{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 84896,
          "databundleVersionId": 10305135,
          "sourceType": "competition"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Colab_backup/blob/main/s04e12_insurance_regression_simple_dl_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Loading Model, Preprocessing"
      ],
      "metadata": {
        "id": "t98ooCYyjDkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries for data manipulation and machine learning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Importing preprocessing and pipeline tools from scikit-learn\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# Importing deep learning tools from TensorFlow/Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Suppressing warnings to keep the output clean\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # Disable warnings during execution\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T11:02:00.107735Z",
          "iopub.execute_input": "2024-12-03T11:02:00.108156Z",
          "iopub.status.idle": "2024-12-03T11:02:12.402725Z",
          "shell.execute_reply.started": "2024-12-03T11:02:00.108119Z",
          "shell.execute_reply": "2024-12-03T11:02:12.402036Z"
        },
        "id": "OYoChFAKjDko"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Constant_Var:\n",
        "    # Defining file paths for the dataset and sample submission file\n",
        "    train_path = \"/kaggle/input/playground-series-s4e12/train.csv\"\n",
        "    test_path = \"/kaggle/input/playground-series-s4e12/test.csv\"\n",
        "    sample_sub_path = \"/kaggle/input/playground-series-s4e12/sample_submission.csv\"\n",
        "\n",
        "    # Defining the target variable name\n",
        "    target = 'Premium Amount'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T11:02:12.404597Z",
          "iopub.execute_input": "2024-12-03T11:02:12.405506Z",
          "iopub.status.idle": "2024-12-03T11:02:12.409938Z",
          "shell.execute_reply.started": "2024-12-03T11:02:12.405466Z",
          "shell.execute_reply": "2024-12-03T11:02:12.408875Z"
        },
        "id": "GtFeT3ytjDku"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    # Read the train and test data\n",
        "    train = pd.read_csv(Constant_Var.train_path, index_col='id')\n",
        "    test = pd.read_csv(Constant_Var.test_path, index_col='id')\n",
        "\n",
        "    # Drop the 'Policy Start Date' column from both training and testing data\n",
        "    train.drop('Policy Start Date', axis=1, inplace=True)\n",
        "    test.drop('Policy Start Date', axis=1, inplace=True)\n",
        "\n",
        "    # Select categorical and numerical columns\n",
        "    categorical_columns = train.select_dtypes(include=['object']).columns.tolist()\n",
        "    # Fix by using remove() to exclude Constant_Var.target from the list\n",
        "    numerical_columns = train.select_dtypes(exclude=['object']).columns.tolist()\n",
        "    numerical_columns.remove(Constant_Var.target) # Removes 'Constant_Var.target' from the list\n",
        "\n",
        "    # Drop the target variable from the feature data\n",
        "    X = train.drop(Constant_Var.target, axis=1)\n",
        "    y = train[Constant_Var.target]\n",
        "    X_test = test.copy()\n",
        "\n",
        "    # Preprocessing for categorical features: Imputation + OneHotEncoding\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Preprocessing for numerical features: Imputation + Scaling\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Combine both transformations\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_columns),\n",
        "            ('cat', categorical_transformer, categorical_columns)\n",
        "        ],\n",
        "        remainder='drop', # Drop any columns not specified in transformers\n",
        "        verbose_feature_names_out=True\n",
        "    )\n",
        "\n",
        "    # Fit and transform the training data\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "    # Create DataFrame for preprocessed training features\n",
        "    X_preprocessed_df = pd.DataFrame(data=X_preprocessed, columns=feature_names)\n",
        "\n",
        "    # Transform the test data using the fitted preprocessor\n",
        "    X_test_preprocessed = preprocessor.transform(X_test)\n",
        "    X_test_preprocessed_df = pd.DataFrame(data=X_test_preprocessed, columns=feature_names)\n",
        "\n",
        "    return X_preprocessed_df, y, X_test_preprocessed_df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T11:02:12.411524Z",
          "iopub.execute_input": "2024-12-03T11:02:12.411876Z",
          "iopub.status.idle": "2024-12-03T11:02:12.446831Z",
          "shell.execute_reply.started": "2024-12-03T11:02:12.411841Z",
          "shell.execute_reply": "2024-12-03T11:02:12.445984Z"
        },
        "id": "owYwDV2wjDkv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Building and Optimizing the Neural Network for Regression"
      ],
      "metadata": {
        "id": "TeAGu7lFjDkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RMSLE loss function\n",
        "def rmsle_1(y_true, y_pred):\n",
        "    # Add a small constant (1.0) to avoid log(0), which is undefined\n",
        "    y_true_log = tf.math.log(y_true + 1.0)\n",
        "    y_pred_log = tf.math.log(y_pred + 1.0)\n",
        "\n",
        "    # Compute the RMSLE\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_true_log - y_pred_log)))\n",
        "\n",
        "# Define the RMSLE loss function\n",
        "def rmsle_2(y_true, y_pred):\n",
        "    # Add a small constant to avoid log(0) which is undefined\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log(y_true + 1.0) - tf.math.log(y_pred + 1.0))))\n",
        "\n",
        "\n",
        "# Build the model\n",
        "def build_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Dense(64, input_dim=input_dim, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(1)  # Output layer for regression (no activation function)\n",
        "    ])\n",
        "\n",
        "    # Compile the model with RMSLE as the loss function\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss=rmsle_1)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Prepare and train the model\n",
        "def train_model(X_train, y_train):\n",
        "    input_dim = X_train.shape[1]  # Number of features\n",
        "    model = build_model(input_dim)\n",
        "\n",
        "    # Early stopping to avoid overfitting\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train, epochs=100, batch_size=512, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T11:02:12.447935Z",
          "iopub.execute_input": "2024-12-03T11:02:12.44861Z",
          "iopub.status.idle": "2024-12-03T11:02:12.461094Z",
          "shell.execute_reply.started": "2024-12-03T11:02:12.448581Z",
          "shell.execute_reply": "2024-12-03T11:02:12.460253Z"
        },
        "id": "SYzfMv8TjDky"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Predictions on the Test Data"
      ],
      "metadata": {
        "id": "TD6H3i5QjDk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Generate predictions from the trained model\n",
        "def make_predictions(model, X_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    return predictions\n",
        "\n",
        "# Step 2: Prepare the final submission DataFrame\n",
        "def create_submission(test, predictions):\n",
        "    submission_df = pd.DataFrame({\n",
        "        'id': test.index + 1200000,  # Use the 'id' column from the test data\n",
        "        'Premium Amount': predictions.flatten()  # Flatten the predictions array to make it 1D\n",
        "    })\n",
        "\n",
        "    # Step 3: Save the predictions to a CSV file for submission\n",
        "    submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "    # Notify the user that the submission file has been successfully created\n",
        "    print(\"Submission file 'submission.csv' created successfully.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T11:02:12.462773Z",
          "iopub.execute_input": "2024-12-03T11:02:12.463042Z",
          "iopub.status.idle": "2024-12-03T11:02:12.473832Z",
          "shell.execute_reply.started": "2024-12-03T11:02:12.462987Z",
          "shell.execute_reply": "2024-12-03T11:02:12.472929Z"
        },
        "id": "hqu7Fz-3jDk5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "X_train, y_train, X_test = get_data()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T11:02:12.474874Z",
          "iopub.execute_input": "2024-12-03T11:02:12.47513Z",
          "iopub.status.idle": "2024-12-03T11:02:29.049284Z",
          "shell.execute_reply.started": "2024-12-03T11:02:12.475107Z",
          "shell.execute_reply": "2024-12-03T11:02:29.048543Z"
        },
        "id": "nLHz2vT_jDk8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = train_model(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "predictions = make_predictions(model, X_test)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T11:02:29.050385Z",
          "iopub.execute_input": "2024-12-03T11:02:29.050744Z",
          "iopub.status.idle": "2024-12-03T11:05:23.120858Z",
          "shell.execute_reply.started": "2024-12-03T11:02:29.050706Z",
          "shell.execute_reply": "2024-12-03T11:05:23.120022Z"
        },
        "id": "8zuNDw4fjDk_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and save the submission file\n",
        "create_submission(X_test, predictions)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T11:05:23.122393Z",
          "iopub.execute_input": "2024-12-03T11:05:23.123409Z",
          "iopub.status.idle": "2024-12-03T11:05:24.099522Z",
          "shell.execute_reply.started": "2024-12-03T11:05:23.123369Z",
          "shell.execute_reply": "2024-12-03T11:05:24.098634Z"
        },
        "id": "EztOInV-jDlA"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}