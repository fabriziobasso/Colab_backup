{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Colab_backup/blob/main/mod_v0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGa3SjrXsW_F"
      },
      "source": [
        "# **S4E10 - LOAN APPROVAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rYYQgW0Rmxv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qq pytorch_tabnet\n",
        "!pip install optuna\n",
        "!pip install catboost\n",
        "!pip install optuna-integration-pytorch-tabnet\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "!pip install category-encoders\n",
        "!pip install optuna-integration\n",
        "!pip install colorama\n",
        "#!pip install pyfiglet\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install keras-nlp\n",
        "!pip install BorutaShap\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install scikit-lego\n",
        "!pip install skops\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIS1habP8JGi"
      },
      "outputs": [],
      "source": [
        "# Setup notebook\n",
        "from pathlib import Path\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pickle import load, dump\n",
        "import json\n",
        "import joblib\n",
        "#import calplot as cal\n",
        "\n",
        "# Graphic Libraries:\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg\n",
        "# Set Style\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5});\n",
        "sns.despine(left=True, bottom=True, top=False, right=False);\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "mpl.rc('axes', labelsize=12);\n",
        "plt.rc('xtick',labelsize=10);\n",
        "plt.rc('ytick',labelsize=10);\n",
        "\n",
        "mpl.rcParams['axes.spines.top'] = False;\n",
        "mpl.rcParams['axes.spines.right'] = False;\n",
        "mpl.rcParams['axes.spines.left'] = True;\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "# tool for Excel:\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "from openpyxl.styles import Border, Side, PatternFill, Font, GradientFill, Alignment\n",
        "from openpyxl.worksheet.cell_range import CellRange\n",
        "\n",
        "from openpyxl.formatting import Rule\n",
        "from openpyxl.styles import Font, PatternFill, Border\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "\n",
        "# Bloomberg\n",
        "#from xbbg import blp\n",
        "from catboost import CatBoostRegressor, Pool, CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "# Time Management\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BMonthEnd, QuarterEnd\n",
        "import datetime\n",
        "from pandas.tseries.offsets import BDay # BDay is business day, not birthday...\n",
        "import datetime as dt\n",
        "import click\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import string\n",
        "\n",
        "from ipywidgets import AppLayout\n",
        "from ipywidgets import Dropdown, Layout, HTML, AppLayout, VBox, Label, HBox, BoundedFloatText, interact, Output\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Reshape\n",
        "from keras.models import Model\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, cross_val_score, cross_validate, GroupKFold, GridSearchCV, RepeatedStratifiedKFold, cross_val_predict\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   QuantileTransformer,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "# metrics\n",
        "import sklearn\n",
        "#import skops.io as sio\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             root_mean_squared_error,\n",
        "                             root_mean_squared_log_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss,\n",
        "                             make_scorer)\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  HuberRegressor,\n",
        "                                  TweedieRegressor,\n",
        "                                  QuantileRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  TheilSenRegressor,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              AdaBoostClassifier,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              GradientBoostingClassifier,\n",
        "                              StackingRegressor,\n",
        "                              StackingClassifier,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              HistGradientBoostingRegressor,\n",
        "                              ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
        "\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import itertools\n",
        "import warnings\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from pylab import rcParams\n",
        "import scipy.stats as ss\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "#import pyfiglet\n",
        "#plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkkRPWKZCkYa"
      },
      "outputs": [],
      "source": [
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.5,\n",
        "         'grid.alpha'           :0.5,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : 'black',\n",
        "         'axes.titlesize'       : 12,\n",
        "         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "# Set Style\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "\n",
        "# import font colors\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "init(autoreset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU7oWpLHRmxy"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Connect to Colab:#\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PuCulFRmx1"
      },
      "source": [
        "<div style=\"text-align:center; border-radius:15px; padding:15px; margin:0; font-size:100%; font-family:Arial, sans-serif; background-color:#A8DADC; color:#1D3557; overflow:hidden; box-shadow:0 3px 6px rgba(0, 0, 0, 0.2);\">\n",
        "    <h3>Loading and Preprocessing Data for Compatibility</h3>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3odgloSjRmx4"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/train.csv')\n",
        "\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/test.csv')\n",
        "\n",
        "df_train_orig = pd.read_csv(\n",
        "    '/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/credit_risk_dataset.csv'\n",
        ")\n",
        "\n",
        "df_subm = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E12/sample_submission.csv\")\n",
        "\n",
        "# df_orig = pd.read_csv(\n",
        "#     \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E12/Insurance Premium Prediction Dataset.csv\",\n",
        "#      parse_dates=['Policy Start Date'],\n",
        "#     #     index_col='id',\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()\n",
        "df_train.shape,df_test.shape"
      ],
      "metadata": {
        "id": "_vwd0o1ph1Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_scatter(x=\"feat1\",y=\"feat2\", df=df_train):\n",
        "\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.scatter(df[x],df[y])\n",
        "  plt.xlabel(x)\n",
        "  plt.ylabel(y)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "njvQSzYr_4BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scatter(x=\"person_age\",y=\"person_income\")"
      ],
      "metadata": {
        "id": "8R5dhMXeAUhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"person_income\"].describe()"
      ],
      "metadata": {
        "id": "pykP87axQAvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.iloc[49179,:]"
      ],
      "metadata": {
        "id": "hREKVL1iPrWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"loat_to_income_diff\"] = (df_train[\"loan_amnt\"] / df_train[\"person_income\"]) - df_train[\"loan_percent_income\"]\n",
        "df_train[\"loat_to_income_real\"] = np.round((df_train[\"loan_amnt\"] / df_train[\"person_income\"]),2)\n",
        "df_train.head()\n",
        "print(df_train.shape)\n",
        "df_train[((df_train[\"loat_to_income_diff\"]<=-0.2)|(df_train[\"loat_to_income_diff\"]>=0.2))]"
      ],
      "metadata": {
        "id": "lWLbHLVPkOd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.loc[df_train[\"loat_to_income_diff\"] <= -0.2, \"loan_amnt\"] = (df_train.loc[df_train[\"loat_to_income_diff\"] <= -0.2, \"person_income\"]*df_train.loc[df_train[\"loat_to_income_diff\"] <= -0.2, \"loan_percent_income\"]).values\n",
        "df_train.loc[df_train[\"loat_to_income_diff\"] >= 0.2, \"person_income\"] = (df_train.loc[df_train[\"loat_to_income_diff\"] >= 0.2, \"loan_amnt\"]/df_train.loc[df_train[\"loat_to_income_diff\"] >= 0.2, \"loan_percent_income\"]).values"
      ],
      "metadata": {
        "id": "-vfGRiTeLCd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train[\"loat_to_income_diff\"] = (df_train[\"loan_amnt\"] / df_train[\"person_income\"]) - df_train[\"loan_percent_income\"]\n",
        "df_train[\"loat_to_income_real\"] = np.round((df_train[\"loan_amnt\"] / df_train[\"person_income\"]),2)\n",
        "df_train.head()\n",
        "print(df_train.shape)\n",
        "df_train[((df_train[\"loat_to_income_diff\"]<=-0.2)|(df_train[\"loat_to_income_diff\"]>=0.2))]"
      ],
      "metadata": {
        "id": "Am9pd9NwKIBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[\"loat_to_income_diff\"] = (df_test[\"loan_amnt\"] / df_test[\"person_income\"]) - df_test[\"loan_percent_income\"]\n",
        "df_test[\"loat_to_income_real\"] = (df_test[\"loan_amnt\"] / df_test[\"person_income\"])\n",
        "\n",
        "df_test[(df_test[\"loat_to_income_diff\"]<=-0.10)|(df_test[\"loat_to_income_diff\"]>=0.10)]"
      ],
      "metadata": {
        "id": "ho9znWqB1KpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Descriptions of Loan Data**\n",
        "\n",
        "Descriptions for the column names based on the data provided:\n",
        "\n",
        "* **id**: Unique identifier for each record.\n",
        "* **person_age**: Age of the individual, categorized into ranges.\n",
        "* **person_income**: Income of the individual, categorized into income ranges.\n",
        "* **person_home_ownership**: Homeownership status, which includes categories like 'RENT', 'MORTGAGE', etc.\n",
        "* **person_emp_length**: Employment length of the individual, categorized into ranges based on years.\n",
        "* **loan_intent**: The purpose of the loan, with categories such as 'EDUCATION', 'MEDICAL', etc.\n",
        "* **loan_grade**: The credit grade of the loan, such as 'A', 'B', etc.\n",
        "* **loan_amnt**: Loan amount, categorized into ranges.\n",
        "* **loan_int_rate**: Loan interest rate, categorized into percentage ranges.\n",
        "* **loan_percent_income**: Percentage of the individual’s income that the loan represents, categorized into - ranges.\n",
        "* **cb_person_default_on_file**: Whether the person has a history of loan default, with values 'true' or 'false'.\n",
        "* **cb_person_cred_hist_length**: Length of the individual’s credit history, categorized into ranges.\n",
        "* **loan_status**: with values representing whether the loan status approval( binary values)\n",
        "\n",
        "The dataset is a about loan applications, including personal, financial, and loan details. It's likely used for predicting whether a person will default on a loan, making it a binary classification problem. The goal is to figure out which applicants are at higher risk of not paying back their loans based on their age, income, employment, loan purpose, credit history, and other related information."
      ],
      "metadata": {
        "id": "LFmR4Gl6JRAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "\n",
        "    state = 42\n",
        "    n_splits = 10\n",
        "    early_stop = 200\n",
        "\n",
        "    target = 'loan_status'\n",
        "    train = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/train.csv')\n",
        "    test = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/test.csv')\n",
        "    submission = pd.read_csv( \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/sample_submission.csv\")\n",
        "    train_org = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/credit_risk_dataset.csv\")\n",
        "\n",
        "    original_data = 'Y'\n",
        "    outliers = 'N'\n",
        "    log_trf = 'Y'\n",
        "    scaler_trf = 'Y'\n",
        "    feature_eng = 'Y'\n",
        "    missing = 'Y'\n",
        "    force_normalization=\"Y\"\n",
        "    log_trans_cols = [\"person_income\",\"loan_amnt\",\"income_per_year_emp\"]\n",
        "    force_norm_cols = [\"person_age\",\"person_emp_length\"]"
      ],
      "metadata": {
        "id": "-_IkGc2Wya01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train = Config.train\n",
        "        self.test = Config.test\n",
        "        self.targets = Config.target\n",
        "\n",
        "        self.prp_data()\n",
        "\n",
        "    def prp_data(self):\n",
        "\n",
        "        if Config.original_data == 'Y':\n",
        "            self.train = pd.concat([self.train, Config.train_org], ignore_index=True).drop_duplicates(ignore_index=True)\n",
        "\n",
        "        self.train = self.train.drop(['id'], axis=1)\n",
        "        self.test = self.test.drop(['id'], axis=1)\n",
        "\n",
        "        self.cat_features = self.train.drop(self.targets, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "        self.num_features = self.train.drop(self.targets, axis=1).select_dtypes(exclude=['object', 'bool']).columns.tolist()\n",
        "\n",
        "        self.train = self.reduce_mem(self.train)\n",
        "        self.test = self.reduce_mem(self.test)\n",
        "        return self\n",
        "\n",
        "    def reduce_mem(self, df):\n",
        "\n",
        "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"]\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_type = df[col].dtypes\n",
        "\n",
        "            if col_type in numerics:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "\n",
        "                if \"int\" in str(col_type):\n",
        "                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)\n",
        "                else:\n",
        "                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    if c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "YjPBIogOJ2mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EDA(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_info()\n",
        "        self.heatmap()\n",
        "        self.dist_plots()\n",
        "        self.cat_feature_plots()\n",
        "        self.target_pie()\n",
        "\n",
        "    def data_info(self):\n",
        "\n",
        "        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n",
        "            table_style = [{'selector': 'th:not(.index_name)',\n",
        "                            'props': [('background-color', 'slategrey'),\n",
        "                                      ('color', '#FFFFFF'),\n",
        "                                      ('font-weight', 'bold'),\n",
        "                                      ('border', '1px solid #DCDCDC'),\n",
        "                                      ('text-align', 'center')]\n",
        "                            },\n",
        "                            {'selector': 'tbody td',\n",
        "                             'props': [('border', '1px solid #DCDCDC'),\n",
        "                                       ('font-weight', 'normal')]\n",
        "                            }]\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} head\\n')\n",
        "            display(data.head().style.set_table_styles(table_style))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} info\\n'+Style.RESET_ALL)\n",
        "            display(data.info())\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} describe\\n')\n",
        "            display(data.describe().drop(index='count', columns=self.targets, errors = 'ignore').T\n",
        "                    .style.set_table_styles(table_style).format('{:.3f}'))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} missing values\\n'+Style.RESET_ALL)\n",
        "            display(data.isna().sum())\n",
        "        return self\n",
        "\n",
        "    def heatmap(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nCorrelation Heatmap\\n')\n",
        "        plt.figure(figsize=(7,7))\n",
        "        corr = self.train.select_dtypes(exclude='object').corr(method='pearson')\n",
        "        sns.heatmap(corr, fmt = '0.2f', cmap = 'Blues', annot=True, cbar=False)\n",
        "        plt.show()\n",
        "\n",
        "    def dist_plots(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Numerical\\n\")\n",
        "        df = pd.concat([self.train[self.num_features].assign(Source = 'Train'),\n",
        "                        self.test[self.num_features].assign(Source = 'Test'),],\n",
        "                        axis=0, ignore_index = True)\n",
        "\n",
        "        fig, axes = plt.subplots(len(self.num_features), 2 ,figsize = (18, len(self.num_features) * 6),\n",
        "                                 gridspec_kw = {'hspace': 0.3,\n",
        "                                                'wspace': 0.2,\n",
        "                                                'width_ratios': [0.70, 0.30]\n",
        "                                               }\n",
        "                                )\n",
        "        for i,col in enumerate(self.num_features):\n",
        "            ax = axes[i,0]\n",
        "            sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source',\n",
        "                        palette = ['royalblue', 'tomato'], ax = ax, alpha=0.7, linewidth = 2\n",
        "                       )\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.grid('--',alpha=0.7)\n",
        "\n",
        "            ax = axes[i,1]\n",
        "            sns.boxplot(data = df, y = col, x=df.Source, width = 0.5,\n",
        "                        linewidth = 1, fliersize= 1,\n",
        "                        ax = ax, palette=['royalblue', 'tomato']\n",
        "                       )\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.tick_params(axis='both', which='major')\n",
        "            ax.set_xticklabels(['Train', 'Test'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def cat_feature_plots(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Categorical\\n\")\n",
        "        fig, axes = plt.subplots(len(self.cat_features), 2 ,figsize = (18, len(self.cat_features) * 6),\n",
        "                                 gridspec_kw = {'hspace': 0.5,\n",
        "                                                'wspace': 0.2,\n",
        "                                               }\n",
        "                                )\n",
        "\n",
        "        for i, col in enumerate(self.cat_features):\n",
        "\n",
        "            ax = axes[i,0]\n",
        "            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='royalblue', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Train\")\n",
        "\n",
        "            ax = axes[i,1]\n",
        "            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='tomato', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Test\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def target_pie(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nTarget feature distribution\\n\")\n",
        "        targets = self.train[self.targets]\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.pie(targets.value_counts(), labels=targets.value_counts().index, autopct='%1.2f%%', colors=palette_9)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ICqfSaYMKIF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 EDA"
      ],
      "metadata": {
        "id": "xIy8MLVmvv3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eda = EDA()"
      ],
      "metadata": {
        "id": "0O1IeGiuKwDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Data Transformation and Feature Negeneering:"
      ],
      "metadata": {
        "id": "6jQmFilAvKM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transform(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        if self.missing == 'Y':\n",
        "            self.missing_values()\n",
        "\n",
        "        self.train_raw = self.train.copy()\n",
        "\n",
        "        if self.feature_eng == 'Y':\n",
        "            self.train = self.new_features(self.train)\n",
        "            self.test = self.new_features(self.test)\n",
        "            self.train_raw = self.new_features(self.train_raw)\n",
        "\n",
        "        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object', 'bool']).columns.tolist()\n",
        "        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "        if self.outliers == 'Y':\n",
        "            self.remove_outliers()\n",
        "\n",
        "        if self.log_trf == 'Y':\n",
        "            self.log_transformation()\n",
        "\n",
        "        if self.force_normalization == 'Y':\n",
        "            self.forced_norm_transformation()\n",
        "\n",
        "        if self.scaler_trf == 'Y':\n",
        "            self.scaler()\n",
        "\n",
        "        if self.outliers == 'Y' or self.log_trf == 'Y' or self.scaler_trf =='Y':\n",
        "            self.distribution()\n",
        "\n",
        "    def __call__(self):\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
        "        data = pd.concat([self.test, self.train])\n",
        "        train_enc, test_enc = self.encode(data)\n",
        "\n",
        "        self.cat_features_card = []\n",
        "        for f in self.cat_features:\n",
        "            self.cat_features_card.append(1 + data[f].max())\n",
        "\n",
        "        self.y = self.train[self.target]\n",
        "        self.train = self.train.drop(self.target, axis=1)\n",
        "        train_enc = train_enc.drop(self.target, axis=1)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        train_enc[self.num_features] = scaler.fit_transform(train_enc[self.num_features])\n",
        "        test_enc[self.num_features] = scaler.transform(test_enc[self.num_features])\n",
        "\n",
        "        return self.train, train_enc, self.y, self.test, test_enc, self.cat_features\n",
        "\n",
        "    def encode(self, data):\n",
        "\n",
        "        oe = OrdinalEncoder()\n",
        "        data[self.cat_features] = oe.fit_transform(data[self.cat_features]).astype('int')\n",
        "\n",
        "        train_enc = data[~data[self.target].isna()]\n",
        "        test_enc = data[data[self.target].isna()].drop(self.target, axis=1)\n",
        "        return train_enc, test_enc\n",
        "\n",
        "    def new_features(self, df):\n",
        "        df['loan_to_income'] = np.round((df[\"loan_amnt\"] / df[\"person_income\"]),2) - df['loan_percent_income']\n",
        "        df.loc[df[\"loan_to_income\"] <= -0.15, \"loan_amnt\"] = (df.loc[df[\"loan_to_income\"] <= -0.15, \"person_income\"]*df.loc[df[\"loan_to_income\"] <= -0.15, \"loan_percent_income\"]).values\n",
        "        df.loc[df[\"loan_to_income\"] >= 0.15, \"person_income\"] = (df.loc[df[\"loan_to_income\"] >= 0.15, \"loan_amnt\"]/df.loc[df[\"loan_to_income\"] >= 0.15, \"loan_percent_income\"]).values\n",
        "        #df['loan_to_income'] = np.round((df[\"loan_amnt\"] / df[\"person_income\"]),2) - df['loan_percent_income']\n",
        "        df=df.drop(columns=['loan_to_income'])\n",
        "\n",
        "        df['interest_to_income'] = df['loan_amnt'] * df['loan_int_rate'] / (df['person_income'])\n",
        "\n",
        "        df['income_per_year_emp'] = df['person_income'] / (df['person_emp_length']+1.0)\n",
        "        df['cred_hist_to_age_ratio'] = df['cb_person_cred_hist_length'] / df['person_age']\n",
        "        # df['int_to_loan_ratio'] = df['loan_int_rate'] / df['loan_amnt']\n",
        "        # df['loan_int_emp_interaction'] = df['loan_int_rate'] * df['person_emp_length']\n",
        "        # df['debt_to_credit_ratio'] = df['loan_amnt'] / df['cb_person_cred_hist_length']\n",
        "        # df['int_to_cred_hist'] = df['loan_int_rate'] / df['cb_person_cred_hist_length']\n",
        "        # df['int_per_year_emp'] = df['loan_int_rate'] / (df['person_emp_length'])\n",
        "        # df['loan_amt_per_emp_year'] = df['loan_amnt'] / (df['person_emp_length'])\n",
        "\n",
        "        df = df[df['person_age']<100]\n",
        "        df = df[df['person_income']<=1900000]\n",
        "        df = df[df['person_emp_length']<50]\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def log_transformation(self):\n",
        "\n",
        "\n",
        "        self.train[self.log_trans_cols] = np.log1p(self.train[self.log_trans_cols])\n",
        "        self.test[self.log_trans_cols] = np.log1p(self.test[self.log_trans_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def forced_norm_transformation(self):\n",
        "\n",
        "        scaler = PowerTransformer(method='yeo-johnson')\n",
        "\n",
        "        #self.train[self.force_norm_cols] = scaler.fit_transform(self.train[self.force_norm_cols])\n",
        "        #self.test[self.force_norm_cols] = scaler.transform(self.test[self.force_norm_cols])\n",
        "\n",
        "        self.train[self.force_norm_cols] = np.sqrt(self.train[self.force_norm_cols]+0.1)\n",
        "        self.test[self.force_norm_cols] = np.sqrt(self.test[self.force_norm_cols]+0.1)\n",
        "\n",
        "\n",
        "        return self\n",
        "\n",
        "    def distribution(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nHistograms of distribution\\n')\n",
        "        fig, axes = plt.subplots(nrows=len(self.num_features), ncols=2, figsize=(15, len(self.num_features)*5))\n",
        "\n",
        "        for (ax_r, ax_n), col in zip(axes, self.num_features):\n",
        "\n",
        "            ax_r.set_title(f'{col} ($\\mu=$ {self.train_raw[col].mean():.2f} and $\\sigma=$ {self.train_raw[col].std():.2f} )')\n",
        "            ax_r.hist(self.train_raw[col], bins=30, color='tomato',alpha=0.7)\n",
        "            ax_r.axvline(self.train_raw[col].mean(), color='r', label='Mean')\n",
        "            ax_r.axvline(self.train_raw[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_r.legend()\n",
        "\n",
        "            ax_n.set_title(f'{col} Normalized ($\\mu=$ {self.train[col].mean():.2f} and $\\sigma=$ {self.train[col].std():.2f} )')\n",
        "            ax_n.hist(self.train[col], bins=30, color='royalblue',alpha=0.7)\n",
        "            ax_n.axvline(self.train[col].mean(), color='r', label='Mean')\n",
        "            ax_n.axvline(self.train[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_n.legend()\n",
        "\n",
        "    def remove_outliers(self):\n",
        "        Q1 = self.train[self.targets].quantile(0.25)\n",
        "        Q3 = self.train[self.targets].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_limit = Q1 - 1.5*IQR\n",
        "        upper_limit = Q3 + 1.5*IQR\n",
        "        self.train = self.train[(self.train[self.targets] >= lower_limit) & (self.train[self.targets] <= upper_limit)]\n",
        "        self.train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def scaler(self):\n",
        "        scaler = StandardScaler()\n",
        "        self.train[self.num_features] = scaler.fit_transform(self.train[self.num_features])\n",
        "        self.test[self.num_features] = scaler.transform(self.test[self.num_features])\n",
        "        return self\n",
        "\n",
        "    def missing_values(self):\n",
        "\n",
        "        self.train[self.num_features] = self.train[self.num_features].fillna(self.train[self.num_features].median())\n",
        "        self.test[self.num_features] = self.test[self.num_features].fillna(self.test[self.num_features].median())\n",
        "        for column in self.cat_features:\n",
        "            self.train[column] = self.train[column].fillna(self.train[column].mode()[0])\n",
        "            self.test[column] = self.test[column].fillna(self.test[column].mode()[0])\n",
        "        return self"
      ],
      "metadata": {
        "id": "gxSqeWx-MZHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = Transform()\n",
        "X, X_enc, y, test, test_enc, cat_features = t()"
      ],
      "metadata": {
        "id": "YimbhYB_v_G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, X_enc.shape, test.shape, test_enc.shape"
      ],
      "metadata": {
        "id": "_J1svia6wHk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc.head()"
      ],
      "metadata": {
        "id": "gbunXYe3vquN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1GyNmMXO7Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[X.loan_to_income<-20]"
      ],
      "metadata": {
        "id": "O6XXbt231ogK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}