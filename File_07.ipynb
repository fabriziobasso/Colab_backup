{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 90274,
          "databundleVersionId": 10951356,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1860.937789,
      "end_time": "2024-10-26T20:02:26.852331",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-10-26T19:31:25.914542",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Colab_backup/blob/main/File_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import packages and modules"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008519,
          "end_time": "2024-10-26T19:31:29.283114",
          "exception": false,
          "start_time": "2024-10-26T19:31:29.274595",
          "status": "completed"
        },
        "tags": [],
        "id": "eFQAje2z1KTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessry to run LGBMRegressor\n",
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd"
      ],
      "metadata": {
        "id": "v0r46Q3q2iUS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!pip install -qq pytorch_tabnet\n",
        "!pip install optuna\n",
        "!pip install catboost\n",
        "#!pip install optuna-integration-pytorch-tabnet\n",
        "\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade\n",
        "\n",
        "!pip install --upgrade category-encoders\n",
        "!pip install optuna-integration\n",
        "!pip install colorama\n",
        "#!pip install pyfiglet\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install keras-nlp\n",
        "!pip install BorutaShap\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install scikit-lego\n",
        "!pip install skops\n",
        "\n",
        "#from pytorch_tabnet.tab_model import TabNetRegressor"
      ],
      "metadata": {
        "id": "VdgjH0IaaV61"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from termcolor import colored\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "from joblib import dump, load\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import iqr, skew, kurtosis, mode\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\", {\"grid.color\": \".1\", \"grid.linestyle\": \":\", \"axes.facecolor\": \".9\"})\n",
        "\n",
        "from warnings import filterwarnings\n",
        "from termcolor import colored\n",
        "from warnings import filterwarnings; filterwarnings(action = 'ignore');\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, log_loss, \\\n",
        "                            mean_squared_error, mean_absolute_error, r2_score, \\\n",
        "                            make_scorer,root_mean_squared_error\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, FunctionTransformer, \\\n",
        "                                  OrdinalEncoder, OneHotEncoder\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.base import clone\n",
        "\n",
        "from category_encoders import TargetEncoder, CatBoostEncoder, LeaveOneOutEncoder, OrdinalEncoder, CountEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold, RepeatedStratifiedKFold, \\\n",
        "                                    cross_val_score, RepeatedKFold\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.cluster import k_means\n",
        "from sklearn.feature_selection import RFECV, SequentialFeatureSelector, mutual_info_classif, mutual_info_regression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier, VotingRegressor, StackingRegressor, \\\n",
        "                             RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor, \\\n",
        "                             HistGradientBoostingClassifier, HistGradientBoostingRegressor \\\n",
        "\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor, early_stopping, log_evaluation\n",
        "import lightgbm as lgb\n",
        "\n",
        "from xgboost import XGBClassifier, XGBRegressor, DMatrix\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "import matplotlib as mpl\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Reshape\n",
        "from keras.models import Model\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch, GridSearch, BayesianOptimization"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:33.914813Z",
          "iopub.execute_input": "2025-02-03T01:57:33.915083Z",
          "iopub.status.idle": "2025-02-03T01:57:42.908563Z",
          "shell.execute_reply.started": "2025-02-03T01:57:33.915055Z",
          "shell.execute_reply": "2025-02-03T01:57:42.907652Z"
        },
        "papermill": {
          "duration": 6.371992,
          "end_time": "2024-10-26T19:31:35.662482",
          "exception": false,
          "start_time": "2024-10-26T19:31:29.29049",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "NoIuwT_21KTa"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.5,\n",
        "         'grid.alpha'           :0.5,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : 'black',\n",
        "         'axes.titlesize'       : 12,\n",
        "#         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "# Set Style\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "\n",
        "# import font colors\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "init(autoreset=True)"
      ],
      "metadata": {
        "id": "L_QF4gmj5M61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31ba9d9b-b747-4a7e-ffae-c65c5d3d733c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 768x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Colab:#\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "87t8frSLdEsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464dc0e7-1c4d-4682-d4f3-82a028877b23"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "pZ1cBlKwHkh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b52e56e-571d-4b18-f376-95c9f795cd01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Keras backend:\", keras.backend.backend())"
      ],
      "metadata": {
        "id": "Ta7Zcr6iHo6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78548d1b-b4a4-40e7-f926-ccbedc44394c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras backend: tensorflow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Define Configuration and Data classes\n",
        "In the code below we load the data and display some basic information about the dataset such as: the number of missing entries, the features types, and the number of unique categories per categorical feature."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007142,
          "end_time": "2024-10-26T19:31:35.677236",
          "exception": false,
          "start_time": "2024-10-26T19:31:35.670094",
          "status": "completed"
        },
        "tags": [],
        "id": "OJLTekfn1KTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    SEED    = 333\n",
        "    CV      = KFold(n_splits=15, shuffle=True, random_state=SEED)\n",
        "    VERSION = '3'\n",
        "\n",
        "class Data:\n",
        "    path       = False\n",
        "    or_path    = ''\n",
        "    to_drop    = False\n",
        "    target     = 'Price'\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train      = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_train_enc_expanded.csv\",index_col=0).drop(columns=self.to_drop) if self.to_drop else pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_train_enc_expanded.csv\")\n",
        "        self.test       = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_test_enc_expanded.csv\",index_col=0).drop(columns=self.to_drop) if self.to_drop else pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_test_enc_expanded.csv\",index_col=0)\n",
        "        self.submission = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\",index_col=0)\n",
        "        self.original   = pd.read_csv(self.or_path) if self.or_path else pd.DataFrame()\n",
        "\n",
        "    @property\n",
        "    def X(self):\n",
        "        return self.train.drop(columns=self.target)\n",
        "    @property\n",
        "    def y(self):\n",
        "        return self.train[[self.target]]\n",
        "    @property\n",
        "    def X_test(self):\n",
        "        return self.test\n",
        "    @property\n",
        "    def X_original(self):\n",
        "        if len(self.original) != 0:\n",
        "            return self.original.drop(columns=self.target)\n",
        "        return pd.DataFrame()\n",
        "    @property\n",
        "    def y_original(self):\n",
        "        if len(self.original) != 0:\n",
        "            return self.original[[self.target]]\n",
        "        return pd.DataFrame()\n",
        "    @property\n",
        "    def cat_features(self):\n",
        "        return self.X.select_dtypes(include=['category', 'bool', 'category','int']).columns.to_list()\n",
        "    @property\n",
        "    def num_features(self):\n",
        "        return self.X.select_dtypes(exclude=['category', 'bool', 'category','int']).columns.to_list()\n",
        "\n",
        "    def submit(self, sub: np.ndarray, desc: str):\n",
        "        '''Submit the predictions in the adequate format'''\n",
        "        self.submission[self.target] = sub\n",
        "        self.submission.to_csv(f'SUB_{CFG.VERSION}_{desc}.csv', index=False)\n",
        "        print(colored('Submission has been made.', color='green', attrs=['bold', 'dark']))\n",
        "\n",
        "    @staticmethod\n",
        "    def sep_line():\n",
        "        print(colored(f'{\"_____\"*14}', color='black'))\n",
        "        print('')\n",
        "    @staticmethod\n",
        "    def head(head_text):\n",
        "        print(colored(f'{\"    \"} ➩ {head_text} ', color='green', attrs=['dark']))\n",
        "\n",
        "    def display_data(self):\n",
        "        self.head(f'𝐃𝐚𝐭𝐚𝐬𝐞𝐭 𝐬𝐡𝐚𝐩𝐞𝐬 — 𝐓𝐫𝐚𝐢𝐧 | 𝐓𝐞𝐬𝐭: {self.train.shape} | {self.test.shape}')\n",
        "        self.sep_line()\n",
        "\n",
        "        self.head('𝐓𝐫𝐚𝐢𝐧 𝐡𝐞𝐚𝐝')\n",
        "        display(self.train.head(5))\n",
        "        self.head('𝐓𝐞𝐬𝐭 𝐡𝐞𝐚𝐝')\n",
        "        display(self.test.head(5))\n",
        "        self.sep_line()\n",
        "\n",
        "        self.head('𝐓𝐫𝐚𝐢𝐧 𝐢𝐧𝐟𝐨')\n",
        "        display(self.train.info())\n",
        "        self.head('𝐓𝐞𝐬𝐭 𝐢𝐧𝐟𝐨')\n",
        "        display(self.test.info())\n",
        "        self.sep_line()\n",
        "\n",
        "        self.head('𝐓𝐫𝐚𝐢𝐧 𝐬𝐮𝐦𝐦𝐚𝐫𝐲 𝐬𝐭𝐚𝐭𝐬')\n",
        "        display(self.train.describe().T)\n",
        "        self.head('𝐓𝐞𝐬𝐭 𝐬𝐮𝐦𝐦𝐚𝐫𝐲 𝐬𝐭𝐚𝐭𝐬')\n",
        "        display(self.test.describe().T)\n",
        "        self.sep_line()\n",
        "\n",
        "        def nunique_null(train, test):\n",
        "            nunique_train, nunique_test = {}, {}\n",
        "            nulls_train, nulls_test = {}, {}\n",
        "\n",
        "            for col in test.columns:\n",
        "                nunique_train[col], nunique_test[col] = train[col].nunique(), test[col].nunique()\n",
        "                nulls_train[col], nulls_test[col] = train[col].isna().sum(), test[col].isna().sum()\n",
        "\n",
        "            df = pd.DataFrame([nunique_train, nunique_test,\n",
        "                               nulls_train, nulls_test],\n",
        "                              index=['Train nunique', 'Test nunique',\n",
        "                                     'Train null', 'Test null'])\n",
        "            return df\n",
        "\n",
        "        self.head('𝐍𝐮𝐧𝐢𝐪𝐮𝐞 𝐚𝐧𝐝 𝐧𝐮𝐥𝐥𝐬')\n",
        "        display(nunique_null(self.train, self.test))\n",
        "        self.sep_line()\n",
        "\n",
        "        self.head('𝐃𝐮𝐩𝐥𝐢𝐜𝐚𝐭𝐞𝐬')\n",
        "        display(f'Train duplicated: {self.train.duplicated().sum()}')\n",
        "        display(f'Test duplicated: {self.test.duplicated().sum()}')\n",
        "        if self.train.duplicated().sum() > 0:\n",
        "            self.train = self.train.drop_duplicates()\n",
        "            print('Train duplicates dropped.')\n",
        "        if self.test.duplicated().sum() > 0:\n",
        "            #self.test = self.test.drop_duplicates()\n",
        "            print('Test duplicates dropped.')\n",
        "        self.sep_line()\n",
        "\n",
        "        self.head('𝐍𝐮𝐧𝐢𝐪𝐮𝐞 𝐢𝐧 𝐭𝐫𝐚𝐢𝐧 𝐧𝐨𝐭 𝐢𝐧 𝐭𝐞𝐬𝐭/𝐢𝐧 𝐭𝐞𝐬𝐭 𝐧𝐨𝐭 𝐢𝐧 𝐭𝐫𝐚𝐢𝐧')\n",
        "        cat_cols = [c for c in self.test.columns if self.train[c].nunique() <= 40 or\n",
        "                    c in self.test.select_dtypes(include=['object', 'category']).columns]\n",
        "\n",
        "        def compare_unique_categories(train, test, cat_cols):\n",
        "            unique_train_dic, unique_test_dic = {}, {}\n",
        "\n",
        "            for c in cat_cols:\n",
        "                unique_train_c = train[c].unique()\n",
        "                unique_test_c = test[c].unique()\n",
        "\n",
        "                count_tr = sum(1 for cat in unique_train_c if cat not in unique_test_c and not pd.isna(cat))\n",
        "                count_te = sum(1 for cat in unique_test_c if (cat not in unique_train_c and not pd.isna(cat)))\n",
        "\n",
        "                unique_train_dic[c] = count_tr\n",
        "                unique_test_dic[c] = count_te\n",
        "\n",
        "            result_df = pd.DataFrame([unique_train_dic, unique_test_dic],\n",
        "                                     index=['in train not in test', 'in test not in train'])\n",
        "\n",
        "            return result_df\n",
        "\n",
        "        display(compare_unique_categories(self.train, self.test, cat_cols))\n",
        "\n",
        "data = Data()\n",
        "data.display_data()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:42.909948Z",
          "iopub.execute_input": "2025-02-03T01:57:42.91046Z",
          "iopub.status.idle": "2025-02-03T01:57:45.254458Z",
          "shell.execute_reply.started": "2025-02-03T01:57:42.910435Z",
          "shell.execute_reply": "2025-02-03T01:57:45.253807Z"
        },
        "papermill": {
          "duration": 0.671233,
          "end_time": "2024-10-26T19:31:36.355611",
          "exception": false,
          "start_time": "2024-10-26T19:31:35.684378",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "PEhAxeTp1KTb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "224b61c9-fc7d-4e97-ff0f-b66d249f70f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ➩ 𝐃𝐚𝐭𝐚𝐬𝐞𝐭 𝐬𝐡𝐚𝐩𝐞𝐬 — 𝐓𝐫𝐚𝐢𝐧 | 𝐓𝐞𝐬𝐭: (3994318, 15) | (200000, 14) \n",
            "______________________________________________________________________\n",
            "\n",
            "     ➩ 𝐓𝐫𝐚𝐢𝐧 𝐡𝐞𝐚𝐝 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  Style  \\\n",
              "0      1         1     1             7                   2           1      3   \n",
              "1      1         0     3             1                   2           2      1   \n",
              "2      5         1     3             2                   2           1      1   \n",
              "3      3         3     3             8                   2           1      1   \n",
              "4      0         0     1             0                   2           2      1   \n",
              "\n",
              "   Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  cheap_flag  \\\n",
              "0      0             -0.917722  0.261445 -0.168518 -0.517618           0   \n",
              "1      3              1.300573  0.621130 -0.846883 -0.613824           0   \n",
              "2      6             -0.196013  0.016408 -0.606337 -1.382314           0   \n",
              "3      3             -0.727615  1.498987 -0.606337 -0.709550           0   \n",
              "4      3             -0.037447  0.016408 -1.185578  0.754615           0   \n",
              "\n",
              "   expansive_flag      Price  \n",
              "0               0  112.15875  \n",
              "1               0   68.88056  \n",
              "2               0   39.17320  \n",
              "3               0   80.60793  \n",
              "4               0   86.02312  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c3ecfdd6-1aba-4c5b-ab01-14928007b513\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.917722</td>\n",
              "      <td>0.261445</td>\n",
              "      <td>-0.168518</td>\n",
              "      <td>-0.517618</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>112.15875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.300573</td>\n",
              "      <td>0.621130</td>\n",
              "      <td>-0.846883</td>\n",
              "      <td>-0.613824</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>68.88056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.196013</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>-1.382314</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>39.17320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.727615</td>\n",
              "      <td>1.498987</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>-0.709550</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>80.60793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.037447</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-1.185578</td>\n",
              "      <td>0.754615</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>86.02312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3ecfdd6-1aba-4c5b-ab01-14928007b513')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c3ecfdd6-1aba-4c5b-ab01-14928007b513 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c3ecfdd6-1aba-4c5b-ab01-14928007b513');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dc4d9673-d907-49cf-99d7-705ac81facaf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc4d9673-d907-49cf-99d7-705ac81facaf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dc4d9673-d907-49cf-99d7-705ac81facaf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8712176067816039,\n        \"min\": -0.9177217,\n        \"max\": 1.3005726,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.3005726\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6196397209916799,\n        \"min\": 0.016407728,\n        \"max\": 1.4989873,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.62113035\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.37278939922472365,\n        \"min\": -1.1855781,\n        \"max\": -0.16851841,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.8468833\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7761308810365135,\n        \"min\": -1.3823137,\n        \"max\": 0.75461453,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.6138238\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26.583283828567343,\n        \"min\": 39.1732,\n        \"max\": 112.15875,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          68.88056\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ➩ 𝐓𝐞𝐬𝐭 𝐡𝐞𝐚𝐝 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  Style  \\\n",
              "0      4         1     3             2                   1           1      3   \n",
              "1      3         0     1             7                   1           2      0   \n",
              "2      0         0     0             9                   1           2      1   \n",
              "3      0         3     0             0                   2           1      1   \n",
              "4      2         3     0             2                   2           2      3   \n",
              "\n",
              "   Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  cheap_flag  \\\n",
              "0      3              0.381607  0.414531  1.019273  0.544976           0   \n",
              "1      3             -0.637706 -0.095293  0.268810 -0.100777           0   \n",
              "2      1             -0.889313  2.164135  0.044250  1.660423           0   \n",
              "3      3              0.066921 -0.516258 -0.385691  1.131928           0   \n",
              "4      0             -1.162081  0.016408 -1.749252  0.496019           0   \n",
              "\n",
              "   expansive_flag  \n",
              "0               0  \n",
              "1               0  \n",
              "2               0  \n",
              "3               0  \n",
              "4               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b128c356-6778-4fa0-96b0-80e26f3493d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.381607</td>\n",
              "      <td>0.414531</td>\n",
              "      <td>1.019273</td>\n",
              "      <td>0.544976</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.637706</td>\n",
              "      <td>-0.095293</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>-0.100777</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.889313</td>\n",
              "      <td>2.164135</td>\n",
              "      <td>0.044250</td>\n",
              "      <td>1.660423</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.066921</td>\n",
              "      <td>-0.516258</td>\n",
              "      <td>-0.385691</td>\n",
              "      <td>1.131928</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.162081</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-1.749252</td>\n",
              "      <td>0.496019</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b128c356-6778-4fa0-96b0-80e26f3493d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b128c356-6778-4fa0-96b0-80e26f3493d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b128c356-6778-4fa0-96b0-80e26f3493d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8f4ce423-596a-4689-ba00-aeb0496b984e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f4ce423-596a-4689-ba00-aeb0496b984e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8f4ce423-596a-4689-ba00-aeb0496b984e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          2,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          7,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6507779735166621,\n        \"min\": -1.1620812,\n        \"max\": 0.38160655,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.6377055,\n          -1.1620812\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0421511934804317,\n        \"min\": -0.51625824,\n        \"max\": 2.164135,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.09529296,\n          0.016407754\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0238332102607681,\n        \"min\": -1.7492516,\n        \"max\": 1.0192734,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.26881,\n          -1.7492516\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.671758935752152,\n        \"min\": -0.100776896,\n        \"max\": 1.6604226,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -0.100776896,\n          0.4960188\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "______________________________________________________________________\n",
            "\n",
            "     ➩ 𝐓𝐫𝐚𝐢𝐧 𝐢𝐧𝐟𝐨 \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 15 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float64\n",
            " 9   TE_wc                 float64\n",
            " 10  skew_0                float64\n",
            " 11  skew_1                float64\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            " 14  Price                 float64\n",
            "dtypes: float64(5), int64(10)\n",
            "memory usage: 457.1 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ➩ 𝐓𝐞𝐬𝐭 𝐢𝐧𝐟𝐨 \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 200000 entries, 0 to 199999\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Non-Null Count   Dtype  \n",
            "---  ------                --------------   -----  \n",
            " 0   Brand                 200000 non-null  int64  \n",
            " 1   Material              200000 non-null  int64  \n",
            " 2   Size                  200000 non-null  int64  \n",
            " 3   Compartments          200000 non-null  int64  \n",
            " 4   Laptop Compartment    200000 non-null  int64  \n",
            " 5   Waterproof            200000 non-null  int64  \n",
            " 6   Style                 200000 non-null  int64  \n",
            " 7   Color                 200000 non-null  int64  \n",
            " 8   Weight Capacity (kg)  200000 non-null  float64\n",
            " 9   TE_wc                 200000 non-null  float64\n",
            " 10  skew_0                200000 non-null  float64\n",
            " 11  skew_1                200000 non-null  float64\n",
            " 12  cheap_flag            200000 non-null  int64  \n",
            " 13  expansive_flag        200000 non-null  int64  \n",
            "dtypes: float64(4), int64(10)\n",
            "memory usage: 22.9 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "______________________________________________________________________\n",
            "\n",
            "     ➩ 𝐓𝐫𝐚𝐢𝐧 𝐬𝐮𝐦𝐦𝐚𝐫𝐲 𝐬𝐭𝐚𝐭𝐬 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                          count          mean        std        min  \\\n",
              "Brand                 3994318.0  2.584763e+00   1.843070   0.000000   \n",
              "Material              3994318.0  2.070342e+00   1.562931   0.000000   \n",
              "Size                  3994318.0  1.314194e+00   1.222013   0.000000   \n",
              "Compartments          3994318.0  4.435745e+00   2.881279   0.000000   \n",
              "Laptop Compartment    3994318.0  1.469268e+00   0.546253   0.000000   \n",
              "Waterproof            3994318.0  1.469387e+00   0.544327   0.000000   \n",
              "Style                 3994318.0  1.359897e+00   1.229806   0.000000   \n",
              "Color                 3994318.0  2.898823e+00   2.082539   0.000000   \n",
              "Weight Capacity (kg)  3994318.0 -4.068731e-09   1.000000  -1.865994   \n",
              "TE_wc                 3994318.0 -9.840794e-11   1.000000  -6.903976   \n",
              "skew_0                3994318.0 -1.227148e-08   1.000000  -5.307906   \n",
              "skew_1                3994318.0  1.205104e-08   1.000000  -5.222957   \n",
              "cheap_flag            3994318.0  1.672125e-03   0.040857   0.000000   \n",
              "expansive_flag        3994318.0  1.369445e-03   0.036981   0.000000   \n",
              "Price                 3994318.0  8.136217e+01  38.938684  15.000000   \n",
              "\n",
              "                            25%        50%         75%         max  \n",
              "Brand                  1.000000   3.000000    4.000000    5.000000  \n",
              "Material               1.000000   3.000000    4.000000    4.000000  \n",
              "Size                   0.000000   1.000000    3.000000    3.000000  \n",
              "Compartments           2.000000   4.000000    7.000000    9.000000  \n",
              "Laptop Compartment     1.000000   1.000000    2.000000    2.000000  \n",
              "Waterproof             1.000000   1.000000    2.000000    2.000000  \n",
              "Style                  0.000000   1.000000    3.000000    3.000000  \n",
              "Color                  1.000000   3.000000    5.000000    6.000000  \n",
              "Weight Capacity (kg)  -0.851663   0.006300    0.856963    1.719577  \n",
              "TE_wc                 -0.471265   0.016408    0.451310    6.731979  \n",
              "skew_0                -0.606337   0.044250    0.746521    5.283896  \n",
              "skew_1                -0.709550  -0.018634    0.648612    3.309770  \n",
              "cheap_flag             0.000000   0.000000    0.000000    1.000000  \n",
              "expansive_flag         0.000000   0.000000    0.000000    1.000000  \n",
              "Price                 47.470020  80.984950  114.855000  150.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7060c918-1045-4239-8d60-9ab05593f75c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Brand</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>2.584763e+00</td>\n",
              "      <td>1.843070</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Material</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>2.070342e+00</td>\n",
              "      <td>1.562931</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>1.314194e+00</td>\n",
              "      <td>1.222013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Compartments</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>4.435745e+00</td>\n",
              "      <td>2.881279</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>1.469268e+00</td>\n",
              "      <td>0.546253</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Waterproof</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>1.469387e+00</td>\n",
              "      <td>0.544327</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Style</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>1.359897e+00</td>\n",
              "      <td>1.229806</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Color</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>2.898823e+00</td>\n",
              "      <td>2.082539</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>-4.068731e-09</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-1.865994</td>\n",
              "      <td>-0.851663</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>0.856963</td>\n",
              "      <td>1.719577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TE_wc</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>-9.840794e-11</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-6.903976</td>\n",
              "      <td>-0.471265</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>0.451310</td>\n",
              "      <td>6.731979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_0</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>-1.227148e-08</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-5.307906</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>0.044250</td>\n",
              "      <td>0.746521</td>\n",
              "      <td>5.283896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_1</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>1.205104e-08</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-5.222957</td>\n",
              "      <td>-0.709550</td>\n",
              "      <td>-0.018634</td>\n",
              "      <td>0.648612</td>\n",
              "      <td>3.309770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheap_flag</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>1.672125e-03</td>\n",
              "      <td>0.040857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>expansive_flag</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>1.369445e-03</td>\n",
              "      <td>0.036981</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Price</th>\n",
              "      <td>3994318.0</td>\n",
              "      <td>8.136217e+01</td>\n",
              "      <td>38.938684</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>47.470020</td>\n",
              "      <td>80.984950</td>\n",
              "      <td>114.855000</td>\n",
              "      <td>150.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7060c918-1045-4239-8d60-9ab05593f75c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7060c918-1045-4239-8d60-9ab05593f75c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7060c918-1045-4239-8d60-9ab05593f75c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7f06c7e8-57fb-4c04-adc4-f5a8622e5923\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7f06c7e8-57fb-4c04-adc4-f5a8622e5923')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7f06c7e8-57fb-4c04-adc4-f5a8622e5923 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 3994318.0,\n        \"max\": 3994318.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3994318.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20.725710736069903,\n        \"min\": -1.2271479111579293e-08,\n        \"max\": 81.36217459394567,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          -9.840793572947853e-11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.787596162356651,\n        \"min\": 0.03698067412936445,\n        \"max\": 38.938684109653224,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          1.0000001250735493\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.857739182625788,\n        \"min\": -6.903976,\n        \"max\": 15.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"25%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.20398961156471,\n        \"min\": -0.85166305,\n        \"max\": 47.47002,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"50%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20.641261085235865,\n        \"min\": -0.018634038,\n        \"max\": 80.98495,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"75%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29.12291874819736,\n        \"min\": 0.0,\n        \"max\": 114.855,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.8569628\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 37.81974143032882,\n        \"min\": 1.0,\n        \"max\": 150.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ➩ 𝐓𝐞𝐬𝐭 𝐬𝐮𝐦𝐦𝐚𝐫𝐲 𝐬𝐭𝐚𝐭𝐬 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                         count      mean       std       min       25%  \\\n",
              "Brand                 200000.0  2.583635  1.845064  0.000000  1.000000   \n",
              "Material              200000.0  2.074010  1.560463  0.000000  1.000000   \n",
              "Size                  200000.0  1.315085  1.222397  0.000000  0.000000   \n",
              "Compartments          200000.0  4.443590  2.879623  0.000000  2.000000   \n",
              "Laptop Compartment    200000.0  1.468485  0.546469  0.000000  1.000000   \n",
              "Waterproof            200000.0  1.468915  0.545110  0.000000  1.000000   \n",
              "Style                 200000.0  1.359800  1.230834  0.000000  0.000000   \n",
              "Color                 200000.0  2.901365  2.081725  0.000000  1.000000   \n",
              "Weight Capacity (kg)  200000.0 -0.002493  0.999763 -1.865994 -0.852032   \n",
              "TE_wc                 200000.0  0.000134  1.000090 -6.756299 -0.475368   \n",
              "skew_0                200000.0 -0.000865  1.001584 -5.307906 -0.606337   \n",
              "skew_1                200000.0  0.000777  0.999486 -5.222957 -0.709550   \n",
              "cheap_flag            200000.0  0.001780  0.042153  0.000000  0.000000   \n",
              "expansive_flag        200000.0  0.001415  0.037590  0.000000  0.000000   \n",
              "\n",
              "                           50%       75%       max  \n",
              "Brand                 3.000000  4.000000  5.000000  \n",
              "Material              3.000000  4.000000  4.000000  \n",
              "Size                  1.000000  3.000000  3.000000  \n",
              "Compartments          4.000000  7.000000  9.000000  \n",
              "Laptop Compartment    1.000000  2.000000  2.000000  \n",
              "Waterproof            1.000000  2.000000  2.000000  \n",
              "Style                 1.000000  3.000000  3.000000  \n",
              "Color                 3.000000  5.000000  6.000000  \n",
              "Weight Capacity (kg)  0.006356  0.854115  1.719577  \n",
              "TE_wc                 0.016408  0.452854  6.500204  \n",
              "skew_0                0.044250  0.746521  5.283896  \n",
              "skew_1               -0.018634  0.648612  3.309770  \n",
              "cheap_flag            0.000000  0.000000  1.000000  \n",
              "expansive_flag        0.000000  0.000000  1.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be71e95b-bea5-4d68-8b26-7618a0887a85\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Brand</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>2.583635</td>\n",
              "      <td>1.845064</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Material</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>2.074010</td>\n",
              "      <td>1.560463</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>1.315085</td>\n",
              "      <td>1.222397</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Compartments</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>4.443590</td>\n",
              "      <td>2.879623</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>1.468485</td>\n",
              "      <td>0.546469</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Waterproof</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>1.468915</td>\n",
              "      <td>0.545110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Style</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>1.359800</td>\n",
              "      <td>1.230834</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Color</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>2.901365</td>\n",
              "      <td>2.081725</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>-0.002493</td>\n",
              "      <td>0.999763</td>\n",
              "      <td>-1.865994</td>\n",
              "      <td>-0.852032</td>\n",
              "      <td>0.006356</td>\n",
              "      <td>0.854115</td>\n",
              "      <td>1.719577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TE_wc</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>1.000090</td>\n",
              "      <td>-6.756299</td>\n",
              "      <td>-0.475368</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>0.452854</td>\n",
              "      <td>6.500204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_0</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>-0.000865</td>\n",
              "      <td>1.001584</td>\n",
              "      <td>-5.307906</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>0.044250</td>\n",
              "      <td>0.746521</td>\n",
              "      <td>5.283896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_1</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.999486</td>\n",
              "      <td>-5.222957</td>\n",
              "      <td>-0.709550</td>\n",
              "      <td>-0.018634</td>\n",
              "      <td>0.648612</td>\n",
              "      <td>3.309770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheap_flag</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>0.001780</td>\n",
              "      <td>0.042153</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>expansive_flag</th>\n",
              "      <td>200000.0</td>\n",
              "      <td>0.001415</td>\n",
              "      <td>0.037590</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be71e95b-bea5-4d68-8b26-7618a0887a85')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be71e95b-bea5-4d68-8b26-7618a0887a85 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be71e95b-bea5-4d68-8b26-7618a0887a85');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cae9b2c4-3e60-45da-8470-8a535431c997\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cae9b2c4-3e60-45da-8470-8a535431c997')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cae9b2c4-3e60-45da-8470-8a535431c997 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 14,\n  \"fields\": [\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 200000.0,\n        \"max\": 200000.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          200000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.383077349501748,\n        \"min\": -0.0024926519217726667,\n        \"max\": 4.44359,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.0001337220113146094\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7749391241912815,\n        \"min\": 0.03758995663765314,\n        \"max\": 2.8796231303833246,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          1.000090100022442\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.4554422553723887,\n        \"min\": -6.756299,\n        \"max\": 0.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -1.8659939\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"25%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.853376046780982,\n        \"min\": -0.85203236,\n        \"max\": 2.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"50%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.420782494442913,\n        \"min\": -0.018634038,\n        \"max\": 4.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"75%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.103566656268112,\n        \"min\": 0.0,\n        \"max\": 7.0,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.64861226\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.3345376602601258,\n        \"min\": 1.0,\n        \"max\": 9.0,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          6.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "______________________________________________________________________\n",
            "\n",
            "     ➩ 𝐍𝐮𝐧𝐢𝐪𝐮𝐞 𝐚𝐧𝐝 𝐧𝐮𝐥𝐥𝐬 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               Brand  Material  Size  Compartments  Laptop Compartment  \\\n",
              "Train nunique      6         5     4            10                   3   \n",
              "Test nunique       6         5     4            10                   3   \n",
              "Train null         0         0     0             0                   0   \n",
              "Test null          0         0     0             0                   0   \n",
              "\n",
              "               Waterproof  Style  Color  Weight Capacity (kg)    TE_wc  \\\n",
              "Train nunique           3      4      7                907130  1803800   \n",
              "Test nunique            3      4      7                108681    78292   \n",
              "Train null              0      0      0                     0        0   \n",
              "Test null               0      0      0                     0        0   \n",
              "\n",
              "               skew_0  skew_1  cheap_flag  expansive_flag  \n",
              "Train nunique      36      72           2               2  \n",
              "Test nunique       36      72           2               2  \n",
              "Train null          0       0           0               0  \n",
              "Test null           0       0           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-64640a4f-4fbb-4295-8aae-f48e5bfdca46\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Train nunique</th>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>907130</td>\n",
              "      <td>1803800</td>\n",
              "      <td>36</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test nunique</th>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>108681</td>\n",
              "      <td>78292</td>\n",
              "      <td>36</td>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Train null</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Test null</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-64640a4f-4fbb-4295-8aae-f48e5bfdca46')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-64640a4f-4fbb-4295-8aae-f48e5bfdca46 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-64640a4f-4fbb-4295-8aae-f48e5bfdca46');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8deb73d2-c63f-4e01-a3a7-e101c8a9b21b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8deb73d2-c63f-4e01-a3a7-e101c8a9b21b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8deb73d2-c63f-4e01-a3a7-e101c8a9b21b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 0,\n        \"max\": 10,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 7,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 438455,\n        \"min\": 0,\n        \"max\": 907130,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          907130,\n          108681\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 889617,\n        \"min\": 0,\n        \"max\": 1803800,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1803800,\n          78292\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 0,\n        \"max\": 36,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          36\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 41,\n        \"min\": 0,\n        \"max\": 72,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          72\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "______________________________________________________________________\n",
            "\n",
            "     ➩ 𝐃𝐮𝐩𝐥𝐢𝐜𝐚𝐭𝐞𝐬 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Train duplicated: 0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'Test duplicated: 1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test duplicates dropped.\n",
            "______________________________________________________________________\n",
            "\n",
            "     ➩ 𝐍𝐮𝐧𝐢𝐪𝐮𝐞 𝐢𝐧 𝐭𝐫𝐚𝐢𝐧 𝐧𝐨𝐭 𝐢𝐧 𝐭𝐞𝐬𝐭/𝐢𝐧 𝐭𝐞𝐬𝐭 𝐧𝐨𝐭 𝐢𝐧 𝐭𝐫𝐚𝐢𝐧 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                      Brand  Material  Size  Compartments  Laptop Compartment  \\\n",
              "in train not in test      0         0     0             0                   0   \n",
              "in test not in train      0         0     0             0                   0   \n",
              "\n",
              "                      Waterproof  Style  Color  skew_0  cheap_flag  \\\n",
              "in train not in test           0      0      0       0           0   \n",
              "in test not in train           0      0      0       0           0   \n",
              "\n",
              "                      expansive_flag  \n",
              "in train not in test               0  \n",
              "in test not in train               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94d5d12f-5df8-4f67-bdc9-4711dd200cfa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>in train not in test</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in test not in train</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94d5d12f-5df8-4f67-bdc9-4711dd200cfa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-94d5d12f-5df8-4f67-bdc9-4711dd200cfa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-94d5d12f-5df8-4f67-bdc9-4711dd200cfa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-136a77a7-d1f9-4faa-9058-760d39d3bedd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-136a77a7-d1f9-4faa-9058-760d39d3bedd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-136a77a7-d1f9-4faa-9058-760d39d3bedd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "data.X.shape, data.y.shape, data.X_test.shape"
      ],
      "metadata": {
        "id": "w1JQWVV9duYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cb23dd7-49c1-453f-eafa-e18ec9c0df63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3994318, 14), (3994318, 1), (200000, 14))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.cat_features)"
      ],
      "metadata": {
        "id": "YuDLMDjOe6x1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62340476-dd8b-4b1e-b715-8ffe4adeb5e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Brand', 'Material', 'Size', 'Compartments', 'Laptop Compartment', 'Waterproof', 'Style', 'Color', 'cheap_flag', 'expansive_flag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.X.info(),data.X_test.info(),"
      ],
      "metadata": {
        "id": "EunCWfhBfcnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea9a3bc8-9d8f-4706-ae67-8bbe585ad904"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float64\n",
            " 9   TE_wc                 float64\n",
            " 10  skew_0                float64\n",
            " 11  skew_1                float64\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            "dtypes: float64(4), int64(10)\n",
            "memory usage: 426.6 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 200000 entries, 0 to 199999\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Non-Null Count   Dtype  \n",
            "---  ------                --------------   -----  \n",
            " 0   Brand                 200000 non-null  int64  \n",
            " 1   Material              200000 non-null  int64  \n",
            " 2   Size                  200000 non-null  int64  \n",
            " 3   Compartments          200000 non-null  int64  \n",
            " 4   Laptop Compartment    200000 non-null  int64  \n",
            " 5   Waterproof            200000 non-null  int64  \n",
            " 6   Style                 200000 non-null  int64  \n",
            " 7   Color                 200000 non-null  int64  \n",
            " 8   Weight Capacity (kg)  200000 non-null  float64\n",
            " 9   TE_wc                 200000 non-null  float64\n",
            " 10  skew_0                200000 non-null  float64\n",
            " 11  skew_1                200000 non-null  float64\n",
            " 12  cheap_flag            200000 non-null  int64  \n",
            " 13  expansive_flag        200000 non-null  int64  \n",
            "dtypes: float64(4), int64(10)\n",
            "memory usage: 22.9 MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Preprocessing\n",
        "Here we define a preprocessing class to handle missing values and perform some feature engineering."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.011742,
          "end_time": "2024-10-26T19:31:36.380662",
          "exception": false,
          "start_time": "2024-10-26T19:31:36.36892",
          "status": "completed"
        },
        "tags": [],
        "id": "BsolTJDo1KTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, data: Data):\n",
        "        self.data = data\n",
        "\n",
        "    def feature_preparation(self):\n",
        "        ''' User-define feature engineering and preprocessing'''\n",
        "        print('⇒ Train-Test-Original shapes before preprocessing:',\n",
        "              self.data.train.shape, self.data.test.shape, self.data.original.shape)\n",
        "\n",
        "        df = pd.concat([self.data.train, self.data.test, self.data.original], axis=0)\n",
        "        ## -- Operations\n",
        "        # Convert categoricals and handle missing\n",
        "        df = self.convert_categorical(df)\n",
        "        df = self.handle_nan(df)\n",
        "\n",
        "        # Retrieve train, test and original\n",
        "        self.data.train    = df[:len(self.data.train)].reset_index(drop=True)\n",
        "        self.data.test     = df[len(self.data.train):len(self.data.train)+\\\n",
        "                                len(self.data.test)].reset_index(drop=True).drop(columns=data.target)\n",
        "        self.data.original = df[len(self.data.train)+len(self.data.test):].reset_index(drop=True)\n",
        "        print('⇒ Train-Test-Original shapes after preprocessing:', self.data.train.shape, self.data.test.shape, self.data.original.shape)\n",
        "        print('⇒ Preprocessing finished')\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_categorical(df):\n",
        "        '''For converting data to categoricals'''\n",
        "        for c in df.select_dtypes(include=['object', 'category', 'bool', 'int']).columns:\n",
        "            df[c] = df[c].astype(str).fillna('missing').astype('int')\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def handle_nan(df):\n",
        "        for c in df.select_dtypes(exclude=['object', 'category', 'bool', 'int']).columns:\n",
        "            df[c] = df[c].fillna(df[c].median())\n",
        "\n",
        "        for c in df.select_dtypes(include=['object', 'category', 'bool', 'int']).columns:\n",
        "            df[c] = df[c].fillna(df[c].mode()[0])\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_fi():\n",
        "\n",
        "        def sorted_fi(fi): # fi is a list of feature importances\n",
        "            fi = list(zip(data.X.columns.to_list(), fi))\n",
        "            return sorted(fi, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Make a pipeline to transform X for those method which can't handle categorical and NaN natively\n",
        "        pipe    = make_pipeline(SimpleImputer(strategy='most_frequent'), CountEncoder(), StandardScaler())\n",
        "        X_FE    = pipe.fit_transform(data.X)\n",
        "\n",
        "        FI_LGBM = sorted_fi(LGBMRegressor(verbose=-1, random_state=CFG.SEED).fit(data.X, data.y).feature_importances_)\n",
        "\n",
        "        FI_CB   = sorted_fi(CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features).fit(data.X, data.y).get_feature_importance())\n",
        "\n",
        "        FI_RF   = sorted_fi(RandomForestRegressor(n_estimators=100, random_state=CFG.SEED).fit(X_FE, data.y).feature_importances_)\n",
        "\n",
        "        FI_MI   = sorted_fi(list(mutual_info_regression(X_FE, data.y)))\n",
        "\n",
        "        FIS     = [FI_LGBM, FI_CB, FI_RF, FI_MI]\n",
        "        titles  = ['LGBM FI', 'CB FI', 'RF FI', 'MI']\n",
        "\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(25, 10))\n",
        "        for i, FI in enumerate(FIS):\n",
        "            # Convert the list of tuples to a DataFrame for easier plotting\n",
        "            FI_df = pd.DataFrame(FI[:15], columns=['Feature', 'Importance'])\n",
        "            sns.barplot(x='Importance', y='Feature', data=FI_df, ax=axes[i], palette='Blues_r')\n",
        "            axes[i].set_title(titles[i])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "data = Data()\n",
        "pp   = Preprocessing(data)\n",
        "pp.feature_preparation()\n",
        "#pp.calculate_fi()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:45.255839Z",
          "iopub.execute_input": "2025-02-03T01:57:45.256124Z",
          "iopub.status.idle": "2025-02-03T01:57:46.490442Z",
          "shell.execute_reply.started": "2025-02-03T01:57:45.256101Z",
          "shell.execute_reply": "2025-02-03T01:57:46.489645Z"
        },
        "papermill": {
          "duration": 26.783101,
          "end_time": "2024-10-26T19:32:03.175845",
          "exception": false,
          "start_time": "2024-10-26T19:31:36.392744",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "25jm-nnR1KTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "861a6c1b-2ee7-4418-feac-2d7dad7ea8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⇒ Train-Test-Original shapes before preprocessing: (3994318, 15) (200000, 14) (0, 0)\n",
            "⇒ Train-Test-Original shapes after preprocessing: (3994318, 15) (200000, 14) (0, 15)\n",
            "⇒ Preprocessing finished\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "data.cat_features,data.num_features"
      ],
      "metadata": {
        "id": "gFvHEDvOe0oU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52722e5-73d6-458d-eafd-7c09a56809c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Brand',\n",
              "  'Material',\n",
              "  'Size',\n",
              "  'Compartments',\n",
              "  'Laptop Compartment',\n",
              "  'Waterproof',\n",
              "  'Style',\n",
              "  'Color',\n",
              "  'cheap_flag',\n",
              "  'expansive_flag'],\n",
              " ['Weight Capacity (kg)', 'TE_wc', 'skew_0', 'skew_1'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.X.info()"
      ],
      "metadata": {
        "id": "EEWwdyAUgkYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c83c2ffc-a074-4977-e0f6-5f450eaeb398"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float64\n",
            " 9   TE_wc                 float64\n",
            " 10  skew_0                float64\n",
            " 11  skew_1                float64\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            "dtypes: float64(4), int64(10)\n",
            "memory usage: 426.6 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that there is no consensus among the different methods to the most important features. 'MI' entry, which stands for the calculation from Mutual Information method, seems quite reasonable: the Overall quality of the house is the most important feature, followed by the Neighborhood, which ultimately reflects its location."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.014801,
          "end_time": "2024-10-26T19:32:03.206725",
          "exception": false,
          "start_time": "2024-10-26T19:32:03.191924",
          "status": "completed"
        },
        "tags": [],
        "id": "YF_REUbY1KTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modeling"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.014189,
          "end_time": "2024-10-26T19:32:03.237017",
          "exception": false,
          "start_time": "2024-10-26T19:32:03.222828",
          "status": "completed"
        },
        "tags": [],
        "id": "7OZ_SPtJ1KTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.0 Neural Network Model Functions:"
      ],
      "metadata": {
        "id": "aG58nE96v8Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features_card = []\n",
        "for f in data.cat_features:\n",
        "  cat_features_card.append(1 + data.X[f].astype(\"int\").max())\n",
        "\n",
        "cat_features_card"
      ],
      "metadata": {
        "id": "XpEkN-4CzCKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################################################################################################################################\n",
        "def build_model_v0(units=512,last_layer = 1, activation=\"relu\",cat_features=data.cat_features,cat_features_card=cat_features_card):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model\n",
        "\n",
        "#####################################################################################################################################################################################\n",
        "def build_model_v1(units=512,last_layer = 1, activation=\"relu\", cat_features=data.cat_features,cat_features_card=cat_features_card, dropout_rate=0.35, reg=0.001):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    reshaped_features = layers.Reshape((1, -1))(x)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x, flattened_attention])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model\n",
        "\n",
        "#####################################################################################################################################################################################\n",
        "def build_model_v2(units=512, last_layer=1, activation=\"relu\", cat_features=data.cat_features,cat_features_card=cat_features_card,\n",
        "                   repeat_att=2, dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.0001): # Reduced transformer_units\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
        "    embs = []\n",
        "    transformer_outputs = [] # List to store transformer outputs for each categorical feature\n",
        "\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:, j])\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        embs.append(x)\n",
        "\n",
        "        # Reshape for Transformer (batch_size, 1, features) - Crucial!\n",
        "        reshaped_embedding = layers.Reshape((1, -1))(x)\n",
        "\n",
        "        # Transformer Layer for each categorical feature\n",
        "        for q in list(range(repeat_att)):\n",
        "          if q == 0:\n",
        "            attention_output = reshaped_embedding\n",
        "\n",
        "          attention_output_ = layers.MultiHeadAttention(num_heads=num_transformer_heads, key_dim=transformer_units,name=f\"mh_{j}_{q}\")(attention_output, attention_output)\n",
        "          attention_output_ = layers.LayerNormalization(name=f\"mh_ln1_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "          attention_output_ = layers.Dense(reshaped_embedding.shape[-1], activation=activation,name=f\"mh_dense_{j}_{q}\")(attention_output_)\n",
        "          attention_output = layers.LayerNormalization(name=f\"mh_ln2_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "\n",
        "        transformer_outputs.append(layers.Flatten()(attention_output)) # Store flattened transformer output\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    x_orig = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    reshaped_features = layers.Reshape((1, -1))(x_orig)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x_orig, flattened_attention])\n",
        "\n",
        "    # Concatenate Transformer outputs and numerical features\n",
        "    all_features = layers.Concatenate(axis=-1)(transformer_outputs + [x])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(all_features)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    #x = layers.Concatenate(axis=-1)([x_orig, x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model\n",
        "\n",
        "  ####################################################\n",
        "\n",
        "class exitesqueeze_layer(layers.Layer):\n",
        "    def __init__(self, exite_units,dropout_rate,activation,reg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.exite_units = exite_units\n",
        "        self.activation=activation\n",
        "        self.reg=reg\n",
        "\n",
        "        self.reshaped_0 = layers.Reshape((-1, 1))\n",
        "        self.reshaped_1 = layers.Reshape((-1, ))\n",
        "\n",
        "        self.exite = layers.Dense(self.exite_units, activation=self.activation)\n",
        "        self.squeeze = layers.Dense(1, activation=\"linear\",kernel_regularizer=keras.regularizers.l2(reg))\n",
        "        self.lnorm_00 = layers.LayerNormalization()\n",
        "        self.lnorm_01 = layers.LayerNormalization()\n",
        "        self.drop = layers.Dropout(rate=dropout_rate)\n",
        "        self.attention = layers.Attention()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.reshaped_0(inputs)\n",
        "        x = self.exite(x)\n",
        "        att_out = self.attention([x,x])\n",
        "        att_out = self.lnorm_00(att_out)\n",
        "        x = layers.add([x, att_out])\n",
        "        x = self.squeeze(x)\n",
        "        x = self.reshaped_1(x)\n",
        "\n",
        "        x = layers.multiply([x, inputs])\n",
        "\n",
        "        x = self.lnorm_01(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Remove build warnings\n",
        "    def build(self):\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "def build_model_v3(units=512,exite_units=64, last_layer = 1, activation=\"relu\", reg=0.001, dropout_rate=0.33):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(data.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(data.cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    x_0 = layers.Concatenate(axis=-1, name=\"input_concat\")(embs+[x_input_nums])\n",
        "\n",
        "    es_0 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_0_concat\")([x_0,es_0])\n",
        "    x = layers.BatchNormalization(name=\"se_0_bn\")(x)\n",
        "\n",
        "    es_1 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_1_concat\")([x,es_1])\n",
        "    x = layers.BatchNormalization(name=\"se_1_bn\")(x)\n",
        "\n",
        "    es_2 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_2_concat\")([x,es_2])\n",
        "    x = layers.BatchNormalization(name=\"se_2_bn\")(x)\n",
        "\n",
        "    x_0 = layers.Dense(units, name=\"dense_0\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_0\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_0\")(x_0)\n",
        "\n",
        "    x_0 = layers.Dense(int(units/last_layer), name=\"dense_1\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_1\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_1\")(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([x_0,x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "hb64lyLLwCoZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Stacking"
      ],
      "metadata": {
        "id": "OOAMQ6WiwE8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainModels:\n",
        "    def __init__(self, X, y, X_test, X_original, y_original, models):\n",
        "        self.models     = models\n",
        "        self.X          = X\n",
        "        self.y          = y\n",
        "        self.X_original = X_original\n",
        "        self.y_original = y_original\n",
        "        self.X_test     = X_test\n",
        "        self._OOF_train = pd.DataFrame()\n",
        "        self._OOF_test  = pd.DataFrame()\n",
        "        self.categorical_features = X.select_dtypes(include=['category', 'bool', 'category','int']).columns.to_list()\n",
        "        self.numerical_features = X.select_dtypes(include=['float']).columns.to_list()\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_cat(df_):\n",
        "        '''\n",
        "        A function to convert dtypes to categorical if the catboost with all categorical\n",
        "        features is called\n",
        "        '''\n",
        "        df = df_.copy()\n",
        "        for c in df.columns.to_list():\n",
        "            df[c] = df[c].astype(str).astype('category')\n",
        "        return df\n",
        "\n",
        "\n",
        "    def fit_model(self, name, model_, train_flag):\n",
        "        oof_train = np.zeros(self.X.shape[0])\n",
        "        oof_test  = np.zeros(self.X_test.shape[0])\n",
        "        scores_train = []\n",
        "        scores_val   = []\n",
        "\n",
        "        CB_CAT_FLAG = True if 'CB_CAT' in name.upper() else False\n",
        "\n",
        "        os.chdir('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E2/layers_3_staked_models_opt')\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(CFG.CV.split(self.X, self.y)):\n",
        "            x_train, y_train = self.X.iloc[train_idx], self.y.iloc[train_idx]\n",
        "            x_val,   y_val   = self.X.iloc[val_idx],   self.y.iloc[val_idx]\n",
        "\n",
        "            # Adds the original data to training set only\n",
        "            if self.X_original is not None:\n",
        "                x_train = pd.concat([x_train, self.X_original], axis=0)\n",
        "                y_train = pd.concat([y_train, self.y_original], axis=0)\n",
        "\n",
        "            # -- Create a special block to convert all features to categorical\n",
        "            if CB_CAT_FLAG:\n",
        "                x_train = self.convert_cat(x_train)\n",
        "                x_val   = self.convert_cat(x_val)\n",
        "            if 'NN_' in name.upper():\n",
        "              model = model_\n",
        "            else:\n",
        "              model = clone(model_)\n",
        "\n",
        "#######################################################################################\n",
        "            if 'NN_' in name.upper():\n",
        "\n",
        "              model = model_\n",
        "\n",
        "              X_train_cat = x_train[self.categorical_features].astype(\"int32\")\n",
        "              X_train_num = x_train[self.numerical_features].astype(\"float\")\n",
        "\n",
        "              X_valid_cat = x_val[self.categorical_features].astype(\"int32\")\n",
        "              X_valid_num = x_val[self.numerical_features].astype(\"float32\")\n",
        "\n",
        "              X_test_cat = self.X_test[self.categorical_features].astype(\"int32\")\n",
        "              X_test_num = self.X_test[self.numerical_features].astype(\"float32\")\n",
        "\n",
        "              # Compile the model\n",
        "              keras.utils.set_random_seed(42)\n",
        "\n",
        "              optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "              model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mse\"),\n",
        "                            metrics=[keras.metrics.RootMeanSquaredError(name=\"rmse\")])\n",
        "\n",
        "              # Fit the model\n",
        "              if train_flag==True:\n",
        "                model.fit([X_train_cat,X_train_num], y_train,\n",
        "                          validation_data=([X_valid_cat, X_valid_num], y_val),\n",
        "                          epochs=101,\n",
        "                          batch_size=2048,\n",
        "                          callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3, monitor=\"val_rmse\", factor=0.3, min_lr=5e-5),\n",
        "                                     keras.callbacks.EarlyStopping(patience=21, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                                    start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "                model.save(f'{name}_{CFG.VERSION}.keras') # Saving the model after training\n",
        "\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = load(f'{name}_{CFG.VERSION}.keras') # Loading the saved model\n",
        "#####################################################################################\n",
        "\n",
        "            elif 'CB' in name.upper():\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train,y_train,eval_set=[(x_val,y_val)], early_stopping_rounds=101)\n",
        "                model.save_model(f\"{name}_{fold}_{CFG.VERSION}.bin\")\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = CatBoostRegressor()\n",
        "                model.load_model(f\"{name}_{fold}_{CFG.VERSION}.bin\")\n",
        "\n",
        "            elif 'LGBMR' in name.upper():\n",
        "              early_stop = early_stopping(stopping_rounds=101)\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train, y_train, eval_set=[(x_val, y_val)], callbacks=[early_stop],categorical_feature=self.categorical_features)\n",
        "                model.booster_.save_model(f'{name}_{fold}_{CFG.VERSION}.txt')\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = lgb.Booster(model_file=f'{name}_{fold}_{CFG.VERSION}.txt')\n",
        "\n",
        "            elif 'LGBMP' in name.upper():\n",
        "              # Fit the model\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train, y_train)\n",
        "                dump(model, f'{name}_{fold}_{CFG.VERSION}.pkl')\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = load(f'{name}_{fold}_{CFG.VERSION}.pkl')\n",
        "\n",
        "            elif 'XGBR' in name.upper():\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train, y_train, eval_set=[(x_val, y_val)],verbose=250)\n",
        "                model.save_model(f'{name}_{fold}_{CFG.VERSION}.json')\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = XGBRegressor()  # Create an instance of the model\n",
        "                model.load_model(f'{name}_{fold}_{CFG.VERSION}.json')\n",
        "\n",
        "            elif 'LGBM_2' in name.upper():\n",
        "              model.fit(x_train, y_train, eval_set=[(x_val, y_val)],verbose=250)\n",
        "\n",
        "\n",
        "            elif 'HGB_' in name.upper():\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train, y_train)\n",
        "                dump(model, f'{name}_{fold}_{CFG.VERSION}.pkl')\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = load(f'{name}_{fold}_{CFG.VERSION}.pkl')\n",
        "\n",
        "            else:\n",
        "\n",
        "              model.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "            #################### Predictions\n",
        "            if 'NN_' in name.upper():\n",
        "              y_pred_train = model.predict([X_train_cat,X_train_num], batch_size=1024).ravel()\n",
        "              y_pred_val   = model.predict([X_valid_cat, X_valid_num], batch_size=1024).ravel()\n",
        "              y_pred_test  = model.predict([X_test_cat, X_test_num], batch_size=1024).ravel()\n",
        "\n",
        "            else:\n",
        "              y_pred_train = model.predict(x_train).ravel()\n",
        "              y_pred_val   = model.predict(x_val).ravel()\n",
        "              y_pred_test  = model.predict(self.X_test if not CB_CAT_FLAG else self.convert_cat(self.X_test)).ravel()\n",
        "\n",
        "            oof_train[val_idx] = y_pred_val\n",
        "            oof_test   += y_pred_test/CFG.CV.get_n_splits()\n",
        "\n",
        "            train_score = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "            val_score   = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "\n",
        "            print(f'Fold {fold+1} → Training set Score: {train_score:.5f} | Validation set Score: {val_score:.5f}')\n",
        "\n",
        "            scores_train.append(train_score)\n",
        "            scores_val.append(val_score)\n",
        "\n",
        "        self._OOF_train[name] = oof_train\n",
        "        self._OOF_test[name]  = oof_test\n",
        "\n",
        "        os.chdir('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2')\n",
        "\n",
        "        print(colored(f'Overall → Training set Score: {np.mean(scores_train):.5f}±{np.std(scores_train):.7f} | Validation set Score: {np.mean(scores_val):.5f}±{np.std(scores_val):.7f}',\n",
        "              color='green', attrs=['bold', 'dark']))\n",
        "\n",
        "    def fit_models(self):\n",
        "        for name, model in list(self.models.items()):\n",
        "            print(colored(f'{\" \"*4} Fitting {name}', color='red', attrs=['dark', 'bold']))\n",
        "            self.fit_model(name, model[0], model[1])\n",
        "            print(\"\")\n",
        "\n",
        "    @property\n",
        "    def OOF_train(self):\n",
        "        return self._OOF_train\n",
        "    @property\n",
        "    def OOF_test(self):\n",
        "        return self._OOF_test\n",
        "\n",
        "    def save_predictions(self):\n",
        "        self._OOF_train.to_csv('OOF_train_many_models.csv', index=False)\n",
        "        self._OOF_test.to_csv('OOF_test_many_models.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:46.491421Z",
          "iopub.execute_input": "2025-02-03T01:57:46.491688Z",
          "iopub.status.idle": "2025-02-03T01:57:46.503925Z",
          "shell.execute_reply.started": "2025-02-03T01:57:46.491663Z",
          "shell.execute_reply": "2025-02-03T01:57:46.50294Z"
        },
        "papermill": {
          "duration": 0.044089,
          "end_time": "2024-10-26T19:32:03.295732",
          "exception": false,
          "start_time": "2024-10-26T19:32:03.251643",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "WypqJp7Q1KTh"
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.1 First Level of Stacking\n",
        "We define a bunch of tree-based methods with many different hyperparameters configurations to aggregate diversity."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.014633,
          "end_time": "2024-10-26T19:32:03.325194",
          "exception": false,
          "start_time": "2024-10-26T19:32:03.310561",
          "status": "completed"
        },
        "tags": [],
        "id": "mFxQScB81KTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2')"
      ],
      "metadata": {
        "id": "XEz8PpD8koh5"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_params_1 = {'num_leaves': 103, 'min_child_samples': 36, 'subsample': 0.9131771240297577, 'subsample_freq': 2, 'colsample_bytree': 0.6190291906152294, 'reg_alpha': 0.03976551748855951, 'reg_lambda': 0.2576052197300848}\n",
        "#lgbm_params_5 = {'max_depth': 5, 'learning_rate': 0.055, 'n_estimators': 3000, 'num_leaves': 31}\n",
        "#lgbm_params_6 = {'max_depth': 6, 'learning_rate': 0.01, 'n_estimators': 3000, 'num_leaves': 31}\n",
        "#lgbm_params_6 = {'max_depth': 9, 'learning_rate': 0.02, 'n_estimators': 2000, 'num_leaves': 31, \"subsample\":0.85, \"colsample_bytree\":0.85,\n",
        "#                 \"reg_alpha\":0.01, \"reg_lambda\":0.01, \"boosting_type\":\"gbdt\" }\n",
        "\n",
        "cb_cat_params_1   = {'iterations': 3000,'learning_rate': 0.025,'depth': 9, 'l2_leaf_reg': 0.004177701145518355,\n",
        "                     'bagging_temperature': 0.5, 'random_strength': 0.5,\"bootstrap_type\": \"Bayesian\",\n",
        "                     'cat_features': data.cat_features,'task_type': 'GPU',\n",
        "                     'random_seed':CFG.SEED,'verbose': 250,\"od_type\":'EBS',\"od_wait\":101,\"use_best_model\":True}\n",
        "\n",
        "xgb_params_1 = {'n_estimators': 3000,'learning_rate': 0.025,'max_depth': 9,'min_child_weight': 9,'subsample': 0.95,'colsample_bytree': 0.75,\n",
        "                'gamma': 0.5511841320880777,'reg_alpha': 0.006948944983035811,'reg_lambda': 0.0015661314558322087,\n",
        "                'objective': \"reg:squarederror\",'eval_metric': \"rmse\", \"early_stopping_rounds\":101, 'device':'gpu', 'tree_method': 'gpu_hist',\n",
        "                'random_state': CFG.SEED,'enable_categorical': True,\"verbosity\":0}\n",
        "\n",
        "nn_params_v0 = {'units': 1024, 'last_layer': 2, 'activation': 'silu'}\n",
        "nn_params_v1 = {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002133354415296107, 'dropout_rate': 0.47090232042440616}\n",
        "nn_params_v2 = {'units': 256, 'last_layer': 1, 'activation': 'silu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.36}\n",
        "\n",
        "models={\n",
        "#        'LGBMR1_opt': [LGBMRegressor(learning_rate=0.02,boosting_type='gbdt',n_estimators=3000,objective=\"regression\",eval_metric=\"rmse\",device='gpu',verbose=-1,random_state= CFG.SEED,**lgbm_params_1),False],\n",
        "        #'LGBMR2': [LGBMRegressor(verbose=-1, random_state=CFG.SEED, boosting_type='dart', device='gpu',n_estimators=211),False],\n",
        "        #'LGBMR3': [LGBMRegressor(verbose=-1, random_state=CFG.SEED, data_sample_strategy='GOSS', device='gpu',n_estimators=211),False],\n",
        "        #'LGBMP4': [make_pipeline(CatBoostEncoder(), LGBMRegressor(verbose=-1, random_state=CFG.SEED, **lgbm_params_4, device='gpu')),False],\n",
        "        #'LGBMR5': [LGBMRegressor(verbose=-1, random_state=CFG.SEED, **lgbm_params_5, device='gpu'),False],\n",
        "        #'LGBMR6': [LGBMRegressor(verbose=-1, random_state=CFG.SEED, **lgbm_params_6, device='gpu'),False],\n",
        "\n",
        "#        'CB1_opt'   :[CatBoostRegressor(objective=\"RMSE\",eval_metric=\"RMSE\", **cb_cat_params_1),False],\n",
        "    #    'CB2'   : [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features, grow_policy='Depthwise', loss_function='RMSE', task_type='GPU', iterations=3000),False],\n",
        "    #    'CB3'   : [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features, grow_policy='Lossguide', loss_function='RMSE', task_type='GPU', iterations=3000),False],\n",
        "    #    'CB4'   : [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features, **cb_cat_params_4, loss_function='RMSE', task_type='GPU', iterations=3000),False],\n",
        "    #    'CB5'   : [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features, iterations=3000, task_type='GPU'),False],\n",
        "    #   'CB_CAT1': [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.test.columns.to_list(), iterations=750, task_type='GPU'),True],\n",
        "    #    'CB_CAT2': [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.test.columns.to_list(), **cb_cat_params_4, loss_function='RMSE', task_type='GPU'),True],\n",
        "\n",
        "#        'XGBR1_opt' : [XGBRegressor(**xgb_params_1),False],\n",
        "    #   'XGBR2' : [XGBRegressor(random_state=CFG.SEED, enable_categorical=True, max_depth=4, learning_rate=0.04, device='gpu', early_stopping_rounds=101,tree_method= 'gpu_hist',n_estimators=2000),True],\n",
        "\n",
        "\n",
        "#        'HGB_TE': [make_pipeline(TargetEncoder(), HistGradientBoostingRegressor(random_state=CFG.SEED, max_iter=5000,\n",
        "#                                                                              learning_rate=0.035, max_depth=8,validation_fraction=0.15,\n",
        "#                                                                              early_stopping=True, l2_regularization=0.03,\n",
        "#                                                                             )),False],\n",
        "\n",
        "      #  'HGB_CE': [make_pipeline(CountEncoder(), HistGradientBoostingRegressor(random_state=CFG.SEED, max_iter=5000,\n",
        "      #                                                                          learning_rate=0.055, max_depth=5,\n",
        "      #                                                                          early_stopping=True,  validation_fraction=0.15\n",
        "      #                                                                        )),True],\n",
        "\n",
        "#        \"NN_v0\" : [build_model_v0(**nn_params_v0),True],\n",
        "#        \"NN_v1\" : [build_model_v1(**nn_params_v1),True],\n",
        "        \"NN_v2\" : [build_model_v3(**nn_params_v2),True]}\n",
        "\n",
        "TM = TrainModels(X=data.X, y=data.y, X_test=data.X_test, X_original=None, y_original=None, models=models)\n",
        "TM.fit_models()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:46.505448Z",
          "iopub.execute_input": "2025-02-03T01:57:46.505649Z",
          "iopub.status.idle": "2025-02-03T02:51:59.869417Z",
          "shell.execute_reply.started": "2025-02-03T01:57:46.505631Z",
          "shell.execute_reply": "2025-02-03T02:51:59.868592Z"
        },
        "papermill": {
          "duration": 1396.693239,
          "end_time": "2024-10-26T19:55:20.033429",
          "exception": false,
          "start_time": "2024-10-26T19:32:03.34019",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "CzGQ1DP71KTi",
        "outputId": "bd97fa19-d103-41b1-f331-b5d93f406eaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Fitting NN_v2\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 48s 17ms/step - loss: 3448.5542 - rmse: 57.3162 - val_loss: 1499.6984 - val_rmse: 38.7254 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1514.1229 - rmse: 38.9112 - val_loss: 1499.1476 - val_rmse: 38.7182 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1505.7306 - rmse: 38.8031 - val_loss: 1498.7871 - val_rmse: 38.7135 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1502.0063 - rmse: 38.7550 - val_loss: 1498.9771 - val_rmse: 38.7159 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1500.4741 - rmse: 38.7352 - val_loss: 1499.0582 - val_rmse: 38.7169 - learning_rate: 0.0010\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1499.5563 - rmse: 38.7233 - val_loss: 1499.3561 - val_rmse: 38.7207 - learning_rate: 0.0010\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.8325 - rmse: 38.7139 - val_loss: 1498.5897 - val_rmse: 38.7107 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.7306 - rmse: 38.7126 - val_loss: 1498.3190 - val_rmse: 38.7072 - learning_rate: 3.0000e-04\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.6294 - rmse: 38.7113 - val_loss: 1498.3408 - val_rmse: 38.7075 - learning_rate: 3.0000e-04\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 18s 10ms/step - loss: 1498.5474 - rmse: 38.7102 - val_loss: 1498.3938 - val_rmse: 38.7082 - learning_rate: 3.0000e-04\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.4368 - rmse: 38.7088 - val_loss: 1498.2972 - val_rmse: 38.7070 - learning_rate: 3.0000e-04\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.2865 - rmse: 38.7068 - val_loss: 1498.3147 - val_rmse: 38.7072 - learning_rate: 3.0000e-04\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.3129 - rmse: 38.7072 - val_loss: 1498.4159 - val_rmse: 38.7085 - learning_rate: 3.0000e-04\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.2454 - rmse: 38.7063 - val_loss: 1498.3154 - val_rmse: 38.7072 - learning_rate: 3.0000e-04\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.1283 - rmse: 38.7048 - val_loss: 1498.2023 - val_rmse: 38.7058 - learning_rate: 9.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.0555 - rmse: 38.7039 - val_loss: 1498.1949 - val_rmse: 38.7057 - learning_rate: 9.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.0739 - rmse: 38.7041 - val_loss: 1498.1830 - val_rmse: 38.7055 - learning_rate: 9.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.0446 - rmse: 38.7037 - val_loss: 1498.1724 - val_rmse: 38.7054 - learning_rate: 9.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.0519 - rmse: 38.7038 - val_loss: 1498.1797 - val_rmse: 38.7055 - learning_rate: 9.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.0071 - rmse: 38.7032 - val_loss: 1498.1722 - val_rmse: 38.7054 - learning_rate: 9.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.9659 - rmse: 38.7027 - val_loss: 1498.1760 - val_rmse: 38.7054 - learning_rate: 9.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.9437 - rmse: 38.7024 - val_loss: 1498.1636 - val_rmse: 38.7053 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1498.0009 - rmse: 38.7032 - val_loss: 1498.1646 - val_rmse: 38.7053 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.9570 - rmse: 38.7026 - val_loss: 1498.1564 - val_rmse: 38.7052 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 18s 10ms/step - loss: 1497.9158 - rmse: 38.7021 - val_loss: 1498.1667 - val_rmse: 38.7053 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.9944 - rmse: 38.7031 - val_loss: 1498.1506 - val_rmse: 38.7051 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.8414 - rmse: 38.7011 - val_loss: 1498.1483 - val_rmse: 38.7051 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.8600 - rmse: 38.7014 - val_loss: 1498.1393 - val_rmse: 38.7050 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.9148 - rmse: 38.7021 - val_loss: 1498.1379 - val_rmse: 38.7050 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.8348 - rmse: 38.7010 - val_loss: 1498.1354 - val_rmse: 38.7049 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.9005 - rmse: 38.7019 - val_loss: 1498.1460 - val_rmse: 38.7051 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.8179 - rmse: 38.7008 - val_loss: 1498.1270 - val_rmse: 38.7048 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8307 - rmse: 38.7010 - val_loss: 1498.1257 - val_rmse: 38.7048 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8912 - rmse: 38.7018 - val_loss: 1498.1301 - val_rmse: 38.7049 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8228 - rmse: 38.7009 - val_loss: 1498.1241 - val_rmse: 38.7048 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 21s 11ms/step - loss: 1497.8213 - rmse: 38.7009 - val_loss: 1498.1193 - val_rmse: 38.7047 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8385 - rmse: 38.7011 - val_loss: 1498.1310 - val_rmse: 38.7049 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.8079 - rmse: 38.7007 - val_loss: 1498.1273 - val_rmse: 38.7048 - learning_rate: 5.0000e-05\n",
            "Epoch 39/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8091 - rmse: 38.7007 - val_loss: 1498.1230 - val_rmse: 38.7048 - learning_rate: 5.0000e-05\n",
            "Epoch 40/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.8376 - rmse: 38.7011 - val_loss: 1498.1182 - val_rmse: 38.7047 - learning_rate: 5.0000e-05\n",
            "Epoch 41/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7924 - rmse: 38.7005 - val_loss: 1498.1144 - val_rmse: 38.7047 - learning_rate: 5.0000e-05\n",
            "Epoch 42/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8113 - rmse: 38.7008 - val_loss: 1498.1053 - val_rmse: 38.7046 - learning_rate: 5.0000e-05\n",
            "Epoch 43/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.8005 - rmse: 38.7006 - val_loss: 1498.1084 - val_rmse: 38.7046 - learning_rate: 5.0000e-05\n",
            "Epoch 44/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7308 - rmse: 38.6997 - val_loss: 1498.1094 - val_rmse: 38.7046 - learning_rate: 5.0000e-05\n",
            "Epoch 45/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7643 - rmse: 38.7001 - val_loss: 1498.1066 - val_rmse: 38.7046 - learning_rate: 5.0000e-05\n",
            "Epoch 46/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7174 - rmse: 38.6995 - val_loss: 1498.1039 - val_rmse: 38.7045 - learning_rate: 5.0000e-05\n",
            "Epoch 47/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7523 - rmse: 38.7000 - val_loss: 1498.1051 - val_rmse: 38.7046 - learning_rate: 5.0000e-05\n",
            "Epoch 48/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7611 - rmse: 38.7001 - val_loss: 1498.0986 - val_rmse: 38.7045 - learning_rate: 5.0000e-05\n",
            "Epoch 49/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.7687 - rmse: 38.7002 - val_loss: 1498.0912 - val_rmse: 38.7044 - learning_rate: 5.0000e-05\n",
            "Epoch 50/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7231 - rmse: 38.6996 - val_loss: 1498.0818 - val_rmse: 38.7043 - learning_rate: 5.0000e-05\n",
            "Epoch 51/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7496 - rmse: 38.7000 - val_loss: 1498.0773 - val_rmse: 38.7042 - learning_rate: 5.0000e-05\n",
            "Epoch 52/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7234 - rmse: 38.6996 - val_loss: 1498.0922 - val_rmse: 38.7044 - learning_rate: 5.0000e-05\n",
            "Epoch 53/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6792 - rmse: 38.6991 - val_loss: 1498.0850 - val_rmse: 38.7043 - learning_rate: 5.0000e-05\n",
            "Epoch 54/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.7638 - rmse: 38.7002 - val_loss: 1498.0773 - val_rmse: 38.7042 - learning_rate: 5.0000e-05\n",
            "Epoch 55/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.6382 - rmse: 38.6985 - val_loss: 1498.0787 - val_rmse: 38.7042 - learning_rate: 5.0000e-05\n",
            "Epoch 56/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7030 - rmse: 38.6994 - val_loss: 1498.0637 - val_rmse: 38.7040 - learning_rate: 5.0000e-05\n",
            "Epoch 57/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6792 - rmse: 38.6991 - val_loss: 1498.0620 - val_rmse: 38.7040 - learning_rate: 5.0000e-05\n",
            "Epoch 58/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7080 - rmse: 38.6994 - val_loss: 1498.0570 - val_rmse: 38.7039 - learning_rate: 5.0000e-05\n",
            "Epoch 59/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7089 - rmse: 38.6995 - val_loss: 1498.0592 - val_rmse: 38.7040 - learning_rate: 5.0000e-05\n",
            "Epoch 60/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.7306 - rmse: 38.6997 - val_loss: 1498.0580 - val_rmse: 38.7040 - learning_rate: 5.0000e-05\n",
            "Epoch 61/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7477 - rmse: 38.7000 - val_loss: 1498.0649 - val_rmse: 38.7041 - learning_rate: 5.0000e-05\n",
            "Epoch 62/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.6583 - rmse: 38.6988 - val_loss: 1498.0533 - val_rmse: 38.7039 - learning_rate: 5.0000e-05\n",
            "Epoch 63/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7490 - rmse: 38.7000 - val_loss: 1498.0626 - val_rmse: 38.7040 - learning_rate: 5.0000e-05\n",
            "Epoch 64/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.6808 - rmse: 38.6991 - val_loss: 1498.0610 - val_rmse: 38.7040 - learning_rate: 5.0000e-05\n",
            "Epoch 65/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7041 - rmse: 38.6994 - val_loss: 1498.0712 - val_rmse: 38.7041 - learning_rate: 5.0000e-05\n",
            "Epoch 66/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7213 - rmse: 38.6996 - val_loss: 1498.0531 - val_rmse: 38.7039 - learning_rate: 5.0000e-05\n",
            "Epoch 67/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7440 - rmse: 38.6999 - val_loss: 1498.0411 - val_rmse: 38.7038 - learning_rate: 5.0000e-05\n",
            "Epoch 68/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7001 - rmse: 38.6993 - val_loss: 1498.0330 - val_rmse: 38.7037 - learning_rate: 5.0000e-05\n",
            "Epoch 69/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.7089 - rmse: 38.6995 - val_loss: 1498.0415 - val_rmse: 38.7038 - learning_rate: 5.0000e-05\n",
            "Epoch 70/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6776 - rmse: 38.6990 - val_loss: 1498.0446 - val_rmse: 38.7038 - learning_rate: 5.0000e-05\n",
            "Epoch 71/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5929 - rmse: 38.6980 - val_loss: 1498.0338 - val_rmse: 38.7037 - learning_rate: 5.0000e-05\n",
            "Epoch 72/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6943 - rmse: 38.6993 - val_loss: 1498.0265 - val_rmse: 38.7036 - learning_rate: 5.0000e-05\n",
            "Epoch 73/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6566 - rmse: 38.6988 - val_loss: 1498.0348 - val_rmse: 38.7037 - learning_rate: 5.0000e-05\n",
            "Epoch 74/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6755 - rmse: 38.6990 - val_loss: 1498.0280 - val_rmse: 38.7036 - learning_rate: 5.0000e-05\n",
            "Epoch 75/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.6675 - rmse: 38.6989 - val_loss: 1498.0312 - val_rmse: 38.7036 - learning_rate: 5.0000e-05\n",
            "Epoch 76/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.6561 - rmse: 38.6988 - val_loss: 1498.0355 - val_rmse: 38.7037 - learning_rate: 5.0000e-05\n",
            "Epoch 77/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6428 - rmse: 38.6986 - val_loss: 1498.0299 - val_rmse: 38.7036 - learning_rate: 5.0000e-05\n",
            "Epoch 78/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.6324 - rmse: 38.6985 - val_loss: 1498.0308 - val_rmse: 38.7036 - learning_rate: 5.0000e-05\n",
            "Epoch 79/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1497.6067 - rmse: 38.6981 - val_loss: 1498.0414 - val_rmse: 38.7038 - learning_rate: 5.0000e-05\n",
            "Epoch 80/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6495 - rmse: 38.6987 - val_loss: 1498.0359 - val_rmse: 38.7037 - learning_rate: 5.0000e-05\n",
            "Epoch 81/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.5811 - rmse: 38.6978 - val_loss: 1498.0144 - val_rmse: 38.7034 - learning_rate: 5.0000e-05\n",
            "Epoch 82/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.5858 - rmse: 38.6979 - val_loss: 1498.0189 - val_rmse: 38.7035 - learning_rate: 5.0000e-05\n",
            "Epoch 83/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6272 - rmse: 38.6984 - val_loss: 1498.0167 - val_rmse: 38.7034 - learning_rate: 5.0000e-05\n",
            "Epoch 84/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5970 - rmse: 38.6980 - val_loss: 1498.0156 - val_rmse: 38.7034 - learning_rate: 5.0000e-05\n",
            "Epoch 85/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5839 - rmse: 38.6979 - val_loss: 1498.0031 - val_rmse: 38.7033 - learning_rate: 5.0000e-05\n",
            "Epoch 86/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.6085 - rmse: 38.6982 - val_loss: 1498.0052 - val_rmse: 38.7033 - learning_rate: 5.0000e-05\n",
            "Epoch 87/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5507 - rmse: 38.6974 - val_loss: 1498.0052 - val_rmse: 38.7033 - learning_rate: 5.0000e-05\n",
            "Epoch 88/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5994 - rmse: 38.6980 - val_loss: 1497.9985 - val_rmse: 38.7032 - learning_rate: 5.0000e-05\n",
            "Epoch 89/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5453 - rmse: 38.6973 - val_loss: 1497.9821 - val_rmse: 38.7030 - learning_rate: 5.0000e-05\n",
            "Epoch 90/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.5914 - rmse: 38.6979 - val_loss: 1497.9985 - val_rmse: 38.7032 - learning_rate: 5.0000e-05\n",
            "Epoch 91/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.5924 - rmse: 38.6979 - val_loss: 1497.9922 - val_rmse: 38.7031 - learning_rate: 5.0000e-05\n",
            "Epoch 92/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6005 - rmse: 38.6981 - val_loss: 1497.9897 - val_rmse: 38.7031 - learning_rate: 5.0000e-05\n",
            "Epoch 93/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5836 - rmse: 38.6978 - val_loss: 1498.0048 - val_rmse: 38.7033 - learning_rate: 5.0000e-05\n",
            "Epoch 94/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6068 - rmse: 38.6982 - val_loss: 1497.9912 - val_rmse: 38.7031 - learning_rate: 5.0000e-05\n",
            "Epoch 95/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5540 - rmse: 38.6975 - val_loss: 1497.9872 - val_rmse: 38.7031 - learning_rate: 5.0000e-05\n",
            "Epoch 96/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.5208 - rmse: 38.6970 - val_loss: 1497.9731 - val_rmse: 38.7029 - learning_rate: 5.0000e-05\n",
            "Epoch 97/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.5569 - rmse: 38.6975 - val_loss: 1497.9751 - val_rmse: 38.7029 - learning_rate: 5.0000e-05\n",
            "Epoch 98/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5216 - rmse: 38.6971 - val_loss: 1497.9886 - val_rmse: 38.7031 - learning_rate: 5.0000e-05\n",
            "Epoch 99/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5312 - rmse: 38.6972 - val_loss: 1497.9838 - val_rmse: 38.7030 - learning_rate: 5.0000e-05\n",
            "Epoch 100/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.5233 - rmse: 38.6971 - val_loss: 1497.9807 - val_rmse: 38.7030 - learning_rate: 5.0000e-05\n",
            "Epoch 101/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.5475 - rmse: 38.6974 - val_loss: 1497.9640 - val_rmse: 38.7028 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 14s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 7ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step\n",
            "Fold 1 → Training set Score: 38.69404 | Validation set Score: 38.70277\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 50s 17ms/step - loss: 1498.6282 - rmse: 38.7113 - val_loss: 1499.4020 - val_rmse: 38.7212 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.6537 - rmse: 38.7115 - val_loss: 1500.3066 - val_rmse: 38.7328 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.6123 - rmse: 38.7109 - val_loss: 1500.0228 - val_rmse: 38.7290 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.6035 - rmse: 38.7107 - val_loss: 1501.3959 - val_rmse: 38.7467 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.3855 - rmse: 38.7078 - val_loss: 1498.8149 - val_rmse: 38.7134 - learning_rate: 3.0000e-04\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.2792 - rmse: 38.7065 - val_loss: 1498.8444 - val_rmse: 38.7138 - learning_rate: 3.0000e-04\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.2789 - rmse: 38.7065 - val_loss: 1499.2837 - val_rmse: 38.7194 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.2474 - rmse: 38.7061 - val_loss: 1498.8301 - val_rmse: 38.7136 - learning_rate: 3.0000e-04\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.1705 - rmse: 38.7051 - val_loss: 1498.6864 - val_rmse: 38.7117 - learning_rate: 9.0000e-05\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.1337 - rmse: 38.7046 - val_loss: 1498.6864 - val_rmse: 38.7117 - learning_rate: 9.0000e-05\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.1217 - rmse: 38.7044 - val_loss: 1498.6647 - val_rmse: 38.7115 - learning_rate: 9.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.1365 - rmse: 38.7046 - val_loss: 1498.6365 - val_rmse: 38.7111 - learning_rate: 9.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0918 - rmse: 38.7040 - val_loss: 1498.6338 - val_rmse: 38.7111 - learning_rate: 9.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0367 - rmse: 38.7033 - val_loss: 1498.8846 - val_rmse: 38.7143 - learning_rate: 9.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0573 - rmse: 38.7036 - val_loss: 1498.7682 - val_rmse: 38.7128 - learning_rate: 9.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0496 - rmse: 38.7035 - val_loss: 1498.6278 - val_rmse: 38.7110 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0350 - rmse: 38.7033 - val_loss: 1498.6624 - val_rmse: 38.7114 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9952 - rmse: 38.7028 - val_loss: 1498.6198 - val_rmse: 38.7109 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9126 - rmse: 38.7018 - val_loss: 1498.6130 - val_rmse: 38.7108 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0388 - rmse: 38.7034 - val_loss: 1498.6182 - val_rmse: 38.7109 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0110 - rmse: 38.7030 - val_loss: 1498.6072 - val_rmse: 38.7107 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9740 - rmse: 38.7026 - val_loss: 1498.6106 - val_rmse: 38.7108 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0592 - rmse: 38.7037 - val_loss: 1498.6512 - val_rmse: 38.7113 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0110 - rmse: 38.7030 - val_loss: 1498.6138 - val_rmse: 38.7108 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9473 - rmse: 38.7022 - val_loss: 1498.6178 - val_rmse: 38.7109 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9951 - rmse: 38.7028 - val_loss: 1498.8094 - val_rmse: 38.7134 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9537 - rmse: 38.7023 - val_loss: 1498.6033 - val_rmse: 38.7107 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9716 - rmse: 38.7025 - val_loss: 1498.6211 - val_rmse: 38.7109 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9410 - rmse: 38.7021 - val_loss: 1498.6107 - val_rmse: 38.7108 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9708 - rmse: 38.7025 - val_loss: 1498.6038 - val_rmse: 38.7107 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9584 - rmse: 38.7024 - val_loss: 1498.6108 - val_rmse: 38.7108 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9745 - rmse: 38.7026 - val_loss: 1498.5959 - val_rmse: 38.7106 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9641 - rmse: 38.7024 - val_loss: 1498.6477 - val_rmse: 38.7113 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9320 - rmse: 38.7020 - val_loss: 1498.6097 - val_rmse: 38.7108 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9719 - rmse: 38.7025 - val_loss: 1498.5929 - val_rmse: 38.7106 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9492 - rmse: 38.7022 - val_loss: 1498.5779 - val_rmse: 38.7104 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9657 - rmse: 38.7025 - val_loss: 1498.6260 - val_rmse: 38.7110 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9495 - rmse: 38.7023 - val_loss: 1498.5874 - val_rmse: 38.7105 - learning_rate: 5.0000e-05\n",
            "Epoch 39/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9189 - rmse: 38.7019 - val_loss: 1498.5875 - val_rmse: 38.7105 - learning_rate: 5.0000e-05\n",
            "Epoch 40/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9043 - rmse: 38.7017 - val_loss: 1498.6270 - val_rmse: 38.7110 - learning_rate: 5.0000e-05\n",
            "Epoch 41/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9293 - rmse: 38.7020 - val_loss: 1498.5789 - val_rmse: 38.7104 - learning_rate: 5.0000e-05\n",
            "Epoch 42/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8979 - rmse: 38.7016 - val_loss: 1498.5793 - val_rmse: 38.7104 - learning_rate: 5.0000e-05\n",
            "Epoch 43/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8997 - rmse: 38.7016 - val_loss: 1498.5878 - val_rmse: 38.7105 - learning_rate: 5.0000e-05\n",
            "Epoch 44/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9219 - rmse: 38.7019 - val_loss: 1498.5808 - val_rmse: 38.7104 - learning_rate: 5.0000e-05\n",
            "Epoch 45/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9326 - rmse: 38.7021 - val_loss: 1498.5732 - val_rmse: 38.7103 - learning_rate: 5.0000e-05\n",
            "Epoch 46/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9423 - rmse: 38.7022 - val_loss: 1498.5742 - val_rmse: 38.7103 - learning_rate: 5.0000e-05\n",
            "Epoch 47/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9381 - rmse: 38.7021 - val_loss: 1498.5983 - val_rmse: 38.7107 - learning_rate: 5.0000e-05\n",
            "Epoch 48/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9681 - rmse: 38.7025 - val_loss: 1498.5675 - val_rmse: 38.7103 - learning_rate: 5.0000e-05\n",
            "Epoch 49/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9598 - rmse: 38.7024 - val_loss: 1498.5640 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 50/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9045 - rmse: 38.7017 - val_loss: 1498.6561 - val_rmse: 38.7114 - learning_rate: 5.0000e-05\n",
            "Epoch 51/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9371 - rmse: 38.7021 - val_loss: 1498.5620 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 52/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8811 - rmse: 38.7014 - val_loss: 1498.5800 - val_rmse: 38.7104 - learning_rate: 5.0000e-05\n",
            "Epoch 53/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9174 - rmse: 38.7019 - val_loss: 1498.5802 - val_rmse: 38.7104 - learning_rate: 5.0000e-05\n",
            "Epoch 54/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9147 - rmse: 38.7018 - val_loss: 1498.5702 - val_rmse: 38.7103 - learning_rate: 5.0000e-05\n",
            "Epoch 55/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8160 - rmse: 38.7006 - val_loss: 1498.5732 - val_rmse: 38.7103 - learning_rate: 5.0000e-05\n",
            "Epoch 56/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8535 - rmse: 38.7010 - val_loss: 1498.5585 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 57/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8761 - rmse: 38.7013 - val_loss: 1498.5763 - val_rmse: 38.7104 - learning_rate: 5.0000e-05\n",
            "Epoch 58/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8358 - rmse: 38.7008 - val_loss: 1498.5638 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 59/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8561 - rmse: 38.7011 - val_loss: 1498.5543 - val_rmse: 38.7101 - learning_rate: 5.0000e-05\n",
            "Epoch 60/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9098 - rmse: 38.7018 - val_loss: 1498.5629 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 61/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8567 - rmse: 38.7011 - val_loss: 1498.5619 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 62/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8774 - rmse: 38.7014 - val_loss: 1498.5585 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 63/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8268 - rmse: 38.7007 - val_loss: 1498.5625 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 64/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8715 - rmse: 38.7013 - val_loss: 1498.5570 - val_rmse: 38.7101 - learning_rate: 5.0000e-05\n",
            "Epoch 65/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8718 - rmse: 38.7013 - val_loss: 1498.6173 - val_rmse: 38.7109 - learning_rate: 5.0000e-05\n",
            "Epoch 66/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8632 - rmse: 38.7012 - val_loss: 1498.5347 - val_rmse: 38.7099 - learning_rate: 5.0000e-05\n",
            "Epoch 67/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8148 - rmse: 38.7006 - val_loss: 1498.5374 - val_rmse: 38.7099 - learning_rate: 5.0000e-05\n",
            "Epoch 68/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8397 - rmse: 38.7009 - val_loss: 1498.5474 - val_rmse: 38.7100 - learning_rate: 5.0000e-05\n",
            "Epoch 69/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8397 - rmse: 38.7009 - val_loss: 1498.5641 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 70/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7457 - rmse: 38.6997 - val_loss: 1498.5421 - val_rmse: 38.7100 - learning_rate: 5.0000e-05\n",
            "Epoch 71/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8511 - rmse: 38.7010 - val_loss: 1498.5391 - val_rmse: 38.7099 - learning_rate: 5.0000e-05\n",
            "Epoch 72/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8368 - rmse: 38.7008 - val_loss: 1498.5995 - val_rmse: 38.7107 - learning_rate: 5.0000e-05\n",
            "Epoch 73/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7944 - rmse: 38.7003 - val_loss: 1498.5361 - val_rmse: 38.7099 - learning_rate: 5.0000e-05\n",
            "Epoch 74/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7889 - rmse: 38.7002 - val_loss: 1498.5851 - val_rmse: 38.7105 - learning_rate: 5.0000e-05\n",
            "Epoch 75/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7955 - rmse: 38.7003 - val_loss: 1498.5387 - val_rmse: 38.7099 - learning_rate: 5.0000e-05\n",
            "Epoch 76/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8447 - rmse: 38.7009 - val_loss: 1498.5432 - val_rmse: 38.7100 - learning_rate: 5.0000e-05\n",
            "Epoch 77/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8759 - rmse: 38.7013 - val_loss: 1498.5320 - val_rmse: 38.7098 - learning_rate: 5.0000e-05\n",
            "Epoch 78/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8562 - rmse: 38.7011 - val_loss: 1498.5509 - val_rmse: 38.7101 - learning_rate: 5.0000e-05\n",
            "Epoch 79/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8320 - rmse: 38.7008 - val_loss: 1498.5289 - val_rmse: 38.7098 - learning_rate: 5.0000e-05\n",
            "Epoch 80/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8203 - rmse: 38.7006 - val_loss: 1498.5574 - val_rmse: 38.7102 - learning_rate: 5.0000e-05\n",
            "Epoch 81/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7875 - rmse: 38.7002 - val_loss: 1498.5286 - val_rmse: 38.7098 - learning_rate: 5.0000e-05\n",
            "Epoch 82/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8049 - rmse: 38.7004 - val_loss: 1498.5701 - val_rmse: 38.7103 - learning_rate: 5.0000e-05\n",
            "Epoch 83/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8422 - rmse: 38.7009 - val_loss: 1498.5326 - val_rmse: 38.7098 - learning_rate: 5.0000e-05\n",
            "Epoch 84/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8645 - rmse: 38.7012 - val_loss: 1498.5526 - val_rmse: 38.7101 - learning_rate: 5.0000e-05\n",
            "Epoch 85/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8679 - rmse: 38.7012 - val_loss: 1498.5925 - val_rmse: 38.7106 - learning_rate: 5.0000e-05\n",
            "Epoch 86/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8036 - rmse: 38.7004 - val_loss: 1498.5757 - val_rmse: 38.7104 - learning_rate: 5.0000e-05\n",
            "Epoch 87/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8073 - rmse: 38.7005 - val_loss: 1498.5310 - val_rmse: 38.7098 - learning_rate: 5.0000e-05\n",
            "Epoch 88/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8517 - rmse: 38.7011 - val_loss: 1498.5410 - val_rmse: 38.7099 - learning_rate: 5.0000e-05\n",
            "Epoch 89/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7836 - rmse: 38.7002 - val_loss: 1498.5380 - val_rmse: 38.7099 - learning_rate: 5.0000e-05\n",
            "Epoch 90/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8362 - rmse: 38.7008 - val_loss: 1498.5400 - val_rmse: 38.7099 - learning_rate: 5.0000e-05\n",
            "Epoch 91/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8152 - rmse: 38.7006 - val_loss: 1498.5289 - val_rmse: 38.7098 - learning_rate: 5.0000e-05\n",
            "Epoch 92/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7888 - rmse: 38.7002 - val_loss: 1498.5193 - val_rmse: 38.7097 - learning_rate: 5.0000e-05\n",
            "Epoch 93/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7659 - rmse: 38.6999 - val_loss: 1498.5277 - val_rmse: 38.7098 - learning_rate: 5.0000e-05\n",
            "Epoch 94/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7992 - rmse: 38.7004 - val_loss: 1498.5146 - val_rmse: 38.7096 - learning_rate: 5.0000e-05\n",
            "Epoch 95/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7253 - rmse: 38.6994 - val_loss: 1498.5143 - val_rmse: 38.7096 - learning_rate: 5.0000e-05\n",
            "Epoch 96/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7411 - rmse: 38.6996 - val_loss: 1498.5316 - val_rmse: 38.7098 - learning_rate: 5.0000e-05\n",
            "Epoch 97/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7543 - rmse: 38.6998 - val_loss: 1498.5143 - val_rmse: 38.7096 - learning_rate: 5.0000e-05\n",
            "Epoch 98/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7946 - rmse: 38.7003 - val_loss: 1498.5209 - val_rmse: 38.7097 - learning_rate: 5.0000e-05\n",
            "Epoch 99/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.8137 - rmse: 38.7006 - val_loss: 1498.5164 - val_rmse: 38.7096 - learning_rate: 5.0000e-05\n",
            "Epoch 100/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7531 - rmse: 38.6998 - val_loss: 1498.5377 - val_rmse: 38.7099 - learning_rate: 5.0000e-05\n",
            "Epoch 101/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7585 - rmse: 38.6999 - val_loss: 1498.5778 - val_rmse: 38.7104 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 14s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 2 → Training set Score: 38.68693 | Validation set Score: 38.70961\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 49s 17ms/step - loss: 1497.5631 - rmse: 38.6973 - val_loss: 1496.2278 - val_rmse: 38.6799 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.6230 - rmse: 38.6979 - val_loss: 1501.3472 - val_rmse: 38.7460 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5643 - rmse: 38.6971 - val_loss: 1496.4324 - val_rmse: 38.6824 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.6182 - rmse: 38.6977 - val_loss: 1496.1653 - val_rmse: 38.6788 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5575 - rmse: 38.6968 - val_loss: 1502.4606 - val_rmse: 38.7601 - learning_rate: 0.0010\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.4366 - rmse: 38.6952 - val_loss: 1496.4911 - val_rmse: 38.6829 - learning_rate: 0.0010\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.4315 - rmse: 38.6951 - val_loss: 1500.0212 - val_rmse: 38.7285 - learning_rate: 0.0010\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.3584 - rmse: 38.6941 - val_loss: 1496.2148 - val_rmse: 38.6793 - learning_rate: 3.0000e-04\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.1617 - rmse: 38.6916 - val_loss: 1496.4015 - val_rmse: 38.6818 - learning_rate: 3.0000e-04\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.1621 - rmse: 38.6916 - val_loss: 1496.5363 - val_rmse: 38.6835 - learning_rate: 3.0000e-04\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.1124 - rmse: 38.6910 - val_loss: 1496.0176 - val_rmse: 38.6768 - learning_rate: 9.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.0643 - rmse: 38.6903 - val_loss: 1496.1462 - val_rmse: 38.6785 - learning_rate: 9.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9951 - rmse: 38.6894 - val_loss: 1496.0552 - val_rmse: 38.6773 - learning_rate: 9.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.0349 - rmse: 38.6900 - val_loss: 1496.0083 - val_rmse: 38.6767 - learning_rate: 9.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9929 - rmse: 38.6894 - val_loss: 1496.0188 - val_rmse: 38.6768 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9889 - rmse: 38.6894 - val_loss: 1496.0061 - val_rmse: 38.6767 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9459 - rmse: 38.6888 - val_loss: 1496.0308 - val_rmse: 38.6770 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9966 - rmse: 38.6895 - val_loss: 1496.0251 - val_rmse: 38.6769 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.0627 - rmse: 38.6903 - val_loss: 1496.0226 - val_rmse: 38.6769 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9619 - rmse: 38.6890 - val_loss: 1496.0038 - val_rmse: 38.6767 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9186 - rmse: 38.6885 - val_loss: 1496.0009 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9167 - rmse: 38.6885 - val_loss: 1496.0149 - val_rmse: 38.6768 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9714 - rmse: 38.6892 - val_loss: 1496.0240 - val_rmse: 38.6770 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9286 - rmse: 38.6886 - val_loss: 1496.0260 - val_rmse: 38.6770 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9233 - rmse: 38.6886 - val_loss: 1496.0006 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8771 - rmse: 38.6880 - val_loss: 1495.9972 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8837 - rmse: 38.6880 - val_loss: 1496.0117 - val_rmse: 38.6768 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9517 - rmse: 38.6889 - val_loss: 1495.9967 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.9331 - rmse: 38.6887 - val_loss: 1495.9917 - val_rmse: 38.6765 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.9064 - rmse: 38.6884 - val_loss: 1496.0383 - val_rmse: 38.6771 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9193 - rmse: 38.6885 - val_loss: 1495.9934 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9594 - rmse: 38.6891 - val_loss: 1496.0692 - val_rmse: 38.6775 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9121 - rmse: 38.6884 - val_loss: 1495.9921 - val_rmse: 38.6765 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9191 - rmse: 38.6885 - val_loss: 1496.0107 - val_rmse: 38.6768 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8740 - rmse: 38.6880 - val_loss: 1496.0044 - val_rmse: 38.6767 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8788 - rmse: 38.6880 - val_loss: 1495.9775 - val_rmse: 38.6764 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8802 - rmse: 38.6880 - val_loss: 1495.9910 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9005 - rmse: 38.6883 - val_loss: 1495.9996 - val_rmse: 38.6767 - learning_rate: 5.0000e-05\n",
            "Epoch 39/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8651 - rmse: 38.6878 - val_loss: 1496.0250 - val_rmse: 38.6770 - learning_rate: 5.0000e-05\n",
            "Epoch 40/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8568 - rmse: 38.6877 - val_loss: 1496.0356 - val_rmse: 38.6771 - learning_rate: 5.0000e-05\n",
            "Epoch 41/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8645 - rmse: 38.6878 - val_loss: 1495.9946 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 42/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8517 - rmse: 38.6877 - val_loss: 1496.0024 - val_rmse: 38.6767 - learning_rate: 5.0000e-05\n",
            "Epoch 43/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8378 - rmse: 38.6875 - val_loss: 1496.1198 - val_rmse: 38.6782 - learning_rate: 5.0000e-05\n",
            "Epoch 44/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8177 - rmse: 38.6873 - val_loss: 1496.0371 - val_rmse: 38.6772 - learning_rate: 5.0000e-05\n",
            "Epoch 45/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7994 - rmse: 38.6870 - val_loss: 1495.9709 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 46/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7803 - rmse: 38.6868 - val_loss: 1496.0048 - val_rmse: 38.6768 - learning_rate: 5.0000e-05\n",
            "Epoch 47/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8691 - rmse: 38.6879 - val_loss: 1496.0145 - val_rmse: 38.6769 - learning_rate: 5.0000e-05\n",
            "Epoch 48/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8173 - rmse: 38.6873 - val_loss: 1495.9741 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 49/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8271 - rmse: 38.6874 - val_loss: 1495.9956 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 50/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8517 - rmse: 38.6877 - val_loss: 1495.9642 - val_rmse: 38.6762 - learning_rate: 5.0000e-05\n",
            "Epoch 51/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8053 - rmse: 38.6871 - val_loss: 1495.9786 - val_rmse: 38.6764 - learning_rate: 5.0000e-05\n",
            "Epoch 52/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7462 - rmse: 38.6863 - val_loss: 1495.9805 - val_rmse: 38.6764 - learning_rate: 5.0000e-05\n",
            "Epoch 53/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8478 - rmse: 38.6876 - val_loss: 1496.0479 - val_rmse: 38.6773 - learning_rate: 5.0000e-05\n",
            "Epoch 54/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8324 - rmse: 38.6874 - val_loss: 1495.9698 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 55/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8073 - rmse: 38.6871 - val_loss: 1495.9750 - val_rmse: 38.6764 - learning_rate: 5.0000e-05\n",
            "Epoch 56/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7913 - rmse: 38.6869 - val_loss: 1495.9875 - val_rmse: 38.6765 - learning_rate: 5.0000e-05\n",
            "Epoch 57/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 21s 11ms/step - loss: 1496.7764 - rmse: 38.6867 - val_loss: 1495.9893 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 58/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7854 - rmse: 38.6869 - val_loss: 1496.0156 - val_rmse: 38.6769 - learning_rate: 5.0000e-05\n",
            "Epoch 59/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8069 - rmse: 38.6871 - val_loss: 1495.9722 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 60/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8694 - rmse: 38.6879 - val_loss: 1495.9719 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 61/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7678 - rmse: 38.6866 - val_loss: 1495.9888 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 62/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7926 - rmse: 38.6869 - val_loss: 1495.9613 - val_rmse: 38.6762 - learning_rate: 5.0000e-05\n",
            "Epoch 63/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8030 - rmse: 38.6871 - val_loss: 1495.9795 - val_rmse: 38.6764 - learning_rate: 5.0000e-05\n",
            "Epoch 64/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7751 - rmse: 38.6867 - val_loss: 1495.9950 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 65/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8540 - rmse: 38.6878 - val_loss: 1495.9879 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 66/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8524 - rmse: 38.6877 - val_loss: 1495.9785 - val_rmse: 38.6764 - learning_rate: 5.0000e-05\n",
            "Epoch 67/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7501 - rmse: 38.6864 - val_loss: 1495.9915 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 68/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7361 - rmse: 38.6862 - val_loss: 1495.9708 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 69/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8011 - rmse: 38.6871 - val_loss: 1496.0331 - val_rmse: 38.6771 - learning_rate: 5.0000e-05\n",
            "Epoch 70/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7390 - rmse: 38.6863 - val_loss: 1495.9694 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 71/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7130 - rmse: 38.6859 - val_loss: 1495.9707 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 72/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7177 - rmse: 38.6860 - val_loss: 1496.0212 - val_rmse: 38.6770 - learning_rate: 5.0000e-05\n",
            "Epoch 73/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7081 - rmse: 38.6859 - val_loss: 1495.9854 - val_rmse: 38.6765 - learning_rate: 5.0000e-05\n",
            "Epoch 74/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7395 - rmse: 38.6863 - val_loss: 1495.9796 - val_rmse: 38.6765 - learning_rate: 5.0000e-05\n",
            "Epoch 75/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7035 - rmse: 38.6858 - val_loss: 1495.9698 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 76/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.6941 - rmse: 38.6857 - val_loss: 1495.9784 - val_rmse: 38.6764 - learning_rate: 5.0000e-05\n",
            "Epoch 77/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7041 - rmse: 38.6858 - val_loss: 1495.9775 - val_rmse: 38.6764 - learning_rate: 5.0000e-05\n",
            "Epoch 78/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.6970 - rmse: 38.6857 - val_loss: 1496.0129 - val_rmse: 38.6769 - learning_rate: 5.0000e-05\n",
            "Epoch 79/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7697 - rmse: 38.6867 - val_loss: 1495.9905 - val_rmse: 38.6766 - learning_rate: 5.0000e-05\n",
            "Epoch 80/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7524 - rmse: 38.6865 - val_loss: 1495.9678 - val_rmse: 38.6763 - learning_rate: 5.0000e-05\n",
            "Epoch 81/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7678 - rmse: 38.6867 - val_loss: 1495.9761 - val_rmse: 38.6764 - learning_rate: 5.0000e-05\n",
            "Epoch 82/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7740 - rmse: 38.6867 - val_loss: 1495.9833 - val_rmse: 38.6765 - learning_rate: 5.0000e-05\n",
            "Epoch 83/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7307 - rmse: 38.6862 - val_loss: 1495.9603 - val_rmse: 38.6762 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 3 → Training set Score: 38.68394 | Validation set Score: 38.67621\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 50s 17ms/step - loss: 1496.4606 - rmse: 38.6826 - val_loss: 1510.9132 - val_rmse: 38.8689 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.5800 - rmse: 38.6841 - val_loss: 1498.7349 - val_rmse: 38.7119 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.5159 - rmse: 38.6832 - val_loss: 1498.0460 - val_rmse: 38.7029 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.5721 - rmse: 38.6838 - val_loss: 1498.0089 - val_rmse: 38.7023 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.5610 - rmse: 38.6836 - val_loss: 1499.1660 - val_rmse: 38.7172 - learning_rate: 0.0010\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.5350 - rmse: 38.6832 - val_loss: 1497.8494 - val_rmse: 38.7002 - learning_rate: 0.0010\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.4816 - rmse: 38.6825 - val_loss: 1498.0558 - val_rmse: 38.7028 - learning_rate: 0.0010\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.3892 - rmse: 38.6812 - val_loss: 1498.5820 - val_rmse: 38.7095 - learning_rate: 0.0010\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.4457 - rmse: 38.6819 - val_loss: 1498.7958 - val_rmse: 38.7122 - learning_rate: 0.0010\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.3120 - rmse: 38.6801 - val_loss: 1497.6864 - val_rmse: 38.6979 - learning_rate: 3.0000e-04\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.0907 - rmse: 38.6773 - val_loss: 1498.6240 - val_rmse: 38.7101 - learning_rate: 3.0000e-04\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0557 - rmse: 38.6769 - val_loss: 1497.7780 - val_rmse: 38.6991 - learning_rate: 3.0000e-04\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.1289 - rmse: 38.6778 - val_loss: 1497.8802 - val_rmse: 38.7005 - learning_rate: 3.0000e-04\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.9912 - rmse: 38.6761 - val_loss: 1497.6694 - val_rmse: 38.6978 - learning_rate: 9.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.9537 - rmse: 38.6756 - val_loss: 1497.7094 - val_rmse: 38.6983 - learning_rate: 9.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.9017 - rmse: 38.6749 - val_loss: 1497.6787 - val_rmse: 38.6979 - learning_rate: 9.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8591 - rmse: 38.6744 - val_loss: 1497.6720 - val_rmse: 38.6978 - learning_rate: 9.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8873 - rmse: 38.6747 - val_loss: 1497.6674 - val_rmse: 38.6978 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8483 - rmse: 38.6742 - val_loss: 1497.6680 - val_rmse: 38.6978 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.8085 - rmse: 38.6737 - val_loss: 1497.6708 - val_rmse: 38.6978 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8872 - rmse: 38.6747 - val_loss: 1497.6912 - val_rmse: 38.6981 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8812 - rmse: 38.6747 - val_loss: 1497.6831 - val_rmse: 38.6980 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8118 - rmse: 38.6738 - val_loss: 1497.6774 - val_rmse: 38.6979 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8583 - rmse: 38.6744 - val_loss: 1497.6764 - val_rmse: 38.6979 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.7803 - rmse: 38.6734 - val_loss: 1497.6715 - val_rmse: 38.6978 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.8215 - rmse: 38.6739 - val_loss: 1497.6862 - val_rmse: 38.6980 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7831 - rmse: 38.6734 - val_loss: 1497.6968 - val_rmse: 38.6982 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8225 - rmse: 38.6739 - val_loss: 1497.6792 - val_rmse: 38.6979 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8767 - rmse: 38.6746 - val_loss: 1497.6738 - val_rmse: 38.6979 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7692 - rmse: 38.6732 - val_loss: 1497.6973 - val_rmse: 38.6982 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.7839 - rmse: 38.6734 - val_loss: 1497.6855 - val_rmse: 38.6980 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7456 - rmse: 38.6730 - val_loss: 1497.6846 - val_rmse: 38.6980 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7827 - rmse: 38.6734 - val_loss: 1497.7122 - val_rmse: 38.6984 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7651 - rmse: 38.6732 - val_loss: 1497.6935 - val_rmse: 38.6981 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.7399 - rmse: 38.6729 - val_loss: 1497.6979 - val_rmse: 38.6982 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8479 - rmse: 38.6743 - val_loss: 1497.6735 - val_rmse: 38.6979 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7842 - rmse: 38.6735 - val_loss: 1497.6780 - val_rmse: 38.6979 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.7001 - rmse: 38.6724 - val_loss: 1497.6805 - val_rmse: 38.6980 - learning_rate: 5.0000e-05\n",
            "Epoch 39/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7119 - rmse: 38.6725 - val_loss: 1497.6648 - val_rmse: 38.6978 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 4 → Training set Score: 38.67825 | Validation set Score: 38.69775\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 49s 17ms/step - loss: 1497.5627 - rmse: 38.6964 - val_loss: 1498.6555 - val_rmse: 38.7104 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7026 - rmse: 38.6981 - val_loss: 1501.0322 - val_rmse: 38.7411 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.7260 - rmse: 38.6984 - val_loss: 1498.9034 - val_rmse: 38.7135 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.6461 - rmse: 38.6973 - val_loss: 1499.1605 - val_rmse: 38.7168 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.4353 - rmse: 38.6945 - val_loss: 1498.5459 - val_rmse: 38.7089 - learning_rate: 3.0000e-04\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2859 - rmse: 38.6926 - val_loss: 1498.5554 - val_rmse: 38.7090 - learning_rate: 3.0000e-04\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2454 - rmse: 38.6921 - val_loss: 1498.5151 - val_rmse: 38.7085 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2565 - rmse: 38.6922 - val_loss: 1498.6708 - val_rmse: 38.7105 - learning_rate: 3.0000e-04\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.1691 - rmse: 38.6911 - val_loss: 1498.5507 - val_rmse: 38.7090 - learning_rate: 3.0000e-04\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.0629 - rmse: 38.6898 - val_loss: 1498.7172 - val_rmse: 38.7111 - learning_rate: 3.0000e-04\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.0453 - rmse: 38.6895 - val_loss: 1498.4407 - val_rmse: 38.7076 - learning_rate: 9.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.0603 - rmse: 38.6897 - val_loss: 1498.4507 - val_rmse: 38.7077 - learning_rate: 9.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.0863 - rmse: 38.6901 - val_loss: 1498.4600 - val_rmse: 38.7078 - learning_rate: 9.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.9491 - rmse: 38.6883 - val_loss: 1498.4437 - val_rmse: 38.7076 - learning_rate: 9.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.0325 - rmse: 38.6894 - val_loss: 1498.4369 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9695 - rmse: 38.6886 - val_loss: 1498.4269 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.9631 - rmse: 38.6885 - val_loss: 1498.4263 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.9504 - rmse: 38.6883 - val_loss: 1498.4310 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.9833 - rmse: 38.6888 - val_loss: 1498.4309 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.9911 - rmse: 38.6889 - val_loss: 1498.4457 - val_rmse: 38.7077 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8969 - rmse: 38.6876 - val_loss: 1498.4333 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.0259 - rmse: 38.6893 - val_loss: 1498.4479 - val_rmse: 38.7077 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.9012 - rmse: 38.6877 - val_loss: 1498.4321 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8827 - rmse: 38.6875 - val_loss: 1498.4188 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8630 - rmse: 38.6872 - val_loss: 1498.4172 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9373 - rmse: 38.6882 - val_loss: 1498.4270 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8834 - rmse: 38.6875 - val_loss: 1498.4240 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8157 - rmse: 38.6866 - val_loss: 1498.4430 - val_rmse: 38.7076 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9032 - rmse: 38.6877 - val_loss: 1498.4149 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8450 - rmse: 38.6870 - val_loss: 1498.4276 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9482 - rmse: 38.6883 - val_loss: 1498.4362 - val_rmse: 38.7076 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8397 - rmse: 38.6869 - val_loss: 1498.4255 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9075 - rmse: 38.6878 - val_loss: 1498.4106 - val_rmse: 38.7072 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 21s 11ms/step - loss: 1496.8109 - rmse: 38.6866 - val_loss: 1498.4172 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.9246 - rmse: 38.6880 - val_loss: 1498.4146 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8789 - rmse: 38.6875 - val_loss: 1498.4159 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8378 - rmse: 38.6869 - val_loss: 1498.4196 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8252 - rmse: 38.6868 - val_loss: 1498.4152 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 39/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7797 - rmse: 38.6862 - val_loss: 1498.4282 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 40/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8340 - rmse: 38.6869 - val_loss: 1498.4172 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 41/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8627 - rmse: 38.6872 - val_loss: 1498.4186 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 42/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8389 - rmse: 38.6869 - val_loss: 1498.4169 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 43/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7881 - rmse: 38.6863 - val_loss: 1498.4254 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 44/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8273 - rmse: 38.6868 - val_loss: 1498.4366 - val_rmse: 38.7076 - learning_rate: 5.0000e-05\n",
            "Epoch 45/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8369 - rmse: 38.6869 - val_loss: 1498.4253 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 46/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8788 - rmse: 38.6875 - val_loss: 1498.4202 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 47/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7911 - rmse: 38.6863 - val_loss: 1498.4312 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 48/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7411 - rmse: 38.6857 - val_loss: 1498.4301 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 49/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7792 - rmse: 38.6862 - val_loss: 1498.4294 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "Epoch 50/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7606 - rmse: 38.6860 - val_loss: 1498.4736 - val_rmse: 38.7081 - learning_rate: 5.0000e-05\n",
            "Epoch 51/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.8580 - rmse: 38.6872 - val_loss: 1498.4329 - val_rmse: 38.7076 - learning_rate: 5.0000e-05\n",
            "Epoch 52/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.7885 - rmse: 38.6863 - val_loss: 1498.4337 - val_rmse: 38.7076 - learning_rate: 5.0000e-05\n",
            "Epoch 53/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8944 - rmse: 38.6877 - val_loss: 1498.4595 - val_rmse: 38.7079 - learning_rate: 5.0000e-05\n",
            "Epoch 54/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.7821 - rmse: 38.6862 - val_loss: 1498.4408 - val_rmse: 38.7077 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 5 → Training set Score: 38.67191 | Validation set Score: 38.70724\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 50s 17ms/step - loss: 1496.2421 - rmse: 38.6792 - val_loss: 1494.0220 - val_rmse: 38.6504 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.3036 - rmse: 38.6799 - val_loss: 1493.9200 - val_rmse: 38.6490 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.2949 - rmse: 38.6797 - val_loss: 1494.6532 - val_rmse: 38.6584 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.2428 - rmse: 38.6789 - val_loss: 1503.1018 - val_rmse: 38.7675 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.2605 - rmse: 38.6791 - val_loss: 1494.2009 - val_rmse: 38.6524 - learning_rate: 0.0010\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0293 - rmse: 38.6761 - val_loss: 1494.1570 - val_rmse: 38.6519 - learning_rate: 3.0000e-04\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.9426 - rmse: 38.6750 - val_loss: 1494.0795 - val_rmse: 38.6509 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.8297 - rmse: 38.6735 - val_loss: 1494.1268 - val_rmse: 38.6515 - learning_rate: 3.0000e-04\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.7684 - rmse: 38.6728 - val_loss: 1494.1101 - val_rmse: 38.6513 - learning_rate: 9.0000e-05\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.7505 - rmse: 38.6725 - val_loss: 1494.0739 - val_rmse: 38.6508 - learning_rate: 9.0000e-05\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6265 - rmse: 38.6709 - val_loss: 1494.0687 - val_rmse: 38.6508 - learning_rate: 9.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6714 - rmse: 38.6715 - val_loss: 1494.0920 - val_rmse: 38.6511 - learning_rate: 5.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6726 - rmse: 38.6715 - val_loss: 1494.0460 - val_rmse: 38.6505 - learning_rate: 5.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6171 - rmse: 38.6708 - val_loss: 1494.0386 - val_rmse: 38.6504 - learning_rate: 5.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6509 - rmse: 38.6713 - val_loss: 1494.0798 - val_rmse: 38.6509 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6689 - rmse: 38.6715 - val_loss: 1494.0540 - val_rmse: 38.6506 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6378 - rmse: 38.6711 - val_loss: 1494.0354 - val_rmse: 38.6504 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5027 - rmse: 38.6693 - val_loss: 1494.0602 - val_rmse: 38.6507 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6666 - rmse: 38.6715 - val_loss: 1494.0635 - val_rmse: 38.6507 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5341 - rmse: 38.6698 - val_loss: 1494.1274 - val_rmse: 38.6516 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5695 - rmse: 38.6702 - val_loss: 1494.0647 - val_rmse: 38.6508 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5668 - rmse: 38.6702 - val_loss: 1494.0422 - val_rmse: 38.6505 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6562 - rmse: 38.6713 - val_loss: 1494.1095 - val_rmse: 38.6513 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4733 - rmse: 38.6690 - val_loss: 1494.0840 - val_rmse: 38.6510 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5663 - rmse: 38.6702 - val_loss: 1494.0657 - val_rmse: 38.6508 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5166 - rmse: 38.6695 - val_loss: 1494.0978 - val_rmse: 38.6512 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5912 - rmse: 38.6705 - val_loss: 1494.0850 - val_rmse: 38.6510 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5105 - rmse: 38.6695 - val_loss: 1494.0774 - val_rmse: 38.6509 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5459 - rmse: 38.6699 - val_loss: 1494.0939 - val_rmse: 38.6512 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5508 - rmse: 38.6700 - val_loss: 1494.1090 - val_rmse: 38.6513 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4546 - rmse: 38.6688 - val_loss: 1494.0952 - val_rmse: 38.6512 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5107 - rmse: 38.6695 - val_loss: 1494.1173 - val_rmse: 38.6515 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4172 - rmse: 38.6683 - val_loss: 1494.0718 - val_rmse: 38.6509 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5077 - rmse: 38.6694 - val_loss: 1494.1226 - val_rmse: 38.6515 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4783 - rmse: 38.6691 - val_loss: 1494.1235 - val_rmse: 38.6516 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4569 - rmse: 38.6688 - val_loss: 1494.1068 - val_rmse: 38.6513 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3962 - rmse: 38.6680 - val_loss: 1494.1526 - val_rmse: 38.6519 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4495 - rmse: 38.6687 - val_loss: 1494.1174 - val_rmse: 38.6515 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 6 → Training set Score: 38.67352 | Validation set Score: 38.65037\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 49s 17ms/step - loss: 1496.1958 - rmse: 38.6783 - val_loss: 1498.5479 - val_rmse: 38.7086 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.1854 - rmse: 38.6781 - val_loss: 1498.7068 - val_rmse: 38.7106 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.2648 - rmse: 38.6790 - val_loss: 1498.2120 - val_rmse: 38.7041 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.2739 - rmse: 38.6791 - val_loss: 1502.2545 - val_rmse: 38.7562 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.2031 - rmse: 38.6781 - val_loss: 1504.4679 - val_rmse: 38.7847 - learning_rate: 0.0010\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.1633 - rmse: 38.6775 - val_loss: 1498.8280 - val_rmse: 38.7119 - learning_rate: 0.0010\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.8154 - rmse: 38.6730 - val_loss: 1498.1506 - val_rmse: 38.7032 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.8236 - rmse: 38.6731 - val_loss: 1498.1885 - val_rmse: 38.7037 - learning_rate: 3.0000e-04\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.7759 - rmse: 38.6725 - val_loss: 1498.2484 - val_rmse: 38.7044 - learning_rate: 3.0000e-04\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.8055 - rmse: 38.6729 - val_loss: 1498.1426 - val_rmse: 38.7031 - learning_rate: 3.0000e-04\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5945 - rmse: 38.6702 - val_loss: 1498.1150 - val_rmse: 38.7027 - learning_rate: 9.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6836 - rmse: 38.6713 - val_loss: 1498.1169 - val_rmse: 38.7028 - learning_rate: 9.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5967 - rmse: 38.6702 - val_loss: 1498.1068 - val_rmse: 38.7026 - learning_rate: 9.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5264 - rmse: 38.6693 - val_loss: 1498.0869 - val_rmse: 38.7024 - learning_rate: 9.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4373 - rmse: 38.6681 - val_loss: 1498.1079 - val_rmse: 38.7027 - learning_rate: 9.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4613 - rmse: 38.6685 - val_loss: 1498.0763 - val_rmse: 38.7022 - learning_rate: 9.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4502 - rmse: 38.6683 - val_loss: 1498.0951 - val_rmse: 38.7025 - learning_rate: 9.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5317 - rmse: 38.6694 - val_loss: 1498.1119 - val_rmse: 38.7027 - learning_rate: 9.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4608 - rmse: 38.6684 - val_loss: 1498.0989 - val_rmse: 38.7026 - learning_rate: 9.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3936 - rmse: 38.6676 - val_loss: 1498.1268 - val_rmse: 38.7029 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3860 - rmse: 38.6675 - val_loss: 1498.0891 - val_rmse: 38.7024 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4729 - rmse: 38.6686 - val_loss: 1498.0886 - val_rmse: 38.7024 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4714 - rmse: 38.6686 - val_loss: 1498.0769 - val_rmse: 38.7023 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.4014 - rmse: 38.6677 - val_loss: 1498.0734 - val_rmse: 38.7022 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.4536 - rmse: 38.6684 - val_loss: 1498.0748 - val_rmse: 38.7023 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3691 - rmse: 38.6673 - val_loss: 1498.0848 - val_rmse: 38.7024 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3406 - rmse: 38.6669 - val_loss: 1498.0894 - val_rmse: 38.7024 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3060 - rmse: 38.6665 - val_loss: 1498.1112 - val_rmse: 38.7027 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3192 - rmse: 38.6667 - val_loss: 1498.0787 - val_rmse: 38.7023 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3339 - rmse: 38.6668 - val_loss: 1498.0859 - val_rmse: 38.7024 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3591 - rmse: 38.6671 - val_loss: 1498.0844 - val_rmse: 38.7024 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1495.3499 - rmse: 38.6670 - val_loss: 1498.0813 - val_rmse: 38.7024 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3087 - rmse: 38.6665 - val_loss: 1498.0906 - val_rmse: 38.7025 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3397 - rmse: 38.6669 - val_loss: 1498.1034 - val_rmse: 38.7026 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3234 - rmse: 38.6667 - val_loss: 1498.0862 - val_rmse: 38.7024 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3304 - rmse: 38.6668 - val_loss: 1498.0886 - val_rmse: 38.7024 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3541 - rmse: 38.6671 - val_loss: 1498.0940 - val_rmse: 38.7025 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3287 - rmse: 38.6668 - val_loss: 1498.1075 - val_rmse: 38.7027 - learning_rate: 5.0000e-05\n",
            "Epoch 39/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3485 - rmse: 38.6670 - val_loss: 1498.1025 - val_rmse: 38.7026 - learning_rate: 5.0000e-05\n",
            "Epoch 40/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3970 - rmse: 38.6677 - val_loss: 1498.1155 - val_rmse: 38.7028 - learning_rate: 5.0000e-05\n",
            "Epoch 41/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.2302 - rmse: 38.6655 - val_loss: 1498.1023 - val_rmse: 38.7026 - learning_rate: 5.0000e-05\n",
            "Epoch 42/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.2839 - rmse: 38.6662 - val_loss: 1498.1025 - val_rmse: 38.7026 - learning_rate: 5.0000e-05\n",
            "Epoch 43/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.1672 - rmse: 38.6647 - val_loss: 1498.0942 - val_rmse: 38.7025 - learning_rate: 5.0000e-05\n",
            "Epoch 44/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1495.3000 - rmse: 38.6664 - val_loss: 1498.0925 - val_rmse: 38.7025 - learning_rate: 5.0000e-05\n",
            "Epoch 45/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2701 - rmse: 38.6660 - val_loss: 1498.1067 - val_rmse: 38.7027 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step\n",
            "Fold 7 → Training set Score: 38.66444 | Validation set Score: 38.70223\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 50s 17ms/step - loss: 1497.8544 - rmse: 38.6994 - val_loss: 1492.8309 - val_rmse: 38.6344 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1498.0374 - rmse: 38.7017 - val_loss: 1494.0421 - val_rmse: 38.6499 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9678 - rmse: 38.7007 - val_loss: 1498.5245 - val_rmse: 38.7078 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.9559 - rmse: 38.7004 - val_loss: 1493.2754 - val_rmse: 38.6399 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.7129 - rmse: 38.6973 - val_loss: 1493.2847 - val_rmse: 38.6400 - learning_rate: 3.0000e-04\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.6501 - rmse: 38.6964 - val_loss: 1493.3805 - val_rmse: 38.6413 - learning_rate: 3.0000e-04\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5836 - rmse: 38.6956 - val_loss: 1493.0128 - val_rmse: 38.6365 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.5038 - rmse: 38.6946 - val_loss: 1492.9385 - val_rmse: 38.6356 - learning_rate: 9.0000e-05\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.4640 - rmse: 38.6941 - val_loss: 1493.0717 - val_rmse: 38.6373 - learning_rate: 9.0000e-05\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.3301 - rmse: 38.6924 - val_loss: 1492.9719 - val_rmse: 38.6360 - learning_rate: 9.0000e-05\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.3333 - rmse: 38.6924 - val_loss: 1492.9008 - val_rmse: 38.6351 - learning_rate: 5.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2260 - rmse: 38.6910 - val_loss: 1492.9714 - val_rmse: 38.6360 - learning_rate: 5.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.3452 - rmse: 38.6926 - val_loss: 1492.8958 - val_rmse: 38.6350 - learning_rate: 5.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.3763 - rmse: 38.6930 - val_loss: 1492.9026 - val_rmse: 38.6351 - learning_rate: 5.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2389 - rmse: 38.6912 - val_loss: 1492.9320 - val_rmse: 38.6355 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.2423 - rmse: 38.6913 - val_loss: 1492.9747 - val_rmse: 38.6361 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.1787 - rmse: 38.6904 - val_loss: 1492.9495 - val_rmse: 38.6357 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2827 - rmse: 38.6918 - val_loss: 1492.9589 - val_rmse: 38.6359 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.2809 - rmse: 38.6918 - val_loss: 1492.9712 - val_rmse: 38.6360 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2228 - rmse: 38.6910 - val_loss: 1492.9143 - val_rmse: 38.6353 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2654 - rmse: 38.6915 - val_loss: 1492.9152 - val_rmse: 38.6353 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.3640 - rmse: 38.6928 - val_loss: 1492.9778 - val_rmse: 38.6361 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.2671 - rmse: 38.6916 - val_loss: 1492.9058 - val_rmse: 38.6352 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.3204 - rmse: 38.6923 - val_loss: 1492.9404 - val_rmse: 38.6356 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2252 - rmse: 38.6911 - val_loss: 1492.9778 - val_rmse: 38.6361 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2104 - rmse: 38.6909 - val_loss: 1492.9816 - val_rmse: 38.6362 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.1625 - rmse: 38.6902 - val_loss: 1492.9366 - val_rmse: 38.6356 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.2313 - rmse: 38.6911 - val_loss: 1492.9353 - val_rmse: 38.6356 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1497.2037 - rmse: 38.6908 - val_loss: 1492.9440 - val_rmse: 38.6357 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.1931 - rmse: 38.6906 - val_loss: 1492.9045 - val_rmse: 38.6352 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.1605 - rmse: 38.6902 - val_loss: 1492.9814 - val_rmse: 38.6362 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.0840 - rmse: 38.6892 - val_loss: 1492.9628 - val_rmse: 38.6359 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.1486 - rmse: 38.6901 - val_loss: 1493.0463 - val_rmse: 38.6370 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1497.1205 - rmse: 38.6897 - val_loss: 1492.9923 - val_rmse: 38.6363 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 7ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 8 → Training set Score: 38.66689 | Validation set Score: 38.63503\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 49s 17ms/step - loss: 1495.6046 - rmse: 38.6700 - val_loss: 1500.7004 - val_rmse: 38.7358 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7773 - rmse: 38.6722 - val_loss: 1501.2689 - val_rmse: 38.7431 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.6902 - rmse: 38.6710 - val_loss: 1501.1736 - val_rmse: 38.7418 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7538 - rmse: 38.6717 - val_loss: 1501.5157 - val_rmse: 38.7461 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.4628 - rmse: 38.6679 - val_loss: 1501.0386 - val_rmse: 38.7400 - learning_rate: 3.0000e-04\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3685 - rmse: 38.6667 - val_loss: 1500.9384 - val_rmse: 38.7387 - learning_rate: 3.0000e-04\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3185 - rmse: 38.6661 - val_loss: 1500.9669 - val_rmse: 38.7391 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.0712 - rmse: 38.6629 - val_loss: 1500.9514 - val_rmse: 38.7389 - learning_rate: 9.0000e-05\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.1039 - rmse: 38.6633 - val_loss: 1500.9512 - val_rmse: 38.7389 - learning_rate: 9.0000e-05\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.0699 - rmse: 38.6629 - val_loss: 1500.9369 - val_rmse: 38.7387 - learning_rate: 9.0000e-05\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.1111 - rmse: 38.6634 - val_loss: 1500.9464 - val_rmse: 38.7388 - learning_rate: 5.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9913 - rmse: 38.6619 - val_loss: 1500.9423 - val_rmse: 38.7388 - learning_rate: 5.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.0579 - rmse: 38.6627 - val_loss: 1500.9354 - val_rmse: 38.7387 - learning_rate: 5.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.0242 - rmse: 38.6623 - val_loss: 1500.9276 - val_rmse: 38.7386 - learning_rate: 5.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9675 - rmse: 38.6616 - val_loss: 1500.9243 - val_rmse: 38.7385 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9761 - rmse: 38.6617 - val_loss: 1500.9218 - val_rmse: 38.7385 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.0742 - rmse: 38.6630 - val_loss: 1500.9191 - val_rmse: 38.7385 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9640 - rmse: 38.6615 - val_loss: 1500.9268 - val_rmse: 38.7386 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.8579 - rmse: 38.6602 - val_loss: 1500.9266 - val_rmse: 38.7386 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9287 - rmse: 38.6611 - val_loss: 1500.9296 - val_rmse: 38.7386 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9016 - rmse: 38.6607 - val_loss: 1500.9293 - val_rmse: 38.7386 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9027 - rmse: 38.6608 - val_loss: 1500.9407 - val_rmse: 38.7388 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9750 - rmse: 38.6617 - val_loss: 1500.9360 - val_rmse: 38.7387 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.8903 - rmse: 38.6606 - val_loss: 1500.9407 - val_rmse: 38.7388 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9170 - rmse: 38.6609 - val_loss: 1500.9469 - val_rmse: 38.7389 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9270 - rmse: 38.6611 - val_loss: 1500.9274 - val_rmse: 38.7386 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9545 - rmse: 38.6614 - val_loss: 1500.9382 - val_rmse: 38.7387 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.7919 - rmse: 38.6593 - val_loss: 1500.9348 - val_rmse: 38.7387 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.8153 - rmse: 38.6596 - val_loss: 1500.9272 - val_rmse: 38.7386 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9133 - rmse: 38.6609 - val_loss: 1500.9417 - val_rmse: 38.7388 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.8064 - rmse: 38.6595 - val_loss: 1500.9297 - val_rmse: 38.7387 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9330 - rmse: 38.6612 - val_loss: 1500.9208 - val_rmse: 38.7385 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9244 - rmse: 38.6611 - val_loss: 1500.9297 - val_rmse: 38.7386 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.7588 - rmse: 38.6589 - val_loss: 1500.9436 - val_rmse: 38.7388 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 10ms/step - loss: 1494.8271 - rmse: 38.6598 - val_loss: 1500.9519 - val_rmse: 38.7389 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.7559 - rmse: 38.6589 - val_loss: 1500.9528 - val_rmse: 38.7390 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.6606 - rmse: 38.6577 - val_loss: 1500.9570 - val_rmse: 38.7390 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.8462 - rmse: 38.6601 - val_loss: 1500.9496 - val_rmse: 38.7389 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step\n",
            "Fold 9 → Training set Score: 38.65638 | Validation set Score: 38.73848\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 49s 17ms/step - loss: 1495.9996 - rmse: 38.6749 - val_loss: 1496.3701 - val_rmse: 38.6796 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.1854 - rmse: 38.6772 - val_loss: 1499.7550 - val_rmse: 38.7233 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.2590 - rmse: 38.6781 - val_loss: 1496.7870 - val_rmse: 38.6849 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.2988 - rmse: 38.6785 - val_loss: 1496.7822 - val_rmse: 38.6847 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.0278 - rmse: 38.6750 - val_loss: 1496.6843 - val_rmse: 38.6835 - learning_rate: 3.0000e-04\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.8409 - rmse: 38.6726 - val_loss: 1496.6758 - val_rmse: 38.6834 - learning_rate: 3.0000e-04\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7759 - rmse: 38.6718 - val_loss: 1496.6078 - val_rmse: 38.6825 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.6230 - rmse: 38.6698 - val_loss: 1496.6929 - val_rmse: 38.6836 - learning_rate: 9.0000e-05\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.5997 - rmse: 38.6695 - val_loss: 1496.7023 - val_rmse: 38.6838 - learning_rate: 9.0000e-05\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.6094 - rmse: 38.6696 - val_loss: 1496.8182 - val_rmse: 38.6853 - learning_rate: 9.0000e-05\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.5354 - rmse: 38.6687 - val_loss: 1496.7089 - val_rmse: 38.6838 - learning_rate: 5.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.6481 - rmse: 38.6701 - val_loss: 1496.7124 - val_rmse: 38.6839 - learning_rate: 5.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.5972 - rmse: 38.6695 - val_loss: 1496.7477 - val_rmse: 38.6843 - learning_rate: 5.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.5125 - rmse: 38.6684 - val_loss: 1496.7045 - val_rmse: 38.6838 - learning_rate: 5.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.4117 - rmse: 38.6671 - val_loss: 1496.6863 - val_rmse: 38.6836 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.4274 - rmse: 38.6673 - val_loss: 1496.7157 - val_rmse: 38.6839 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3646 - rmse: 38.6665 - val_loss: 1496.6992 - val_rmse: 38.6837 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.5085 - rmse: 38.6683 - val_loss: 1496.6818 - val_rmse: 38.6835 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3834 - rmse: 38.6667 - val_loss: 1496.7236 - val_rmse: 38.6841 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3761 - rmse: 38.6666 - val_loss: 1496.6827 - val_rmse: 38.6835 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4668 - rmse: 38.6678 - val_loss: 1496.6851 - val_rmse: 38.6836 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5011 - rmse: 38.6683 - val_loss: 1496.7180 - val_rmse: 38.6840 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3635 - rmse: 38.6665 - val_loss: 1496.7023 - val_rmse: 38.6838 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5176 - rmse: 38.6685 - val_loss: 1496.6963 - val_rmse: 38.6837 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4137 - rmse: 38.6671 - val_loss: 1496.7152 - val_rmse: 38.6840 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4115 - rmse: 38.6671 - val_loss: 1496.7410 - val_rmse: 38.6843 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3063 - rmse: 38.6658 - val_loss: 1496.7145 - val_rmse: 38.6840 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4432 - rmse: 38.6675 - val_loss: 1496.7317 - val_rmse: 38.6842 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 10 → Training set Score: 38.65933 | Validation set Score: 38.68252\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 50s 17ms/step - loss: 1496.8390 - rmse: 38.6855 - val_loss: 1499.2882 - val_rmse: 38.7171 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8370 - rmse: 38.6854 - val_loss: 1498.7527 - val_rmse: 38.7101 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8604 - rmse: 38.6856 - val_loss: 1501.2517 - val_rmse: 38.7423 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8524 - rmse: 38.6854 - val_loss: 1498.6798 - val_rmse: 38.7090 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8647 - rmse: 38.6855 - val_loss: 1498.3475 - val_rmse: 38.7046 - learning_rate: 0.0010\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8447 - rmse: 38.6852 - val_loss: 1499.0900 - val_rmse: 38.7141 - learning_rate: 0.0010\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8243 - rmse: 38.6848 - val_loss: 1498.5947 - val_rmse: 38.7077 - learning_rate: 0.0010\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.8628 - rmse: 38.6853 - val_loss: 1498.7107 - val_rmse: 38.7091 - learning_rate: 0.0010\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.6215 - rmse: 38.6821 - val_loss: 1498.5680 - val_rmse: 38.7073 - learning_rate: 3.0000e-04\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.3175 - rmse: 38.6782 - val_loss: 1498.5007 - val_rmse: 38.7064 - learning_rate: 3.0000e-04\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.3530 - rmse: 38.6786 - val_loss: 1498.5864 - val_rmse: 38.7075 - learning_rate: 3.0000e-04\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.1483 - rmse: 38.6760 - val_loss: 1498.5999 - val_rmse: 38.7077 - learning_rate: 9.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.1051 - rmse: 38.6755 - val_loss: 1498.5679 - val_rmse: 38.7073 - learning_rate: 9.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.1417 - rmse: 38.6759 - val_loss: 1498.5983 - val_rmse: 38.7077 - learning_rate: 9.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0562 - rmse: 38.6748 - val_loss: 1498.5696 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.1267 - rmse: 38.6757 - val_loss: 1498.5605 - val_rmse: 38.7072 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0063 - rmse: 38.6742 - val_loss: 1498.5591 - val_rmse: 38.7072 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0869 - rmse: 38.6752 - val_loss: 1498.5756 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0570 - rmse: 38.6749 - val_loss: 1498.5570 - val_rmse: 38.7072 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0223 - rmse: 38.6744 - val_loss: 1498.5686 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0629 - rmse: 38.6749 - val_loss: 1498.5719 - val_rmse: 38.7074 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0059 - rmse: 38.6742 - val_loss: 1498.5522 - val_rmse: 38.7071 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0127 - rmse: 38.6743 - val_loss: 1498.5665 - val_rmse: 38.7073 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.9526 - rmse: 38.6735 - val_loss: 1498.5619 - val_rmse: 38.7072 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.9338 - rmse: 38.6733 - val_loss: 1498.5447 - val_rmse: 38.7070 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1496.0078 - rmse: 38.6742 - val_loss: 1498.5789 - val_rmse: 38.7075 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 14s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 11 → Training set Score: 38.66004 | Validation set Score: 38.70462\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 49s 17ms/step - loss: 1495.9590 - rmse: 38.6737 - val_loss: 1495.0740 - val_rmse: 38.6623 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.9398 - rmse: 38.6734 - val_loss: 1495.4915 - val_rmse: 38.6676 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1496.0237 - rmse: 38.6744 - val_loss: 1495.5383 - val_rmse: 38.6681 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.9353 - rmse: 38.6732 - val_loss: 1495.5461 - val_rmse: 38.6682 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.7606 - rmse: 38.6709 - val_loss: 1495.3638 - val_rmse: 38.6658 - learning_rate: 3.0000e-04\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5544 - rmse: 38.6683 - val_loss: 1495.3689 - val_rmse: 38.6659 - learning_rate: 3.0000e-04\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4664 - rmse: 38.6672 - val_loss: 1495.3202 - val_rmse: 38.6653 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2346 - rmse: 38.6642 - val_loss: 1495.3120 - val_rmse: 38.6652 - learning_rate: 9.0000e-05\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2590 - rmse: 38.6645 - val_loss: 1495.2808 - val_rmse: 38.6648 - learning_rate: 9.0000e-05\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.3340 - rmse: 38.6655 - val_loss: 1495.2899 - val_rmse: 38.6649 - learning_rate: 9.0000e-05\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2208 - rmse: 38.6640 - val_loss: 1495.3096 - val_rmse: 38.6652 - learning_rate: 5.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3030 - rmse: 38.6651 - val_loss: 1495.3086 - val_rmse: 38.6651 - learning_rate: 5.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2633 - rmse: 38.6646 - val_loss: 1495.3008 - val_rmse: 38.6651 - learning_rate: 5.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2473 - rmse: 38.6644 - val_loss: 1495.2930 - val_rmse: 38.6650 - learning_rate: 5.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2457 - rmse: 38.6643 - val_loss: 1495.2791 - val_rmse: 38.6648 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.1486 - rmse: 38.6631 - val_loss: 1495.2882 - val_rmse: 38.6649 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.1858 - rmse: 38.6636 - val_loss: 1495.2871 - val_rmse: 38.6649 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.1088 - rmse: 38.6626 - val_loss: 1495.2849 - val_rmse: 38.6649 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.1263 - rmse: 38.6628 - val_loss: 1495.2816 - val_rmse: 38.6648 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.1510 - rmse: 38.6631 - val_loss: 1495.2869 - val_rmse: 38.6649 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2028 - rmse: 38.6638 - val_loss: 1495.2828 - val_rmse: 38.6649 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.1624 - rmse: 38.6633 - val_loss: 1495.2981 - val_rmse: 38.6651 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.0540 - rmse: 38.6619 - val_loss: 1495.2671 - val_rmse: 38.6646 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.0197 - rmse: 38.6614 - val_loss: 1495.2955 - val_rmse: 38.6650 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.1149 - rmse: 38.6627 - val_loss: 1495.3029 - val_rmse: 38.6651 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.1144 - rmse: 38.6627 - val_loss: 1495.2866 - val_rmse: 38.6649 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9647 - rmse: 38.6607 - val_loss: 1495.2920 - val_rmse: 38.6650 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.0985 - rmse: 38.6625 - val_loss: 1495.2944 - val_rmse: 38.6650 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.0764 - rmse: 38.6622 - val_loss: 1495.3254 - val_rmse: 38.6654 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.0059 - rmse: 38.6613 - val_loss: 1495.3130 - val_rmse: 38.6653 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9386 - rmse: 38.6604 - val_loss: 1495.2947 - val_rmse: 38.6650 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.0383 - rmse: 38.6617 - val_loss: 1495.3121 - val_rmse: 38.6653 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9417 - rmse: 38.6605 - val_loss: 1495.2991 - val_rmse: 38.6651 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9409 - rmse: 38.6605 - val_loss: 1495.3260 - val_rmse: 38.6654 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.0513 - rmse: 38.6619 - val_loss: 1495.3159 - val_rmse: 38.6653 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9814 - rmse: 38.6610 - val_loss: 1495.3268 - val_rmse: 38.6655 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9264 - rmse: 38.6603 - val_loss: 1495.3213 - val_rmse: 38.6654 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.8864 - rmse: 38.6598 - val_loss: 1495.3000 - val_rmse: 38.6651 - learning_rate: 5.0000e-05\n",
            "Epoch 39/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.8844 - rmse: 38.6597 - val_loss: 1495.3132 - val_rmse: 38.6653 - learning_rate: 5.0000e-05\n",
            "Epoch 40/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.8971 - rmse: 38.6599 - val_loss: 1495.3086 - val_rmse: 38.6652 - learning_rate: 5.0000e-05\n",
            "Epoch 41/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.8887 - rmse: 38.6598 - val_loss: 1495.3153 - val_rmse: 38.6653 - learning_rate: 5.0000e-05\n",
            "Epoch 42/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.8806 - rmse: 38.6597 - val_loss: 1495.3311 - val_rmse: 38.6655 - learning_rate: 5.0000e-05\n",
            "Epoch 43/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1494.9066 - rmse: 38.6600 - val_loss: 1495.3322 - val_rmse: 38.6655 - learning_rate: 5.0000e-05\n",
            "Epoch 44/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9346 - rmse: 38.6604 - val_loss: 1495.3264 - val_rmse: 38.6655 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 12 → Training set Score: 38.65385 | Validation set Score: 38.66466\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 48s 16ms/step - loss: 1495.4170 - rmse: 38.6666 - val_loss: 1488.8348 - val_rmse: 38.5813 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4755 - rmse: 38.6672 - val_loss: 1488.4425 - val_rmse: 38.5761 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 19s 11ms/step - loss: 1495.4377 - rmse: 38.6667 - val_loss: 1488.5437 - val_rmse: 38.5774 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.5244 - rmse: 38.6677 - val_loss: 1489.3507 - val_rmse: 38.5877 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4664 - rmse: 38.6669 - val_loss: 1488.7798 - val_rmse: 38.5803 - learning_rate: 0.0010\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2008 - rmse: 38.6634 - val_loss: 1488.8439 - val_rmse: 38.5811 - learning_rate: 3.0000e-04\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.0989 - rmse: 38.6621 - val_loss: 1488.6008 - val_rmse: 38.5780 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.8323 - rmse: 38.6587 - val_loss: 1488.6122 - val_rmse: 38.5782 - learning_rate: 3.0000e-04\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9497 - rmse: 38.6602 - val_loss: 1488.5333 - val_rmse: 38.5771 - learning_rate: 9.0000e-05\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9049 - rmse: 38.6596 - val_loss: 1488.5360 - val_rmse: 38.5772 - learning_rate: 9.0000e-05\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.7229 - rmse: 38.6573 - val_loss: 1488.5042 - val_rmse: 38.5768 - learning_rate: 9.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.7715 - rmse: 38.6579 - val_loss: 1488.4968 - val_rmse: 38.5767 - learning_rate: 5.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.7919 - rmse: 38.6582 - val_loss: 1488.4985 - val_rmse: 38.5767 - learning_rate: 5.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6885 - rmse: 38.6569 - val_loss: 1488.5198 - val_rmse: 38.5770 - learning_rate: 5.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.7667 - rmse: 38.6578 - val_loss: 1488.4841 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5988 - rmse: 38.6557 - val_loss: 1488.5038 - val_rmse: 38.5768 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.7104 - rmse: 38.6571 - val_loss: 1488.4827 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6776 - rmse: 38.6567 - val_loss: 1488.4817 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6312 - rmse: 38.6561 - val_loss: 1488.4823 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5911 - rmse: 38.6556 - val_loss: 1488.4940 - val_rmse: 38.5767 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6006 - rmse: 38.6557 - val_loss: 1488.4740 - val_rmse: 38.5764 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6038 - rmse: 38.6558 - val_loss: 1488.4863 - val_rmse: 38.5766 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5997 - rmse: 38.6557 - val_loss: 1488.4844 - val_rmse: 38.5766 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6100 - rmse: 38.6558 - val_loss: 1488.4823 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6196 - rmse: 38.6560 - val_loss: 1488.4767 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6478 - rmse: 38.6564 - val_loss: 1488.4598 - val_rmse: 38.5762 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6283 - rmse: 38.6561 - val_loss: 1488.4802 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5360 - rmse: 38.6549 - val_loss: 1488.4645 - val_rmse: 38.5763 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6891 - rmse: 38.6569 - val_loss: 1488.4769 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6023 - rmse: 38.6558 - val_loss: 1488.4669 - val_rmse: 38.5763 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5128 - rmse: 38.6546 - val_loss: 1488.4713 - val_rmse: 38.5764 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4669 - rmse: 38.6540 - val_loss: 1488.4746 - val_rmse: 38.5764 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4052 - rmse: 38.6532 - val_loss: 1488.4950 - val_rmse: 38.5767 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5317 - rmse: 38.6549 - val_loss: 1488.4803 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4528 - rmse: 38.6539 - val_loss: 1488.4821 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4989 - rmse: 38.6544 - val_loss: 1488.4878 - val_rmse: 38.5766 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5275 - rmse: 38.6548 - val_loss: 1488.4880 - val_rmse: 38.5766 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4761 - rmse: 38.6542 - val_loss: 1488.4742 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 39/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4579 - rmse: 38.6539 - val_loss: 1488.4666 - val_rmse: 38.5764 - learning_rate: 5.0000e-05\n",
            "Epoch 40/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4479 - rmse: 38.6538 - val_loss: 1488.4681 - val_rmse: 38.5764 - learning_rate: 5.0000e-05\n",
            "Epoch 41/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5549 - rmse: 38.6552 - val_loss: 1488.4692 - val_rmse: 38.5764 - learning_rate: 5.0000e-05\n",
            "Epoch 42/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2838 - rmse: 38.6517 - val_loss: 1488.4789 - val_rmse: 38.5765 - learning_rate: 5.0000e-05\n",
            "Epoch 43/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4747 - rmse: 38.6541 - val_loss: 1488.5114 - val_rmse: 38.5770 - learning_rate: 5.0000e-05\n",
            "Epoch 44/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3894 - rmse: 38.6530 - val_loss: 1488.5057 - val_rmse: 38.5769 - learning_rate: 5.0000e-05\n",
            "Epoch 45/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6072 - rmse: 38.6559 - val_loss: 1488.4884 - val_rmse: 38.5767 - learning_rate: 5.0000e-05\n",
            "Epoch 46/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3820 - rmse: 38.6530 - val_loss: 1488.4889 - val_rmse: 38.5767 - learning_rate: 5.0000e-05\n",
            "Epoch 47/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3864 - rmse: 38.6530 - val_loss: 1488.4944 - val_rmse: 38.5767 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 13s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 13 → Training set Score: 38.65543 | Validation set Score: 38.57624\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 49s 17ms/step - loss: 1495.1776 - rmse: 38.6632 - val_loss: 1505.5049 - val_rmse: 38.7965 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3636 - rmse: 38.6655 - val_loss: 1496.5464 - val_rmse: 38.6807 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4924 - rmse: 38.6671 - val_loss: 1496.1913 - val_rmse: 38.6761 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4146 - rmse: 38.6660 - val_loss: 1495.8544 - val_rmse: 38.6717 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4847 - rmse: 38.6668 - val_loss: 1496.0713 - val_rmse: 38.6744 - learning_rate: 0.0010\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4142 - rmse: 38.6659 - val_loss: 1496.3116 - val_rmse: 38.6774 - learning_rate: 0.0010\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.4061 - rmse: 38.6657 - val_loss: 1496.2703 - val_rmse: 38.6768 - learning_rate: 0.0010\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.0864 - rmse: 38.6615 - val_loss: 1495.9254 - val_rmse: 38.6724 - learning_rate: 3.0000e-04\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9291 - rmse: 38.6595 - val_loss: 1496.0132 - val_rmse: 38.6735 - learning_rate: 3.0000e-04\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.9458 - rmse: 38.6597 - val_loss: 1496.5037 - val_rmse: 38.6799 - learning_rate: 3.0000e-04\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6847 - rmse: 38.6563 - val_loss: 1495.8230 - val_rmse: 38.6711 - learning_rate: 9.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.6642 - rmse: 38.6561 - val_loss: 1495.8861 - val_rmse: 38.6719 - learning_rate: 9.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5917 - rmse: 38.6551 - val_loss: 1495.8218 - val_rmse: 38.6711 - learning_rate: 9.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5410 - rmse: 38.6545 - val_loss: 1495.7982 - val_rmse: 38.6708 - learning_rate: 9.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5090 - rmse: 38.6541 - val_loss: 1495.7874 - val_rmse: 38.6706 - learning_rate: 9.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5378 - rmse: 38.6545 - val_loss: 1495.8217 - val_rmse: 38.6711 - learning_rate: 9.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5144 - rmse: 38.6542 - val_loss: 1495.8137 - val_rmse: 38.6710 - learning_rate: 9.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4873 - rmse: 38.6538 - val_loss: 1495.7548 - val_rmse: 38.6702 - learning_rate: 9.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4912 - rmse: 38.6539 - val_loss: 1495.8589 - val_rmse: 38.6716 - learning_rate: 9.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4899 - rmse: 38.6539 - val_loss: 1495.7932 - val_rmse: 38.6707 - learning_rate: 9.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4429 - rmse: 38.6533 - val_loss: 1495.8398 - val_rmse: 38.6713 - learning_rate: 9.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2953 - rmse: 38.6514 - val_loss: 1495.7484 - val_rmse: 38.6702 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3857 - rmse: 38.6525 - val_loss: 1495.7821 - val_rmse: 38.6706 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4784 - rmse: 38.6537 - val_loss: 1495.7690 - val_rmse: 38.6704 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3837 - rmse: 38.6525 - val_loss: 1495.7573 - val_rmse: 38.6703 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5596 - rmse: 38.6548 - val_loss: 1495.7787 - val_rmse: 38.6706 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4518 - rmse: 38.6534 - val_loss: 1495.8459 - val_rmse: 38.6714 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4703 - rmse: 38.6536 - val_loss: 1495.7782 - val_rmse: 38.6706 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2946 - rmse: 38.6514 - val_loss: 1495.7900 - val_rmse: 38.6707 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2844 - rmse: 38.6512 - val_loss: 1495.7871 - val_rmse: 38.6707 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3346 - rmse: 38.6519 - val_loss: 1495.7733 - val_rmse: 38.6705 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2959 - rmse: 38.6514 - val_loss: 1495.7737 - val_rmse: 38.6705 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3051 - rmse: 38.6515 - val_loss: 1495.8051 - val_rmse: 38.6709 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 21s 11ms/step - loss: 1494.2007 - rmse: 38.6502 - val_loss: 1495.7969 - val_rmse: 38.6708 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2571 - rmse: 38.6509 - val_loss: 1495.7861 - val_rmse: 38.6707 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2764 - rmse: 38.6512 - val_loss: 1495.8193 - val_rmse: 38.6711 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2618 - rmse: 38.6510 - val_loss: 1495.7815 - val_rmse: 38.6706 - learning_rate: 5.0000e-05\n",
            "Epoch 38/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2908 - rmse: 38.6514 - val_loss: 1495.8009 - val_rmse: 38.6709 - learning_rate: 5.0000e-05\n",
            "Epoch 39/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2211 - rmse: 38.6504 - val_loss: 1495.7660 - val_rmse: 38.6704 - learning_rate: 5.0000e-05\n",
            "Epoch 40/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2314 - rmse: 38.6506 - val_loss: 1495.7723 - val_rmse: 38.6705 - learning_rate: 5.0000e-05\n",
            "Epoch 41/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2045 - rmse: 38.6502 - val_loss: 1495.8151 - val_rmse: 38.6711 - learning_rate: 5.0000e-05\n",
            "Epoch 42/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.1208 - rmse: 38.6492 - val_loss: 1495.7902 - val_rmse: 38.6708 - learning_rate: 5.0000e-05\n",
            "Epoch 43/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3049 - rmse: 38.6515 - val_loss: 1495.7911 - val_rmse: 38.6708 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 14s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 7ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step\n",
            "Fold 14 → Training set Score: 38.64379 | Validation set Score: 38.67016\n",
            "Epoch 1/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 51s 18ms/step - loss: 1494.9807 - rmse: 38.6602 - val_loss: 1498.4550 - val_rmse: 38.7050 - learning_rate: 0.0010\n",
            "Epoch 2/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2080 - rmse: 38.6631 - val_loss: 1495.3766 - val_rmse: 38.6652 - learning_rate: 0.0010\n",
            "Epoch 3/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2831 - rmse: 38.6640 - val_loss: 1495.9810 - val_rmse: 38.6729 - learning_rate: 0.0010\n",
            "Epoch 4/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.2305 - rmse: 38.6632 - val_loss: 1495.7362 - val_rmse: 38.6697 - learning_rate: 0.0010\n",
            "Epoch 5/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.3883 - rmse: 38.6652 - val_loss: 1495.9189 - val_rmse: 38.6720 - learning_rate: 0.0010\n",
            "Epoch 6/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1495.0378 - rmse: 38.6606 - val_loss: 1496.1707 - val_rmse: 38.6752 - learning_rate: 3.0000e-04\n",
            "Epoch 7/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.7544 - rmse: 38.6569 - val_loss: 1495.8834 - val_rmse: 38.6716 - learning_rate: 3.0000e-04\n",
            "Epoch 8/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.7256 - rmse: 38.6566 - val_loss: 1495.9943 - val_rmse: 38.6730 - learning_rate: 3.0000e-04\n",
            "Epoch 9/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5873 - rmse: 38.6548 - val_loss: 1495.6996 - val_rmse: 38.6692 - learning_rate: 9.0000e-05\n",
            "Epoch 10/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5151 - rmse: 38.6539 - val_loss: 1495.8148 - val_rmse: 38.6707 - learning_rate: 9.0000e-05\n",
            "Epoch 11/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5323 - rmse: 38.6541 - val_loss: 1495.6735 - val_rmse: 38.6688 - learning_rate: 9.0000e-05\n",
            "Epoch 12/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5363 - rmse: 38.6541 - val_loss: 1495.6167 - val_rmse: 38.6681 - learning_rate: 5.0000e-05\n",
            "Epoch 13/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.5065 - rmse: 38.6538 - val_loss: 1495.6365 - val_rmse: 38.6684 - learning_rate: 5.0000e-05\n",
            "Epoch 14/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4142 - rmse: 38.6526 - val_loss: 1495.5886 - val_rmse: 38.6678 - learning_rate: 5.0000e-05\n",
            "Epoch 15/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4224 - rmse: 38.6527 - val_loss: 1495.6040 - val_rmse: 38.6680 - learning_rate: 5.0000e-05\n",
            "Epoch 16/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3748 - rmse: 38.6521 - val_loss: 1495.5656 - val_rmse: 38.6675 - learning_rate: 5.0000e-05\n",
            "Epoch 17/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.4574 - rmse: 38.6531 - val_loss: 1495.5680 - val_rmse: 38.6675 - learning_rate: 5.0000e-05\n",
            "Epoch 18/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3312 - rmse: 38.6515 - val_loss: 1495.5924 - val_rmse: 38.6678 - learning_rate: 5.0000e-05\n",
            "Epoch 19/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2664 - rmse: 38.6507 - val_loss: 1495.6046 - val_rmse: 38.6680 - learning_rate: 5.0000e-05\n",
            "Epoch 20/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2335 - rmse: 38.6503 - val_loss: 1495.6157 - val_rmse: 38.6681 - learning_rate: 5.0000e-05\n",
            "Epoch 21/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3135 - rmse: 38.6513 - val_loss: 1495.6038 - val_rmse: 38.6680 - learning_rate: 5.0000e-05\n",
            "Epoch 22/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2766 - rmse: 38.6508 - val_loss: 1495.6085 - val_rmse: 38.6681 - learning_rate: 5.0000e-05\n",
            "Epoch 23/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2814 - rmse: 38.6509 - val_loss: 1495.5782 - val_rmse: 38.6677 - learning_rate: 5.0000e-05\n",
            "Epoch 24/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.3330 - rmse: 38.6515 - val_loss: 1495.6040 - val_rmse: 38.6680 - learning_rate: 5.0000e-05\n",
            "Epoch 25/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.1802 - rmse: 38.6496 - val_loss: 1495.6051 - val_rmse: 38.6680 - learning_rate: 5.0000e-05\n",
            "Epoch 26/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.1564 - rmse: 38.6493 - val_loss: 1495.5952 - val_rmse: 38.6679 - learning_rate: 5.0000e-05\n",
            "Epoch 27/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.0671 - rmse: 38.6481 - val_loss: 1495.6268 - val_rmse: 38.6683 - learning_rate: 5.0000e-05\n",
            "Epoch 28/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.1199 - rmse: 38.6488 - val_loss: 1495.6115 - val_rmse: 38.6681 - learning_rate: 5.0000e-05\n",
            "Epoch 29/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.1956 - rmse: 38.6498 - val_loss: 1495.6166 - val_rmse: 38.6682 - learning_rate: 5.0000e-05\n",
            "Epoch 30/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.0995 - rmse: 38.6485 - val_loss: 1495.6005 - val_rmse: 38.6680 - learning_rate: 5.0000e-05\n",
            "Epoch 31/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.1958 - rmse: 38.6498 - val_loss: 1495.6001 - val_rmse: 38.6680 - learning_rate: 5.0000e-05\n",
            "Epoch 32/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2174 - rmse: 38.6501 - val_loss: 1495.6100 - val_rmse: 38.6681 - learning_rate: 5.0000e-05\n",
            "Epoch 33/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.1517 - rmse: 38.6492 - val_loss: 1495.6378 - val_rmse: 38.6685 - learning_rate: 5.0000e-05\n",
            "Epoch 34/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.0709 - rmse: 38.6482 - val_loss: 1495.5917 - val_rmse: 38.6679 - learning_rate: 5.0000e-05\n",
            "Epoch 35/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2189 - rmse: 38.6501 - val_loss: 1495.6399 - val_rmse: 38.6685 - learning_rate: 5.0000e-05\n",
            "Epoch 36/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.2260 - rmse: 38.6502 - val_loss: 1495.5935 - val_rmse: 38.6679 - learning_rate: 5.0000e-05\n",
            "Epoch 37/101\n",
            "1821/1821 ━━━━━━━━━━━━━━━━━━━━ 20s 11ms/step - loss: 1494.0850 - rmse: 38.6484 - val_loss: 1495.6108 - val_rmse: 38.6681 - learning_rate: 5.0000e-05\n",
            "3641/3641 ━━━━━━━━━━━━━━━━━━━━ 14s 3ms/step\n",
            "261/261 ━━━━━━━━━━━━━━━━━━━━ 2s 6ms/step\n",
            "196/196 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step\n",
            "Fold 15 → Training set Score: 38.64230 | Validation set Score: 38.66748\n",
            "Overall → Training set Score: 38.66607±0.0147821 | Validation set Score: 38.67902±0.0375371\n",
            "\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.2 Second Level of Stacking\n",
        "We take the out-of-folds predictions from the previous models and fed them into simple linear methods like Ridge and Lasso. The alpha parameters have been tuned manually (i.e., trial and error)."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.033525,
          "end_time": "2024-10-26T19:55:20.101705",
          "exception": false,
          "start_time": "2024-10-26T19:55:20.06818",
          "status": "completed"
        },
        "tags": [],
        "id": "Tl19oKm91KTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models2 = {\n",
        "          'Ridge2': Ridge(alpha=1),\n",
        "          'Lasso2': Lasso(alpha=0.0002),\n",
        "          'LGBM_2' : LGBMRegressor(max_depth=2, random_state=CFG.SEED, verbose=-1, device='gpu'),\n",
        "          }\n",
        "\n",
        "TM2 = TrainModels(X=TM.OOF_train, y=data.y, X_test=TM.OOF_test, X_original=None, y_original=None, models=models2)\n",
        "TM2.fit_models()\n",
        "data.submit(sub=TM2.OOF_test.mean(axis=1), desc='lasso_ridge_mean')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T02:51:59.870325Z",
          "iopub.execute_input": "2025-02-03T02:51:59.870552Z",
          "iopub.status.idle": "2025-02-03T02:52:39.627066Z",
          "shell.execute_reply.started": "2025-02-03T02:51:59.870532Z",
          "shell.execute_reply": "2025-02-03T02:52:39.626195Z"
        },
        "papermill": {
          "duration": 1.891352,
          "end_time": "2024-10-26T19:55:22.027553",
          "exception": false,
          "start_time": "2024-10-26T19:55:20.136201",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "AcuYSPyM1KTj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.3 Third Level of Stacking\n",
        "At this stage, the first level and second level out-of-fold predictions are incorporated into the original training data. This is then fed into another level of models to output the final stage of out-of-fold predictions which are finally averaged."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.034722,
          "end_time": "2024-10-26T19:55:22.099225",
          "exception": false,
          "start_time": "2024-10-26T19:55:22.064503",
          "status": "completed"
        },
        "tags": [],
        "id": "0IuFkaqj1KTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([data.X,      TM.OOF_train, TM2.OOF_train], axis=1)\n",
        "test  = pd.concat([data.X_test, TM.OOF_test,  TM2.OOF_test], axis=1)\n",
        "\n",
        "models3 = {\n",
        "          'LGBM_21' : LGBMRegressor(verbose=-1, random_state=CFG.SEED, max_depth=4, device='gpu'),\n",
        "          'CB_3'   : CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features, task_type='GPU'),\n",
        "          'Ridge_3': make_pipeline(TargetEncoder(), Ridge(alpha=5)),\n",
        "          'Lasso_3': make_pipeline(TargetEncoder(), Lasso(alpha=0.001)),\n",
        "          }\n",
        "\n",
        "TM3  = TrainModels(X=train, y=data.y, X_test=test, X_original=None, y_original=None, models=models3)\n",
        "TM3.fit_models()\n",
        "\n",
        "TM3.save_predictions()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T02:52:39.628014Z",
          "iopub.execute_input": "2025-02-03T02:52:39.628303Z",
          "iopub.status.idle": "2025-02-03T03:02:09.91606Z",
          "shell.execute_reply.started": "2025-02-03T02:52:39.628278Z",
          "shell.execute_reply": "2025-02-03T03:02:09.915226Z"
        },
        "papermill": {
          "duration": 423.130115,
          "end_time": "2024-10-26T20:02:25.264332",
          "exception": false,
          "start_time": "2024-10-26T19:55:22.134217",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "7XVP_opO1KTk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.submit(sub=TM3.OOF_test.mean(axis=1), desc='final_layer')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T03:02:09.91696Z",
          "iopub.execute_input": "2025-02-03T03:02:09.917291Z",
          "iopub.status.idle": "2025-02-03T03:02:10.278403Z",
          "shell.execute_reply.started": "2025-02-03T03:02:09.917259Z",
          "shell.execute_reply": "2025-02-03T03:02:10.277614Z"
        },
        "papermill": {
          "duration": 0.06412,
          "end_time": "2024-10-26T20:02:25.427822",
          "exception": false,
          "start_time": "2024-10-26T20:02:25.363702",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "Ui09Yz5y1KTk",
        "outputId": "3d53f3d6-fd20-49b9-d491-de4333641e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission has been made.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "TM3.OOF_test.mean(axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T03:12:48.702102Z",
          "iopub.execute_input": "2025-02-03T03:12:48.702378Z",
          "iopub.status.idle": "2025-02-03T03:12:48.737359Z",
          "shell.execute_reply.started": "2025-02-03T03:12:48.702357Z",
          "shell.execute_reply": "2025-02-03T03:12:48.736785Z"
        },
        "id": "3UwUPsOx1KTl",
        "outputId": "0e138e2c-b662-4c55-ee3e-ffe7a764a949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         81.828801\n",
              "1         81.913722\n",
              "2         88.117746\n",
              "3         78.654571\n",
              "4         80.203378\n",
              "            ...    \n",
              "199995    83.698665\n",
              "199996    79.667135\n",
              "199997    83.700374\n",
              "199998    82.015075\n",
              "199999    72.587452\n",
              "Length: 200000, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81.828801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>81.913722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>88.117746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>78.654571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80.203378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199995</th>\n",
              "      <td>83.698665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199996</th>\n",
              "      <td>79.667135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199997</th>\n",
              "      <td>83.700374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199998</th>\n",
              "      <td>82.015075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199999</th>\n",
              "      <td>72.587452</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200000 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "TM3.save_predictions()"
      ],
      "metadata": {
        "trusted": true,
        "id": "XMtQ8Rf-1KTl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "TM3.OOF_test.mean(axis=1)\n",
        "Data.submission"
      ],
      "metadata": {
        "trusted": true,
        "id": "q-VVElGU1KTm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "b477c9e4-cced-4592-b38d-2b57f5d52937"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "type object 'Data' has no attribute 'submission'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-07a9415cd8e8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTM3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOOF_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: type object 'Data' has no attribute 'submission'"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\",index_col=0)\n",
        "submission[\"Price\"] = TM3.OOF_test.mean(axis=1).values\n",
        "submission.to_csv(\"SUB_01_Final_Layer.csv\")\n",
        "final_layer_output = submission.copy()\n",
        "display(submission.head())\n",
        "submission[\"Price\"] = TM2.OOF_test.mean(axis=1).values\n",
        "submission.to_csv(\"SUB_01_Second_Layer.csv\")\n",
        "display(submission.head())\n",
        "second_layer_output = submission.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "DBNe-yhLnsdv",
        "outputId": "2f1f300f-62fd-40ad-9a41-c5b033b23b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            Price\n",
              "id               \n",
              "300000  81.828801\n",
              "300001  81.913722\n",
              "300002  88.117746\n",
              "300003  78.654571\n",
              "300004  80.203378"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02f4822e-1dcc-49f1-a7ee-903d90122ff8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>300000</th>\n",
              "      <td>81.828801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300001</th>\n",
              "      <td>81.913722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300002</th>\n",
              "      <td>88.117746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300003</th>\n",
              "      <td>78.654571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300004</th>\n",
              "      <td>80.203378</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02f4822e-1dcc-49f1-a7ee-903d90122ff8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-02f4822e-1dcc-49f1-a7ee-903d90122ff8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-02f4822e-1dcc-49f1-a7ee-903d90122ff8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a2b7c61d-9392-46a5-8347-2c04db282244\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a2b7c61d-9392-46a5-8347-2c04db282244')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a2b7c61d-9392-46a5-8347-2c04db282244 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"second_layer_output = submission\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 300000,\n        \"max\": 300004,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          300001,\n          300004,\n          300002\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.597921422819125,\n        \"min\": 78.65457076117863,\n        \"max\": 88.11774644285369,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          81.91372159718601,\n          80.20337807835446,\n          88.11774644285369\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            Price\n",
              "id               \n",
              "300000  81.825387\n",
              "300001  82.018160\n",
              "300002  88.450856\n",
              "300003  78.714707\n",
              "300004  80.377172"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4a861c3-5b47-4f8c-b386-52ff7f2a2416\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>300000</th>\n",
              "      <td>81.825387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300001</th>\n",
              "      <td>82.018160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300002</th>\n",
              "      <td>88.450856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300003</th>\n",
              "      <td>78.714707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300004</th>\n",
              "      <td>80.377172</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4a861c3-5b47-4f8c-b386-52ff7f2a2416')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a4a861c3-5b47-4f8c-b386-52ff7f2a2416 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a4a861c3-5b47-4f8c-b386-52ff7f2a2416');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-249563bf-49e2-4096-aa3c-96faf0813635\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-249563bf-49e2-4096-aa3c-96faf0813635')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-249563bf-49e2-4096-aa3c-96faf0813635 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"second_layer_output = submission\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 300000,\n        \"max\": 300004,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          300001,\n          300004,\n          300002\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.6975237335713826,\n        \"min\": 78.71470705439974,\n        \"max\": 88.45085585587763,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          82.0181600142701,\n          80.37717172847412,\n          88.45085585587763\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(final_layer_output.values, second_layer_output.values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "m-GKbMevoq9K",
        "outputId": "f1fc328c-6fa1-4f30-c6a7-d2b08e02c6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x78a0d8344390>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z4Z1f8r7phKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jReIEHz6wysG"
      },
      "source": [
        "#### **4.6.4 NeuralNetwork v3**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.cat_features,data.num_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyU08ZbXPTEe",
        "outputId": "ea01b98d-d0dd-4902-e811-29c398d04126"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Brand',\n",
              "  'Material',\n",
              "  'Size',\n",
              "  'Compartments',\n",
              "  'Laptop Compartment',\n",
              "  'Waterproof',\n",
              "  'Style',\n",
              "  'Color',\n",
              "  'cheap_flag',\n",
              "  'expansive_flag'],\n",
              " ['Weight Capacity (kg)', 'TE_wc', 'skew_0', 'skew_1'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62af9d57-6501-43c0-99c8-e2b4bde6108c",
        "id": "QfBbW0LUwysH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float64\n",
            " 9   TE_wc                 float64\n",
            " 10  skew_0                float64\n",
            " 11  skew_1                float64\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            "dtypes: float64(4), int64(10)\n",
            "memory usage: 426.6 MB\n"
          ]
        }
      ],
      "source": [
        "data.X.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class exitesqueeze_layer(layers.Layer):\n",
        "    def __init__(self, exite_units,dropout_rate,activation,reg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.exite_units = exite_units\n",
        "        self.activation=activation\n",
        "        self.reg=reg\n",
        "\n",
        "        self.reshaped_0 = layers.Reshape((-1, 1))\n",
        "        self.reshaped_1 = layers.Reshape((-1, ))\n",
        "\n",
        "        self.exite = layers.Dense(self.exite_units, activation=self.activation)\n",
        "        self.squeeze = layers.Dense(1, activation=\"linear\",kernel_regularizer=keras.regularizers.l2(reg))\n",
        "        self.lnorm_00 = layers.LayerNormalization()\n",
        "        self.lnorm_01 = layers.LayerNormalization()\n",
        "        self.drop = layers.Dropout(rate=dropout_rate)\n",
        "        self.attention = layers.Attention()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.reshaped_0(inputs)\n",
        "        x = self.exite(x)\n",
        "        att_out = self.attention([x,x])\n",
        "        att_out = self.lnorm_00(att_out)\n",
        "        x = layers.add([x, att_out])\n",
        "        x = self.squeeze(x)\n",
        "        x = self.reshaped_1(x)\n",
        "\n",
        "        x = layers.multiply([x, inputs])\n",
        "\n",
        "        x = self.lnorm_01(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Remove build warnings\n",
        "    def build(self):\n",
        "        self.built = True"
      ],
      "metadata": {
        "id": "UJ5OQf-rDnO8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512,exite_units=64, last_layer = 1, activation=\"relu\", reg=0.001, dropout_rate=0.33):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(data.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(data.cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    x_0 = layers.Concatenate(axis=-1, name=\"input_concat\")(embs+[x_input_nums])\n",
        "\n",
        "    es_0 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_0_concat\")([x_0,es_0])\n",
        "    x = layers.BatchNormalization(name=\"se_0_bn\")(x)\n",
        "\n",
        "    es_1 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_1_concat\")([x,es_1])\n",
        "    x = layers.BatchNormalization(name=\"se_1_bn\")(x)\n",
        "\n",
        "    es_2 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_2_concat\")([x,es_2])\n",
        "    x = layers.BatchNormalization(name=\"se_2_bn\")(x)\n",
        "\n",
        "    x_0 = layers.Dense(units, name=\"dense_0\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_0\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_0\")(x_0)\n",
        "\n",
        "    x_0 = layers.Dense(int(units/last_layer), name=\"dense_1\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_1\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_1\")(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([x_0,x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "avZAgIvUwysH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model(units=64,exite_units=16, last_layer = 2, activation=\"relu\", reg=0.001, dropout_rate=0.33)\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "cjfFlBj5wysI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d7ebd156-6313-42c6-d1a8-ede566419634"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item (\u001b[38;5;33mGetItem\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_1 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_2 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_3 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_4 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_5 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_6 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_7 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_8 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_9 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m18\u001b[0m │ get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m15\u001b[0m │ get_item_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m8\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m40\u001b[0m │ get_item_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m6\u001b[0m │ get_item_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m6\u001b[0m │ get_item_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m8\u001b[0m │ get_item_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m21\u001b[0m │ get_item_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m4\u001b[0m │ get_item_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m4\u001b[0m │ get_item_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ embedding_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_concat              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)             │                        │                │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ flatten_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ flatten_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ flatten_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ flatten_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ flatten_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ flatten_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ flatten_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
              "│                           │                        │                │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ exitesqueeze_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │            \u001b[38;5;34m139\u001b[0m │ input_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mexitesqueeze_layer\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_0_concat (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ input_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                           │                        │                │ exitesqueeze_layer[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_0_bn                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)             │            \u001b[38;5;34m232\u001b[0m │ se_0_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_0 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │          \u001b[38;5;34m1,920\u001b[0m │ input_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ exitesqueeze_layer_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)             │            \u001b[38;5;34m197\u001b[0m │ se_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mexitesqueeze_layer\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bn_0 (\u001b[38;5;33mBatchNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │            \u001b[38;5;34m256\u001b[0m │ dense_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_1_concat (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m116\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ se_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │                        │                │ exitesqueeze_layer_1[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ do_0 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ bn_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_1_bn                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m116\u001b[0m)            │            \u001b[38;5;34m464\u001b[0m │ se_1_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │          \u001b[38;5;34m2,080\u001b[0m │ do_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ exitesqueeze_layer_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m116\u001b[0m)            │            \u001b[38;5;34m313\u001b[0m │ se_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mexitesqueeze_layer\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bn_1 (\u001b[38;5;33mBatchNormalization\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │            \u001b[38;5;34m128\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_2_concat (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ se_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         │\n",
              "│                           │                        │                │ exitesqueeze_layer_2[\u001b[38;5;34m…\u001b[0m │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ do_1 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ bn_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_2_bn                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)            │            \u001b[38;5;34m928\u001b[0m │ se_2_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m264\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ do_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],            │\n",
              "│                           │                        │                │ se_2_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m265\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span> │ get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span> │ get_item_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │ get_item_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ get_item_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │ get_item_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │ get_item_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │ get_item_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │ get_item_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │ get_item_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_concat              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)             │                        │                │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ flatten_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ flatten_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ flatten_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ flatten_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ flatten_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ flatten_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ flatten_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
              "│                           │                        │                │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ exitesqueeze_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">139</span> │ input_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">exitesqueeze_layer</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_0_concat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                           │                        │                │ exitesqueeze_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_0_bn                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span> │ se_0_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span> │ input_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ exitesqueeze_layer_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span> │ se_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">exitesqueeze_layer</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bn_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_1_concat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">116</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ se_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │                        │                │ exitesqueeze_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ do_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bn_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_1_bn                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">116</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">464</span> │ se_1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │ do_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ exitesqueeze_layer_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">116</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span> │ se_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">exitesqueeze_layer</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_2_concat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ se_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         │\n",
              "│                           │                        │                │ exitesqueeze_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ do_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bn_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ se_2_bn                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)            │            <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span> │ se_2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ do_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],            │\n",
              "│                           │                        │                │ se_2_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,052\u001b[0m (27.55 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,052</span> (27.55 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,048\u001b[0m (23.62 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,048</span> (23.62 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,004\u001b[0m (3.92 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,004</span> (3.92 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(mod_test, show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"TB\")"
      ],
      "metadata": {
        "id": "gxWhVnRkwysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "id": "hTJyGENowysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "697CMLBjwysI"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ujnWEIfcwysI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7796b1-99b5-4f3a-b575-71eb567164b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Dtype\n",
            "---  ------              -----\n",
            " 0   Brand               int32\n",
            " 1   Material            int32\n",
            " 2   Size                int32\n",
            " 3   Compartments        int32\n",
            " 4   Laptop Compartment  int32\n",
            " 5   Waterproof          int32\n",
            " 6   Style               int32\n",
            " 7   Color               int32\n",
            " 8   cheap_flag          int32\n",
            " 9   expansive_flag      int32\n",
            "dtypes: int32(10)\n",
            "memory usage: 152.4 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Weight Capacity (kg)  float32\n",
            " 1   TE_wc                 float32\n",
            " 2   skew_0                float32\n",
            " 3   skew_1                float32\n",
            "dtypes: float32(4)\n",
            "memory usage: 60.9 MB\n"
          ]
        }
      ],
      "source": [
        "categorical_feat = data.cat_features.copy()\n",
        "numerical_feat = data.num_features.copy()\n",
        "\n",
        "X_train_cat = data.X[categorical_feat].astype(\"int32\")\n",
        "X_train_num = data.X[numerical_feat].astype(\"float32\")\n",
        "\n",
        "X_test_cat = data.X_test[categorical_feat].astype(\"int32\")\n",
        "X_test_num = data.X_test[numerical_feat].astype(\"float32\")\n",
        "\n",
        "X_train_cat.info()\n",
        "X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DLdEjBBgwysI"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "#(units=512,exite_units=64, last_layer = 1, activation=\"relu\",  reg=0.001, dropout_rate=0.33)\n",
        "    categorical_features = data.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [64,128,256]),#\n",
        "              'last_layer': trial.suggest_int('last_layer',1,2),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': trial.suggest_categorical('reg', [0.00001,0.0001,0.001,0.01]),\n",
        "              \"exite_units\": trial.suggest_categorical('exite_units', [16,32,64]),#\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.51,step=0.03)\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = data.cat_features.copy()\n",
        "        numerical_feat = data.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, min_lr=0.00005),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num], batch_size=1024)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "dyp2r5IkwysI"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "457907b4-369d-4a40-c91b-7adfcb33f4e6",
        "id": "ZVi6cBIJwysI"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-23 00:48:01,308] A new study created in memory with name: no-name-49cc8d06-a9d2-4b18-a629-5429ff0b69e4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 89s 24ms/step - RMSE: 42.4820 - loss: 1849.0151 - val_RMSE: 38.6950 - val_loss: 1497.3987 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7643 - loss: 1502.7723 - val_RMSE: 38.6973 - val_loss: 1497.6093 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7584 - loss: 1502.3545 - val_RMSE: 38.7013 - val_loss: 1497.9435 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7337 - loss: 1500.4503 - val_RMSE: 38.6851 - val_loss: 1496.6754 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7285 - loss: 1500.0334 - val_RMSE: 38.6838 - val_loss: 1496.5654 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7267 - loss: 1499.8818 - val_RMSE: 38.6836 - val_loss: 1496.5461 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7260 - loss: 1499.8240 - val_RMSE: 38.6836 - val_loss: 1496.5377 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7247 - loss: 1499.7201 - val_RMSE: 38.6833 - val_loss: 1496.5115 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7235 - loss: 1499.6259 - val_RMSE: 38.6822 - val_loss: 1496.4254 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7226 - loss: 1499.5543 - val_RMSE: 38.6820 - val_loss: 1496.4128 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7229 - loss: 1499.5792 - val_RMSE: 38.6820 - val_loss: 1496.4115 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7213 - loss: 1499.4486 - val_RMSE: 38.6823 - val_loss: 1496.4305 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7213 - loss: 1499.4484 - val_RMSE: 38.6815 - val_loss: 1496.3699 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7205 - loss: 1499.3862 - val_RMSE: 38.6808 - val_loss: 1496.3170 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7203 - loss: 1499.3701 - val_RMSE: 38.6810 - val_loss: 1496.3275 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7183 - loss: 1499.2162 - val_RMSE: 38.6807 - val_loss: 1496.3052 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7195 - loss: 1499.3116 - val_RMSE: 38.6803 - val_loss: 1496.2815 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7184 - loss: 1499.2308 - val_RMSE: 38.6800 - val_loss: 1496.2510 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7181 - loss: 1499.2056 - val_RMSE: 38.6798 - val_loss: 1496.2366 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7181 - loss: 1499.2031 - val_RMSE: 38.6798 - val_loss: 1496.2410 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7173 - loss: 1499.1437 - val_RMSE: 38.6794 - val_loss: 1496.2090 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7164 - loss: 1499.0756 - val_RMSE: 38.6799 - val_loss: 1496.2482 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7171 - loss: 1499.1323 - val_RMSE: 38.6795 - val_loss: 1496.2233 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7134 - loss: 1498.8466 - val_RMSE: 38.6788 - val_loss: 1496.1687 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7126 - loss: 1498.7839 - val_RMSE: 38.6787 - val_loss: 1496.1608 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 80s 23ms/step - RMSE: 42.4247 - loss: 1843.6702 - val_RMSE: 38.7366 - val_loss: 1500.6244 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7121 - loss: 1498.7377 - val_RMSE: 38.7524 - val_loss: 1501.8823 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7088 - loss: 1498.5143 - val_RMSE: 38.7570 - val_loss: 1502.2683 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6827 - loss: 1496.5149 - val_RMSE: 38.7146 - val_loss: 1498.9722 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6776 - loss: 1496.1021 - val_RMSE: 38.7128 - val_loss: 1498.8168 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6758 - loss: 1495.9513 - val_RMSE: 38.7134 - val_loss: 1498.8564 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6751 - loss: 1495.8888 - val_RMSE: 38.7134 - val_loss: 1498.8539 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.6712 - loss: 1495.5881 - val_RMSE: 38.7105 - val_loss: 1498.6288 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6706 - loss: 1495.5381 - val_RMSE: 38.7103 - val_loss: 1498.6129 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6704 - loss: 1495.5260 - val_RMSE: 38.7103 - val_loss: 1498.6077 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.6703 - loss: 1495.5165 - val_RMSE: 38.7102 - val_loss: 1498.5996 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6701 - loss: 1495.4979 - val_RMSE: 38.7101 - val_loss: 1498.5952 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6698 - loss: 1495.4750 - val_RMSE: 38.7101 - val_loss: 1498.5907 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6702 - loss: 1495.5060 - val_RMSE: 38.7101 - val_loss: 1498.5917 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6697 - loss: 1495.4684 - val_RMSE: 38.7100 - val_loss: 1498.5874 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6701 - loss: 1495.4957 - val_RMSE: 38.7100 - val_loss: 1498.5812 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6702 - loss: 1495.5033 - val_RMSE: 38.7100 - val_loss: 1498.5809 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6695 - loss: 1495.4473 - val_RMSE: 38.7099 - val_loss: 1498.5745 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6692 - loss: 1495.4271 - val_RMSE: 38.7099 - val_loss: 1498.5753 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6692 - loss: 1495.4258 - val_RMSE: 38.7098 - val_loss: 1498.5662 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6689 - loss: 1495.4042 - val_RMSE: 38.7098 - val_loss: 1498.5645 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.6683 - loss: 1495.3568 - val_RMSE: 38.7098 - val_loss: 1498.5646 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.6680 - loss: 1495.3293 - val_RMSE: 38.7097 - val_loss: 1498.5571 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.6687 - loss: 1495.3885 - val_RMSE: 38.7097 - val_loss: 1498.5549 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6686 - loss: 1495.3801 - val_RMSE: 38.7096 - val_loss: 1498.5498 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 73s 21ms/step - RMSE: 42.4194 - loss: 1842.6129 - val_RMSE: 38.7171 - val_loss: 1499.1094 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7375 - loss: 1500.6925 - val_RMSE: 38.7167 - val_loss: 1499.1079 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7339 - loss: 1500.4492 - val_RMSE: 38.7428 - val_loss: 1501.1506 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7314 - loss: 1500.2762 - val_RMSE: 38.7193 - val_loss: 1499.3578 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7102 - loss: 1498.6562 - val_RMSE: 38.7064 - val_loss: 1498.3414 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7041 - loss: 1498.1598 - val_RMSE: 38.7040 - val_loss: 1498.1326 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7033 - loss: 1498.0787 - val_RMSE: 38.7054 - val_loss: 1498.2351 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7016 - loss: 1497.9431 - val_RMSE: 38.7043 - val_loss: 1498.1437 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6992 - loss: 1497.7507 - val_RMSE: 38.7029 - val_loss: 1498.0378 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6989 - loss: 1497.7303 - val_RMSE: 38.7029 - val_loss: 1498.0343 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6977 - loss: 1497.6364 - val_RMSE: 38.7027 - val_loss: 1498.0215 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6984 - loss: 1497.6879 - val_RMSE: 38.7027 - val_loss: 1498.0214 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6979 - loss: 1497.6522 - val_RMSE: 38.7026 - val_loss: 1498.0148 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6976 - loss: 1497.6228 - val_RMSE: 38.7026 - val_loss: 1498.0106 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6974 - loss: 1497.6090 - val_RMSE: 38.7025 - val_loss: 1498.0051 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6970 - loss: 1497.5808 - val_RMSE: 38.7024 - val_loss: 1497.9978 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6978 - loss: 1497.6420 - val_RMSE: 38.7024 - val_loss: 1497.9916 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6970 - loss: 1497.5778 - val_RMSE: 38.7023 - val_loss: 1497.9875 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6973 - loss: 1497.6034 - val_RMSE: 38.7022 - val_loss: 1497.9801 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6962 - loss: 1497.5107 - val_RMSE: 38.7021 - val_loss: 1497.9735 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6963 - loss: 1497.5177 - val_RMSE: 38.7020 - val_loss: 1497.9645 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6959 - loss: 1497.4894 - val_RMSE: 38.7020 - val_loss: 1497.9650 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6964 - loss: 1497.5277 - val_RMSE: 38.7020 - val_loss: 1497.9578 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6958 - loss: 1497.4850 - val_RMSE: 38.7018 - val_loss: 1497.9451 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6955 - loss: 1497.4617 - val_RMSE: 38.7018 - val_loss: 1497.9464 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-23 01:37:20,570] Trial 0 finished with value: 38.696710393788 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.3}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 106s 29ms/step - RMSE: 42.8164 - loss: 1882.1709 - val_RMSE: 38.7010 - val_loss: 1497.8066 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7621 - loss: 1502.5391 - val_RMSE: 38.7258 - val_loss: 1499.7379 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7587 - loss: 1502.2994 - val_RMSE: 38.6948 - val_loss: 1497.3663 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7572 - loss: 1502.2126 - val_RMSE: 38.7404 - val_loss: 1500.9222 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7552 - loss: 1502.0767 - val_RMSE: 38.6933 - val_loss: 1497.2971 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7546 - loss: 1502.0524 - val_RMSE: 38.6903 - val_loss: 1497.0726 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7542 - loss: 1502.0365 - val_RMSE: 38.7018 - val_loss: 1497.9854 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7523 - loss: 1501.9016 - val_RMSE: 38.6882 - val_loss: 1496.9431 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7514 - loss: 1501.8436 - val_RMSE: 38.6919 - val_loss: 1497.2362 - learning_rate: 0.0100\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7485 - loss: 1501.6241 - val_RMSE: 38.6887 - val_loss: 1496.9851 - learning_rate: 0.0100\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7299 - loss: 1500.1770 - val_RMSE: 38.6821 - val_loss: 1496.4714 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7252 - loss: 1499.8090 - val_RMSE: 38.6816 - val_loss: 1496.4259 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7244 - loss: 1499.7352 - val_RMSE: 38.6816 - val_loss: 1496.4189 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7236 - loss: 1499.6748 - val_RMSE: 38.6813 - val_loss: 1496.3898 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7224 - loss: 1499.5723 - val_RMSE: 38.6811 - val_loss: 1496.3757 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7219 - loss: 1499.5355 - val_RMSE: 38.6813 - val_loss: 1496.3892 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7230 - loss: 1499.6177 - val_RMSE: 38.6810 - val_loss: 1496.3685 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7212 - loss: 1499.4780 - val_RMSE: 38.6812 - val_loss: 1496.3773 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7218 - loss: 1499.5179 - val_RMSE: 38.6810 - val_loss: 1496.3646 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7206 - loss: 1499.4260 - val_RMSE: 38.6807 - val_loss: 1496.3335 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7213 - loss: 1499.4817 - val_RMSE: 38.6806 - val_loss: 1496.3281 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7204 - loss: 1499.4081 - val_RMSE: 38.6805 - val_loss: 1496.3203 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7199 - loss: 1499.3707 - val_RMSE: 38.6804 - val_loss: 1496.3104 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7201 - loss: 1499.3818 - val_RMSE: 38.6805 - val_loss: 1496.3124 - learning_rate: 1.0000e-03\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7193 - loss: 1499.3165 - val_RMSE: 38.6805 - val_loss: 1496.3121 - learning_rate: 1.0000e-03\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 97s 29ms/step - RMSE: 42.7462 - loss: 1875.4493 - val_RMSE: 38.7299 - val_loss: 1500.0410 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7105 - loss: 1498.5444 - val_RMSE: 38.7730 - val_loss: 1503.4012 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7083 - loss: 1498.3927 - val_RMSE: 38.7280 - val_loss: 1499.9419 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7064 - loss: 1498.2743 - val_RMSE: 38.7408 - val_loss: 1500.9567 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7047 - loss: 1498.1681 - val_RMSE: 38.7196 - val_loss: 1499.3341 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7033 - loss: 1498.0754 - val_RMSE: 38.7237 - val_loss: 1499.6682 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7017 - loss: 1497.9659 - val_RMSE: 38.7341 - val_loss: 1500.4824 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6829 - loss: 1496.5222 - val_RMSE: 38.7127 - val_loss: 1498.8252 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.6763 - loss: 1496.0035 - val_RMSE: 38.7135 - val_loss: 1498.8756 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6752 - loss: 1495.9086 - val_RMSE: 38.7131 - val_loss: 1498.8395 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.6723 - loss: 1495.6816 - val_RMSE: 38.7105 - val_loss: 1498.6359 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.6720 - loss: 1495.6570 - val_RMSE: 38.7104 - val_loss: 1498.6313 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.6714 - loss: 1495.6160 - val_RMSE: 38.7104 - val_loss: 1498.6274 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.6715 - loss: 1495.6215 - val_RMSE: 38.7104 - val_loss: 1498.6322 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.6719 - loss: 1495.6495 - val_RMSE: 38.7104 - val_loss: 1498.6273 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6718 - loss: 1495.6442 - val_RMSE: 38.7104 - val_loss: 1498.6322 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6712 - loss: 1495.5950 - val_RMSE: 38.7104 - val_loss: 1498.6278 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6717 - loss: 1495.6316 - val_RMSE: 38.7102 - val_loss: 1498.6147 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6704 - loss: 1495.5332 - val_RMSE: 38.7102 - val_loss: 1498.6151 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6709 - loss: 1495.5732 - val_RMSE: 38.7102 - val_loss: 1498.6152 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.6710 - loss: 1495.5824 - val_RMSE: 38.7102 - val_loss: 1498.6160 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6711 - loss: 1495.5835 - val_RMSE: 38.7103 - val_loss: 1498.6169 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6709 - loss: 1495.5712 - val_RMSE: 38.7103 - val_loss: 1498.6172 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.6703 - loss: 1495.5249 - val_RMSE: 38.7103 - val_loss: 1498.6188 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.6707 - loss: 1495.5533 - val_RMSE: 38.7103 - val_loss: 1498.6179 - learning_rate: 1.0000e-08\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 93s 27ms/step - RMSE: 42.7461 - loss: 1875.0154 - val_RMSE: 38.7228 - val_loss: 1499.4884 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7354 - loss: 1500.4702 - val_RMSE: 38.7158 - val_loss: 1498.9646 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7351 - loss: 1500.4685 - val_RMSE: 38.7327 - val_loss: 1500.3063 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7329 - loss: 1500.3193 - val_RMSE: 38.7139 - val_loss: 1498.8657 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.7317 - loss: 1500.2466 - val_RMSE: 38.7134 - val_loss: 1498.8428 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.7303 - loss: 1500.1475 - val_RMSE: 38.7132 - val_loss: 1498.8361 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.7294 - loss: 1500.0947 - val_RMSE: 38.7143 - val_loss: 1498.9318 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.7290 - loss: 1500.0731 - val_RMSE: 38.7246 - val_loss: 1499.7401 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.7087 - loss: 1498.5093 - val_RMSE: 38.7077 - val_loss: 1498.4230 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7033 - loss: 1498.0768 - val_RMSE: 38.7065 - val_loss: 1498.3247 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7024 - loss: 1498.0061 - val_RMSE: 38.7057 - val_loss: 1498.2574 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.7015 - loss: 1497.9355 - val_RMSE: 38.7068 - val_loss: 1498.3416 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7012 - loss: 1497.9077 - val_RMSE: 38.7059 - val_loss: 1498.2736 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6982 - loss: 1497.6713 - val_RMSE: 38.7042 - val_loss: 1498.1398 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6986 - loss: 1497.7032 - val_RMSE: 38.7042 - val_loss: 1498.1400 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6979 - loss: 1497.6528 - val_RMSE: 38.7042 - val_loss: 1498.1411 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6978 - loss: 1497.6459 - val_RMSE: 38.7041 - val_loss: 1498.1317 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 19ms/step - RMSE: 38.6979 - loss: 1497.6497 - val_RMSE: 38.7041 - val_loss: 1498.1315 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6983 - loss: 1497.6803 - val_RMSE: 38.7041 - val_loss: 1498.1305 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 19ms/step - RMSE: 38.6976 - loss: 1497.6292 - val_RMSE: 38.7041 - val_loss: 1498.1295 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.6981 - loss: 1497.6667 - val_RMSE: 38.7041 - val_loss: 1498.1304 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.6980 - loss: 1497.6545 - val_RMSE: 38.7041 - val_loss: 1498.1296 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6984 - loss: 1497.6913 - val_RMSE: 38.7041 - val_loss: 1498.1298 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.6977 - loss: 1497.6385 - val_RMSE: 38.7041 - val_loss: 1498.1290 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.6981 - loss: 1497.6677 - val_RMSE: 38.7041 - val_loss: 1498.1292 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-23 02:40:56,141] Trial 1 finished with value: 38.69827949014242 and parameters: {'units': 64, 'last_layer': 2, 'activation': 'relu', 'reg': 0.0001, 'exite_units': 64, 'dropout_rate': 0.32999999999999996}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 71s 20ms/step - RMSE: 42.2698 - loss: 1827.8013 - val_RMSE: 38.7159 - val_loss: 1498.9497 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7686 - loss: 1503.0403 - val_RMSE: 38.7155 - val_loss: 1498.9373 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.7599 - loss: 1502.3832 - val_RMSE: 38.7054 - val_loss: 1498.1785 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7571 - loss: 1502.1864 - val_RMSE: 38.6970 - val_loss: 1497.5791 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7573 - loss: 1502.2693 - val_RMSE: 38.6962 - val_loss: 1497.5505 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7547 - loss: 1502.0929 - val_RMSE: 38.6913 - val_loss: 1497.1942 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7529 - loss: 1501.9653 - val_RMSE: 38.6960 - val_loss: 1497.5803 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.7534 - loss: 1502.0240 - val_RMSE: 38.6928 - val_loss: 1497.3304 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7312 - loss: 1500.3070 - val_RMSE: 38.6833 - val_loss: 1496.5812 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7269 - loss: 1499.9508 - val_RMSE: 38.6828 - val_loss: 1496.5087 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.7258 - loss: 1499.8344 - val_RMSE: 38.6822 - val_loss: 1496.4392 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7256 - loss: 1499.7952 - val_RMSE: 38.6821 - val_loss: 1496.4185 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7248 - loss: 1499.7262 - val_RMSE: 38.6817 - val_loss: 1496.3850 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7240 - loss: 1499.6583 - val_RMSE: 38.6816 - val_loss: 1496.3684 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7241 - loss: 1499.6578 - val_RMSE: 38.6814 - val_loss: 1496.3579 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.7237 - loss: 1499.6261 - val_RMSE: 38.6812 - val_loss: 1496.3412 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.7231 - loss: 1499.5842 - val_RMSE: 38.6813 - val_loss: 1496.3440 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7227 - loss: 1499.5471 - val_RMSE: 38.6811 - val_loss: 1496.3318 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7220 - loss: 1499.4935 - val_RMSE: 38.6812 - val_loss: 1496.3333 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7220 - loss: 1499.4910 - val_RMSE: 38.6811 - val_loss: 1496.3274 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7223 - loss: 1499.5160 - val_RMSE: 38.6810 - val_loss: 1496.3199 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7209 - loss: 1499.4097 - val_RMSE: 38.6808 - val_loss: 1496.3054 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7207 - loss: 1499.3905 - val_RMSE: 38.6809 - val_loss: 1496.3071 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7202 - loss: 1499.3506 - val_RMSE: 38.6803 - val_loss: 1496.2628 - learning_rate: 1.0000e-03\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.7202 - loss: 1499.3534 - val_RMSE: 38.6804 - val_loss: 1496.2720 - learning_rate: 1.0000e-03\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 73s 21ms/step - RMSE: 42.2533 - loss: 1826.6449 - val_RMSE: 38.7324 - val_loss: 1500.2362 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7177 - loss: 1499.0962 - val_RMSE: 38.7399 - val_loss: 1500.8389 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 16ms/step - RMSE: 38.7100 - loss: 1498.5261 - val_RMSE: 38.7393 - val_loss: 1500.8158 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.6839 - loss: 1496.5221 - val_RMSE: 38.7162 - val_loss: 1499.0144 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.6779 - loss: 1496.0554 - val_RMSE: 38.7151 - val_loss: 1498.9281 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.6771 - loss: 1495.9874 - val_RMSE: 38.7153 - val_loss: 1498.9307 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.6777 - loss: 1496.0298 - val_RMSE: 38.7152 - val_loss: 1498.9207 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6760 - loss: 1495.8905 - val_RMSE: 38.7146 - val_loss: 1498.8782 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6754 - loss: 1495.8419 - val_RMSE: 38.7140 - val_loss: 1498.8285 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6755 - loss: 1495.8524 - val_RMSE: 38.7138 - val_loss: 1498.8175 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6753 - loss: 1495.8375 - val_RMSE: 38.7136 - val_loss: 1498.7961 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6742 - loss: 1495.7543 - val_RMSE: 38.7131 - val_loss: 1498.7581 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.6741 - loss: 1495.7432 - val_RMSE: 38.7130 - val_loss: 1498.7545 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6746 - loss: 1495.7841 - val_RMSE: 38.7128 - val_loss: 1498.7339 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6733 - loss: 1495.6848 - val_RMSE: 38.7120 - val_loss: 1498.6743 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6725 - loss: 1495.6208 - val_RMSE: 38.7127 - val_loss: 1498.7372 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6727 - loss: 1495.6384 - val_RMSE: 38.7118 - val_loss: 1498.6656 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6737 - loss: 1495.7144 - val_RMSE: 38.7125 - val_loss: 1498.7224 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6718 - loss: 1495.5717 - val_RMSE: 38.7131 - val_loss: 1498.7607 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6695 - loss: 1495.3894 - val_RMSE: 38.7089 - val_loss: 1498.4351 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6686 - loss: 1495.3196 - val_RMSE: 38.7088 - val_loss: 1498.4285 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6681 - loss: 1495.2847 - val_RMSE: 38.7087 - val_loss: 1498.4224 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.6683 - loss: 1495.2991 - val_RMSE: 38.7086 - val_loss: 1498.4152 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.6678 - loss: 1495.2559 - val_RMSE: 38.7086 - val_loss: 1498.4155 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.6672 - loss: 1495.2159 - val_RMSE: 38.7087 - val_loss: 1498.4177 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 71s 20ms/step - RMSE: 42.2177 - loss: 1822.5948 - val_RMSE: 38.7197 - val_loss: 1499.2439 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7445 - loss: 1501.1740 - val_RMSE: 38.7174 - val_loss: 1499.0885 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7356 - loss: 1500.5012 - val_RMSE: 38.7175 - val_loss: 1499.1161 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7331 - loss: 1500.3325 - val_RMSE: 38.7131 - val_loss: 1498.8256 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7331 - loss: 1500.3876 - val_RMSE: 38.7129 - val_loss: 1498.8663 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7316 - loss: 1500.3193 - val_RMSE: 38.7155 - val_loss: 1499.0817 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7109 - loss: 1498.7258 - val_RMSE: 38.7065 - val_loss: 1498.3695 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7062 - loss: 1498.3402 - val_RMSE: 38.7060 - val_loss: 1498.2947 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7041 - loss: 1498.1410 - val_RMSE: 38.7054 - val_loss: 1498.2233 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7038 - loss: 1498.1005 - val_RMSE: 38.7050 - val_loss: 1498.1808 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7026 - loss: 1498.0020 - val_RMSE: 38.7051 - val_loss: 1498.1898 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.7019 - loss: 1497.9443 - val_RMSE: 38.7054 - val_loss: 1498.2079 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.7003 - loss: 1497.8160 - val_RMSE: 38.7039 - val_loss: 1498.0918 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.6997 - loss: 1497.7692 - val_RMSE: 38.7039 - val_loss: 1498.0905 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.6988 - loss: 1497.6956 - val_RMSE: 38.7039 - val_loss: 1498.0870 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.6994 - loss: 1497.7419 - val_RMSE: 38.7037 - val_loss: 1498.0760 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.6994 - loss: 1497.7401 - val_RMSE: 38.7037 - val_loss: 1498.0732 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.6988 - loss: 1497.6986 - val_RMSE: 38.7037 - val_loss: 1498.0737 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.6982 - loss: 1497.6439 - val_RMSE: 38.7037 - val_loss: 1498.0693 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.6992 - loss: 1497.7277 - val_RMSE: 38.7036 - val_loss: 1498.0653 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.6982 - loss: 1497.6470 - val_RMSE: 38.7036 - val_loss: 1498.0645 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.6986 - loss: 1497.6779 - val_RMSE: 38.7034 - val_loss: 1498.0509 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6989 - loss: 1497.7020 - val_RMSE: 38.7035 - val_loss: 1498.0540 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6977 - loss: 1497.6106 - val_RMSE: 38.7034 - val_loss: 1498.0492 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6981 - loss: 1497.6398 - val_RMSE: 38.7033 - val_loss: 1498.0435 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 9s 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-23 03:31:36,391] Trial 2 finished with value: 38.69746866921932 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'selu', 'reg': 1e-05, 'exite_units': 16, 'dropout_rate': 0.32999999999999996}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 87s 26ms/step - RMSE: 42.5331 - loss: 1857.6256 - val_RMSE: 38.7029 - val_loss: 1498.8004 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7660 - loss: 1503.6312 - val_RMSE: 38.7071 - val_loss: 1498.9594 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7629 - loss: 1503.1954 - val_RMSE: 38.7172 - val_loss: 1499.3317 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7423 - loss: 1501.1727 - val_RMSE: 38.6924 - val_loss: 1497.2329 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 21ms/step - RMSE: 38.7370 - loss: 1500.6921 - val_RMSE: 38.6907 - val_loss: 1497.1075 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7354 - loss: 1500.5786 - val_RMSE: 38.6886 - val_loss: 1497.0002 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7338 - loss: 1500.5101 - val_RMSE: 38.6875 - val_loss: 1496.9410 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 21ms/step - RMSE: 38.7328 - loss: 1500.4564 - val_RMSE: 38.6877 - val_loss: 1496.9974 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7311 - loss: 1500.3612 - val_RMSE: 38.6867 - val_loss: 1496.9261 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7314 - loss: 1500.3887 - val_RMSE: 38.6861 - val_loss: 1496.9017 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7303 - loss: 1500.3297 - val_RMSE: 38.6850 - val_loss: 1496.8044 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7300 - loss: 1500.2885 - val_RMSE: 38.6849 - val_loss: 1496.7899 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7297 - loss: 1500.2695 - val_RMSE: 38.6842 - val_loss: 1496.7518 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7297 - loss: 1500.2727 - val_RMSE: 38.6852 - val_loss: 1496.8187 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7301 - loss: 1500.3002 - val_RMSE: 38.6839 - val_loss: 1496.7202 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 21ms/step - RMSE: 38.7291 - loss: 1500.2291 - val_RMSE: 38.6841 - val_loss: 1496.7401 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7293 - loss: 1500.2443 - val_RMSE: 38.6849 - val_loss: 1496.7924 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7262 - loss: 1499.9830 - val_RMSE: 38.6834 - val_loss: 1496.6483 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7257 - loss: 1499.9177 - val_RMSE: 38.6830 - val_loss: 1496.6039 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7245 - loss: 1499.8148 - val_RMSE: 38.6826 - val_loss: 1496.5623 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7241 - loss: 1499.7783 - val_RMSE: 38.6827 - val_loss: 1496.5591 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7236 - loss: 1499.7306 - val_RMSE: 38.6826 - val_loss: 1496.5459 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7239 - loss: 1499.7408 - val_RMSE: 38.6825 - val_loss: 1496.5325 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7240 - loss: 1499.7468 - val_RMSE: 38.6823 - val_loss: 1496.5167 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7243 - loss: 1499.7656 - val_RMSE: 38.6823 - val_loss: 1496.5103 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 84s 25ms/step - RMSE: 42.4997 - loss: 1854.5194 - val_RMSE: 38.7866 - val_loss: 1505.5142 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7149 - loss: 1499.9308 - val_RMSE: 38.7601 - val_loss: 1503.1824 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7112 - loss: 1499.3557 - val_RMSE: 38.7442 - val_loss: 1501.8669 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7114 - loss: 1499.1324 - val_RMSE: 38.7538 - val_loss: 1502.2819 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7103 - loss: 1499.2418 - val_RMSE: 38.7339 - val_loss: 1500.9067 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7095 - loss: 1499.0214 - val_RMSE: 38.7401 - val_loss: 1501.4121 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7092 - loss: 1499.2059 - val_RMSE: 38.7343 - val_loss: 1501.0375 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6878 - loss: 1497.1428 - val_RMSE: 38.7201 - val_loss: 1499.3942 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6849 - loss: 1496.6644 - val_RMSE: 38.7187 - val_loss: 1499.2871 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6826 - loss: 1496.5038 - val_RMSE: 38.7175 - val_loss: 1499.2335 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6829 - loss: 1496.5505 - val_RMSE: 38.7177 - val_loss: 1499.2590 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6811 - loss: 1496.4346 - val_RMSE: 38.7184 - val_loss: 1499.3409 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6792 - loss: 1496.2946 - val_RMSE: 38.7145 - val_loss: 1498.9988 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 19ms/step - RMSE: 38.6777 - loss: 1496.1490 - val_RMSE: 38.7142 - val_loss: 1498.9569 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6772 - loss: 1496.0941 - val_RMSE: 38.7141 - val_loss: 1498.9402 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6769 - loss: 1496.0630 - val_RMSE: 38.7138 - val_loss: 1498.9106 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6761 - loss: 1495.9951 - val_RMSE: 38.7138 - val_loss: 1498.9066 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6759 - loss: 1495.9720 - val_RMSE: 38.7137 - val_loss: 1498.8925 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 19ms/step - RMSE: 38.6756 - loss: 1495.9500 - val_RMSE: 38.7136 - val_loss: 1498.8839 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.6757 - loss: 1495.9510 - val_RMSE: 38.7134 - val_loss: 1498.8668 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.6759 - loss: 1495.9673 - val_RMSE: 38.7133 - val_loss: 1498.8618 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6748 - loss: 1495.8796 - val_RMSE: 38.7133 - val_loss: 1498.8597 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 19ms/step - RMSE: 38.6762 - loss: 1495.9849 - val_RMSE: 38.7131 - val_loss: 1498.8413 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 19ms/step - RMSE: 38.6751 - loss: 1495.8988 - val_RMSE: 38.7131 - val_loss: 1498.8361 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6755 - loss: 1495.9335 - val_RMSE: 38.7132 - val_loss: 1498.8451 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 85s 26ms/step - RMSE: 42.4700 - loss: 1851.1406 - val_RMSE: 38.7215 - val_loss: 1500.4347 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7406 - loss: 1501.8497 - val_RMSE: 38.7209 - val_loss: 1500.4225 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7381 - loss: 1501.4828 - val_RMSE: 38.7420 - val_loss: 1501.7203 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7366 - loss: 1501.1229 - val_RMSE: 38.7201 - val_loss: 1499.6532 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7370 - loss: 1501.6343 - val_RMSE: 38.7155 - val_loss: 1499.4756 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7346 - loss: 1500.9377 - val_RMSE: 38.7153 - val_loss: 1499.7268 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7338 - loss: 1501.3444 - val_RMSE: 38.7211 - val_loss: 1500.3397 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7133 - loss: 1499.3811 - val_RMSE: 38.7095 - val_loss: 1498.6930 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7086 - loss: 1498.6021 - val_RMSE: 38.7088 - val_loss: 1498.5582 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7083 - loss: 1498.5232 - val_RMSE: 38.7089 - val_loss: 1498.5668 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7074 - loss: 1498.4475 - val_RMSE: 38.7084 - val_loss: 1498.5155 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7073 - loss: 1498.4293 - val_RMSE: 38.7082 - val_loss: 1498.5002 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7070 - loss: 1498.4091 - val_RMSE: 38.7077 - val_loss: 1498.4774 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7063 - loss: 1498.3724 - val_RMSE: 38.7076 - val_loss: 1498.4663 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7059 - loss: 1498.3369 - val_RMSE: 38.7077 - val_loss: 1498.4944 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7062 - loss: 1498.3883 - val_RMSE: 38.7075 - val_loss: 1498.5116 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7036 - loss: 1498.1904 - val_RMSE: 38.7071 - val_loss: 1498.4423 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 19ms/step - RMSE: 38.7027 - loss: 1498.0885 - val_RMSE: 38.7069 - val_loss: 1498.4041 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 19ms/step - RMSE: 38.7026 - loss: 1498.0627 - val_RMSE: 38.7069 - val_loss: 1498.3876 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.7028 - loss: 1498.0635 - val_RMSE: 38.7068 - val_loss: 1498.3689 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7025 - loss: 1498.0364 - val_RMSE: 38.7068 - val_loss: 1498.3604 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7020 - loss: 1497.9885 - val_RMSE: 38.7068 - val_loss: 1498.3534 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7029 - loss: 1498.0565 - val_RMSE: 38.7066 - val_loss: 1498.3428 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7025 - loss: 1498.0205 - val_RMSE: 38.7066 - val_loss: 1498.3359 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7019 - loss: 1497.9686 - val_RMSE: 38.7066 - val_loss: 1498.3324 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 04:39:30,347] Trial 3 finished with value: 38.70071321047837 and parameters: {'units': 256, 'last_layer': 2, 'activation': 'silu', 'reg': 0.01, 'exite_units': 64, 'dropout_rate': 0.44999999999999996}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 87s 25ms/step - RMSE: 42.5152 - loss: 1855.0037 - val_RMSE: 38.6971 - val_loss: 1498.1749 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7645 - loss: 1503.4374 - val_RMSE: 38.7003 - val_loss: 1498.4285 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7620 - loss: 1503.0951 - val_RMSE: 38.6982 - val_loss: 1497.8918 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7609 - loss: 1502.8010 - val_RMSE: 38.6993 - val_loss: 1498.0414 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7604 - loss: 1502.7922 - val_RMSE: 38.7367 - val_loss: 1500.6361 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7402 - loss: 1500.8724 - val_RMSE: 38.6923 - val_loss: 1497.1276 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7373 - loss: 1500.6074 - val_RMSE: 38.6908 - val_loss: 1497.0087 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7356 - loss: 1500.4858 - val_RMSE: 38.6915 - val_loss: 1497.0875 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7342 - loss: 1500.3971 - val_RMSE: 38.6929 - val_loss: 1497.2013 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7331 - loss: 1500.3169 - val_RMSE: 38.6882 - val_loss: 1496.8309 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7314 - loss: 1500.1766 - val_RMSE: 38.6876 - val_loss: 1496.7853 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7313 - loss: 1500.1636 - val_RMSE: 38.6876 - val_loss: 1496.7823 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7317 - loss: 1500.1973 - val_RMSE: 38.6874 - val_loss: 1496.7621 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7300 - loss: 1500.0597 - val_RMSE: 38.6872 - val_loss: 1496.7443 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7305 - loss: 1500.1008 - val_RMSE: 38.6871 - val_loss: 1496.7346 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7302 - loss: 1500.0775 - val_RMSE: 38.6871 - val_loss: 1496.7346 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7301 - loss: 1500.0635 - val_RMSE: 38.6876 - val_loss: 1496.7697 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7302 - loss: 1500.0757 - val_RMSE: 38.6869 - val_loss: 1496.7195 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7296 - loss: 1500.0275 - val_RMSE: 38.6868 - val_loss: 1496.7135 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7299 - loss: 1500.0472 - val_RMSE: 38.6869 - val_loss: 1496.7185 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7300 - loss: 1500.0568 - val_RMSE: 38.6869 - val_loss: 1496.7162 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7292 - loss: 1499.9941 - val_RMSE: 38.6868 - val_loss: 1496.7112 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.7295 - loss: 1500.0189 - val_RMSE: 38.6868 - val_loss: 1496.7107 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7292 - loss: 1499.9897 - val_RMSE: 38.6868 - val_loss: 1496.7117 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7299 - loss: 1500.0482 - val_RMSE: 38.6868 - val_loss: 1496.7111 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 88s 26ms/step - RMSE: 42.4745 - loss: 1851.3224 - val_RMSE: 38.7555 - val_loss: 1502.6718 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7140 - loss: 1499.5286 - val_RMSE: 38.7496 - val_loss: 1502.2341 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7129 - loss: 1499.3351 - val_RMSE: 38.7416 - val_loss: 1501.4338 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7108 - loss: 1498.9656 - val_RMSE: 38.7427 - val_loss: 1501.3374 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.7097 - loss: 1498.7053 - val_RMSE: 38.7344 - val_loss: 1500.4271 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7103 - loss: 1498.5872 - val_RMSE: 38.7314 - val_loss: 1500.2881 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7092 - loss: 1498.5204 - val_RMSE: 38.7428 - val_loss: 1501.0284 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7076 - loss: 1498.3370 - val_RMSE: 38.7290 - val_loss: 1500.0647 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7072 - loss: 1498.3662 - val_RMSE: 38.7280 - val_loss: 1499.9524 - learning_rate: 0.0100\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7051 - loss: 1498.1534 - val_RMSE: 39.0392 - val_loss: 1524.0747 - learning_rate: 0.0100\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7063 - loss: 1498.2115 - val_RMSE: 38.7367 - val_loss: 1500.5745 - learning_rate: 0.0100\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6887 - loss: 1496.8457 - val_RMSE: 38.7231 - val_loss: 1499.4939 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6855 - loss: 1496.5886 - val_RMSE: 38.7228 - val_loss: 1499.4678 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6839 - loss: 1496.4614 - val_RMSE: 38.7304 - val_loss: 1500.0635 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6848 - loss: 1496.5286 - val_RMSE: 38.7218 - val_loss: 1499.3905 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6838 - loss: 1496.4517 - val_RMSE: 38.7250 - val_loss: 1499.6359 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6832 - loss: 1496.3982 - val_RMSE: 38.7237 - val_loss: 1499.5319 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6820 - loss: 1496.3094 - val_RMSE: 38.7185 - val_loss: 1499.1345 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6814 - loss: 1496.2582 - val_RMSE: 38.7184 - val_loss: 1499.1252 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6816 - loss: 1496.2764 - val_RMSE: 38.7184 - val_loss: 1499.1233 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6814 - loss: 1496.2592 - val_RMSE: 38.7184 - val_loss: 1499.1224 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6812 - loss: 1496.2461 - val_RMSE: 38.7185 - val_loss: 1499.1279 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6816 - loss: 1496.2740 - val_RMSE: 38.7183 - val_loss: 1499.1178 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6815 - loss: 1496.2699 - val_RMSE: 38.7183 - val_loss: 1499.1134 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6812 - loss: 1496.2416 - val_RMSE: 38.7184 - val_loss: 1499.1193 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 89s 26ms/step - RMSE: 42.4488 - loss: 1848.1752 - val_RMSE: 38.7343 - val_loss: 1501.0629 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.7393 - loss: 1501.4487 - val_RMSE: 38.7418 - val_loss: 1501.5641 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7390 - loss: 1501.2889 - val_RMSE: 38.7410 - val_loss: 1501.1160 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7149 - loss: 1499.0292 - val_RMSE: 38.7120 - val_loss: 1498.7460 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7102 - loss: 1498.6107 - val_RMSE: 38.7100 - val_loss: 1498.6067 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.7088 - loss: 1498.5186 - val_RMSE: 38.7122 - val_loss: 1498.7849 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7082 - loss: 1498.4769 - val_RMSE: 38.7111 - val_loss: 1498.6980 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7050 - loss: 1498.2263 - val_RMSE: 38.7063 - val_loss: 1498.3215 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7034 - loss: 1498.0985 - val_RMSE: 38.7060 - val_loss: 1498.2903 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7028 - loss: 1498.0472 - val_RMSE: 38.7055 - val_loss: 1498.2521 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7026 - loss: 1498.0217 - val_RMSE: 38.7055 - val_loss: 1498.2474 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7026 - loss: 1498.0178 - val_RMSE: 38.7055 - val_loss: 1498.2472 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7026 - loss: 1498.0247 - val_RMSE: 38.7053 - val_loss: 1498.2244 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7022 - loss: 1497.9846 - val_RMSE: 38.7050 - val_loss: 1498.2021 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7013 - loss: 1497.9139 - val_RMSE: 38.7050 - val_loss: 1498.2035 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7002 - loss: 1497.8263 - val_RMSE: 38.7048 - val_loss: 1498.1869 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7012 - loss: 1497.9021 - val_RMSE: 38.7048 - val_loss: 1498.1869 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.7011 - loss: 1497.8975 - val_RMSE: 38.7046 - val_loss: 1498.1677 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.7020 - loss: 1497.9658 - val_RMSE: 38.7046 - val_loss: 1498.1625 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.7004 - loss: 1497.8368 - val_RMSE: 38.7048 - val_loss: 1498.1758 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.7000 - loss: 1497.8053 - val_RMSE: 38.7046 - val_loss: 1498.1602 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.6998 - loss: 1497.7916 - val_RMSE: 38.7048 - val_loss: 1498.1740 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7011 - loss: 1497.8901 - val_RMSE: 38.7047 - val_loss: 1498.1681 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.6997 - loss: 1497.7805 - val_RMSE: 38.7045 - val_loss: 1498.1556 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.6995 - loss: 1497.7646 - val_RMSE: 38.7045 - val_loss: 1498.1503 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 05:42:18,563] Trial 4 finished with value: 38.703210553627535 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'relu', 'reg': 0.01, 'exite_units': 64, 'dropout_rate': 0.42}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 85s 26ms/step - RMSE: 42.8359 - loss: 1884.2311 - val_RMSE: 38.6991 - val_loss: 1497.8054 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7621 - loss: 1502.6879 - val_RMSE: 38.7360 - val_loss: 1500.6530 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7608 - loss: 1502.5908 - val_RMSE: 38.6965 - val_loss: 1497.6302 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7592 - loss: 1502.4574 - val_RMSE: 38.7141 - val_loss: 1498.9359 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7595 - loss: 1502.4913 - val_RMSE: 38.7001 - val_loss: 1497.8936 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7400 - loss: 1500.9700 - val_RMSE: 38.6891 - val_loss: 1496.9796 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7349 - loss: 1500.5216 - val_RMSE: 38.6880 - val_loss: 1496.8695 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7331 - loss: 1500.3611 - val_RMSE: 38.6894 - val_loss: 1496.9691 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7323 - loss: 1500.2896 - val_RMSE: 38.6873 - val_loss: 1496.7994 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7314 - loss: 1500.2133 - val_RMSE: 38.6870 - val_loss: 1496.7736 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7321 - loss: 1500.2600 - val_RMSE: 38.6887 - val_loss: 1496.9005 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7304 - loss: 1500.1292 - val_RMSE: 38.6862 - val_loss: 1496.7043 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7316 - loss: 1500.2172 - val_RMSE: 38.6866 - val_loss: 1496.7343 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7309 - loss: 1500.1661 - val_RMSE: 38.6916 - val_loss: 1497.1152 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7289 - loss: 1500.0114 - val_RMSE: 38.6853 - val_loss: 1496.6315 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7282 - loss: 1499.9553 - val_RMSE: 38.6851 - val_loss: 1496.6138 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7291 - loss: 1500.0229 - val_RMSE: 38.6850 - val_loss: 1496.6046 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7288 - loss: 1500.0006 - val_RMSE: 38.6850 - val_loss: 1496.6051 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7275 - loss: 1499.8912 - val_RMSE: 38.6851 - val_loss: 1496.6134 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7276 - loss: 1499.9010 - val_RMSE: 38.6849 - val_loss: 1496.5950 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7276 - loss: 1499.9017 - val_RMSE: 38.6849 - val_loss: 1496.5956 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7273 - loss: 1499.8770 - val_RMSE: 38.6849 - val_loss: 1496.5942 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7271 - loss: 1499.8610 - val_RMSE: 38.6849 - val_loss: 1496.5947 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7279 - loss: 1499.9211 - val_RMSE: 38.6849 - val_loss: 1496.5946 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7264 - loss: 1499.8104 - val_RMSE: 38.6849 - val_loss: 1496.5950 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 87s 26ms/step - RMSE: 42.7727 - loss: 1878.1031 - val_RMSE: 38.7401 - val_loss: 1500.9688 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7127 - loss: 1498.8345 - val_RMSE: 38.7408 - val_loss: 1501.0044 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7105 - loss: 1498.6627 - val_RMSE: 38.7365 - val_loss: 1500.6627 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7098 - loss: 1498.5868 - val_RMSE: 38.7312 - val_loss: 1500.2499 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7075 - loss: 1498.4166 - val_RMSE: 39.0688 - val_loss: 1526.4871 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7071 - loss: 1498.3643 - val_RMSE: 39.2152 - val_loss: 1537.9830 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 21ms/step - RMSE: 38.6858 - loss: 1496.7236 - val_RMSE: 38.7187 - val_loss: 1499.2399 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6806 - loss: 1496.2860 - val_RMSE: 38.7175 - val_loss: 1499.1294 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6809 - loss: 1496.2927 - val_RMSE: 38.7223 - val_loss: 1499.4957 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6806 - loss: 1496.2672 - val_RMSE: 38.7212 - val_loss: 1499.4028 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6786 - loss: 1496.1058 - val_RMSE: 38.7151 - val_loss: 1498.9301 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6780 - loss: 1496.0599 - val_RMSE: 38.7150 - val_loss: 1498.9259 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6783 - loss: 1496.0792 - val_RMSE: 38.7149 - val_loss: 1498.9108 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6770 - loss: 1495.9849 - val_RMSE: 38.7148 - val_loss: 1498.9056 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6780 - loss: 1496.0629 - val_RMSE: 38.7147 - val_loss: 1498.8988 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6779 - loss: 1496.0488 - val_RMSE: 38.7147 - val_loss: 1498.8949 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6776 - loss: 1496.0267 - val_RMSE: 38.7147 - val_loss: 1498.8931 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6778 - loss: 1496.0453 - val_RMSE: 38.7148 - val_loss: 1498.9003 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6769 - loss: 1495.9746 - val_RMSE: 38.7146 - val_loss: 1498.8861 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6771 - loss: 1495.9857 - val_RMSE: 38.7145 - val_loss: 1498.8804 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6769 - loss: 1495.9731 - val_RMSE: 38.7146 - val_loss: 1498.8866 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6765 - loss: 1495.9398 - val_RMSE: 38.7146 - val_loss: 1498.8904 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6776 - loss: 1496.0255 - val_RMSE: 38.7145 - val_loss: 1498.8760 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6766 - loss: 1495.9478 - val_RMSE: 38.7145 - val_loss: 1498.8754 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6767 - loss: 1495.9525 - val_RMSE: 38.7144 - val_loss: 1498.8762 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 85s 26ms/step - RMSE: 42.7515 - loss: 1875.5496 - val_RMSE: 38.7365 - val_loss: 1500.6808 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7378 - loss: 1500.7648 - val_RMSE: 38.7273 - val_loss: 1499.9550 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7366 - loss: 1500.6769 - val_RMSE: 38.7180 - val_loss: 1499.2604 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7354 - loss: 1500.6154 - val_RMSE: 38.7476 - val_loss: 1501.5543 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7365 - loss: 1500.6907 - val_RMSE: 38.7169 - val_loss: 1499.1890 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7340 - loss: 1500.4796 - val_RMSE: 38.7491 - val_loss: 1501.6305 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7346 - loss: 1500.5399 - val_RMSE: 38.7342 - val_loss: 1500.5038 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7157 - loss: 1499.0540 - val_RMSE: 38.7100 - val_loss: 1498.5774 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7101 - loss: 1498.5813 - val_RMSE: 38.7105 - val_loss: 1498.5913 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7090 - loss: 1498.4742 - val_RMSE: 38.7093 - val_loss: 1498.4957 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7078 - loss: 1498.3806 - val_RMSE: 38.7080 - val_loss: 1498.3873 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7077 - loss: 1498.3657 - val_RMSE: 38.7077 - val_loss: 1498.3618 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7077 - loss: 1498.3594 - val_RMSE: 38.7073 - val_loss: 1498.3285 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7068 - loss: 1498.2869 - val_RMSE: 38.7069 - val_loss: 1498.2948 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7058 - loss: 1498.2103 - val_RMSE: 38.7084 - val_loss: 1498.4165 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7060 - loss: 1498.2255 - val_RMSE: 38.7066 - val_loss: 1498.2745 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7055 - loss: 1498.1940 - val_RMSE: 38.7082 - val_loss: 1498.3987 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7061 - loss: 1498.2355 - val_RMSE: 38.7070 - val_loss: 1498.3070 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7040 - loss: 1498.0778 - val_RMSE: 38.7057 - val_loss: 1498.2079 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7040 - loss: 1498.0714 - val_RMSE: 38.7058 - val_loss: 1498.2115 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7028 - loss: 1497.9790 - val_RMSE: 38.7056 - val_loss: 1498.1932 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7027 - loss: 1497.9752 - val_RMSE: 38.7055 - val_loss: 1498.1882 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7023 - loss: 1497.9415 - val_RMSE: 38.7054 - val_loss: 1498.1785 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7022 - loss: 1497.9333 - val_RMSE: 38.7054 - val_loss: 1498.1763 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7028 - loss: 1497.9794 - val_RMSE: 38.7054 - val_loss: 1498.1812 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 06:50:44,443] Trial 5 finished with value: 38.70159366861271 and parameters: {'units': 64, 'last_layer': 2, 'activation': 'selu', 'reg': 0.001, 'exite_units': 64, 'dropout_rate': 0.39}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 88s 26ms/step - RMSE: 42.8238 - loss: 1882.8636 - val_RMSE: 38.6974 - val_loss: 1497.4934 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7601 - loss: 1502.3468 - val_RMSE: 38.7120 - val_loss: 1498.6277 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7581 - loss: 1502.1991 - val_RMSE: 38.6949 - val_loss: 1497.3098 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 21ms/step - RMSE: 38.7573 - loss: 1502.1389 - val_RMSE: 38.6996 - val_loss: 1497.6754 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7560 - loss: 1502.0472 - val_RMSE: 38.7039 - val_loss: 1498.0144 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7350 - loss: 1500.4221 - val_RMSE: 38.6852 - val_loss: 1496.5675 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7299 - loss: 1500.0291 - val_RMSE: 38.6848 - val_loss: 1496.5348 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7280 - loss: 1499.8838 - val_RMSE: 38.6844 - val_loss: 1496.5070 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7277 - loss: 1499.8589 - val_RMSE: 38.6840 - val_loss: 1496.4775 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7265 - loss: 1499.7667 - val_RMSE: 38.6840 - val_loss: 1496.4738 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7262 - loss: 1499.7440 - val_RMSE: 38.6835 - val_loss: 1496.4391 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7266 - loss: 1499.7692 - val_RMSE: 38.6843 - val_loss: 1496.5000 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7272 - loss: 1499.8220 - val_RMSE: 38.6836 - val_loss: 1496.4407 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7240 - loss: 1499.5686 - val_RMSE: 38.6834 - val_loss: 1496.4216 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 21ms/step - RMSE: 38.7235 - loss: 1499.5331 - val_RMSE: 38.6832 - val_loss: 1496.4088 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7229 - loss: 1499.4885 - val_RMSE: 38.6831 - val_loss: 1496.4034 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 55s 21ms/step - RMSE: 38.7235 - loss: 1499.5330 - val_RMSE: 38.6830 - val_loss: 1496.4009 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7240 - loss: 1499.5710 - val_RMSE: 38.6830 - val_loss: 1496.3984 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7236 - loss: 1499.5394 - val_RMSE: 38.6829 - val_loss: 1496.3907 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7219 - loss: 1499.4091 - val_RMSE: 38.6829 - val_loss: 1496.3882 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7225 - loss: 1499.4539 - val_RMSE: 38.6828 - val_loss: 1496.3854 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7221 - loss: 1499.4236 - val_RMSE: 38.6828 - val_loss: 1496.3835 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7231 - loss: 1499.5049 - val_RMSE: 38.6828 - val_loss: 1496.3827 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7227 - loss: 1499.4700 - val_RMSE: 38.6828 - val_loss: 1496.3834 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7223 - loss: 1499.4386 - val_RMSE: 38.6827 - val_loss: 1496.3749 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 85s 26ms/step - RMSE: 42.7629 - loss: 1876.9761 - val_RMSE: 38.7353 - val_loss: 1500.4301 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7091 - loss: 1498.3959 - val_RMSE: 38.7787 - val_loss: 1503.7964 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7084 - loss: 1498.3475 - val_RMSE: 38.7299 - val_loss: 1500.0220 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7072 - loss: 1498.2668 - val_RMSE: 38.7530 - val_loss: 1501.8135 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7044 - loss: 1498.0533 - val_RMSE: 38.7336 - val_loss: 1500.3162 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.6834 - loss: 1496.4301 - val_RMSE: 38.7146 - val_loss: 1498.8428 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6780 - loss: 1496.0096 - val_RMSE: 38.7134 - val_loss: 1498.7535 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6759 - loss: 1495.8489 - val_RMSE: 38.7128 - val_loss: 1498.7013 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6765 - loss: 1495.8997 - val_RMSE: 38.7126 - val_loss: 1498.6888 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6761 - loss: 1495.8629 - val_RMSE: 38.7115 - val_loss: 1498.6034 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6750 - loss: 1495.7798 - val_RMSE: 38.7118 - val_loss: 1498.6243 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6760 - loss: 1495.8539 - val_RMSE: 38.7107 - val_loss: 1498.5455 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6742 - loss: 1495.7197 - val_RMSE: 38.7114 - val_loss: 1498.5981 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6747 - loss: 1495.7562 - val_RMSE: 38.7108 - val_loss: 1498.5487 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6720 - loss: 1495.5503 - val_RMSE: 38.7098 - val_loss: 1498.4719 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6718 - loss: 1495.5327 - val_RMSE: 38.7098 - val_loss: 1498.4731 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6714 - loss: 1495.4987 - val_RMSE: 38.7097 - val_loss: 1498.4670 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6708 - loss: 1495.4569 - val_RMSE: 38.7097 - val_loss: 1498.4691 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6706 - loss: 1495.4403 - val_RMSE: 38.7098 - val_loss: 1498.4698 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6706 - loss: 1495.4407 - val_RMSE: 38.7097 - val_loss: 1498.4653 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6709 - loss: 1495.4655 - val_RMSE: 38.7097 - val_loss: 1498.4646 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6709 - loss: 1495.4669 - val_RMSE: 38.7097 - val_loss: 1498.4646 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6711 - loss: 1495.4795 - val_RMSE: 38.7097 - val_loss: 1498.4635 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6712 - loss: 1495.4839 - val_RMSE: 38.7097 - val_loss: 1498.4634 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6699 - loss: 1495.3864 - val_RMSE: 38.7097 - val_loss: 1498.4640 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 87s 26ms/step - RMSE: 42.7639 - loss: 1876.6530 - val_RMSE: 38.7212 - val_loss: 1499.3348 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7356 - loss: 1500.4546 - val_RMSE: 38.7330 - val_loss: 1500.2500 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7341 - loss: 1500.3363 - val_RMSE: 38.7213 - val_loss: 1499.3514 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7111 - loss: 1498.5660 - val_RMSE: 38.7066 - val_loss: 1498.2152 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7066 - loss: 1498.2118 - val_RMSE: 38.7060 - val_loss: 1498.1722 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7056 - loss: 1498.1364 - val_RMSE: 38.7056 - val_loss: 1498.1360 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7046 - loss: 1498.0615 - val_RMSE: 38.7066 - val_loss: 1498.2156 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7045 - loss: 1498.0490 - val_RMSE: 38.7057 - val_loss: 1498.1467 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7007 - loss: 1497.7605 - val_RMSE: 38.7045 - val_loss: 1498.0516 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7013 - loss: 1497.8015 - val_RMSE: 38.7045 - val_loss: 1498.0537 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7022 - loss: 1497.8710 - val_RMSE: 38.7045 - val_loss: 1498.0485 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7015 - loss: 1497.8225 - val_RMSE: 38.7044 - val_loss: 1498.0424 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7013 - loss: 1497.8044 - val_RMSE: 38.7045 - val_loss: 1498.0493 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7021 - loss: 1497.8660 - val_RMSE: 38.7043 - val_loss: 1498.0382 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7014 - loss: 1497.8113 - val_RMSE: 38.7044 - val_loss: 1498.0468 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6994 - loss: 1497.6569 - val_RMSE: 38.7043 - val_loss: 1498.0402 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6997 - loss: 1497.6799 - val_RMSE: 38.7042 - val_loss: 1498.0266 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7003 - loss: 1497.7271 - val_RMSE: 38.7042 - val_loss: 1498.0256 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7012 - loss: 1497.7947 - val_RMSE: 38.7042 - val_loss: 1498.0255 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7000 - loss: 1497.7041 - val_RMSE: 38.7042 - val_loss: 1498.0250 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7003 - loss: 1497.7274 - val_RMSE: 38.7041 - val_loss: 1498.0222 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7002 - loss: 1497.7223 - val_RMSE: 38.7041 - val_loss: 1498.0198 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7002 - loss: 1497.7162 - val_RMSE: 38.7041 - val_loss: 1498.0215 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7007 - loss: 1497.7576 - val_RMSE: 38.7041 - val_loss: 1498.0220 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7006 - loss: 1497.7524 - val_RMSE: 38.7041 - val_loss: 1498.0215 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 07:59:21,449] Trial 6 finished with value: 38.69884438684419 and parameters: {'units': 64, 'last_layer': 2, 'activation': 'gelu', 'reg': 1e-05, 'exite_units': 64, 'dropout_rate': 0.42}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 84s 23ms/step - RMSE: 42.5193 - loss: 1852.4968 - val_RMSE: 38.6972 - val_loss: 1497.4923 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7633 - loss: 1502.6157 - val_RMSE: 38.7080 - val_loss: 1498.3389 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7582 - loss: 1502.2375 - val_RMSE: 38.7092 - val_loss: 1498.4543 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7337 - loss: 1500.3571 - val_RMSE: 38.6840 - val_loss: 1496.5054 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7282 - loss: 1499.9219 - val_RMSE: 38.6832 - val_loss: 1496.4414 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7260 - loss: 1499.7520 - val_RMSE: 38.6827 - val_loss: 1496.4021 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7262 - loss: 1499.7684 - val_RMSE: 38.6826 - val_loss: 1496.3953 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7248 - loss: 1499.6625 - val_RMSE: 38.6824 - val_loss: 1496.3726 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7240 - loss: 1499.5947 - val_RMSE: 38.6820 - val_loss: 1496.3425 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7241 - loss: 1499.6038 - val_RMSE: 38.6821 - val_loss: 1496.3522 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7234 - loss: 1499.5527 - val_RMSE: 38.6818 - val_loss: 1496.3267 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7224 - loss: 1499.4694 - val_RMSE: 38.6820 - val_loss: 1496.3452 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7230 - loss: 1499.5183 - val_RMSE: 38.6815 - val_loss: 1496.3096 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7223 - loss: 1499.4642 - val_RMSE: 38.6813 - val_loss: 1496.2899 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7212 - loss: 1499.3822 - val_RMSE: 38.6811 - val_loss: 1496.2762 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7198 - loss: 1499.2726 - val_RMSE: 38.6812 - val_loss: 1496.2810 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7197 - loss: 1499.2648 - val_RMSE: 38.6804 - val_loss: 1496.2261 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7195 - loss: 1499.2504 - val_RMSE: 38.6807 - val_loss: 1496.2466 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7187 - loss: 1499.1871 - val_RMSE: 38.6806 - val_loss: 1496.2367 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7161 - loss: 1498.9852 - val_RMSE: 38.6804 - val_loss: 1496.2197 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7157 - loss: 1498.9586 - val_RMSE: 38.6802 - val_loss: 1496.2075 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7153 - loss: 1498.9246 - val_RMSE: 38.6802 - val_loss: 1496.2061 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7151 - loss: 1498.9070 - val_RMSE: 38.6802 - val_loss: 1496.2091 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7141 - loss: 1498.8324 - val_RMSE: 38.6801 - val_loss: 1496.1962 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7138 - loss: 1498.8049 - val_RMSE: 38.6802 - val_loss: 1496.2056 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 78s 22ms/step - RMSE: 42.4645 - loss: 1847.3851 - val_RMSE: 38.7350 - val_loss: 1500.4203 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7107 - loss: 1498.5421 - val_RMSE: 38.7616 - val_loss: 1502.4910 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7086 - loss: 1498.3950 - val_RMSE: 38.7313 - val_loss: 1500.1664 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7057 - loss: 1498.1943 - val_RMSE: 38.7307 - val_loss: 1500.1517 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7051 - loss: 1498.1760 - val_RMSE: 38.7459 - val_loss: 1501.3590 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.7039 - loss: 1498.1096 - val_RMSE: 38.7239 - val_loss: 1499.6777 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 13ms/step - RMSE: 38.7024 - loss: 1498.0146 - val_RMSE: 38.7250 - val_loss: 1499.7791 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 13ms/step - RMSE: 38.7003 - loss: 1497.8743 - val_RMSE: 38.7379 - val_loss: 1500.7903 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6808 - loss: 1496.3726 - val_RMSE: 38.7114 - val_loss: 1498.7333 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6749 - loss: 1495.9078 - val_RMSE: 38.7112 - val_loss: 1498.7057 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.6742 - loss: 1495.8359 - val_RMSE: 38.7106 - val_loss: 1498.6436 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6726 - loss: 1495.6995 - val_RMSE: 38.7116 - val_loss: 1498.7115 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6725 - loss: 1495.6884 - val_RMSE: 38.7110 - val_loss: 1498.6669 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6688 - loss: 1495.4025 - val_RMSE: 38.7095 - val_loss: 1498.5449 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6679 - loss: 1495.3278 - val_RMSE: 38.7094 - val_loss: 1498.5374 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.6673 - loss: 1495.2803 - val_RMSE: 38.7094 - val_loss: 1498.5347 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.6676 - loss: 1495.3060 - val_RMSE: 38.7093 - val_loss: 1498.5283 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6675 - loss: 1495.2928 - val_RMSE: 38.7093 - val_loss: 1498.5286 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6672 - loss: 1495.2743 - val_RMSE: 38.7092 - val_loss: 1498.5260 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6674 - loss: 1495.2903 - val_RMSE: 38.7092 - val_loss: 1498.5251 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6677 - loss: 1495.3132 - val_RMSE: 38.7093 - val_loss: 1498.5276 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6669 - loss: 1495.2517 - val_RMSE: 38.7093 - val_loss: 1498.5292 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6676 - loss: 1495.3043 - val_RMSE: 38.7093 - val_loss: 1498.5304 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6671 - loss: 1495.2688 - val_RMSE: 38.7094 - val_loss: 1498.5343 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6674 - loss: 1495.2881 - val_RMSE: 38.7093 - val_loss: 1498.5273 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 73s 21ms/step - RMSE: 42.4448 - loss: 1844.8795 - val_RMSE: 38.7182 - val_loss: 1499.1188 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7385 - loss: 1500.6954 - val_RMSE: 38.7162 - val_loss: 1498.9740 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7347 - loss: 1500.4165 - val_RMSE: 38.7143 - val_loss: 1498.8514 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7317 - loss: 1500.2057 - val_RMSE: 38.7411 - val_loss: 1500.9603 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7330 - loss: 1500.3412 - val_RMSE: 38.7086 - val_loss: 1498.4810 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7298 - loss: 1500.1244 - val_RMSE: 38.7233 - val_loss: 1499.6420 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7285 - loss: 1500.0448 - val_RMSE: 38.8628 - val_loss: 1510.4805 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7088 - loss: 1498.5281 - val_RMSE: 38.7046 - val_loss: 1498.1943 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7026 - loss: 1498.0376 - val_RMSE: 38.7032 - val_loss: 1498.0758 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7014 - loss: 1497.9279 - val_RMSE: 38.7037 - val_loss: 1498.0979 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7007 - loss: 1497.8715 - val_RMSE: 38.7032 - val_loss: 1498.0576 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6997 - loss: 1497.7883 - val_RMSE: 38.7028 - val_loss: 1498.0227 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6988 - loss: 1497.7155 - val_RMSE: 38.7033 - val_loss: 1498.0581 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6983 - loss: 1497.6691 - val_RMSE: 38.7030 - val_loss: 1498.0308 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6955 - loss: 1497.4583 - val_RMSE: 38.7018 - val_loss: 1497.9386 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6959 - loss: 1497.4863 - val_RMSE: 38.7018 - val_loss: 1497.9404 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6966 - loss: 1497.5420 - val_RMSE: 38.7018 - val_loss: 1497.9412 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6960 - loss: 1497.4912 - val_RMSE: 38.7018 - val_loss: 1497.9379 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6955 - loss: 1497.4547 - val_RMSE: 38.7017 - val_loss: 1497.9364 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6950 - loss: 1497.4188 - val_RMSE: 38.7017 - val_loss: 1497.9342 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6943 - loss: 1497.3622 - val_RMSE: 38.7017 - val_loss: 1497.9343 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6943 - loss: 1497.3591 - val_RMSE: 38.7017 - val_loss: 1497.9337 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6949 - loss: 1497.4081 - val_RMSE: 38.7017 - val_loss: 1497.9330 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.6953 - loss: 1497.4349 - val_RMSE: 38.7017 - val_loss: 1497.9324 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 13ms/step - RMSE: 38.6958 - loss: 1497.4756 - val_RMSE: 38.7017 - val_loss: 1497.9333 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 08:47:50,991] Trial 7 finished with value: 38.69705476562388 and parameters: {'units': 256, 'last_layer': 2, 'activation': 'relu', 'reg': 1e-05, 'exite_units': 16, 'dropout_rate': 0.42}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 84s 25ms/step - RMSE: 42.8132 - loss: 1881.8291 - val_RMSE: 38.6952 - val_loss: 1497.3544 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7608 - loss: 1502.4475 - val_RMSE: 38.7217 - val_loss: 1499.4329 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7594 - loss: 1502.3654 - val_RMSE: 38.7167 - val_loss: 1499.0803 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7344 - loss: 1500.4436 - val_RMSE: 38.6851 - val_loss: 1496.6221 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7296 - loss: 1500.0714 - val_RMSE: 38.6862 - val_loss: 1496.7089 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7279 - loss: 1499.9352 - val_RMSE: 38.6843 - val_loss: 1496.5609 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7275 - loss: 1499.9015 - val_RMSE: 38.6840 - val_loss: 1496.5331 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7271 - loss: 1499.8715 - val_RMSE: 38.6829 - val_loss: 1496.4496 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7271 - loss: 1499.8713 - val_RMSE: 38.6858 - val_loss: 1496.6664 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7252 - loss: 1499.7245 - val_RMSE: 38.6824 - val_loss: 1496.4071 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7255 - loss: 1499.7490 - val_RMSE: 38.7217 - val_loss: 1499.4449 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7247 - loss: 1499.6760 - val_RMSE: 38.6901 - val_loss: 1496.9993 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7232 - loss: 1499.5656 - val_RMSE: 38.6816 - val_loss: 1496.3483 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7227 - loss: 1499.5228 - val_RMSE: 38.6815 - val_loss: 1496.3354 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7211 - loss: 1499.4036 - val_RMSE: 38.6816 - val_loss: 1496.3430 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 21ms/step - RMSE: 38.7227 - loss: 1499.5275 - val_RMSE: 38.6816 - val_loss: 1496.3396 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7219 - loss: 1499.4602 - val_RMSE: 38.6813 - val_loss: 1496.3193 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7220 - loss: 1499.4703 - val_RMSE: 38.6813 - val_loss: 1496.3184 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7215 - loss: 1499.4283 - val_RMSE: 38.6813 - val_loss: 1496.3175 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7221 - loss: 1499.4760 - val_RMSE: 38.6813 - val_loss: 1496.3175 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7212 - loss: 1499.4048 - val_RMSE: 38.6813 - val_loss: 1496.3163 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7217 - loss: 1499.4435 - val_RMSE: 38.6813 - val_loss: 1496.3175 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7221 - loss: 1499.4751 - val_RMSE: 38.6813 - val_loss: 1496.3174 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7223 - loss: 1499.4897 - val_RMSE: 38.6813 - val_loss: 1496.3170 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7216 - loss: 1499.4393 - val_RMSE: 38.6813 - val_loss: 1496.3165 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 8ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 87s 26ms/step - RMSE: 42.7547 - loss: 1876.2645 - val_RMSE: 38.7303 - val_loss: 1500.0701 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 55s 21ms/step - RMSE: 38.7098 - loss: 1498.4850 - val_RMSE: 38.7345 - val_loss: 1500.4180 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7083 - loss: 1498.3979 - val_RMSE: 38.7576 - val_loss: 1502.2328 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.6843 - loss: 1496.5530 - val_RMSE: 38.7148 - val_loss: 1498.9120 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.6787 - loss: 1496.1179 - val_RMSE: 38.7144 - val_loss: 1498.8822 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.6776 - loss: 1496.0298 - val_RMSE: 38.7155 - val_loss: 1498.9626 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6768 - loss: 1495.9716 - val_RMSE: 38.7143 - val_loss: 1498.8667 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6755 - loss: 1495.8713 - val_RMSE: 38.7135 - val_loss: 1498.8022 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6762 - loss: 1495.9187 - val_RMSE: 38.7128 - val_loss: 1498.7484 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6759 - loss: 1495.8927 - val_RMSE: 38.7129 - val_loss: 1498.7593 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6751 - loss: 1495.8340 - val_RMSE: 38.7124 - val_loss: 1498.7218 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6745 - loss: 1495.7834 - val_RMSE: 38.7119 - val_loss: 1498.6829 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6749 - loss: 1495.8197 - val_RMSE: 38.7137 - val_loss: 1498.8163 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6746 - loss: 1495.7908 - val_RMSE: 38.7155 - val_loss: 1498.9609 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6714 - loss: 1495.5471 - val_RMSE: 38.7105 - val_loss: 1498.5693 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6716 - loss: 1495.5613 - val_RMSE: 38.7102 - val_loss: 1498.5492 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.6706 - loss: 1495.4891 - val_RMSE: 38.7101 - val_loss: 1498.5389 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6711 - loss: 1495.5212 - val_RMSE: 38.7101 - val_loss: 1498.5378 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6705 - loss: 1495.4749 - val_RMSE: 38.7101 - val_loss: 1498.5398 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6708 - loss: 1495.4982 - val_RMSE: 38.7100 - val_loss: 1498.5311 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6706 - loss: 1495.4812 - val_RMSE: 38.7100 - val_loss: 1498.5337 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6703 - loss: 1495.4625 - val_RMSE: 38.7100 - val_loss: 1498.5332 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6699 - loss: 1495.4318 - val_RMSE: 38.7097 - val_loss: 1498.5060 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.6703 - loss: 1495.4631 - val_RMSE: 38.7097 - val_loss: 1498.5063 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.6703 - loss: 1495.4606 - val_RMSE: 38.7097 - val_loss: 1498.5057 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 84s 25ms/step - RMSE: 42.7440 - loss: 1874.7086 - val_RMSE: 38.7147 - val_loss: 1498.8625 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7358 - loss: 1500.5005 - val_RMSE: 38.7285 - val_loss: 1499.9546 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7343 - loss: 1500.4078 - val_RMSE: 38.7160 - val_loss: 1499.0100 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7121 - loss: 1498.7075 - val_RMSE: 38.7058 - val_loss: 1498.2150 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7060 - loss: 1498.2312 - val_RMSE: 38.7048 - val_loss: 1498.1383 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7045 - loss: 1498.1121 - val_RMSE: 38.7113 - val_loss: 1498.6320 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7037 - loss: 1498.0502 - val_RMSE: 38.7059 - val_loss: 1498.2172 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7019 - loss: 1497.9093 - val_RMSE: 38.7040 - val_loss: 1498.0670 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7016 - loss: 1497.8853 - val_RMSE: 38.7038 - val_loss: 1498.0549 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7009 - loss: 1497.8325 - val_RMSE: 38.7038 - val_loss: 1498.0511 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7006 - loss: 1497.8082 - val_RMSE: 38.7038 - val_loss: 1498.0568 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7002 - loss: 1497.7745 - val_RMSE: 38.7037 - val_loss: 1498.0432 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7009 - loss: 1497.8311 - val_RMSE: 38.7037 - val_loss: 1498.0446 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7007 - loss: 1497.8162 - val_RMSE: 38.7037 - val_loss: 1498.0455 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7008 - loss: 1497.8217 - val_RMSE: 38.7035 - val_loss: 1498.0334 - learning_rate: 1.0000e-05\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7000 - loss: 1497.7616 - val_RMSE: 38.7035 - val_loss: 1498.0325 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7001 - loss: 1497.7705 - val_RMSE: 38.7035 - val_loss: 1498.0312 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.6997 - loss: 1497.7382 - val_RMSE: 38.7035 - val_loss: 1498.0287 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7005 - loss: 1497.7947 - val_RMSE: 38.7034 - val_loss: 1498.0267 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.6997 - loss: 1497.7365 - val_RMSE: 38.7035 - val_loss: 1498.0283 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 21ms/step - RMSE: 38.7002 - loss: 1497.7784 - val_RMSE: 38.7035 - val_loss: 1498.0287 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7003 - loss: 1497.7844 - val_RMSE: 38.7034 - val_loss: 1498.0260 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.7001 - loss: 1497.7677 - val_RMSE: 38.7034 - val_loss: 1498.0259 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 52s 20ms/step - RMSE: 38.6997 - loss: 1497.7366 - val_RMSE: 38.7034 - val_loss: 1498.0265 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 53s 20ms/step - RMSE: 38.7006 - loss: 1497.8046 - val_RMSE: 38.7034 - val_loss: 1498.0261 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 09:56:13,165] Trial 8 finished with value: 38.698128415832144 and parameters: {'units': 64, 'last_layer': 2, 'activation': 'silu', 'reg': 0.0001, 'exite_units': 64, 'dropout_rate': 0.36}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 71s 20ms/step - RMSE: 42.7173 - loss: 1873.8325 - val_RMSE: 38.7139 - val_loss: 1499.1440 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7637 - loss: 1502.9199 - val_RMSE: 38.6987 - val_loss: 1497.7234 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7624 - loss: 1502.6368 - val_RMSE: 38.7074 - val_loss: 1498.2804 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7597 - loss: 1502.3391 - val_RMSE: 38.7032 - val_loss: 1498.1995 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7406 - loss: 1500.9136 - val_RMSE: 38.6931 - val_loss: 1497.1761 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7366 - loss: 1500.5479 - val_RMSE: 38.6921 - val_loss: 1497.1167 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7354 - loss: 1500.4658 - val_RMSE: 38.6914 - val_loss: 1497.0570 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7356 - loss: 1500.4753 - val_RMSE: 38.6914 - val_loss: 1497.0659 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7360 - loss: 1500.5146 - val_RMSE: 38.6914 - val_loss: 1497.0505 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7360 - loss: 1500.5066 - val_RMSE: 38.6906 - val_loss: 1496.9900 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7348 - loss: 1500.4196 - val_RMSE: 38.6908 - val_loss: 1497.0049 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7352 - loss: 1500.4399 - val_RMSE: 38.6904 - val_loss: 1496.9805 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7355 - loss: 1500.4680 - val_RMSE: 38.6907 - val_loss: 1497.0037 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7355 - loss: 1500.4680 - val_RMSE: 38.6908 - val_loss: 1497.0083 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7339 - loss: 1500.3420 - val_RMSE: 38.6902 - val_loss: 1496.9518 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7332 - loss: 1500.2822 - val_RMSE: 38.6901 - val_loss: 1496.9445 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7325 - loss: 1500.2245 - val_RMSE: 38.6900 - val_loss: 1496.9324 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7336 - loss: 1500.3073 - val_RMSE: 38.6900 - val_loss: 1496.9288 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7328 - loss: 1500.2433 - val_RMSE: 38.6899 - val_loss: 1496.9231 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.7323 - loss: 1500.2046 - val_RMSE: 38.6899 - val_loss: 1496.9215 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.7324 - loss: 1500.2136 - val_RMSE: 38.6898 - val_loss: 1496.9159 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 13ms/step - RMSE: 38.7328 - loss: 1500.2494 - val_RMSE: 38.6897 - val_loss: 1496.9121 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.7325 - loss: 1500.2200 - val_RMSE: 38.6897 - val_loss: 1496.9105 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7319 - loss: 1500.1820 - val_RMSE: 38.6897 - val_loss: 1496.9067 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7321 - loss: 1500.1934 - val_RMSE: 38.6896 - val_loss: 1496.9011 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 69s 20ms/step - RMSE: 42.6560 - loss: 1868.1293 - val_RMSE: 38.7314 - val_loss: 1500.5179 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7131 - loss: 1499.0630 - val_RMSE: 38.7449 - val_loss: 1501.4584 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7122 - loss: 1498.8007 - val_RMSE: 38.7347 - val_loss: 1500.4071 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7113 - loss: 1498.6045 - val_RMSE: 38.7556 - val_loss: 1502.2769 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7102 - loss: 1498.8422 - val_RMSE: 38.7279 - val_loss: 1499.9310 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7092 - loss: 1498.5536 - val_RMSE: 38.7286 - val_loss: 1500.0421 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7081 - loss: 1498.5548 - val_RMSE: 38.7360 - val_loss: 1500.7999 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6868 - loss: 1496.8345 - val_RMSE: 38.7208 - val_loss: 1499.3496 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6842 - loss: 1496.5160 - val_RMSE: 38.7206 - val_loss: 1499.3463 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6834 - loss: 1496.4720 - val_RMSE: 38.7202 - val_loss: 1499.3505 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6829 - loss: 1496.4637 - val_RMSE: 38.7215 - val_loss: 1499.4647 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6804 - loss: 1496.2736 - val_RMSE: 38.7173 - val_loss: 1499.1245 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6800 - loss: 1496.2325 - val_RMSE: 38.7172 - val_loss: 1499.1024 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6794 - loss: 1496.1737 - val_RMSE: 38.7170 - val_loss: 1499.0839 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6796 - loss: 1496.1866 - val_RMSE: 38.7169 - val_loss: 1499.0725 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6790 - loss: 1496.1365 - val_RMSE: 38.7167 - val_loss: 1499.0509 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6790 - loss: 1496.1346 - val_RMSE: 38.7165 - val_loss: 1499.0359 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6789 - loss: 1496.1218 - val_RMSE: 38.7164 - val_loss: 1499.0236 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6778 - loss: 1496.0393 - val_RMSE: 38.7162 - val_loss: 1499.0076 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6778 - loss: 1496.0400 - val_RMSE: 38.7161 - val_loss: 1498.9993 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6779 - loss: 1496.0457 - val_RMSE: 38.7160 - val_loss: 1498.9940 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6778 - loss: 1496.0374 - val_RMSE: 38.7160 - val_loss: 1498.9954 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.6776 - loss: 1496.0234 - val_RMSE: 38.7159 - val_loss: 1498.9839 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6775 - loss: 1496.0114 - val_RMSE: 38.7157 - val_loss: 1498.9705 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.6779 - loss: 1496.0482 - val_RMSE: 38.7155 - val_loss: 1498.9553 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 9s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 69s 19ms/step - RMSE: 42.6386 - loss: 1865.9310 - val_RMSE: 38.7677 - val_loss: 1503.2617 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7395 - loss: 1501.0195 - val_RMSE: 38.7737 - val_loss: 1503.6061 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7376 - loss: 1500.7192 - val_RMSE: 38.7311 - val_loss: 1500.1249 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7373 - loss: 1500.6101 - val_RMSE: 38.8369 - val_loss: 1508.7581 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.7378 - loss: 1501.0789 - val_RMSE: 38.7762 - val_loss: 1503.6615 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7166 - loss: 1499.0188 - val_RMSE: 38.7148 - val_loss: 1498.8707 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7130 - loss: 1498.7327 - val_RMSE: 38.7166 - val_loss: 1499.0153 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7118 - loss: 1498.6461 - val_RMSE: 38.7143 - val_loss: 1498.8529 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7112 - loss: 1498.6129 - val_RMSE: 38.7200 - val_loss: 1499.3071 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7116 - loss: 1498.6566 - val_RMSE: 38.7161 - val_loss: 1499.0031 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7096 - loss: 1498.4961 - val_RMSE: 38.7119 - val_loss: 1498.6637 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7093 - loss: 1498.4604 - val_RMSE: 38.7119 - val_loss: 1498.6578 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7090 - loss: 1498.4293 - val_RMSE: 38.7117 - val_loss: 1498.6396 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7085 - loss: 1498.3887 - val_RMSE: 38.7116 - val_loss: 1498.6301 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7076 - loss: 1498.3199 - val_RMSE: 38.7116 - val_loss: 1498.6254 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7078 - loss: 1498.3289 - val_RMSE: 38.7116 - val_loss: 1498.6288 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7081 - loss: 1498.3529 - val_RMSE: 38.7115 - val_loss: 1498.6158 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.7074 - loss: 1498.3009 - val_RMSE: 38.7115 - val_loss: 1498.6173 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7083 - loss: 1498.3722 - val_RMSE: 38.7113 - val_loss: 1498.6039 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7080 - loss: 1498.3456 - val_RMSE: 38.7114 - val_loss: 1498.6102 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7074 - loss: 1498.2996 - val_RMSE: 38.7111 - val_loss: 1498.5894 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7078 - loss: 1498.3275 - val_RMSE: 38.7112 - val_loss: 1498.5907 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7075 - loss: 1498.3054 - val_RMSE: 38.7111 - val_loss: 1498.5896 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.7069 - loss: 1498.2659 - val_RMSE: 38.7110 - val_loss: 1498.5809 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.7073 - loss: 1498.2888 - val_RMSE: 38.7110 - val_loss: 1498.5790 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 9s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 10:44:33,607] Trial 9 finished with value: 38.70537030317054 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'selu', 'reg': 0.01, 'exite_units': 16, 'dropout_rate': 0.44999999999999996}. Best is trial 0 with value: 38.696710393788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 75s 22ms/step - RMSE: 42.5453 - loss: 1854.9423 - val_RMSE: 38.6972 - val_loss: 1497.5632 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7627 - loss: 1502.6490 - val_RMSE: 38.6943 - val_loss: 1497.3860 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7600 - loss: 1502.4985 - val_RMSE: 38.6998 - val_loss: 1497.8654 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7583 - loss: 1502.4227 - val_RMSE: 38.7016 - val_loss: 1498.1123 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7360 - loss: 1500.7708 - val_RMSE: 38.6850 - val_loss: 1496.7909 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7305 - loss: 1500.3047 - val_RMSE: 38.6840 - val_loss: 1496.6688 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7296 - loss: 1500.1926 - val_RMSE: 38.6833 - val_loss: 1496.5922 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7276 - loss: 1500.0206 - val_RMSE: 38.6828 - val_loss: 1496.5424 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7266 - loss: 1499.9338 - val_RMSE: 38.6825 - val_loss: 1496.5126 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7265 - loss: 1499.9181 - val_RMSE: 38.6819 - val_loss: 1496.4622 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7249 - loss: 1499.7893 - val_RMSE: 38.6819 - val_loss: 1496.4526 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7240 - loss: 1499.7142 - val_RMSE: 38.6819 - val_loss: 1496.4513 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7243 - loss: 1499.7301 - val_RMSE: 38.6812 - val_loss: 1496.3894 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7236 - loss: 1499.6754 - val_RMSE: 38.6814 - val_loss: 1496.4034 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7220 - loss: 1499.5444 - val_RMSE: 38.6804 - val_loss: 1496.3224 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7229 - loss: 1499.6166 - val_RMSE: 38.6806 - val_loss: 1496.3365 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7218 - loss: 1499.5306 - val_RMSE: 38.6799 - val_loss: 1496.2842 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7212 - loss: 1499.4789 - val_RMSE: 38.6798 - val_loss: 1496.2772 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7221 - loss: 1499.5481 - val_RMSE: 38.6800 - val_loss: 1496.2858 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7219 - loss: 1499.5312 - val_RMSE: 38.6798 - val_loss: 1496.2749 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7212 - loss: 1499.4799 - val_RMSE: 38.6794 - val_loss: 1496.2396 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7212 - loss: 1499.4786 - val_RMSE: 38.6796 - val_loss: 1496.2616 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7206 - loss: 1499.4299 - val_RMSE: 38.6791 - val_loss: 1496.2186 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7195 - loss: 1499.3446 - val_RMSE: 38.6790 - val_loss: 1496.2103 - learning_rate: 1.0000e-03\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7193 - loss: 1499.3271 - val_RMSE: 38.6789 - val_loss: 1496.2045 - learning_rate: 1.0000e-03\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 76s 22ms/step - RMSE: 42.4923 - loss: 1849.9899 - val_RMSE: 38.7530 - val_loss: 1501.8778 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7112 - loss: 1498.6517 - val_RMSE: 38.7576 - val_loss: 1502.2882 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7100 - loss: 1498.6174 - val_RMSE: 38.7444 - val_loss: 1501.3198 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7075 - loss: 1498.4807 - val_RMSE: 38.7372 - val_loss: 1500.8384 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7075 - loss: 1498.5643 - val_RMSE: 38.7276 - val_loss: 1500.1282 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7071 - loss: 1498.5563 - val_RMSE: 38.7296 - val_loss: 1500.3225 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7047 - loss: 1498.3976 - val_RMSE: 38.7338 - val_loss: 1500.6517 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6827 - loss: 1496.6865 - val_RMSE: 38.7157 - val_loss: 1499.2051 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6783 - loss: 1496.2980 - val_RMSE: 38.7146 - val_loss: 1499.0732 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6763 - loss: 1496.1047 - val_RMSE: 38.7149 - val_loss: 1499.0725 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6763 - loss: 1496.0881 - val_RMSE: 38.7127 - val_loss: 1498.8885 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6750 - loss: 1495.9744 - val_RMSE: 38.7132 - val_loss: 1498.9188 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6747 - loss: 1495.9399 - val_RMSE: 38.7128 - val_loss: 1498.8790 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6741 - loss: 1495.8801 - val_RMSE: 38.7127 - val_loss: 1498.8689 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6747 - loss: 1495.9240 - val_RMSE: 38.7130 - val_loss: 1498.8849 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6733 - loss: 1495.8093 - val_RMSE: 38.7111 - val_loss: 1498.7313 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6729 - loss: 1495.7709 - val_RMSE: 38.7137 - val_loss: 1498.9222 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6731 - loss: 1495.7882 - val_RMSE: 38.7104 - val_loss: 1498.6625 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6723 - loss: 1495.7175 - val_RMSE: 38.7121 - val_loss: 1498.7937 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6716 - loss: 1495.6625 - val_RMSE: 38.7135 - val_loss: 1498.9083 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6694 - loss: 1495.4897 - val_RMSE: 38.7082 - val_loss: 1498.4894 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6683 - loss: 1495.4072 - val_RMSE: 38.7080 - val_loss: 1498.4753 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6686 - loss: 1495.4318 - val_RMSE: 38.7081 - val_loss: 1498.4879 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6684 - loss: 1495.4108 - val_RMSE: 38.7079 - val_loss: 1498.4685 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6681 - loss: 1495.3905 - val_RMSE: 38.7080 - val_loss: 1498.4772 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 81s 23ms/step - RMSE: 42.4911 - loss: 1849.3804 - val_RMSE: 38.7262 - val_loss: 1499.8074 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7395 - loss: 1500.8468 - val_RMSE: 38.7207 - val_loss: 1499.4390 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7356 - loss: 1500.6118 - val_RMSE: 38.8290 - val_loss: 1507.8951 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7342 - loss: 1500.5566 - val_RMSE: 38.7583 - val_loss: 1502.5004 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7134 - loss: 1499.0168 - val_RMSE: 38.7083 - val_loss: 1498.5903 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7063 - loss: 1498.4282 - val_RMSE: 38.7096 - val_loss: 1498.6537 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7049 - loss: 1498.2827 - val_RMSE: 38.7047 - val_loss: 1498.2502 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7037 - loss: 1498.1743 - val_RMSE: 38.7182 - val_loss: 1499.2894 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7028 - loss: 1498.0930 - val_RMSE: 38.7030 - val_loss: 1498.1019 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7036 - loss: 1498.1472 - val_RMSE: 38.7184 - val_loss: 1499.2845 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7027 - loss: 1498.0725 - val_RMSE: 38.7061 - val_loss: 1498.3250 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6997 - loss: 1497.8362 - val_RMSE: 38.7024 - val_loss: 1498.0426 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6982 - loss: 1497.7177 - val_RMSE: 38.7024 - val_loss: 1498.0432 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6994 - loss: 1497.8125 - val_RMSE: 38.7024 - val_loss: 1498.0378 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6989 - loss: 1497.7664 - val_RMSE: 38.7023 - val_loss: 1498.0353 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6988 - loss: 1497.7571 - val_RMSE: 38.7023 - val_loss: 1498.0291 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6985 - loss: 1497.7329 - val_RMSE: 38.7023 - val_loss: 1498.0256 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6976 - loss: 1497.6660 - val_RMSE: 38.7023 - val_loss: 1498.0256 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6982 - loss: 1497.7141 - val_RMSE: 38.7023 - val_loss: 1498.0283 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6979 - loss: 1497.6898 - val_RMSE: 38.7023 - val_loss: 1498.0269 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6975 - loss: 1497.6591 - val_RMSE: 38.7023 - val_loss: 1498.0253 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6978 - loss: 1497.6821 - val_RMSE: 38.7023 - val_loss: 1498.0237 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6974 - loss: 1497.6461 - val_RMSE: 38.7022 - val_loss: 1498.0233 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6974 - loss: 1497.6486 - val_RMSE: 38.7022 - val_loss: 1498.0236 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6964 - loss: 1497.5709 - val_RMSE: 38.7022 - val_loss: 1498.0227 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 11:41:25,962] Trial 10 finished with value: 38.69639193901346 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.51}. Best is trial 10 with value: 38.69639193901346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 75s 22ms/step - RMSE: 42.5454 - loss: 1854.9484 - val_RMSE: 38.7009 - val_loss: 1497.8472 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7630 - loss: 1502.6708 - val_RMSE: 38.7061 - val_loss: 1498.3007 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7593 - loss: 1502.4410 - val_RMSE: 38.7013 - val_loss: 1497.9832 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7357 - loss: 1500.6432 - val_RMSE: 38.6858 - val_loss: 1496.7700 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7308 - loss: 1500.2440 - val_RMSE: 38.6850 - val_loss: 1496.6818 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7289 - loss: 1500.0743 - val_RMSE: 38.6842 - val_loss: 1496.6123 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7287 - loss: 1500.0516 - val_RMSE: 38.6841 - val_loss: 1496.6001 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7269 - loss: 1499.9116 - val_RMSE: 38.6830 - val_loss: 1496.5077 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7264 - loss: 1499.8688 - val_RMSE: 38.6824 - val_loss: 1496.4624 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7260 - loss: 1499.8369 - val_RMSE: 38.6828 - val_loss: 1496.4885 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7251 - loss: 1499.7644 - val_RMSE: 38.6822 - val_loss: 1496.4391 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7243 - loss: 1499.6985 - val_RMSE: 38.6820 - val_loss: 1496.4224 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7249 - loss: 1499.7466 - val_RMSE: 38.6817 - val_loss: 1496.3948 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7242 - loss: 1499.6913 - val_RMSE: 38.6814 - val_loss: 1496.3695 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7231 - loss: 1499.6044 - val_RMSE: 38.6811 - val_loss: 1496.3511 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7235 - loss: 1499.6350 - val_RMSE: 38.6814 - val_loss: 1496.3746 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7227 - loss: 1499.5714 - val_RMSE: 38.6814 - val_loss: 1496.3690 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7200 - loss: 1499.3574 - val_RMSE: 38.6810 - val_loss: 1496.3364 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7210 - loss: 1499.4349 - val_RMSE: 38.6808 - val_loss: 1496.3210 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7197 - loss: 1499.3375 - val_RMSE: 38.6806 - val_loss: 1496.3112 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7183 - loss: 1499.2239 - val_RMSE: 38.6805 - val_loss: 1496.3021 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7198 - loss: 1499.3451 - val_RMSE: 38.6805 - val_loss: 1496.2993 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7178 - loss: 1499.1884 - val_RMSE: 38.6805 - val_loss: 1496.2968 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7183 - loss: 1499.2242 - val_RMSE: 38.6804 - val_loss: 1496.2935 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7188 - loss: 1499.2664 - val_RMSE: 38.6803 - val_loss: 1496.2843 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 22ms/step - RMSE: 42.4965 - loss: 1850.3501 - val_RMSE: 38.7438 - val_loss: 1501.1680 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7110 - loss: 1498.6396 - val_RMSE: 38.8131 - val_loss: 1506.6025 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7089 - loss: 1498.5377 - val_RMSE: 38.7293 - val_loss: 1500.1580 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7074 - loss: 1498.4834 - val_RMSE: 38.7264 - val_loss: 1500.0273 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7066 - loss: 1498.5096 - val_RMSE: 38.7238 - val_loss: 1499.8674 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7058 - loss: 1498.4758 - val_RMSE: 38.7509 - val_loss: 1501.9724 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7054 - loss: 1498.4551 - val_RMSE: 38.7612 - val_loss: 1502.7667 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6837 - loss: 1496.7593 - val_RMSE: 38.7148 - val_loss: 1499.1294 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6780 - loss: 1496.2766 - val_RMSE: 38.7126 - val_loss: 1498.9199 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6772 - loss: 1496.1807 - val_RMSE: 38.7129 - val_loss: 1498.9282 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6760 - loss: 1496.0665 - val_RMSE: 38.7113 - val_loss: 1498.7910 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6755 - loss: 1496.0186 - val_RMSE: 38.7119 - val_loss: 1498.8243 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6756 - loss: 1496.0171 - val_RMSE: 38.7120 - val_loss: 1498.8256 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6730 - loss: 1495.8080 - val_RMSE: 38.7096 - val_loss: 1498.6415 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6720 - loss: 1495.7289 - val_RMSE: 38.7095 - val_loss: 1498.6293 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6712 - loss: 1495.6644 - val_RMSE: 38.7094 - val_loss: 1498.6241 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6716 - loss: 1495.6954 - val_RMSE: 38.7094 - val_loss: 1498.6243 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6709 - loss: 1495.6407 - val_RMSE: 38.7094 - val_loss: 1498.6206 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6708 - loss: 1495.6284 - val_RMSE: 38.7094 - val_loss: 1498.6172 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6712 - loss: 1495.6613 - val_RMSE: 38.7093 - val_loss: 1498.6138 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6701 - loss: 1495.5746 - val_RMSE: 38.7092 - val_loss: 1498.6051 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6699 - loss: 1495.5565 - val_RMSE: 38.7092 - val_loss: 1498.6021 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6697 - loss: 1495.5455 - val_RMSE: 38.7092 - val_loss: 1498.5959 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6701 - loss: 1495.5693 - val_RMSE: 38.7091 - val_loss: 1498.5930 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6696 - loss: 1495.5349 - val_RMSE: 38.7091 - val_loss: 1498.5863 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 22ms/step - RMSE: 42.4924 - loss: 1849.4830 - val_RMSE: 38.7188 - val_loss: 1499.2375 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7392 - loss: 1500.8226 - val_RMSE: 38.7197 - val_loss: 1499.3613 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7361 - loss: 1500.6488 - val_RMSE: 38.7140 - val_loss: 1498.9790 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7343 - loss: 1500.5675 - val_RMSE: 38.7124 - val_loss: 1498.9442 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7339 - loss: 1500.6177 - val_RMSE: 38.7120 - val_loss: 1498.9441 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7324 - loss: 1500.5244 - val_RMSE: 38.7200 - val_loss: 1499.5984 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7320 - loss: 1500.5154 - val_RMSE: 38.7109 - val_loss: 1498.8954 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7302 - loss: 1500.3951 - val_RMSE: 38.7111 - val_loss: 1498.9221 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7294 - loss: 1500.3396 - val_RMSE: 38.7103 - val_loss: 1498.8551 - learning_rate: 0.0100\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7279 - loss: 1500.2227 - val_RMSE: 38.7088 - val_loss: 1498.7704 - learning_rate: 0.0100\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7273 - loss: 1500.2106 - val_RMSE: 38.7176 - val_loss: 1499.4921 - learning_rate: 0.0100\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7250 - loss: 1500.0508 - val_RMSE: 38.7097 - val_loss: 1498.8783 - learning_rate: 0.0100\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7093 - loss: 1498.8319 - val_RMSE: 38.7046 - val_loss: 1498.4355 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7032 - loss: 1498.3120 - val_RMSE: 38.7037 - val_loss: 1498.3142 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7016 - loss: 1498.1428 - val_RMSE: 38.7032 - val_loss: 1498.2351 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6999 - loss: 1497.9823 - val_RMSE: 38.7032 - val_loss: 1498.2278 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7004 - loss: 1498.0023 - val_RMSE: 38.7027 - val_loss: 1498.1763 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7002 - loss: 1497.9768 - val_RMSE: 38.7027 - val_loss: 1498.1683 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6992 - loss: 1497.8938 - val_RMSE: 38.7028 - val_loss: 1498.1672 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6981 - loss: 1497.8031 - val_RMSE: 38.7024 - val_loss: 1498.1268 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6992 - loss: 1497.8782 - val_RMSE: 38.7026 - val_loss: 1498.1331 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6981 - loss: 1497.7848 - val_RMSE: 38.7024 - val_loss: 1498.1127 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6973 - loss: 1497.7227 - val_RMSE: 38.7025 - val_loss: 1498.1165 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6977 - loss: 1497.7505 - val_RMSE: 38.7022 - val_loss: 1498.0885 - learning_rate: 1.0000e-03\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6968 - loss: 1497.6738 - val_RMSE: 38.7017 - val_loss: 1498.0480 - learning_rate: 1.0000e-03\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 12:39:06,721] Trial 11 finished with value: 38.697016951832914 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.51}. Best is trial 10 with value: 38.69639193901346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 74s 21ms/step - RMSE: 42.5454 - loss: 1854.9471 - val_RMSE: 38.6995 - val_loss: 1497.7373 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7627 - loss: 1502.6493 - val_RMSE: 38.6971 - val_loss: 1497.6045 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7592 - loss: 1502.4337 - val_RMSE: 38.7496 - val_loss: 1501.7234 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7583 - loss: 1502.4194 - val_RMSE: 38.7543 - val_loss: 1502.1705 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7362 - loss: 1500.7649 - val_RMSE: 38.6852 - val_loss: 1496.7837 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7308 - loss: 1500.3091 - val_RMSE: 38.6849 - val_loss: 1496.7288 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7288 - loss: 1500.1240 - val_RMSE: 38.6829 - val_loss: 1496.5559 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7276 - loss: 1500.0189 - val_RMSE: 38.6828 - val_loss: 1496.5422 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7259 - loss: 1499.8719 - val_RMSE: 38.6826 - val_loss: 1496.5211 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7259 - loss: 1499.8679 - val_RMSE: 38.6829 - val_loss: 1496.5366 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7252 - loss: 1499.8112 - val_RMSE: 38.6820 - val_loss: 1496.4558 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7246 - loss: 1499.7582 - val_RMSE: 38.6812 - val_loss: 1496.3927 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7242 - loss: 1499.7212 - val_RMSE: 38.6823 - val_loss: 1496.4783 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7238 - loss: 1499.6906 - val_RMSE: 38.6807 - val_loss: 1496.3491 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7232 - loss: 1499.6357 - val_RMSE: 38.6805 - val_loss: 1496.3297 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7232 - loss: 1499.6365 - val_RMSE: 38.6818 - val_loss: 1496.4272 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7233 - loss: 1499.6421 - val_RMSE: 38.6816 - val_loss: 1496.4155 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7194 - loss: 1499.3417 - val_RMSE: 38.6799 - val_loss: 1496.2826 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7207 - loss: 1499.4402 - val_RMSE: 38.6798 - val_loss: 1496.2714 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7195 - loss: 1499.3458 - val_RMSE: 38.6798 - val_loss: 1496.2712 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7187 - loss: 1499.2803 - val_RMSE: 38.6797 - val_loss: 1496.2595 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7197 - loss: 1499.3569 - val_RMSE: 38.6798 - val_loss: 1496.2697 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7185 - loss: 1499.2653 - val_RMSE: 38.6797 - val_loss: 1496.2593 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7193 - loss: 1499.3254 - val_RMSE: 38.6797 - val_loss: 1496.2588 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7183 - loss: 1499.2494 - val_RMSE: 38.6796 - val_loss: 1496.2535 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 22ms/step - RMSE: 42.4935 - loss: 1850.0814 - val_RMSE: 38.7650 - val_loss: 1502.8108 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7115 - loss: 1498.6793 - val_RMSE: 38.7378 - val_loss: 1500.7560 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7101 - loss: 1498.6288 - val_RMSE: 38.7700 - val_loss: 1503.3134 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7092 - loss: 1498.6194 - val_RMSE: 38.7873 - val_loss: 1504.7332 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6848 - loss: 1496.7802 - val_RMSE: 38.7151 - val_loss: 1499.1012 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6799 - loss: 1496.3698 - val_RMSE: 38.7161 - val_loss: 1499.1444 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6780 - loss: 1496.1886 - val_RMSE: 38.7140 - val_loss: 1498.9586 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6769 - loss: 1496.0789 - val_RMSE: 38.7124 - val_loss: 1498.8197 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6766 - loss: 1496.0444 - val_RMSE: 38.7126 - val_loss: 1498.8304 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6754 - loss: 1495.9482 - val_RMSE: 38.7128 - val_loss: 1498.8425 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6726 - loss: 1495.7325 - val_RMSE: 38.7105 - val_loss: 1498.6559 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6718 - loss: 1495.6639 - val_RMSE: 38.7102 - val_loss: 1498.6392 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6720 - loss: 1495.6777 - val_RMSE: 38.7102 - val_loss: 1498.6306 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6712 - loss: 1495.6161 - val_RMSE: 38.7102 - val_loss: 1498.6342 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6718 - loss: 1495.6638 - val_RMSE: 38.7101 - val_loss: 1498.6268 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6712 - loss: 1495.6180 - val_RMSE: 38.7100 - val_loss: 1498.6187 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6708 - loss: 1495.5818 - val_RMSE: 38.7101 - val_loss: 1498.6206 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6708 - loss: 1495.5814 - val_RMSE: 38.7100 - val_loss: 1498.6171 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6708 - loss: 1495.5808 - val_RMSE: 38.7099 - val_loss: 1498.6080 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6703 - loss: 1495.5425 - val_RMSE: 38.7098 - val_loss: 1498.5999 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6694 - loss: 1495.4768 - val_RMSE: 38.7099 - val_loss: 1498.6024 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6703 - loss: 1495.5464 - val_RMSE: 38.7100 - val_loss: 1498.6090 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6697 - loss: 1495.4938 - val_RMSE: 38.7097 - val_loss: 1498.5909 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6698 - loss: 1495.5034 - val_RMSE: 38.7097 - val_loss: 1498.5928 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6707 - loss: 1495.5710 - val_RMSE: 38.7097 - val_loss: 1498.5911 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 76s 22ms/step - RMSE: 42.4911 - loss: 1849.3811 - val_RMSE: 38.7246 - val_loss: 1499.6827 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7394 - loss: 1500.8400 - val_RMSE: 38.7207 - val_loss: 1499.4369 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7360 - loss: 1500.6440 - val_RMSE: 38.7172 - val_loss: 1499.2285 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7335 - loss: 1500.5105 - val_RMSE: 38.7140 - val_loss: 1499.0635 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7334 - loss: 1500.5758 - val_RMSE: 38.7293 - val_loss: 1500.2998 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7323 - loss: 1500.5310 - val_RMSE: 38.7175 - val_loss: 1499.3792 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7100 - loss: 1498.7938 - val_RMSE: 38.7051 - val_loss: 1498.3750 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7060 - loss: 1498.4349 - val_RMSE: 38.7050 - val_loss: 1498.3312 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7039 - loss: 1498.2365 - val_RMSE: 38.7038 - val_loss: 1498.2150 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7032 - loss: 1498.1660 - val_RMSE: 38.7038 - val_loss: 1498.1979 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7037 - loss: 1498.1863 - val_RMSE: 38.7045 - val_loss: 1498.2383 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7022 - loss: 1498.0634 - val_RMSE: 38.7038 - val_loss: 1498.1794 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7016 - loss: 1498.0115 - val_RMSE: 38.7037 - val_loss: 1498.1683 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7017 - loss: 1498.0099 - val_RMSE: 38.7023 - val_loss: 1498.0494 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7009 - loss: 1497.9409 - val_RMSE: 38.7025 - val_loss: 1498.0668 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6998 - loss: 1497.8529 - val_RMSE: 38.7052 - val_loss: 1498.2646 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6978 - loss: 1497.6929 - val_RMSE: 38.7017 - val_loss: 1497.9984 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6965 - loss: 1497.5956 - val_RMSE: 38.7016 - val_loss: 1497.9866 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6972 - loss: 1497.6445 - val_RMSE: 38.7015 - val_loss: 1497.9839 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6969 - loss: 1497.6234 - val_RMSE: 38.7015 - val_loss: 1497.9811 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6967 - loss: 1497.6066 - val_RMSE: 38.7014 - val_loss: 1497.9744 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6967 - loss: 1497.6079 - val_RMSE: 38.7015 - val_loss: 1497.9800 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6968 - loss: 1497.6155 - val_RMSE: 38.7014 - val_loss: 1497.9667 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6965 - loss: 1497.5942 - val_RMSE: 38.7013 - val_loss: 1497.9626 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6966 - loss: 1497.5950 - val_RMSE: 38.7014 - val_loss: 1497.9647 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 13:36:03,173] Trial 12 finished with value: 38.69689101585856 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.51}. Best is trial 10 with value: 38.69639193901346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 79s 23ms/step - RMSE: 42.4840 - loss: 1849.2191 - val_RMSE: 38.6934 - val_loss: 1497.2781 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7648 - loss: 1502.8129 - val_RMSE: 38.7082 - val_loss: 1498.4542 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7573 - loss: 1502.2740 - val_RMSE: 38.6985 - val_loss: 1497.7383 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7313 - loss: 1500.2755 - val_RMSE: 38.6825 - val_loss: 1496.4785 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7261 - loss: 1499.8523 - val_RMSE: 38.6816 - val_loss: 1496.3989 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7243 - loss: 1499.7042 - val_RMSE: 38.6818 - val_loss: 1496.4041 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7246 - loss: 1499.7197 - val_RMSE: 38.6813 - val_loss: 1496.3663 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7236 - loss: 1499.6399 - val_RMSE: 38.6808 - val_loss: 1496.3215 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7221 - loss: 1499.5208 - val_RMSE: 38.6805 - val_loss: 1496.2953 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7216 - loss: 1499.4825 - val_RMSE: 38.6805 - val_loss: 1496.2943 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7210 - loss: 1499.4325 - val_RMSE: 38.6803 - val_loss: 1496.2826 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7202 - loss: 1499.3728 - val_RMSE: 38.6801 - val_loss: 1496.2661 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7203 - loss: 1499.3779 - val_RMSE: 38.6799 - val_loss: 1496.2491 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7201 - loss: 1499.3601 - val_RMSE: 38.6799 - val_loss: 1496.2450 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7195 - loss: 1499.3156 - val_RMSE: 38.6797 - val_loss: 1496.2339 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7190 - loss: 1499.2764 - val_RMSE: 38.6793 - val_loss: 1496.2025 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7189 - loss: 1499.2657 - val_RMSE: 38.6792 - val_loss: 1496.1925 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7180 - loss: 1499.1997 - val_RMSE: 38.6794 - val_loss: 1496.2124 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7178 - loss: 1499.1833 - val_RMSE: 38.6792 - val_loss: 1496.1980 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7150 - loss: 1498.9705 - val_RMSE: 38.6788 - val_loss: 1496.1636 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7146 - loss: 1498.9388 - val_RMSE: 38.6787 - val_loss: 1496.1552 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7137 - loss: 1498.8654 - val_RMSE: 38.6786 - val_loss: 1496.1539 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7135 - loss: 1498.8470 - val_RMSE: 38.6785 - val_loss: 1496.1456 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7132 - loss: 1498.8281 - val_RMSE: 38.6785 - val_loss: 1496.1425 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7130 - loss: 1498.8148 - val_RMSE: 38.6785 - val_loss: 1496.1400 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 22ms/step - RMSE: 42.4352 - loss: 1844.7493 - val_RMSE: 38.7370 - val_loss: 1500.6536 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7122 - loss: 1498.7462 - val_RMSE: 38.9202 - val_loss: 1514.9114 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7080 - loss: 1498.4517 - val_RMSE: 38.7975 - val_loss: 1505.4037 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6815 - loss: 1496.4142 - val_RMSE: 38.7121 - val_loss: 1498.7675 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6766 - loss: 1496.0156 - val_RMSE: 38.7128 - val_loss: 1498.8048 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6750 - loss: 1495.8822 - val_RMSE: 38.7119 - val_loss: 1498.7301 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6747 - loss: 1495.8561 - val_RMSE: 38.7140 - val_loss: 1498.8893 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6729 - loss: 1495.7103 - val_RMSE: 38.7100 - val_loss: 1498.5811 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6726 - loss: 1495.6862 - val_RMSE: 38.7130 - val_loss: 1498.8066 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6722 - loss: 1495.6589 - val_RMSE: 38.7102 - val_loss: 1498.5916 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6697 - loss: 1495.4633 - val_RMSE: 38.7088 - val_loss: 1498.4849 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6677 - loss: 1495.3060 - val_RMSE: 38.7087 - val_loss: 1498.4746 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6677 - loss: 1495.3052 - val_RMSE: 38.7086 - val_loss: 1498.4678 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6677 - loss: 1495.3059 - val_RMSE: 38.7087 - val_loss: 1498.4773 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6675 - loss: 1495.2896 - val_RMSE: 38.7087 - val_loss: 1498.4790 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.6673 - loss: 1495.2719 - val_RMSE: 38.7084 - val_loss: 1498.4567 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6674 - loss: 1495.2827 - val_RMSE: 38.7085 - val_loss: 1498.4569 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6671 - loss: 1495.2559 - val_RMSE: 38.7084 - val_loss: 1498.4562 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6669 - loss: 1495.2449 - val_RMSE: 38.7085 - val_loss: 1498.4579 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6676 - loss: 1495.2941 - val_RMSE: 38.7085 - val_loss: 1498.4585 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6664 - loss: 1495.2024 - val_RMSE: 38.7085 - val_loss: 1498.4572 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6665 - loss: 1495.2089 - val_RMSE: 38.7085 - val_loss: 1498.4578 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6667 - loss: 1495.2236 - val_RMSE: 38.7085 - val_loss: 1498.4581 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6674 - loss: 1495.2811 - val_RMSE: 38.7085 - val_loss: 1498.4581 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6672 - loss: 1495.2626 - val_RMSE: 38.7085 - val_loss: 1498.4592 - learning_rate: 1.0000e-08\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 79s 23ms/step - RMSE: 42.4154 - loss: 1842.2605 - val_RMSE: 38.7378 - val_loss: 1500.7169 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7380 - loss: 1500.7380 - val_RMSE: 38.7167 - val_loss: 1499.1106 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7332 - loss: 1500.3988 - val_RMSE: 38.7221 - val_loss: 1499.5569 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7315 - loss: 1500.2834 - val_RMSE: 38.7282 - val_loss: 1500.0714 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7095 - loss: 1498.6190 - val_RMSE: 38.7052 - val_loss: 1498.2667 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7037 - loss: 1498.1505 - val_RMSE: 38.7050 - val_loss: 1498.2356 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7024 - loss: 1498.0319 - val_RMSE: 38.7082 - val_loss: 1498.4716 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7004 - loss: 1497.8693 - val_RMSE: 38.7076 - val_loss: 1498.4224 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6985 - loss: 1497.7156 - val_RMSE: 38.7030 - val_loss: 1498.0621 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6978 - loss: 1497.6650 - val_RMSE: 38.7030 - val_loss: 1498.0579 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6970 - loss: 1497.6001 - val_RMSE: 38.7029 - val_loss: 1498.0542 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6973 - loss: 1497.6226 - val_RMSE: 38.7029 - val_loss: 1498.0524 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6966 - loss: 1497.5688 - val_RMSE: 38.7028 - val_loss: 1498.0455 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6965 - loss: 1497.5565 - val_RMSE: 38.7028 - val_loss: 1498.0475 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6967 - loss: 1497.5741 - val_RMSE: 38.7028 - val_loss: 1498.0427 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6956 - loss: 1497.4869 - val_RMSE: 38.7027 - val_loss: 1498.0345 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6959 - loss: 1497.5134 - val_RMSE: 38.7027 - val_loss: 1498.0358 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6963 - loss: 1497.5385 - val_RMSE: 38.7028 - val_loss: 1498.0405 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6953 - loss: 1497.4650 - val_RMSE: 38.7026 - val_loss: 1498.0243 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6956 - loss: 1497.4849 - val_RMSE: 38.7026 - val_loss: 1498.0255 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6953 - loss: 1497.4639 - val_RMSE: 38.7026 - val_loss: 1498.0247 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6955 - loss: 1497.4780 - val_RMSE: 38.7025 - val_loss: 1498.0232 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6957 - loss: 1497.4924 - val_RMSE: 38.7025 - val_loss: 1498.0223 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6955 - loss: 1497.4799 - val_RMSE: 38.7026 - val_loss: 1498.0228 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6956 - loss: 1497.4906 - val_RMSE: 38.7025 - val_loss: 1498.0214 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 14:34:32,143] Trial 13 finished with value: 38.69649960900768 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.3}. Best is trial 10 with value: 38.69639193901346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 22ms/step - RMSE: 42.5287 - loss: 1853.8055 - val_RMSE: 38.7054 - val_loss: 1498.5800 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7630 - loss: 1503.0297 - val_RMSE: 38.6975 - val_loss: 1497.9768 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7657 - loss: 1503.3575 - val_RMSE: 38.7002 - val_loss: 1498.6407 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7660 - loss: 1503.6688 - val_RMSE: 38.7020 - val_loss: 1498.3610 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7356 - loss: 1500.9191 - val_RMSE: 38.6849 - val_loss: 1496.8853 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7299 - loss: 1500.3511 - val_RMSE: 38.6835 - val_loss: 1496.7177 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7287 - loss: 1500.2053 - val_RMSE: 38.6832 - val_loss: 1496.6615 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7277 - loss: 1500.1017 - val_RMSE: 38.6828 - val_loss: 1496.6113 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7275 - loss: 1500.0709 - val_RMSE: 38.6824 - val_loss: 1496.5712 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7263 - loss: 1499.9668 - val_RMSE: 38.6821 - val_loss: 1496.5414 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7255 - loss: 1499.8954 - val_RMSE: 38.6819 - val_loss: 1496.5217 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7252 - loss: 1499.8729 - val_RMSE: 38.6817 - val_loss: 1496.5000 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7256 - loss: 1499.8992 - val_RMSE: 38.6820 - val_loss: 1496.5269 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7241 - loss: 1499.7891 - val_RMSE: 38.6811 - val_loss: 1496.4558 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7243 - loss: 1499.8026 - val_RMSE: 38.6819 - val_loss: 1496.5179 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7245 - loss: 1499.8167 - val_RMSE: 38.6807 - val_loss: 1496.4263 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7233 - loss: 1499.7279 - val_RMSE: 38.6832 - val_loss: 1496.6207 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7233 - loss: 1499.7249 - val_RMSE: 38.6808 - val_loss: 1496.4384 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7209 - loss: 1499.5383 - val_RMSE: 38.6802 - val_loss: 1496.3885 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7207 - loss: 1499.5215 - val_RMSE: 38.6800 - val_loss: 1496.3699 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7193 - loss: 1499.4126 - val_RMSE: 38.6799 - val_loss: 1496.3608 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7189 - loss: 1499.3756 - val_RMSE: 38.6799 - val_loss: 1496.3528 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7193 - loss: 1499.4061 - val_RMSE: 38.6797 - val_loss: 1496.3370 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7179 - loss: 1499.2958 - val_RMSE: 38.6795 - val_loss: 1496.3242 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7187 - loss: 1499.3562 - val_RMSE: 38.6794 - val_loss: 1496.3119 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 23ms/step - RMSE: 42.4861 - loss: 1849.8591 - val_RMSE: 38.7356 - val_loss: 1500.9111 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7117 - loss: 1499.0421 - val_RMSE: 38.7597 - val_loss: 1502.7534 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7095 - loss: 1498.9026 - val_RMSE: 38.7462 - val_loss: 1501.7158 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6874 - loss: 1497.1250 - val_RMSE: 38.7172 - val_loss: 1499.3347 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6812 - loss: 1496.5389 - val_RMSE: 38.7161 - val_loss: 1499.2028 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6787 - loss: 1496.2966 - val_RMSE: 38.7183 - val_loss: 1499.3510 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6776 - loss: 1496.1989 - val_RMSE: 38.7220 - val_loss: 1499.6229 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6741 - loss: 1495.9147 - val_RMSE: 38.7121 - val_loss: 1498.8541 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6734 - loss: 1495.8586 - val_RMSE: 38.7121 - val_loss: 1498.8473 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6738 - loss: 1495.8831 - val_RMSE: 38.7120 - val_loss: 1498.8340 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6723 - loss: 1495.7631 - val_RMSE: 38.7119 - val_loss: 1498.8274 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6727 - loss: 1495.7920 - val_RMSE: 38.7119 - val_loss: 1498.8209 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6721 - loss: 1495.7458 - val_RMSE: 38.7118 - val_loss: 1498.8116 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6731 - loss: 1495.8184 - val_RMSE: 38.7119 - val_loss: 1498.8229 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6733 - loss: 1495.8342 - val_RMSE: 38.7117 - val_loss: 1498.8049 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6716 - loss: 1495.6973 - val_RMSE: 38.7118 - val_loss: 1498.8026 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6720 - loss: 1495.7220 - val_RMSE: 38.7122 - val_loss: 1498.8331 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6716 - loss: 1495.6985 - val_RMSE: 38.7118 - val_loss: 1498.8062 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6713 - loss: 1495.6648 - val_RMSE: 38.7112 - val_loss: 1498.7495 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6713 - loss: 1495.6708 - val_RMSE: 38.7111 - val_loss: 1498.7501 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6704 - loss: 1495.6027 - val_RMSE: 38.7111 - val_loss: 1498.7461 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6711 - loss: 1495.6525 - val_RMSE: 38.7111 - val_loss: 1498.7478 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6711 - loss: 1495.6549 - val_RMSE: 38.7110 - val_loss: 1498.7437 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6716 - loss: 1495.6941 - val_RMSE: 38.7111 - val_loss: 1498.7457 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6716 - loss: 1495.6864 - val_RMSE: 38.7111 - val_loss: 1498.7423 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 23ms/step - RMSE: 42.4760 - loss: 1848.3408 - val_RMSE: 38.7197 - val_loss: 1499.6753 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7383 - loss: 1501.0938 - val_RMSE: 38.7257 - val_loss: 1500.1675 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7364 - loss: 1501.0111 - val_RMSE: 38.7149 - val_loss: 1499.3143 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7342 - loss: 1500.8146 - val_RMSE: 38.7126 - val_loss: 1499.1143 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7345 - loss: 1500.9221 - val_RMSE: 38.7247 - val_loss: 1500.1985 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7330 - loss: 1500.8319 - val_RMSE: 38.7125 - val_loss: 1499.2510 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7118 - loss: 1499.1384 - val_RMSE: 38.7075 - val_loss: 1498.6589 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7064 - loss: 1498.5497 - val_RMSE: 38.7074 - val_loss: 1498.5823 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7036 - loss: 1498.2848 - val_RMSE: 38.7068 - val_loss: 1498.5071 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7034 - loss: 1498.2386 - val_RMSE: 38.7051 - val_loss: 1498.3574 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7028 - loss: 1498.1736 - val_RMSE: 38.7045 - val_loss: 1498.2965 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7023 - loss: 1498.1298 - val_RMSE: 38.7043 - val_loss: 1498.2754 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7019 - loss: 1498.0898 - val_RMSE: 38.7040 - val_loss: 1498.2423 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7013 - loss: 1498.0333 - val_RMSE: 38.7041 - val_loss: 1498.2477 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7014 - loss: 1498.0406 - val_RMSE: 38.7031 - val_loss: 1498.1689 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7010 - loss: 1498.0054 - val_RMSE: 38.7024 - val_loss: 1498.1147 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7003 - loss: 1497.9468 - val_RMSE: 38.7021 - val_loss: 1498.0864 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7007 - loss: 1497.9839 - val_RMSE: 38.7016 - val_loss: 1498.0537 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6991 - loss: 1497.8586 - val_RMSE: 38.7018 - val_loss: 1498.0659 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7004 - loss: 1497.9569 - val_RMSE: 38.7014 - val_loss: 1498.0419 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6993 - loss: 1497.8721 - val_RMSE: 38.7019 - val_loss: 1498.0808 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6993 - loss: 1497.8809 - val_RMSE: 38.7020 - val_loss: 1498.0894 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6959 - loss: 1497.6172 - val_RMSE: 38.7005 - val_loss: 1497.9667 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6954 - loss: 1497.5730 - val_RMSE: 38.7004 - val_loss: 1497.9561 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6948 - loss: 1497.5210 - val_RMSE: 38.7005 - val_loss: 1497.9589 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 15:32:16,743] Trial 14 finished with value: 38.6969810338204 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.001, 'exite_units': 32, 'dropout_rate': 0.48}. Best is trial 10 with value: 38.69639193901346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 80s 23ms/step - RMSE: 42.4857 - loss: 1849.3625 - val_RMSE: 38.6971 - val_loss: 1497.5635 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7649 - loss: 1502.8230 - val_RMSE: 38.7054 - val_loss: 1498.2340 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7576 - loss: 1502.2932 - val_RMSE: 38.7002 - val_loss: 1497.8624 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7315 - loss: 1500.2834 - val_RMSE: 38.6832 - val_loss: 1496.5336 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7269 - loss: 1499.9165 - val_RMSE: 38.6823 - val_loss: 1496.4458 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7257 - loss: 1499.8042 - val_RMSE: 38.6819 - val_loss: 1496.4099 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7251 - loss: 1499.7568 - val_RMSE: 38.6815 - val_loss: 1496.3754 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7240 - loss: 1499.6635 - val_RMSE: 38.6809 - val_loss: 1496.3256 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7227 - loss: 1499.5636 - val_RMSE: 38.6810 - val_loss: 1496.3370 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7228 - loss: 1499.5741 - val_RMSE: 38.6810 - val_loss: 1496.3340 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7193 - loss: 1499.3010 - val_RMSE: 38.6804 - val_loss: 1496.2889 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7187 - loss: 1499.2478 - val_RMSE: 38.6802 - val_loss: 1496.2727 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7192 - loss: 1499.2906 - val_RMSE: 38.6802 - val_loss: 1496.2681 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7186 - loss: 1499.2443 - val_RMSE: 38.6801 - val_loss: 1496.2627 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7181 - loss: 1499.2064 - val_RMSE: 38.6801 - val_loss: 1496.2590 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7181 - loss: 1499.2058 - val_RMSE: 38.6799 - val_loss: 1496.2482 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7182 - loss: 1499.2100 - val_RMSE: 38.6800 - val_loss: 1496.2495 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7171 - loss: 1499.1285 - val_RMSE: 38.6800 - val_loss: 1496.2487 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7171 - loss: 1499.1272 - val_RMSE: 38.6800 - val_loss: 1496.2480 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7170 - loss: 1499.1135 - val_RMSE: 38.6800 - val_loss: 1496.2528 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7169 - loss: 1499.1067 - val_RMSE: 38.6800 - val_loss: 1496.2521 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7174 - loss: 1499.1467 - val_RMSE: 38.6800 - val_loss: 1496.2518 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7177 - loss: 1499.1696 - val_RMSE: 38.6800 - val_loss: 1496.2506 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7171 - loss: 1499.1250 - val_RMSE: 38.6800 - val_loss: 1496.2516 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7180 - loss: 1499.1942 - val_RMSE: 38.6800 - val_loss: 1496.2527 - learning_rate: 1.0000e-07\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 76s 22ms/step - RMSE: 42.4345 - loss: 1844.6891 - val_RMSE: 38.7381 - val_loss: 1500.7383 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7136 - loss: 1498.8483 - val_RMSE: 38.7610 - val_loss: 1502.5520 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7077 - loss: 1498.4336 - val_RMSE: 38.7396 - val_loss: 1500.9247 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6832 - loss: 1496.5571 - val_RMSE: 38.7141 - val_loss: 1498.9348 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6768 - loss: 1496.0408 - val_RMSE: 38.7123 - val_loss: 1498.7817 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6750 - loss: 1495.8918 - val_RMSE: 38.7116 - val_loss: 1498.7147 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6742 - loss: 1495.8232 - val_RMSE: 38.7211 - val_loss: 1499.4482 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6736 - loss: 1495.7708 - val_RMSE: 38.7184 - val_loss: 1499.2363 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6701 - loss: 1495.4993 - val_RMSE: 38.7101 - val_loss: 1498.5948 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6699 - loss: 1495.4857 - val_RMSE: 38.7100 - val_loss: 1498.5852 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6691 - loss: 1495.4248 - val_RMSE: 38.7100 - val_loss: 1498.5825 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6684 - loss: 1495.3688 - val_RMSE: 38.7098 - val_loss: 1498.5714 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6692 - loss: 1495.4316 - val_RMSE: 38.7097 - val_loss: 1498.5641 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6688 - loss: 1495.3966 - val_RMSE: 38.7098 - val_loss: 1498.5649 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6683 - loss: 1495.3616 - val_RMSE: 38.7097 - val_loss: 1498.5625 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6691 - loss: 1495.4211 - val_RMSE: 38.7097 - val_loss: 1498.5587 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6687 - loss: 1495.3888 - val_RMSE: 38.7098 - val_loss: 1498.5665 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6680 - loss: 1495.3335 - val_RMSE: 38.7096 - val_loss: 1498.5536 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6681 - loss: 1495.3417 - val_RMSE: 38.7096 - val_loss: 1498.5553 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6682 - loss: 1495.3475 - val_RMSE: 38.7096 - val_loss: 1498.5521 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6678 - loss: 1495.3213 - val_RMSE: 38.7096 - val_loss: 1498.5496 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6671 - loss: 1495.2609 - val_RMSE: 38.7100 - val_loss: 1498.5808 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6670 - loss: 1495.2559 - val_RMSE: 38.7096 - val_loss: 1498.5516 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6677 - loss: 1495.3126 - val_RMSE: 38.7095 - val_loss: 1498.5397 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6675 - loss: 1495.2911 - val_RMSE: 38.7094 - val_loss: 1498.5386 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 79s 22ms/step - RMSE: 42.4150 - loss: 1842.2375 - val_RMSE: 38.7279 - val_loss: 1499.9456 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7383 - loss: 1500.7593 - val_RMSE: 38.7185 - val_loss: 1499.2551 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7335 - loss: 1500.4197 - val_RMSE: 38.7176 - val_loss: 1499.2031 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7316 - loss: 1500.2904 - val_RMSE: 38.7206 - val_loss: 1499.4814 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7317 - loss: 1500.3628 - val_RMSE: 38.7119 - val_loss: 1498.8555 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7290 - loss: 1500.1753 - val_RMSE: 38.7801 - val_loss: 1504.1531 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7300 - loss: 1500.2683 - val_RMSE: 38.7432 - val_loss: 1501.3348 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7076 - loss: 1498.5702 - val_RMSE: 38.7043 - val_loss: 1498.2850 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7032 - loss: 1498.1859 - val_RMSE: 38.7035 - val_loss: 1498.1843 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7009 - loss: 1497.9799 - val_RMSE: 38.7051 - val_loss: 1498.2916 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6998 - loss: 1497.8763 - val_RMSE: 38.7052 - val_loss: 1498.2866 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6966 - loss: 1497.6306 - val_RMSE: 38.7024 - val_loss: 1498.0695 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6965 - loss: 1497.6145 - val_RMSE: 38.7024 - val_loss: 1498.0725 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6961 - loss: 1497.5856 - val_RMSE: 38.7023 - val_loss: 1498.0599 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6944 - loss: 1497.4553 - val_RMSE: 38.7022 - val_loss: 1498.0576 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.6962 - loss: 1497.5906 - val_RMSE: 38.7023 - val_loss: 1498.0608 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6955 - loss: 1497.5365 - val_RMSE: 38.7023 - val_loss: 1498.0621 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6953 - loss: 1497.5198 - val_RMSE: 38.7021 - val_loss: 1498.0496 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6944 - loss: 1497.4528 - val_RMSE: 38.7021 - val_loss: 1498.0499 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6947 - loss: 1497.4705 - val_RMSE: 38.7021 - val_loss: 1498.0460 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6943 - loss: 1497.4470 - val_RMSE: 38.7021 - val_loss: 1498.0466 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6942 - loss: 1497.4380 - val_RMSE: 38.7021 - val_loss: 1498.0433 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6946 - loss: 1497.4672 - val_RMSE: 38.7021 - val_loss: 1498.0447 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6944 - loss: 1497.4514 - val_RMSE: 38.7021 - val_loss: 1498.0441 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6946 - loss: 1497.4648 - val_RMSE: 38.7021 - val_loss: 1498.0432 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 16:30:50,070] Trial 15 finished with value: 38.697166689195434 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.3}. Best is trial 10 with value: 38.69639193901346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 23ms/step - RMSE: 42.4981 - loss: 1850.5037 - val_RMSE: 38.6963 - val_loss: 1497.5016 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7623 - loss: 1502.6176 - val_RMSE: 38.7209 - val_loss: 1499.4478 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7574 - loss: 1502.2920 - val_RMSE: 38.6940 - val_loss: 1497.4031 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7550 - loss: 1502.1379 - val_RMSE: 38.7279 - val_loss: 1500.0826 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7554 - loss: 1502.2418 - val_RMSE: 38.6965 - val_loss: 1497.6833 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7317 - loss: 1500.4010 - val_RMSE: 38.6830 - val_loss: 1496.6028 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7268 - loss: 1499.9836 - val_RMSE: 38.6823 - val_loss: 1496.5134 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7254 - loss: 1499.8448 - val_RMSE: 38.6819 - val_loss: 1496.4703 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7236 - loss: 1499.6942 - val_RMSE: 38.6818 - val_loss: 1496.4542 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7232 - loss: 1499.6616 - val_RMSE: 38.6815 - val_loss: 1496.4235 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7222 - loss: 1499.5798 - val_RMSE: 38.6816 - val_loss: 1496.4313 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7210 - loss: 1499.4852 - val_RMSE: 38.6813 - val_loss: 1496.4041 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7215 - loss: 1499.5121 - val_RMSE: 38.6817 - val_loss: 1496.4287 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7205 - loss: 1499.4357 - val_RMSE: 38.6811 - val_loss: 1496.3839 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7200 - loss: 1499.3917 - val_RMSE: 38.6813 - val_loss: 1496.3939 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7204 - loss: 1499.4235 - val_RMSE: 38.6811 - val_loss: 1496.3776 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7200 - loss: 1499.3947 - val_RMSE: 38.6811 - val_loss: 1496.3811 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7181 - loss: 1499.2395 - val_RMSE: 38.6813 - val_loss: 1496.3960 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7156 - loss: 1499.0453 - val_RMSE: 38.6809 - val_loss: 1496.3636 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7165 - loss: 1499.1150 - val_RMSE: 38.6808 - val_loss: 1496.3486 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7141 - loss: 1498.9266 - val_RMSE: 38.6807 - val_loss: 1496.3439 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7148 - loss: 1498.9814 - val_RMSE: 38.6808 - val_loss: 1496.3488 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7139 - loss: 1498.9164 - val_RMSE: 38.6806 - val_loss: 1496.3385 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7141 - loss: 1498.9318 - val_RMSE: 38.6807 - val_loss: 1496.3427 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7145 - loss: 1498.9629 - val_RMSE: 38.6806 - val_loss: 1496.3375 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 75s 22ms/step - RMSE: 42.4445 - loss: 1845.5342 - val_RMSE: 38.8032 - val_loss: 1505.7772 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7114 - loss: 1498.6771 - val_RMSE: 38.7484 - val_loss: 1501.5647 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7080 - loss: 1498.4508 - val_RMSE: 38.7318 - val_loss: 1500.3206 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7062 - loss: 1498.3441 - val_RMSE: 38.7752 - val_loss: 1503.7255 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7043 - loss: 1498.2444 - val_RMSE: 38.7236 - val_loss: 1499.7758 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7038 - loss: 1498.2539 - val_RMSE: 38.7191 - val_loss: 1499.4360 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7022 - loss: 1498.1375 - val_RMSE: 38.7444 - val_loss: 1501.4236 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7007 - loss: 1498.0527 - val_RMSE: 38.7353 - val_loss: 1500.7444 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6800 - loss: 1496.4556 - val_RMSE: 38.7110 - val_loss: 1498.8225 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6754 - loss: 1496.0608 - val_RMSE: 38.7107 - val_loss: 1498.7576 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6728 - loss: 1495.8158 - val_RMSE: 38.7099 - val_loss: 1498.6703 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6713 - loss: 1495.6868 - val_RMSE: 38.7098 - val_loss: 1498.6604 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6713 - loss: 1495.6786 - val_RMSE: 38.7094 - val_loss: 1498.6149 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6704 - loss: 1495.6036 - val_RMSE: 38.7097 - val_loss: 1498.6421 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6702 - loss: 1495.5826 - val_RMSE: 38.7091 - val_loss: 1498.5927 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6691 - loss: 1495.4940 - val_RMSE: 38.7105 - val_loss: 1498.6881 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6692 - loss: 1495.4949 - val_RMSE: 38.7099 - val_loss: 1498.6399 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6655 - loss: 1495.2065 - val_RMSE: 38.7080 - val_loss: 1498.4945 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6659 - loss: 1495.2365 - val_RMSE: 38.7080 - val_loss: 1498.4926 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6649 - loss: 1495.1605 - val_RMSE: 38.7081 - val_loss: 1498.5062 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.6648 - loss: 1495.1509 - val_RMSE: 38.7081 - val_loss: 1498.5032 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6649 - loss: 1495.1566 - val_RMSE: 38.7082 - val_loss: 1498.5077 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6643 - loss: 1495.1102 - val_RMSE: 38.7082 - val_loss: 1498.5088 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6653 - loss: 1495.1877 - val_RMSE: 38.7081 - val_loss: 1498.5046 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6643 - loss: 1495.1132 - val_RMSE: 38.7082 - val_loss: 1498.5049 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 78s 22ms/step - RMSE: 42.4260 - loss: 1843.2390 - val_RMSE: 38.7210 - val_loss: 1499.4088 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7385 - loss: 1500.7737 - val_RMSE: 38.7177 - val_loss: 1499.2023 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7336 - loss: 1500.4418 - val_RMSE: 38.7119 - val_loss: 1498.7880 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7324 - loss: 1500.3851 - val_RMSE: 38.7148 - val_loss: 1499.0641 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7318 - loss: 1500.3986 - val_RMSE: 38.7109 - val_loss: 1498.8004 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7075 - loss: 1498.5331 - val_RMSE: 38.7048 - val_loss: 1498.2982 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7023 - loss: 1498.0941 - val_RMSE: 38.7045 - val_loss: 1498.2368 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7009 - loss: 1497.9562 - val_RMSE: 38.7037 - val_loss: 1498.1660 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6999 - loss: 1497.8639 - val_RMSE: 38.7026 - val_loss: 1498.0702 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6992 - loss: 1497.8096 - val_RMSE: 38.7025 - val_loss: 1498.0599 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6998 - loss: 1497.8486 - val_RMSE: 38.7018 - val_loss: 1498.0046 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6986 - loss: 1497.7546 - val_RMSE: 38.7017 - val_loss: 1497.9908 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6983 - loss: 1497.7262 - val_RMSE: 38.7024 - val_loss: 1498.0382 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6977 - loss: 1497.6782 - val_RMSE: 38.7019 - val_loss: 1498.0034 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6940 - loss: 1497.3931 - val_RMSE: 38.7012 - val_loss: 1497.9509 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6941 - loss: 1497.4016 - val_RMSE: 38.7013 - val_loss: 1497.9520 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6942 - loss: 1497.4048 - val_RMSE: 38.7013 - val_loss: 1497.9541 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6940 - loss: 1497.3899 - val_RMSE: 38.7013 - val_loss: 1497.9535 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6941 - loss: 1497.3928 - val_RMSE: 38.7013 - val_loss: 1497.9519 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6934 - loss: 1497.3378 - val_RMSE: 38.7013 - val_loss: 1497.9541 - learning_rate: 1.0000e-06\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.6931 - loss: 1497.3193 - val_RMSE: 38.7013 - val_loss: 1497.9536 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.6926 - loss: 1497.2772 - val_RMSE: 38.7013 - val_loss: 1497.9498 - learning_rate: 1.0000e-07\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6935 - loss: 1497.3521 - val_RMSE: 38.7013 - val_loss: 1497.9507 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6936 - loss: 1497.3589 - val_RMSE: 38.7013 - val_loss: 1497.9536 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6938 - loss: 1497.3738 - val_RMSE: 38.7013 - val_loss: 1497.9504 - learning_rate: 1.0000e-08\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 17:29:30,938] Trial 16 finished with value: 38.696688941023034 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.36}. Best is trial 10 with value: 38.69639193901346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 79s 23ms/step - RMSE: 42.5283 - loss: 1853.3069 - val_RMSE: 38.7070 - val_loss: 1498.3182 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7635 - loss: 1502.7095 - val_RMSE: 38.7113 - val_loss: 1498.7063 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7589 - loss: 1502.4154 - val_RMSE: 38.7004 - val_loss: 1497.9165 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7574 - loss: 1502.3427 - val_RMSE: 38.7119 - val_loss: 1498.8721 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7580 - loss: 1502.4866 - val_RMSE: 38.6895 - val_loss: 1497.2085 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7575 - loss: 1502.4888 - val_RMSE: 38.7197 - val_loss: 1499.5643 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7555 - loss: 1502.3405 - val_RMSE: 38.7006 - val_loss: 1498.0774 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7340 - loss: 1500.6576 - val_RMSE: 38.6841 - val_loss: 1496.7596 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7288 - loss: 1500.2091 - val_RMSE: 38.6827 - val_loss: 1496.6069 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7271 - loss: 1500.0425 - val_RMSE: 38.6823 - val_loss: 1496.5549 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7251 - loss: 1499.8676 - val_RMSE: 38.6825 - val_loss: 1496.5554 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7251 - loss: 1499.8508 - val_RMSE: 38.6819 - val_loss: 1496.5000 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7247 - loss: 1499.8108 - val_RMSE: 38.6816 - val_loss: 1496.4758 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7243 - loss: 1499.7806 - val_RMSE: 38.6811 - val_loss: 1496.4242 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7245 - loss: 1499.7888 - val_RMSE: 38.6824 - val_loss: 1496.5200 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7228 - loss: 1499.6500 - val_RMSE: 38.6815 - val_loss: 1496.4432 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7214 - loss: 1499.5366 - val_RMSE: 38.6805 - val_loss: 1496.3679 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7207 - loss: 1499.4800 - val_RMSE: 38.6801 - val_loss: 1496.3372 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7205 - loss: 1499.4657 - val_RMSE: 38.6801 - val_loss: 1496.3385 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7198 - loss: 1499.4092 - val_RMSE: 38.6801 - val_loss: 1496.3373 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7195 - loss: 1499.3844 - val_RMSE: 38.6801 - val_loss: 1496.3325 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7189 - loss: 1499.3392 - val_RMSE: 38.6801 - val_loss: 1496.3318 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7198 - loss: 1499.4050 - val_RMSE: 38.6801 - val_loss: 1496.3319 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7192 - loss: 1499.3622 - val_RMSE: 38.6801 - val_loss: 1496.3317 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7189 - loss: 1499.3389 - val_RMSE: 38.6800 - val_loss: 1496.3308 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 76s 22ms/step - RMSE: 42.4837 - loss: 1849.2068 - val_RMSE: 38.7347 - val_loss: 1500.4691 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7111 - loss: 1498.6539 - val_RMSE: 38.7722 - val_loss: 1503.4164 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7089 - loss: 1498.5344 - val_RMSE: 38.7720 - val_loss: 1503.4480 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6866 - loss: 1496.8278 - val_RMSE: 38.7180 - val_loss: 1499.2413 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6803 - loss: 1496.3236 - val_RMSE: 38.7156 - val_loss: 1499.0381 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6790 - loss: 1496.2024 - val_RMSE: 38.7184 - val_loss: 1499.2455 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6779 - loss: 1496.1122 - val_RMSE: 38.7141 - val_loss: 1498.9045 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6764 - loss: 1495.9913 - val_RMSE: 38.7146 - val_loss: 1498.9406 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6754 - loss: 1495.9106 - val_RMSE: 38.7128 - val_loss: 1498.8013 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6757 - loss: 1495.9258 - val_RMSE: 38.7128 - val_loss: 1498.7947 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6745 - loss: 1495.8353 - val_RMSE: 38.7121 - val_loss: 1498.7437 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6750 - loss: 1495.8712 - val_RMSE: 38.7147 - val_loss: 1498.9425 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6741 - loss: 1495.8060 - val_RMSE: 38.7126 - val_loss: 1498.7836 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6715 - loss: 1495.5968 - val_RMSE: 38.7099 - val_loss: 1498.5729 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6718 - loss: 1495.6216 - val_RMSE: 38.7099 - val_loss: 1498.5669 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6706 - loss: 1495.5270 - val_RMSE: 38.7097 - val_loss: 1498.5568 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6705 - loss: 1495.5242 - val_RMSE: 38.7096 - val_loss: 1498.5480 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6700 - loss: 1495.4823 - val_RMSE: 38.7098 - val_loss: 1498.5569 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6700 - loss: 1495.4823 - val_RMSE: 38.7098 - val_loss: 1498.5565 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6690 - loss: 1495.4070 - val_RMSE: 38.7094 - val_loss: 1498.5333 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6694 - loss: 1495.4336 - val_RMSE: 38.7094 - val_loss: 1498.5319 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6691 - loss: 1495.4104 - val_RMSE: 38.7094 - val_loss: 1498.5316 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6700 - loss: 1495.4777 - val_RMSE: 38.7094 - val_loss: 1498.5303 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6700 - loss: 1495.4830 - val_RMSE: 38.7094 - val_loss: 1498.5320 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6699 - loss: 1495.4760 - val_RMSE: 38.7094 - val_loss: 1498.5311 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 79s 23ms/step - RMSE: 42.4739 - loss: 1847.7129 - val_RMSE: 38.7266 - val_loss: 1499.8342 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7375 - loss: 1500.6921 - val_RMSE: 38.7292 - val_loss: 1500.0906 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7348 - loss: 1500.5436 - val_RMSE: 38.7181 - val_loss: 1499.2861 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7335 - loss: 1500.4980 - val_RMSE: 38.7363 - val_loss: 1500.7673 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7373 - loss: 1500.8741 - val_RMSE: 38.7143 - val_loss: 1499.2373 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7424 - loss: 1501.4255 - val_RMSE: 38.7305 - val_loss: 1500.4683 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7314 - loss: 1500.5087 - val_RMSE: 38.7135 - val_loss: 1499.0763 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7304 - loss: 1500.3705 - val_RMSE: 38.7094 - val_loss: 1498.7418 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7289 - loss: 1500.2509 - val_RMSE: 38.7084 - val_loss: 1498.6833 - learning_rate: 0.0100\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7281 - loss: 1500.2133 - val_RMSE: 38.7223 - val_loss: 1499.7800 - learning_rate: 0.0100\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7263 - loss: 1500.0819 - val_RMSE: 38.7183 - val_loss: 1499.4838 - learning_rate: 0.0100\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7079 - loss: 1498.6655 - val_RMSE: 38.7047 - val_loss: 1498.3898 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7038 - loss: 1498.3033 - val_RMSE: 38.7035 - val_loss: 1498.2546 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7019 - loss: 1498.1228 - val_RMSE: 38.7037 - val_loss: 1498.2418 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7011 - loss: 1498.0441 - val_RMSE: 38.7033 - val_loss: 1498.2034 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.6999 - loss: 1497.9335 - val_RMSE: 38.7032 - val_loss: 1498.1840 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6995 - loss: 1497.8944 - val_RMSE: 38.7028 - val_loss: 1498.1489 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6992 - loss: 1497.8635 - val_RMSE: 38.7028 - val_loss: 1498.1383 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6997 - loss: 1497.8965 - val_RMSE: 38.7028 - val_loss: 1498.1293 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6988 - loss: 1497.8237 - val_RMSE: 38.7019 - val_loss: 1498.0594 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6976 - loss: 1497.7235 - val_RMSE: 38.7019 - val_loss: 1498.0576 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6975 - loss: 1497.7128 - val_RMSE: 38.7016 - val_loss: 1498.0305 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6974 - loss: 1497.7034 - val_RMSE: 38.7015 - val_loss: 1498.0170 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6953 - loss: 1497.5345 - val_RMSE: 38.7014 - val_loss: 1498.0031 - learning_rate: 1.0000e-03\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6953 - loss: 1497.5374 - val_RMSE: 38.7014 - val_loss: 1498.0051 - learning_rate: 1.0000e-03\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 18:27:55,817] Trial 17 finished with value: 38.69696801004246 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.48}. Best is trial 10 with value: 38.69639193901346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 78s 23ms/step - RMSE: 42.5061 - loss: 1851.7075 - val_RMSE: 38.6990 - val_loss: 1498.0698 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7645 - loss: 1503.0989 - val_RMSE: 38.6957 - val_loss: 1497.7089 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7588 - loss: 1502.6493 - val_RMSE: 38.6955 - val_loss: 1497.7131 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7568 - loss: 1502.4940 - val_RMSE: 38.6979 - val_loss: 1497.8549 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7350 - loss: 1500.7108 - val_RMSE: 38.6850 - val_loss: 1496.7921 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7293 - loss: 1500.2126 - val_RMSE: 38.6837 - val_loss: 1496.6650 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7278 - loss: 1500.0745 - val_RMSE: 38.6831 - val_loss: 1496.6100 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7269 - loss: 1499.9949 - val_RMSE: 38.6825 - val_loss: 1496.5526 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7262 - loss: 1499.9364 - val_RMSE: 38.6825 - val_loss: 1496.5453 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7252 - loss: 1499.8481 - val_RMSE: 38.6825 - val_loss: 1496.5387 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7240 - loss: 1499.7505 - val_RMSE: 38.6821 - val_loss: 1496.5103 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7238 - loss: 1499.7389 - val_RMSE: 38.6813 - val_loss: 1496.4490 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7232 - loss: 1499.6880 - val_RMSE: 38.6811 - val_loss: 1496.4296 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7225 - loss: 1499.6344 - val_RMSE: 38.6811 - val_loss: 1496.4340 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7231 - loss: 1499.6840 - val_RMSE: 38.6804 - val_loss: 1496.3774 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7220 - loss: 1499.6027 - val_RMSE: 38.6805 - val_loss: 1496.3872 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7219 - loss: 1499.5956 - val_RMSE: 38.6801 - val_loss: 1496.3568 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7207 - loss: 1499.5057 - val_RMSE: 38.6795 - val_loss: 1496.3143 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7211 - loss: 1499.5413 - val_RMSE: 38.6799 - val_loss: 1496.3553 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7212 - loss: 1499.5505 - val_RMSE: 38.6804 - val_loss: 1496.3926 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7181 - loss: 1499.3075 - val_RMSE: 38.6795 - val_loss: 1496.3206 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7175 - loss: 1499.2621 - val_RMSE: 38.6791 - val_loss: 1496.2885 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7165 - loss: 1499.1849 - val_RMSE: 38.6790 - val_loss: 1496.2748 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7165 - loss: 1499.1821 - val_RMSE: 38.6788 - val_loss: 1496.2572 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7154 - loss: 1499.0953 - val_RMSE: 38.6787 - val_loss: 1496.2468 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 80s 23ms/step - RMSE: 42.4479 - loss: 1846.2544 - val_RMSE: 38.7589 - val_loss: 1502.7336 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7128 - loss: 1499.1208 - val_RMSE: 38.7634 - val_loss: 1503.0193 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7083 - loss: 1498.7671 - val_RMSE: 38.7413 - val_loss: 1501.3091 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7080 - loss: 1498.7466 - val_RMSE: 38.7560 - val_loss: 1502.4315 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7068 - loss: 1498.7327 - val_RMSE: 38.7301 - val_loss: 1500.6193 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7055 - loss: 1498.6884 - val_RMSE: 38.7365 - val_loss: 1501.0447 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7047 - loss: 1498.6017 - val_RMSE: 38.7262 - val_loss: 1500.1936 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7049 - loss: 1498.5911 - val_RMSE: 38.7314 - val_loss: 1500.6517 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7028 - loss: 1498.4716 - val_RMSE: 38.7252 - val_loss: 1500.1234 - learning_rate: 0.0100\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7022 - loss: 1498.3450 - val_RMSE: 38.7215 - val_loss: 1499.8618 - learning_rate: 0.0100\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7012 - loss: 1498.2882 - val_RMSE: 38.7201 - val_loss: 1499.7931 - learning_rate: 0.0100\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7004 - loss: 1498.2216 - val_RMSE: 38.7654 - val_loss: 1503.2148 - learning_rate: 0.0100\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7008 - loss: 1498.2169 - val_RMSE: 38.7249 - val_loss: 1500.0605 - learning_rate: 0.0100\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6819 - loss: 1496.6937 - val_RMSE: 38.7161 - val_loss: 1499.2471 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6781 - loss: 1496.2817 - val_RMSE: 38.7154 - val_loss: 1499.1365 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6761 - loss: 1496.0863 - val_RMSE: 38.7138 - val_loss: 1498.9910 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6751 - loss: 1495.9968 - val_RMSE: 38.7147 - val_loss: 1499.0527 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6742 - loss: 1495.9199 - val_RMSE: 38.7135 - val_loss: 1498.9578 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6741 - loss: 1495.9039 - val_RMSE: 38.7124 - val_loss: 1498.8708 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6734 - loss: 1495.8490 - val_RMSE: 38.7149 - val_loss: 1499.0560 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6737 - loss: 1495.8649 - val_RMSE: 38.7162 - val_loss: 1499.1527 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6709 - loss: 1495.6525 - val_RMSE: 38.7105 - val_loss: 1498.7070 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6709 - loss: 1495.6454 - val_RMSE: 38.7102 - val_loss: 1498.6865 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6705 - loss: 1495.6151 - val_RMSE: 38.7100 - val_loss: 1498.6692 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6687 - loss: 1495.4760 - val_RMSE: 38.7099 - val_loss: 1498.6586 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 22ms/step - RMSE: 42.4451 - loss: 1845.4678 - val_RMSE: 38.7166 - val_loss: 1499.4392 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7386 - loss: 1501.0864 - val_RMSE: 38.7157 - val_loss: 1499.2799 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7345 - loss: 1500.7500 - val_RMSE: 38.7148 - val_loss: 1499.2269 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7326 - loss: 1500.5975 - val_RMSE: 38.7146 - val_loss: 1499.1930 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7333 - loss: 1500.7269 - val_RMSE: 38.7158 - val_loss: 1499.4921 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7321 - loss: 1500.7062 - val_RMSE: 38.7147 - val_loss: 1499.2947 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7094 - loss: 1498.8455 - val_RMSE: 38.7075 - val_loss: 1498.6033 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7058 - loss: 1498.4537 - val_RMSE: 38.7064 - val_loss: 1498.4634 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7032 - loss: 1498.2118 - val_RMSE: 38.7049 - val_loss: 1498.3282 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7025 - loss: 1498.1329 - val_RMSE: 38.7041 - val_loss: 1498.2537 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7018 - loss: 1498.0669 - val_RMSE: 38.7043 - val_loss: 1498.2545 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7010 - loss: 1497.9972 - val_RMSE: 38.7036 - val_loss: 1498.1958 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7015 - loss: 1498.0317 - val_RMSE: 38.7030 - val_loss: 1498.1445 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7010 - loss: 1497.9912 - val_RMSE: 38.7065 - val_loss: 1498.4138 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6996 - loss: 1497.8779 - val_RMSE: 38.7033 - val_loss: 1498.1672 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6974 - loss: 1497.7065 - val_RMSE: 38.7024 - val_loss: 1498.0897 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6974 - loss: 1497.7035 - val_RMSE: 38.7024 - val_loss: 1498.0869 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6964 - loss: 1497.6232 - val_RMSE: 38.7022 - val_loss: 1498.0720 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6968 - loss: 1497.6494 - val_RMSE: 38.7023 - val_loss: 1498.0758 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6972 - loss: 1497.6832 - val_RMSE: 38.7021 - val_loss: 1498.0609 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6963 - loss: 1497.6111 - val_RMSE: 38.7020 - val_loss: 1498.0531 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6958 - loss: 1497.5699 - val_RMSE: 38.7020 - val_loss: 1498.0553 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6962 - loss: 1497.5994 - val_RMSE: 38.7020 - val_loss: 1498.0526 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6949 - loss: 1497.4979 - val_RMSE: 38.7019 - val_loss: 1498.0376 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6951 - loss: 1497.5172 - val_RMSE: 38.7018 - val_loss: 1498.0341 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 19:25:47,956] Trial 18 finished with value: 38.696786141502315 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.001, 'exite_units': 32, 'dropout_rate': 0.39}. Best is trial 10 with value: 38.69639193901346.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 76s 22ms/step - RMSE: 42.2596 - loss: 1826.8907 - val_RMSE: 38.7030 - val_loss: 1498.1605 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7667 - loss: 1503.0985 - val_RMSE: 38.7197 - val_loss: 1499.4653 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7578 - loss: 1502.4265 - val_RMSE: 38.7032 - val_loss: 1498.1886 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7325 - loss: 1500.4470 - val_RMSE: 38.6839 - val_loss: 1496.6523 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7279 - loss: 1500.0575 - val_RMSE: 38.6838 - val_loss: 1496.6176 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7261 - loss: 1499.8948 - val_RMSE: 38.6824 - val_loss: 1496.5101 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7249 - loss: 1499.7965 - val_RMSE: 38.6820 - val_loss: 1496.4692 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7241 - loss: 1499.7239 - val_RMSE: 38.6817 - val_loss: 1496.4395 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7234 - loss: 1499.6686 - val_RMSE: 38.6814 - val_loss: 1496.4132 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7223 - loss: 1499.5791 - val_RMSE: 38.6811 - val_loss: 1496.3928 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7213 - loss: 1499.4993 - val_RMSE: 38.6805 - val_loss: 1496.3463 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7211 - loss: 1499.4890 - val_RMSE: 38.6810 - val_loss: 1496.3818 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7206 - loss: 1499.4484 - val_RMSE: 38.6804 - val_loss: 1496.3390 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7200 - loss: 1499.4000 - val_RMSE: 38.6798 - val_loss: 1496.2900 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7184 - loss: 1499.2738 - val_RMSE: 38.6801 - val_loss: 1496.3134 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7184 - loss: 1499.2775 - val_RMSE: 38.6798 - val_loss: 1496.2877 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7176 - loss: 1499.2166 - val_RMSE: 38.6793 - val_loss: 1496.2537 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7179 - loss: 1499.2440 - val_RMSE: 38.6794 - val_loss: 1496.2660 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7174 - loss: 1499.2043 - val_RMSE: 38.6791 - val_loss: 1496.2463 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7168 - loss: 1499.1664 - val_RMSE: 38.6796 - val_loss: 1496.2866 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7165 - loss: 1499.1411 - val_RMSE: 38.6795 - val_loss: 1496.2784 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7136 - loss: 1498.9175 - val_RMSE: 38.6790 - val_loss: 1496.2412 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7118 - loss: 1498.7797 - val_RMSE: 38.6789 - val_loss: 1496.2355 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7121 - loss: 1498.7971 - val_RMSE: 38.6789 - val_loss: 1496.2308 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7112 - loss: 1498.7284 - val_RMSE: 38.6788 - val_loss: 1496.2190 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 80s 23ms/step - RMSE: 42.2221 - loss: 1823.6859 - val_RMSE: 38.7356 - val_loss: 1500.6785 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7167 - loss: 1499.2234 - val_RMSE: 38.7623 - val_loss: 1502.7638 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7091 - loss: 1498.6442 - val_RMSE: 38.7866 - val_loss: 1504.6658 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6834 - loss: 1496.6667 - val_RMSE: 38.7145 - val_loss: 1499.0364 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6776 - loss: 1496.1733 - val_RMSE: 38.7129 - val_loss: 1498.8763 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6753 - loss: 1495.9658 - val_RMSE: 38.7139 - val_loss: 1498.9427 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6755 - loss: 1495.9700 - val_RMSE: 38.7130 - val_loss: 1498.8636 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6739 - loss: 1495.8367 - val_RMSE: 38.7128 - val_loss: 1498.8413 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6733 - loss: 1495.7880 - val_RMSE: 38.7135 - val_loss: 1498.8927 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6728 - loss: 1495.7408 - val_RMSE: 38.7124 - val_loss: 1498.8075 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6725 - loss: 1495.7163 - val_RMSE: 38.7130 - val_loss: 1498.8531 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6715 - loss: 1495.6411 - val_RMSE: 38.7136 - val_loss: 1498.8971 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6689 - loss: 1495.4404 - val_RMSE: 38.7090 - val_loss: 1498.5405 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6685 - loss: 1495.4094 - val_RMSE: 38.7089 - val_loss: 1498.5243 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6671 - loss: 1495.2983 - val_RMSE: 38.7088 - val_loss: 1498.5200 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6675 - loss: 1495.3268 - val_RMSE: 38.7087 - val_loss: 1498.5079 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6658 - loss: 1495.1924 - val_RMSE: 38.7086 - val_loss: 1498.5051 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6665 - loss: 1495.2423 - val_RMSE: 38.7086 - val_loss: 1498.5018 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6651 - loss: 1495.1401 - val_RMSE: 38.7085 - val_loss: 1498.4987 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6653 - loss: 1495.1565 - val_RMSE: 38.7085 - val_loss: 1498.4933 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6660 - loss: 1495.2108 - val_RMSE: 38.7085 - val_loss: 1498.4916 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6657 - loss: 1495.1871 - val_RMSE: 38.7084 - val_loss: 1498.4844 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6652 - loss: 1495.1425 - val_RMSE: 38.7084 - val_loss: 1498.4834 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6647 - loss: 1495.1033 - val_RMSE: 38.7084 - val_loss: 1498.4841 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6652 - loss: 1495.1442 - val_RMSE: 38.7084 - val_loss: 1498.4836 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 75s 22ms/step - RMSE: 42.2043 - loss: 1821.3567 - val_RMSE: 38.7190 - val_loss: 1499.4084 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7427 - loss: 1501.2462 - val_RMSE: 38.7159 - val_loss: 1499.1814 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7338 - loss: 1500.5707 - val_RMSE: 38.7208 - val_loss: 1499.5519 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7312 - loss: 1500.3630 - val_RMSE: 38.7111 - val_loss: 1498.9686 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7312 - loss: 1500.5360 - val_RMSE: 38.7455 - val_loss: 1501.7017 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7307 - loss: 1500.5471 - val_RMSE: 38.7082 - val_loss: 1498.7764 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7289 - loss: 1500.4121 - val_RMSE: 38.7138 - val_loss: 1499.2311 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7275 - loss: 1500.3201 - val_RMSE: 38.8571 - val_loss: 1510.4482 - learning_rate: 0.0100\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7072 - loss: 1498.7853 - val_RMSE: 38.7030 - val_loss: 1498.3658 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7016 - loss: 1498.2239 - val_RMSE: 38.7025 - val_loss: 1498.2224 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7002 - loss: 1498.0286 - val_RMSE: 38.7042 - val_loss: 1498.3174 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6980 - loss: 1497.8267 - val_RMSE: 38.7024 - val_loss: 1498.1636 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6984 - loss: 1497.8510 - val_RMSE: 38.7035 - val_loss: 1498.2350 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6966 - loss: 1497.7029 - val_RMSE: 38.7027 - val_loss: 1498.1692 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6940 - loss: 1497.4894 - val_RMSE: 38.7016 - val_loss: 1498.0807 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6940 - loss: 1497.4934 - val_RMSE: 38.7015 - val_loss: 1498.0668 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6930 - loss: 1497.4071 - val_RMSE: 38.7016 - val_loss: 1498.0757 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6935 - loss: 1497.4493 - val_RMSE: 38.7014 - val_loss: 1498.0610 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.6925 - loss: 1497.3715 - val_RMSE: 38.7013 - val_loss: 1498.0496 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6926 - loss: 1497.3751 - val_RMSE: 38.7012 - val_loss: 1498.0427 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6920 - loss: 1497.3301 - val_RMSE: 38.7013 - val_loss: 1498.0470 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6927 - loss: 1497.3774 - val_RMSE: 38.7013 - val_loss: 1498.0482 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6922 - loss: 1497.3398 - val_RMSE: 38.7012 - val_loss: 1498.0342 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6916 - loss: 1497.2914 - val_RMSE: 38.7011 - val_loss: 1498.0325 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.6919 - loss: 1497.3201 - val_RMSE: 38.7011 - val_loss: 1498.0332 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 20:23:05,143] Trial 19 finished with value: 38.69609437218236 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'silu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.36}. Best is trial 19 with value: 38.69609437218236.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 75s 22ms/step - RMSE: 42.2576 - loss: 1826.7124 - val_RMSE: 38.7088 - val_loss: 1498.6077 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7666 - loss: 1503.0922 - val_RMSE: 38.6959 - val_loss: 1497.6295 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7582 - loss: 1502.4547 - val_RMSE: 38.6998 - val_loss: 1497.9158 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7563 - loss: 1502.3163 - val_RMSE: 38.7285 - val_loss: 1500.3530 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7335 - loss: 1500.7147 - val_RMSE: 38.6838 - val_loss: 1496.7849 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7282 - loss: 1500.2028 - val_RMSE: 38.6832 - val_loss: 1496.6654 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7254 - loss: 1499.9237 - val_RMSE: 38.6826 - val_loss: 1496.5865 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7250 - loss: 1499.8701 - val_RMSE: 38.6825 - val_loss: 1496.5641 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7241 - loss: 1499.7871 - val_RMSE: 38.6819 - val_loss: 1496.5118 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7237 - loss: 1499.7430 - val_RMSE: 38.6818 - val_loss: 1496.4907 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7224 - loss: 1499.6340 - val_RMSE: 38.6814 - val_loss: 1496.4498 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7229 - loss: 1499.6694 - val_RMSE: 38.6812 - val_loss: 1496.4292 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7219 - loss: 1499.5784 - val_RMSE: 38.6817 - val_loss: 1496.4668 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7216 - loss: 1499.5542 - val_RMSE: 38.6814 - val_loss: 1496.4427 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7190 - loss: 1499.3499 - val_RMSE: 38.6803 - val_loss: 1496.3544 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7181 - loss: 1499.2795 - val_RMSE: 38.6801 - val_loss: 1496.3353 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7187 - loss: 1499.3228 - val_RMSE: 38.6799 - val_loss: 1496.3260 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7174 - loss: 1499.2222 - val_RMSE: 38.6798 - val_loss: 1496.3142 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7179 - loss: 1499.2611 - val_RMSE: 38.6797 - val_loss: 1496.3014 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7172 - loss: 1499.2062 - val_RMSE: 38.6797 - val_loss: 1496.3003 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7178 - loss: 1499.2548 - val_RMSE: 38.6796 - val_loss: 1496.2928 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7167 - loss: 1499.1689 - val_RMSE: 38.6795 - val_loss: 1496.2852 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7170 - loss: 1499.1871 - val_RMSE: 38.6796 - val_loss: 1496.2911 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7163 - loss: 1499.1316 - val_RMSE: 38.6795 - val_loss: 1496.2839 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7155 - loss: 1499.0669 - val_RMSE: 38.6794 - val_loss: 1496.2783 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 75s 22ms/step - RMSE: 42.2221 - loss: 1823.6876 - val_RMSE: 38.7411 - val_loss: 1501.0986 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7162 - loss: 1499.1786 - val_RMSE: 38.8396 - val_loss: 1508.7660 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7081 - loss: 1498.5634 - val_RMSE: 38.8577 - val_loss: 1510.1698 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6835 - loss: 1496.6495 - val_RMSE: 38.7140 - val_loss: 1498.9836 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6780 - loss: 1496.1849 - val_RMSE: 38.7147 - val_loss: 1499.0038 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6758 - loss: 1495.9949 - val_RMSE: 38.7149 - val_loss: 1499.0138 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6740 - loss: 1495.8485 - val_RMSE: 38.7111 - val_loss: 1498.7238 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6720 - loss: 1495.6885 - val_RMSE: 38.7111 - val_loss: 1498.7147 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6721 - loss: 1495.6946 - val_RMSE: 38.7109 - val_loss: 1498.6993 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6716 - loss: 1495.6570 - val_RMSE: 38.7108 - val_loss: 1498.6869 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6719 - loss: 1495.6770 - val_RMSE: 38.7107 - val_loss: 1498.6775 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6718 - loss: 1495.6674 - val_RMSE: 38.7105 - val_loss: 1498.6641 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6710 - loss: 1495.6083 - val_RMSE: 38.7105 - val_loss: 1498.6622 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6702 - loss: 1495.5413 - val_RMSE: 38.7103 - val_loss: 1498.6483 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6704 - loss: 1495.5566 - val_RMSE: 38.7103 - val_loss: 1498.6396 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6704 - loss: 1495.5580 - val_RMSE: 38.7104 - val_loss: 1498.6497 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6698 - loss: 1495.5120 - val_RMSE: 38.7101 - val_loss: 1498.6281 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6698 - loss: 1495.5062 - val_RMSE: 38.7100 - val_loss: 1498.6216 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6687 - loss: 1495.4230 - val_RMSE: 38.7100 - val_loss: 1498.6193 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6687 - loss: 1495.4221 - val_RMSE: 38.7099 - val_loss: 1498.6057 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6694 - loss: 1495.4744 - val_RMSE: 38.7098 - val_loss: 1498.5973 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6688 - loss: 1495.4249 - val_RMSE: 38.7097 - val_loss: 1498.5961 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6685 - loss: 1495.4053 - val_RMSE: 38.7095 - val_loss: 1498.5763 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6683 - loss: 1495.3903 - val_RMSE: 38.7094 - val_loss: 1498.5691 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6677 - loss: 1495.3428 - val_RMSE: 38.7096 - val_loss: 1498.5812 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 6ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 22ms/step - RMSE: 42.2040 - loss: 1821.3322 - val_RMSE: 38.7241 - val_loss: 1499.7968 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7426 - loss: 1501.2448 - val_RMSE: 38.7138 - val_loss: 1499.0154 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7335 - loss: 1500.5367 - val_RMSE: 38.7123 - val_loss: 1498.8828 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7315 - loss: 1500.3817 - val_RMSE: 38.7218 - val_loss: 1499.7744 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7312 - loss: 1500.5320 - val_RMSE: 38.7139 - val_loss: 1499.2284 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7079 - loss: 1498.7435 - val_RMSE: 38.7039 - val_loss: 1498.3431 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7033 - loss: 1498.2673 - val_RMSE: 38.7039 - val_loss: 1498.2560 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7021 - loss: 1498.1074 - val_RMSE: 38.7036 - val_loss: 1498.2042 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7008 - loss: 1497.9880 - val_RMSE: 38.7037 - val_loss: 1498.2014 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6998 - loss: 1497.8951 - val_RMSE: 38.7036 - val_loss: 1498.1851 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6987 - loss: 1497.8047 - val_RMSE: 38.7023 - val_loss: 1498.0748 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6982 - loss: 1497.7563 - val_RMSE: 38.7020 - val_loss: 1498.0487 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6981 - loss: 1497.7427 - val_RMSE: 38.7023 - val_loss: 1498.0659 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6966 - loss: 1497.6221 - val_RMSE: 38.7026 - val_loss: 1498.0859 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6941 - loss: 1497.4341 - val_RMSE: 38.7010 - val_loss: 1497.9648 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6940 - loss: 1497.4241 - val_RMSE: 38.7009 - val_loss: 1497.9573 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.6933 - loss: 1497.3661 - val_RMSE: 38.7010 - val_loss: 1497.9554 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6937 - loss: 1497.3969 - val_RMSE: 38.7009 - val_loss: 1497.9480 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6928 - loss: 1497.3215 - val_RMSE: 38.7008 - val_loss: 1497.9445 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6930 - loss: 1497.3395 - val_RMSE: 38.7008 - val_loss: 1497.9440 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6928 - loss: 1497.3235 - val_RMSE: 38.7007 - val_loss: 1497.9327 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6938 - loss: 1497.3983 - val_RMSE: 38.7006 - val_loss: 1497.9216 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6925 - loss: 1497.2990 - val_RMSE: 38.7006 - val_loss: 1497.9249 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6918 - loss: 1497.2468 - val_RMSE: 38.7006 - val_loss: 1497.9225 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.6920 - loss: 1497.2616 - val_RMSE: 38.7005 - val_loss: 1497.9160 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 10s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-23 21:20:29,865] Trial 20 finished with value: 38.69650599915822 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'silu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.36}. Best is trial 19 with value: 38.69609437218236.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 77s 22ms/step - RMSE: 42.2441 - loss: 1825.4763 - val_RMSE: 38.7147 - val_loss: 1499.0643 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7674 - loss: 1503.1533 - val_RMSE: 38.7027 - val_loss: 1498.1351 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7575 - loss: 1502.3794 - val_RMSE: 38.7063 - val_loss: 1498.3782 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7555 - loss: 1502.2064 - val_RMSE: 38.6996 - val_loss: 1497.9674 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7543 - loss: 1502.2875 - val_RMSE: 38.6910 - val_loss: 1497.4161 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7533 - loss: 1502.2555 - val_RMSE: 38.6973 - val_loss: 1497.9465 - learning_rate: 0.0100\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7529 - loss: 1502.2533 - val_RMSE: 38.7014 - val_loss: 1498.2733 - learning_rate: 0.0100\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7316 - loss: 1500.5817 - val_RMSE: 38.6822 - val_loss: 1496.6818 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7248 - loss: 1499.9592 - val_RMSE: 38.6816 - val_loss: 1496.5549 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7241 - loss: 1499.8304 - val_RMSE: 38.6813 - val_loss: 1496.4999 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7219 - loss: 1499.6379 - val_RMSE: 38.6807 - val_loss: 1496.4379 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2598/2601 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - RMSE: 38.7216 - loss: 1499.5988"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-23 21:30:03,040] Trial 21 failed with parameters: {'units': 256, 'last_layer': 1, 'activation': 'silu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.32999999999999996} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-34-1f75ff93dfcb>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-31-92b760355d9d>\", line 52, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 395, in fit\n",
            "    val_logs = self.evaluate(\n",
            "               ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 484, in evaluate\n",
            "    logs = self.test_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
            "    return function._call_flat(  # pylint: disable=protected-access\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1322, in _call_flat\n",
            "    return self._inference_function.call_preflattened(args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 217, in call_preflattened\n",
            "    return self.function_type.pack_output(flat_outputs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 446, in pack_output\n",
            "    return self.output.from_tensors(iter(flat_values))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\", line 262, in from_tensors\n",
            "    self._component_specs\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\", line 244, in _component_specs\n",
            "    return [tensor_spec.TensorSpec((), dtypes.variant)]\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 865, in __init__\n",
            "    self._shape = tensor_shape.TensorShape(shape)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 830, in __init__\n",
            "    self._dims = tuple(as_dimension(d).value for d in dims)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-02-23 21:30:03,044] Trial 21 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0c79ea6780ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-1f75ff93dfcb>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-1f75ff93dfcb>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-92b760355d9d>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     53\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    393\u001b[0m                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                     )\n\u001b[0;32m--> 395\u001b[0;31m                 val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    396\u001b[0m                     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_evaluating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36mpack_output\u001b[0;34m(self, flat_values)\u001b[0m\n\u001b[1;32m    444\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not pack outputs for undefined output type.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    260\u001b[0m     components = nest.map_structure(\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_specs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     )\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36m_component_specs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_component_specs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, dtype, name)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32mnot\u001b[0m \u001b[0mconvertible\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \"\"\"\n\u001b[0;32m--> 865\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \"\"\"\n\u001b[1;32m    829\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Most common case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(data.X, data.y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 19 finished with value: 38.69609437218236\n",
        "- parameters: {'units': 256, 'last_layer': 1, 'activation': 'silu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.36}."
      ],
      "metadata": {
        "id": "tSHfbDyPOWgl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TKkv5dORbuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}