{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Colab_backup/blob/main/File_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGa3SjrXsW_F"
      },
      "source": [
        "# **S5E2 - Backpack Prices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4M8cw5duVkf"
      },
      "outputs": [],
      "source": [
        "# Necessry to run LGBMRegressor\n",
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rYYQgW0Rmxv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qq pytorch_tabnet\n",
        "!pip install optuna\n",
        "!pip install catboost\n",
        "#!pip install optuna-integration-pytorch-tabnet\n",
        "\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "!pip install --upgrade category-encoders\n",
        "!pip install optuna-integration\n",
        "!pip install colorama\n",
        "#!pip install pyfiglet\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install keras-nlp\n",
        "!pip install BorutaShap\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install scikit-lego\n",
        "!pip install skops\n",
        "\n",
        "#from pytorch_tabnet.tab_model import TabNetRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIS1habP8JGi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "08492e0d-0051-4cd4-a430-38d346e51ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Setup notebook\n",
        "from pathlib import Path\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pickle import load, dump\n",
        "import json\n",
        "import joblib\n",
        "#import calplot as cal\n",
        "\n",
        "# Graphic Libraries:\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg\n",
        "# Set Style\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5});\n",
        "sns.despine(left=True, bottom=True, top=False, right=False);\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "mpl.rc('axes', labelsize=12);\n",
        "plt.rc('xtick',labelsize=10);\n",
        "plt.rc('ytick',labelsize=10);\n",
        "\n",
        "mpl.rcParams['axes.spines.top'] = False;\n",
        "mpl.rcParams['axes.spines.right'] = False;\n",
        "mpl.rcParams['axes.spines.left'] = True;\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "# tool for Excel:\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "from openpyxl.styles import Border, Side, PatternFill, Font, GradientFill, Alignment\n",
        "from openpyxl.worksheet.cell_range import CellRange\n",
        "\n",
        "from openpyxl.formatting import Rule\n",
        "from openpyxl.styles import Font, PatternFill, Border\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "\n",
        "# Bloomberg\n",
        "#from xbbg import blp\n",
        "from catboost import CatBoostRegressor, Pool, CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "# Time Management\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BMonthEnd, QuarterEnd\n",
        "import datetime\n",
        "from pandas.tseries.offsets import BDay # BDay is business day, not birthday...\n",
        "import datetime as dt\n",
        "import click\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import string\n",
        "\n",
        "from ipywidgets import AppLayout\n",
        "from ipywidgets import Dropdown, Layout, HTML, AppLayout, VBox, Label, HBox, BoundedFloatText, interact, Output\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Reshape\n",
        "from keras.models import Model\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, cross_val_score, cross_validate, GroupKFold, GridSearchCV, RepeatedStratifiedKFold, cross_val_predict\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   QuantileTransformer,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "# metrics\n",
        "import sklearn\n",
        "#import skops.io as sio\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             root_mean_squared_error,\n",
        "                             root_mean_squared_log_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss,\n",
        "                             make_scorer)\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  HuberRegressor,\n",
        "                                  TweedieRegressor,\n",
        "                                  QuantileRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  TheilSenRegressor,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              AdaBoostClassifier,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              GradientBoostingClassifier,\n",
        "                              StackingRegressor,\n",
        "                              StackingClassifier,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              HistGradientBoostingRegressor,\n",
        "                              ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
        "\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import itertools\n",
        "import warnings\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from pylab import rcParams\n",
        "import scipy.stats as ss\n",
        "\n",
        "from category_encoders.cat_boost import CatBoostEncoder\n",
        "from category_encoders.wrapper import PolynomialWrapper\n",
        "from category_encoders.count import CountEncoder\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "#import pyfiglet\n",
        "#plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkkRPWKZCkYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7421bbe0-e91e-4f8b-f662-84ab25a66bbf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 960x660 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.5,\n",
        "         'grid.alpha'           :0.5,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : 'black',\n",
        "         'axes.titlesize'       : 12,\n",
        "#         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "# Set Style\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "\n",
        "# import font colors\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "init(autoreset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU7oWpLHRmxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393a3119-5c4f-488c-fa29-5bc894a285b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Connect to Colab:#\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PuCulFRmx1"
      },
      "source": [
        "<div style=\"text-align:center; border-radius:15px; padding:15px; margin:0; font-size:100%; font-family:Arial, sans-serif; background-color:#A8DADC; color:#1D3557; overflow:hidden; box-shadow:0 3px 6px rgba(0, 0, 0, 0.2);\">\n",
        "    <h3>Loading and Preprocessing Data for Compatibility</h3>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3odgloSjRmx4"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_enc_ext.csv\",index_col=0)\n",
        "\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/test_enc_ext.csv\",index_col=0)\n",
        "\n",
        "# df_train_orig = pd.read_csv(\n",
        "#     '/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/credit_risk_dataset.csv'\n",
        "# )\n",
        "\n",
        "df_subm = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\",index_col=0)\n",
        "\n",
        "# df_orig = pd.read_csv(\n",
        "#     \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E12/Insurance Premium Prediction Dataset.csv\",\n",
        "#      parse_dates=['Policy Start Date'],\n",
        "#     #     index_col='id',\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vwd0o1ph1Ai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6cf5c9-7f5e-470c-80b5-81c3dbe8c1e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3994318, 31), (200000, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_train.head()\n",
        "df_train.shape,df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tcWfu7npSHN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "99ad6134-6391-4d1d-fae0-ce6faca4eb2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  Style  \\\n",
              "0      1         1     1             7                   2           1      3   \n",
              "1      1         0     3             1                   2           2      1   \n",
              "2      5         1     3             2                   2           1      1   \n",
              "3      3         3     3             8                   2           1      1   \n",
              "4      0         0     1             0                   2           2      1   \n",
              "\n",
              "   Color  Weight Capacity (kg)  Weight Capacity (kg)_missing  Mat_Siz_Col  \\\n",
              "0      0             -0.917722                             0            0   \n",
              "1      3              1.300573                             0            0   \n",
              "2      6             -0.196013                             0            0   \n",
              "3      3             -0.727615                             0            0   \n",
              "4      3             -0.037447                             0            0   \n",
              "\n",
              "   Siz_Lap_Col  Bra_Siz_Wat  Siz_Lap_Wat  Mat_Lap_Wat  Bra_Siz_Sty  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Bra_Lap_Wat  Siz_Com_Lap  Siz_Lap_Sty  Mat_Com_Lap  Mat_Siz_Com  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Bra_Siz_Com  Com_Lap_Wat  Bra_Siz_Lap  Bra_Mat_Siz  Siz_Com_Wat  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Siz_Com_Sty     TE_wc    skew_0    skew_1      Price  \n",
              "0            0  0.261445 -0.292388 -0.428820  112.15875  \n",
              "1            0  0.621130 -0.302957 -0.460902   68.88056  \n",
              "2            0  0.016408 -0.301780 -1.112454   39.17320  \n",
              "3            0  1.498987 -0.301780 -0.551413   80.60793  \n",
              "4            0  0.016408 -0.375870  0.519525   86.02312  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ea05a6b-2d08-49e3-8922-e29669f6347c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>Weight Capacity (kg)_missing</th>\n",
              "      <th>Mat_Siz_Col</th>\n",
              "      <th>Siz_Lap_Col</th>\n",
              "      <th>Bra_Siz_Wat</th>\n",
              "      <th>Siz_Lap_Wat</th>\n",
              "      <th>Mat_Lap_Wat</th>\n",
              "      <th>Bra_Siz_Sty</th>\n",
              "      <th>Bra_Lap_Wat</th>\n",
              "      <th>Siz_Com_Lap</th>\n",
              "      <th>Siz_Lap_Sty</th>\n",
              "      <th>Mat_Com_Lap</th>\n",
              "      <th>Mat_Siz_Com</th>\n",
              "      <th>Bra_Siz_Com</th>\n",
              "      <th>Com_Lap_Wat</th>\n",
              "      <th>Bra_Siz_Lap</th>\n",
              "      <th>Bra_Mat_Siz</th>\n",
              "      <th>Siz_Com_Wat</th>\n",
              "      <th>Siz_Com_Sty</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.917722</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.261445</td>\n",
              "      <td>-0.292388</td>\n",
              "      <td>-0.428820</td>\n",
              "      <td>112.15875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.300573</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.621130</td>\n",
              "      <td>-0.302957</td>\n",
              "      <td>-0.460902</td>\n",
              "      <td>68.88056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.196013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-0.301780</td>\n",
              "      <td>-1.112454</td>\n",
              "      <td>39.17320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.727615</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.498987</td>\n",
              "      <td>-0.301780</td>\n",
              "      <td>-0.551413</td>\n",
              "      <td>80.60793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.037447</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-0.375870</td>\n",
              "      <td>0.519525</td>\n",
              "      <td>86.02312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ea05a6b-2d08-49e3-8922-e29669f6347c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1ea05a6b-2d08-49e3-8922-e29669f6347c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1ea05a6b-2d08-49e3-8922-e29669f6347c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a51f6529-5e95-4a88-95f6-10fd174893c8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a51f6529-5e95-4a88-95f6-10fd174893c8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a51f6529-5e95-4a88-95f6-10fd174893c8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#df_train_orig.isna().sum()\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njvQSzYr_4BD"
      },
      "outputs": [],
      "source": [
        "def plot_scatter(x=\"Price\",y=\"TE_wc\", df=df_train):\n",
        "\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.scatter(df[x],df[y])\n",
        "  plt.xlabel(x)\n",
        "  plt.ylabel(y)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R5dhMXeAUhG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "8c477f7b-fbed-4b5e-a4e6-6b2b4c0af57b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_scatter(x=\"Price\",y=\"TE_wc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pykP87axQAvp"
      },
      "outputs": [],
      "source": [
        "#df_train[\"person_emp_length\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hREKVL1iPrWR"
      },
      "outputs": [],
      "source": [
        "#df_train[df_train[\"cb_person_cred_hist_length\"]>0.75*df_train[\"person_age\"]]\n",
        "#np.round(df_train[\"cb_person_cred_hist_length\"].mean(),0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vfGRiTeLCd2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am9pd9NwKIBl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho9znWqB1KpL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFmR4Gl6JRAl"
      },
      "source": [
        "**Descriptions of Loan Data**\n",
        "\n",
        "Descriptions for the column names based on the data provided:\n",
        "\n",
        "* **id**: Unique identifier for each record.\n",
        "* **person_age**: Age of the individual, categorized into ranges.\n",
        "* **person_income**: Income of the individual, categorized into income ranges.\n",
        "* **person_home_ownership**: Homeownership status, which includes categories like 'RENT', 'MORTGAGE', etc.\n",
        "* **person_emp_length**: Employment length of the individual, categorized into ranges based on years.\n",
        "* **loan_intent**: The purpose of the loan, with categories such as 'EDUCATION', 'MEDICAL', etc.\n",
        "* **loan_grade**: The credit grade of the loan, such as 'A', 'B', etc.\n",
        "* **loan_amnt**: Loan amount, categorized into ranges.\n",
        "* **loan_int_rate**: Loan interest rate, categorized into percentage ranges.\n",
        "* **loan_percent_income**: Percentage of the individuals income that the loan represents, categorized into - ranges.\n",
        "* **cb_person_default_on_file**: Whether the person has a history of loan default, with values 'true' or 'false'.\n",
        "* **cb_person_cred_hist_length**: Length of the individuals credit history, categorized into ranges.\n",
        "* **loan_status**: with values representing whether the loan status approval( binary values)\n",
        "\n",
        "The dataset is a about loan applications, including personal, financial, and loan details. It's likely used for predicting whether a person will default on a loan, making it a binary classification problem. The goal is to figure out which applicants are at higher risk of not paying back their loans based on their age, income, employment, loan purpose, credit history, and other related information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_IkGc2Wya01"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "\n",
        "    state = 42\n",
        "    n_splits = 10\n",
        "    early_stop = 200\n",
        "\n",
        "    target = 'Price'\n",
        "    problem = \"Regression\"\n",
        "    train = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_enc_ext.csv', index_col=0)\n",
        "    test = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/test_enc_ext.csv', index_col=0)\n",
        "    submission = pd.read_csv( \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\", index_col=0)\n",
        "    train_org = None\n",
        "\n",
        "    original_data = 'N'\n",
        "    outliers = 'N'\n",
        "    log_trf = 'N'\n",
        "    scaler_trf = 'Y'\n",
        "    feature_eng = 'Y'\n",
        "    missing = 'Y'\n",
        "    force_normalization=\"N\"\n",
        "    impose_normalization=\"Y\"\n",
        "    trg_enc = \"N\"\n",
        "    metric_goal=\"rmse\"\n",
        "    direction_=\"minimize\"\n",
        "    log_trans_cols = []\n",
        "    force_norm_cols = []\n",
        "    impose_norm_cols = [\"skew_0\",\"skew_1\"]\n",
        "    trg_enc_feat = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjPBIogOJ2mz"
      },
      "outputs": [],
      "source": [
        "class Preprocessing():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train = Config.train\n",
        "        self.test = Config.test\n",
        "        self.targets = Config.target\n",
        "        self.problem = Config.problem\n",
        "        self.submission = Config.submission\n",
        "\n",
        "        self.prp_data()\n",
        "\n",
        "    def prp_data(self):\n",
        "\n",
        "        if Config.original_data == 'Y':\n",
        "            self.train = pd.concat([self.train, Config.train_org], ignore_index=True).drop_duplicates(ignore_index=True)\n",
        "        if 'id' in self.train.columns:\n",
        "            self.train = self.train.drop(['id'], axis=1)\n",
        "            self.test = self.test.drop(['id'], axis=1)\n",
        "\n",
        "        self.cat_features = self.train.drop(self.targets, axis=1).select_dtypes(include=['object', 'bool', 'int', 'category']).columns.tolist()\n",
        "        self.num_features = self.train.drop(self.targets, axis=1).select_dtypes(exclude=['object', 'bool', 'int', 'category']).columns.tolist()\n",
        "\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
        "\n",
        "        self.train = self.reduce_mem(self.train)\n",
        "        self.test = self.reduce_mem(self.test)\n",
        "        return self\n",
        "\n",
        "    def reduce_mem(self, df):\n",
        "\n",
        "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"]\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_type = df[col].dtypes\n",
        "\n",
        "            if col_type in numerics:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "\n",
        "                if \"int\" in str(col_type):\n",
        "                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)\n",
        "                else:\n",
        "                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    if c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICqfSaYMKIF8"
      },
      "outputs": [],
      "source": [
        "class EDA(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_info()\n",
        "        self.heatmap()\n",
        "        self.dist_plots()\n",
        "        self.cat_feature_plots()\n",
        "        self.target_pie()\n",
        "\n",
        "    def data_info(self):\n",
        "\n",
        "        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n",
        "            table_style = [{'selector': 'th:not(.index_name)',\n",
        "                            'props': [('background-color', 'slategrey'),\n",
        "                                      ('color', '#FFFFFF'),\n",
        "                                      ('font-weight', 'bold'),\n",
        "                                      ('border', '1px solid #DCDCDC'),\n",
        "                                      ('text-align', 'center')]\n",
        "                            },\n",
        "                            {'selector': 'tbody td',\n",
        "                             'props': [('border', '1px solid #DCDCDC'),\n",
        "                                       ('font-weight', 'normal')]\n",
        "                            }]\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} head\\n')\n",
        "            display(data.head().style.set_table_styles(table_style))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} info\\n'+Style.RESET_ALL)\n",
        "            display(data.info())\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} describe\\n')\n",
        "            display(data.describe().drop(index='count', columns=self.targets, errors = 'ignore').T\n",
        "                    .style.set_table_styles(table_style).format('{:.3f}'))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} missing values\\n'+Style.RESET_ALL)\n",
        "            display(data.isna().sum())\n",
        "        return self\n",
        "\n",
        "    def heatmap(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nCorrelation Heatmap\\n')\n",
        "        plt.figure(figsize=(7,7))\n",
        "        corr = self.train.select_dtypes(exclude=['object', 'category']).corr(method='pearson')\n",
        "        sns.heatmap(corr, fmt = '0.2f', cmap = 'Blues', annot=True, cbar=False)\n",
        "        plt.show()\n",
        "\n",
        "    def dist_plots(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Numerical\\n\")\n",
        "        df = pd.concat([self.train[self.num_features].assign(Source = 'Train'),\n",
        "                        self.test[self.num_features].assign(Source = 'Test'),],\n",
        "                        axis=0, ignore_index = True)\n",
        "\n",
        "        fig, axes = plt.subplots(len(self.num_features), 2 ,figsize = (18, len(self.num_features) * 6),\n",
        "                                 gridspec_kw = {'hspace': 0.3,\n",
        "                                                'wspace': 0.2,\n",
        "                                                'width_ratios': [0.70, 0.30]\n",
        "                                               }\n",
        "                                )\n",
        "        for i,col in enumerate(self.num_features):\n",
        "            ax = axes[i,0]\n",
        "            sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source',\n",
        "                        palette = ['royalblue', 'tomato'], ax = ax, alpha=0.7, linewidth = 2\n",
        "                       )\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.grid('--',alpha=0.7)\n",
        "\n",
        "            ax = axes[i,1]\n",
        "            sns.boxplot(data = df, y = col, x=df.Source, width = 0.5,\n",
        "                        linewidth = 1, fliersize= 1,\n",
        "                        ax = ax, palette=['royalblue', 'tomato']\n",
        "                       )\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.tick_params(axis='both', which='major')\n",
        "            ax.set_xticklabels(['Train', 'Test'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def cat_feature_plots(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Categorical\\n\")\n",
        "        fig, axes = plt.subplots(len(self.cat_features), 2 ,figsize = (18, len(self.cat_features) * 6),\n",
        "                                 gridspec_kw = {'hspace': 0.5,\n",
        "                                                'wspace': 0.2,\n",
        "                                               }\n",
        "                                )\n",
        "\n",
        "        for i, col in enumerate(self.cat_features):\n",
        "\n",
        "            ax = axes[i,0]\n",
        "            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='royalblue', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Train\")\n",
        "\n",
        "            ax = axes[i,1]\n",
        "            sns.barplot(data=self.test[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='tomato', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Test\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def target_pie(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nTarget feature distribution\\n\")\n",
        "        targets = self.train[self.targets]\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        if self.problem==\"Regression\":\n",
        "          plt.hist(targets, bins=35, color='royalblue',alpha=0.7)\n",
        "        else:\n",
        "          plt.pie(targets.value_counts(), labels=targets.value_counts().index, autopct='%1.2f%%', colors=palette_9)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIy8MLVmvv3N"
      },
      "source": [
        "## 1.0 EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O1IeGiuKwDA"
      },
      "outputs": [],
      "source": [
        "eda = EDA()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jQmFilAvKM4"
      },
      "source": [
        "## 2.0 Data Transformation and Feature Engeneering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxSqeWx-MZHk"
      },
      "outputs": [],
      "source": [
        "class Transform(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        if self.missing == 'Y':\n",
        "            self.missing_values()\n",
        "\n",
        "        self.train_raw = self.train.copy()\n",
        "\n",
        "        if self.feature_eng == 'Y':\n",
        "            self.train = self.new_features(self.train)\n",
        "            self.test = self.new_features(self.test)\n",
        "            self.train_raw = self.new_features(self.train_raw)\n",
        "\n",
        "        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object', 'bool', 'category']).columns.tolist()\n",
        "        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool', 'category']).columns.tolist()\n",
        "\n",
        "        if self.outliers == 'Y':\n",
        "            self.remove_outliers()\n",
        "\n",
        "        if self.log_trf == 'Y':\n",
        "            self.log_transformation()\n",
        "\n",
        "        if self.force_normalization == 'Y':\n",
        "            self.forced_norm_transformation()\n",
        "\n",
        "        if self.impose_normalization == 'Y':\n",
        "            self.impose_normalization_transformation()\n",
        "\n",
        "        if self.trg_enc == 'Y':\n",
        "            self.target_encoding()\n",
        "\n",
        "        if self.scaler_trf == 'Y':\n",
        "            self.scaler()\n",
        "\n",
        "        if self.outliers == 'Y' or self.log_trf == 'Y' or self.scaler_trf =='Y':\n",
        "            self.distribution()\n",
        "\n",
        "    def __call__(self):\n",
        "\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
        "        data = pd.concat([self.test, self.train])\n",
        "        self.train_enc, self.test_enc = self.encode(data)\n",
        "\n",
        "        self.cat_features_card = []\n",
        "        for f in self.cat_features:\n",
        "            self.cat_features_card.append(1 + data[f].max())\n",
        "\n",
        "        self.y = self.train[self.target]\n",
        "        self.train = self.train.drop(self.target, axis=1)\n",
        "        self.train_enc = self.train_enc.drop(self.target, axis=1)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        self.train_enc[self.num_features] = scaler.fit_transform(self.train_enc[self.num_features])\n",
        "        self.test_enc[self.num_features] = scaler.transform(self.test_enc[self.num_features])\n",
        "\n",
        "        return self.train, self.train_enc, self.y, self.test, self.test_enc, self.cat_features\n",
        "\n",
        "    def encode(self, data):\n",
        "\n",
        "        oe = OrdinalEncoder()\n",
        "        data[self.cat_features] = oe.fit_transform(data[self.cat_features]).astype('int')\n",
        "\n",
        "        train_enc = data[~data[self.target].isna()]\n",
        "        test_enc = data[data[self.target].isna()].drop(self.target, axis=1)\n",
        "        return train_enc, test_enc\n",
        "\n",
        "    def new_features(self, df):\n",
        "\n",
        "        price_flags = [\"Mat_Siz_Col\",\t\"Siz_Lap_Col\",\t\"Bra_Siz_Wat\",\t\"Siz_Lap_Wat\",\t\"Mat_Lap_Wat\",\t\"Bra_Siz_Sty\",\t\"Bra_Lap_Wat\",\t\"Siz_Com_Lap\",\t\"Siz_Lap_Sty\",\n",
        "                       \"Mat_Com_Lap\",\t\"Mat_Siz_Com\",\t\"Bra_Siz_Com\",\t\"Com_Lap_Wat\",\t\"Bra_Siz_Lap\",\t\"Bra_Mat_Siz\",\t\"Siz_Com_Wat\",\t\"Siz_Com_Sty\"]\n",
        "\n",
        "        df['cheap_flag'] = df[price_flags].apply(lambda row: 1 in row.values, axis=1).astype(\"category\")\n",
        "        df['expansive_flag'] = df[price_flags].apply(lambda row: 2 in row.values, axis=1).astype(\"category\")\n",
        "\n",
        "        df = df.drop(columns=price_flags)\n",
        "        df = df.drop(columns=[\"Weight Capacity (kg)_missing\"])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def log_transformation(self):\n",
        "\n",
        "        self.train[self.log_trans_cols] = np.log1p(self.train[self.log_trans_cols])\n",
        "        self.test[self.log_trans_cols] = np.log1p(self.test[self.log_trans_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def forced_norm_transformation(self):\n",
        "\n",
        "        self.train[self.force_norm_cols] = np.sqrt(self.train[self.force_norm_cols]+0.1)\n",
        "        self.test[self.force_norm_cols] = np.sqrt(self.test[self.force_norm_cols]+0.1)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impose_normalization_transformation(self):\n",
        "\n",
        "        scaler = QuantileTransformer(output_distribution='normal',subsample=20_000,random_state=42)\n",
        "        self.train[self.impose_norm_cols] = scaler.fit_transform(self.train[self.impose_norm_cols])\n",
        "        self.test[self.impose_norm_cols] = scaler.transform(self.test[self.impose_norm_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def distribution(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nHistograms of distribution\\n')\n",
        "        fig, axes = plt.subplots(nrows=len(self.num_features), ncols=2, figsize=(15, len(self.num_features)*5))\n",
        "\n",
        "        for (ax_r, ax_n), col in zip(axes, self.num_features):\n",
        "\n",
        "            ax_r.set_title(f'{col} ($\\mu=$ {self.train_raw[col].mean():.2f} and $\\sigma=$ {self.train_raw[col].std():.2f} )')\n",
        "            ax_r.hist(self.train_raw[col], bins=30, color='tomato',alpha=0.7)\n",
        "            ax_r.axvline(self.train_raw[col].mean(), color='r', label='Mean')\n",
        "            ax_r.axvline(self.train_raw[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_r.legend()\n",
        "\n",
        "            ax_n.set_title(f'{col} Normalized ($\\mu=$ {self.train[col].mean():.2f} and $\\sigma=$ {self.train[col].std():.2f} )')\n",
        "            ax_n.hist(self.train[col], bins=30, color='royalblue',alpha=0.7)\n",
        "            ax_n.axvline(self.train[col].mean(), color='r', label='Mean')\n",
        "            ax_n.axvline(self.train[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_n.legend()\n",
        "\n",
        "    def remove_outliers(self):\n",
        "        Q1 = self.train[self.targets].quantile(0.25)\n",
        "        Q3 = self.train[self.targets].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_limit = Q1 - 1.5*IQR\n",
        "        upper_limit = Q3 + 1.5*IQR\n",
        "        self.train = self.train[(self.train[self.targets] >= lower_limit) & (self.train[self.targets] <= upper_limit)]\n",
        "        self.train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def scaler(self):\n",
        "        scaler = StandardScaler()\n",
        "        self.train[self.num_features] = scaler.fit_transform(self.train[self.num_features])\n",
        "        self.test[self.num_features] = scaler.transform(self.test[self.num_features])\n",
        "        return self\n",
        "\n",
        "    def missing_values(self):\n",
        "\n",
        "        self.train[self.num_features] = self.train[self.num_features].fillna(self.train[self.num_features].median())\n",
        "        self.test[self.num_features] = self.test[self.num_features].fillna(self.test[self.num_features].median())\n",
        "        for column in self.cat_features:\n",
        "            self.train[column] = self.train[column].fillna(self.train[column].mode()[0])\n",
        "            self.test[column] = self.test[column].fillna(self.test[column].mode()[0])\n",
        "        return self\n",
        "\n",
        "    def target_encoding(self):\n",
        "        te = TargetEncoder()\n",
        "        self.train[self.trg_enc_feat] = te.fit_transform(self.train[self.trg_enc_feat],self.train[self.target])\n",
        "        self.test[self.trg_enc_feat] = te.transform(self.test[self.trg_enc_feat])\n",
        "\n",
        "        for a in self.cat_features:\n",
        "            self.cat_features.remove(a)\n",
        "\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def cat_features(self):\n",
        "        return self._cat_features\n",
        "\n",
        "    @cat_features.setter\n",
        "    def cat_features(self, cat_features):\n",
        "        self._cat_features = cat_features\n",
        "\n",
        "    @property\n",
        "    def num_features(self):\n",
        "        return self._num_features\n",
        "\n",
        "    @num_features.setter\n",
        "    def num_features(self, num_features):\n",
        "        self._num_features = num_features\n",
        "\n",
        "    @property\n",
        "    def cat_features_card(self):\n",
        "        return self._cat_features_card\n",
        "\n",
        "    @cat_features_card.setter\n",
        "    def cat_features_card(self, cat_features_card):\n",
        "        self._cat_features_card = cat_features_card\n",
        "\n",
        "    @property\n",
        "    def train(self):\n",
        "        return self._train\n",
        "\n",
        "    @train.setter\n",
        "    def train(self, train):\n",
        "        self._train = train\n",
        "\n",
        "    @property\n",
        "    def direction(self):\n",
        "        return self._direction\n",
        "\n",
        "    @direction.setter\n",
        "    def direction(self, direction):\n",
        "        self._direction= direction\n",
        "\n",
        "\n",
        "class MixedDataImputer:\n",
        "    \"\"\"\n",
        "    Imputes missing values in mixed-data train and test DataFrames using\n",
        "    separate IterativeImputers for numerical and categorical features.\n",
        "\n",
        "    Args:\n",
        "      train_df: Pandas DataFrame with training data.\n",
        "      test_df: Pandas DataFrame with test data.\n",
        "      target_feature: Name of the target feature column.\n",
        "      random_state: Random state for reproducibility (default=42).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_df, test_df, target_feature=None, random_state=42):\n",
        "        super().__init__()\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.target_feature = target_feature\n",
        "        self.random_state = random_state\n",
        "        self.num_features = None\n",
        "        self.cat_features = None\n",
        "\n",
        "    def _identify_features(self):\n",
        "        \"\"\"Identifies numerical and categorical features.\"\"\"\n",
        "        self.num_features = self.train_df.select_dtypes(include=['number']).columns.tolist()\n",
        "        self.cat_features = self.train_df.select_dtypes(exclude=['number']).columns.tolist()\n",
        "        #self.num_features.remove(self.target_feature)  # Remove target from numerical features\n",
        "\n",
        "    def _impute_data(self, df):\n",
        "        \"\"\"Imputes missing values in a DataFrame.\"\"\"\n",
        "        df_num = df[self.num_features].copy()\n",
        "        df_cat = df[self.cat_features].copy()\n",
        "\n",
        "        # Impute numerical features only if there are missing values\n",
        "        if df_num.isnull().values.any():\n",
        "            num_imputer = IterativeImputer(estimator=BayesianRidge(),\n",
        "                                          random_state=self.random_state)\n",
        "            df_num_imputed = pd.DataFrame(num_imputer.fit_transform(df_num),\n",
        "                                         columns=self.num_features)\n",
        "        else:\n",
        "            df_num_imputed = df_num  # No imputation needed\n",
        "\n",
        "        # Impute categorical features only if there are missing values\n",
        "        if df_cat.isnull().values.any():\n",
        "            cat_imputer = IterativeImputer(estimator=LogisticRegression(),\n",
        "                                          initial_strategy='most_frequent',\n",
        "                                          random_state=self.random_state)\n",
        "            df_cat_imputed = pd.DataFrame(cat_imputer.fit_transform(df_cat),\n",
        "                                         columns=self.cat_features)\n",
        "\n",
        "            # Convert categorical features back to their original datatype\n",
        "            for feature in self.cat_features:\n",
        "                df_cat_imputed[feature] = df_cat_imputed[feature].astype(df[feature].dtype)\n",
        "        else:\n",
        "            df_cat_imputed = df_cat  # No imputation needed\n",
        "\n",
        "        # Concatenate the imputed DataFrames\n",
        "        df_imputed = pd.concat([df_num_imputed, df_cat_imputed], axis=1)\n",
        "\n",
        "        return df_imputed\n",
        "\n",
        "    def transform(self):\n",
        "        \"\"\"\n",
        "        Imputes missing values in both train and test DataFrames.\n",
        "\n",
        "        Returns:\n",
        "          train_df_imputed: Pandas DataFrame with imputed training data.\n",
        "          test_df_imputed: Pandas DataFrame with imputed test data.\n",
        "        \"\"\"\n",
        "        self._identify_features()\n",
        "        train_df_imputed = self._impute_data(self.train_df)\n",
        "        test_df_imputed = self._impute_data(self.test_df)\n",
        "        return train_df_imputed, test_df_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YimbhYB_v_G5"
      },
      "outputs": [],
      "source": [
        "t = Transform()\n",
        "X, X_enc, y, test, test_enc, cat_features = t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQFVvKjm560K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9331b95-991e-4cbb-ac15-0de65535bf23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Brand',\n",
              "  'Material',\n",
              "  'Size',\n",
              "  'Compartments',\n",
              "  'Laptop Compartment',\n",
              "  'Waterproof',\n",
              "  'Style',\n",
              "  'Color',\n",
              "  'cheap_flag',\n",
              "  'expansive_flag'],\n",
              " [6, 5, 4, 10, 3, 3, 4, 7, 2, 2],\n",
              " (3994318, 14),\n",
              " 'minimize')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "t.cat_features, t.cat_features_card, t.train.shape, t.direction_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J1svia6wHk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a46afb3b-0c21-4111-e3bb-2fbb0c5d4388"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3994318, 14), (3994318, 14), (200000, 14), (200000, 14))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "X.shape, X_enc.shape, test.shape, test_enc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BsRXz1PiZfT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "aa4a1aca-94d7-4d42-a72c-f6d98bf70dbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Brand                   0\n",
              "Material                0\n",
              "Size                    0\n",
              "Compartments            0\n",
              "Laptop Compartment      0\n",
              "Waterproof              0\n",
              "Style                   0\n",
              "Color                   0\n",
              "Weight Capacity (kg)    0\n",
              "TE_wc                   0\n",
              "skew_0                  0\n",
              "skew_1                  0\n",
              "cheap_flag              0\n",
              "expansive_flag          0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Brand</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Material</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Compartments</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Waterproof</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Style</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Color</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TE_wc</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheap_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>expansive_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "X_enc.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4StkPOujlvR"
      },
      "outputs": [],
      "source": [
        "imputer = MixedDataImputer(X_enc, test_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6vWq-ulkDQB"
      },
      "outputs": [],
      "source": [
        "train_df_imputed, test_df_imputed = imputer.transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iF1T6CCtD-B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "d98ccc09-ba98-438e-a053-c675fd6958be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Brand                   0\n",
              "Material                0\n",
              "Size                    0\n",
              "Compartments            0\n",
              "Laptop Compartment      0\n",
              "Waterproof              0\n",
              "Style                   0\n",
              "Color                   0\n",
              "Weight Capacity (kg)    0\n",
              "TE_wc                   0\n",
              "skew_0                  0\n",
              "skew_1                  0\n",
              "cheap_flag              0\n",
              "expansive_flag          0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Brand</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Material</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Compartments</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Waterproof</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Style</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Color</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TE_wc</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheap_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>expansive_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_df_imputed.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZsrhGGvPt39"
      },
      "source": [
        "## 3.0 Advanced Feature Engeneering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1kqkztpPy6m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "ca24c119-7677-4cf6-8de0-7752c733902e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "1599364      3         3     0             9                   2           1   \n",
              "195290       0         0     3             2                   2           2   \n",
              "2789824      1         0     2             4                   2           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "1599364      1      2             -1.192626 -0.179441 -0.385691 -0.428730   \n",
              "195290       3      4             -1.428271 -0.571992 -0.846883  0.867903   \n",
              "2789824      0      3              1.702300  1.417259  2.147031  2.077067   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "1599364           0               0  \n",
              "195290            0               0  \n",
              "2789824           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4cabb9d0-cdda-40bd-a501-ff3ec5d194ca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1599364</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.192626</td>\n",
              "      <td>-0.179441</td>\n",
              "      <td>-0.385691</td>\n",
              "      <td>-0.428730</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195290</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.428271</td>\n",
              "      <td>-0.571992</td>\n",
              "      <td>-0.846883</td>\n",
              "      <td>0.867903</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2789824</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.702300</td>\n",
              "      <td>1.417259</td>\n",
              "      <td>2.147031</td>\n",
              "      <td>2.077067</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4cabb9d0-cdda-40bd-a501-ff3ec5d194ca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4cabb9d0-cdda-40bd-a501-ff3ec5d194ca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4cabb9d0-cdda-40bd-a501-ff3ec5d194ca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ba417630-1bf3-4a10-82a2-3ab62a170b7d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba417630-1bf3-4a10-82a2-3ab62a170b7d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ba417630-1bf3-4a10-82a2-3ab62a170b7d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          9,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.1926263570785522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.1794411987066269\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.38569140434265137\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.42872950434684753\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLNavxj3aTVe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "4970f937-360e-44d8-f3b2-2aa8a7242b98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "3920860      5         1     0             7                   2           1   \n",
              "2580869      5         1     1             5                   2           1   \n",
              "2596813      0         0     1             9                   1           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "3920860      1      2             -1.155698  0.016408 -0.385691 -1.621354   \n",
              "2580869      3      1              0.108897 -0.252539 -0.168518 -2.060967   \n",
              "2596813      1      3              0.467736  2.692105  0.483511  0.987293   \n",
              "\n",
              "         cheap_flag  expansive_flag       Price  \n",
              "3920860           0               0  131.990707  \n",
              "2580869           0               0   97.162819  \n",
              "2596813           0               0  105.449501  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1d13c3f-4f1c-4e40-b3e6-a400436234ef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3920860</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.155698</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-0.385691</td>\n",
              "      <td>-1.621354</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>131.990707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2580869</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.108897</td>\n",
              "      <td>-0.252539</td>\n",
              "      <td>-0.168518</td>\n",
              "      <td>-2.060967</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>97.162819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2596813</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.467736</td>\n",
              "      <td>2.692105</td>\n",
              "      <td>0.483511</td>\n",
              "      <td>0.987293</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>105.449501</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1d13c3f-4f1c-4e40-b3e6-a400436234ef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b1d13c3f-4f1c-4e40-b3e6-a400436234ef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b1d13c3f-4f1c-4e40-b3e6-a400436234ef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a327f3f2-516d-42d0-be47-7f7a3993d8cd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a327f3f2-516d-42d0-be47-7f7a3993d8cd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a327f3f2-516d-42d0-be47-7f7a3993d8cd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc_y\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 5,\n        \"max\": 9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          7,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.155698299407959\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.01640772819519043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.38569140434265137\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.621354103088379\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          131.99070739746094\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "X_enc_y = pd.concat([X_enc, y], axis=1)\n",
        "X_enc_y.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1eGrjSSNBTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08398a4-2621-4a2d-ed04-1497942b6440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 15 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float32\n",
            " 9   TE_wc                 float32\n",
            " 10  skew_0                float32\n",
            " 11  skew_1                float32\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            " 14  Price                 float32\n",
            "dtypes: float32(5), int64(10)\n",
            "memory usage: 411.4 MB\n"
          ]
        }
      ],
      "source": [
        "X_enc_y.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzFLG4OrlysF"
      },
      "outputs": [],
      "source": [
        "class plot_class():\n",
        "\n",
        "    def __init__(self,df, target_variable, features_cat,features_num):\n",
        "      self.df = df\n",
        "      self.target_variable = target_variable\n",
        "      self.features_cat = features_cat\n",
        "      self.features_num = features_num\n",
        "\n",
        "    @classmethod\n",
        "    def plot_categorical_features(cls, df, target_variable, features_cat, features_num):\n",
        "        \"\"\"\n",
        "        Plots the frequency of the target variable for each value of multiple categorical features.\n",
        "\n",
        "        Args:\n",
        "          df: Pandas DataFrame containing the data.\n",
        "          target_variable: Name of the target variable column in the DataFrame.\n",
        "          features: List of names of the categorical feature columns to plot.\n",
        "        \"\"\"\n",
        "\n",
        "        num_features = len(features_cat)\n",
        "        num_rows = (num_features + 1) // 2  # Calculate the number of rows needed\n",
        "\n",
        "        fig, axes = plt.subplots(num_rows, 2, figsize=(12, 4 * num_rows))\n",
        "        axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
        "\n",
        "        for i, feature in enumerate(features_cat):\n",
        "            cross_tab = pd.crosstab(df[feature], df[target_variable])\n",
        "            cross_tab.plot(kind='bar', stacked=False, position=0.3, width=0.4, ax=axes[i],colormap=palette_1, alpha=0.6)\n",
        "            axes[i].set_xlabel(feature)\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "            axes[i].set_title(f'Frequency of {target_variable} by {feature}')\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for i in range(num_features, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return cls(df, target_variable, features_cat, features_num)\n",
        "\n",
        "    @classmethod\n",
        "    def plot_numerical_features(cls, df, target_variable, features_cat, features_num):\n",
        "        \"\"\"\n",
        "        Generates violin plots for numerical features, showing the distribution for each target class.\n",
        "\n",
        "        Args:\n",
        "          df: Pandas DataFrame containing the data.\n",
        "          target_variable: Name of the target variable column in the DataFrame.\n",
        "          features: List of names of the numerical feature columns to plot.\n",
        "        \"\"\"\n",
        "\n",
        "        num_features = len(features_num)\n",
        "        num_rows = (num_features + 1) // 2  # Calculate the number of rows needed\n",
        "\n",
        "        fig, axes = plt.subplots(num_rows, 2, figsize=(12, 4 * num_rows))\n",
        "        axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
        "\n",
        "        for i, feature in enumerate(features_num):\n",
        "            sns.violinplot(x=target_variable, y=feature, data=df, ax=axes[i],\n",
        "                           hue=target_variable,  # Use 'hue' to color by target class\n",
        "                           palette=palette_9)\n",
        "            axes[i].set_xlabel(target_variable)\n",
        "            axes[i].set_ylabel(feature)\n",
        "            axes[i].set_title(f'Distribution of {feature} by {target_variable}')\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for i in range(num_features, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return cls(df, target_variable, features_cat, features_num)\n",
        "\n",
        "    def scatter_comp(self, feat_01, feat_02, hue_def):\n",
        "        \"\"\"\n",
        "        Generates a scatter plot between two features, colored by a third\n",
        "        categorical feature using Seaborn.\n",
        "\n",
        "        Args:\n",
        "          df: Pandas DataFrame containing the data.\n",
        "          x_feature: Name of the feature to plot on the x-axis.\n",
        "          y_feature: Name of the feature to plot on the y-axis.\n",
        "          color_feature: Name of the categorical feature to use for coloring.\n",
        "        \"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.scatterplot(\n",
        "            x=feat_01,\n",
        "            y=feat_02,\n",
        "            hue=hue_def,  # Use 'hue' for color encoding\n",
        "            data=self.df,\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        plt.xlabel(feat_01)\n",
        "        plt.ylabel(feat_02)\n",
        "        plt.title(f'ScatterPlot of {feat_01} vs. {feat_02} colored by {hue_def}')\n",
        "        plt.show()\n",
        "\n",
        "    def heatmap_corr(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nCorrelation Heatmap\\n')\n",
        "        plt.figure(figsize=(7,7))\n",
        "        corr = self.df.select_dtypes(exclude='int').corr(method='pearson')\n",
        "        sns.heatmap(corr, fmt = '0.2f', cmap = \"Reds\", annot=True, cbar=False)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YShjpCmaPB7"
      },
      "outputs": [],
      "source": [
        "#plot_instance = plot_class.plot_categorical_features(df=X_enc_y, target_variable=\"loan_status\", features_cat=t.cat_features, features_num=t.num_features);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYJ72HImPy3N"
      },
      "outputs": [],
      "source": [
        "#plot_instance.plot_numerical_features(df=X_enc_y, target_variable=\"loan_status\", features_cat=t.cat_features, features_num=t.num_features);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thXxmDkdPy0L"
      },
      "outputs": [],
      "source": [
        "#plot_instance.scatter_comp(feat_01=\"loan_sustainability\", feat_02=\"loan_grade\", hue_def=\"loan_status\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLWYiuaaPyxD"
      },
      "outputs": [],
      "source": [
        "#plot_instance.heatmap_corr()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc.info()"
      ],
      "metadata": {
        "id": "Y20xEnaSuowD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dab5cc9-6300-4265-993d-3c60469bb0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float32\n",
            " 9   TE_wc                 float32\n",
            " 10  skew_0                float32\n",
            " 11  skew_1                float32\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            "dtypes: float32(4), int64(10)\n",
            "memory usage: 396.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKAbo5tTtczL"
      },
      "outputs": [],
      "source": [
        "X_enc_y.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_train_enc_expanded.csv\", index=False)\n",
        "test_enc.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_test_enc_expanded.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMb-65cVOuzp"
      },
      "source": [
        "## **4.0 MODELS**\n",
        "\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmZnoc4eO1_E"
      },
      "source": [
        "### **4.1 TREE BASED MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1GyNmMXO7Dw"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "# class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# sample_pos_weight = class_weights[1]/class_weights[0]\n",
        "# sample_pos_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qrTmdIJRsR-"
      },
      "source": [
        "#### 4.1.1 CatBoostClassifier:\n",
        "\n",
        "    class CatBoostClassifier(iterations=None,\n",
        "                            learning_rate=None,\n",
        "                            depth=None,\n",
        "                            l2_leaf_reg=None,\n",
        "                            model_size_reg=None,\n",
        "                            rsm=None,\n",
        "                            loss_function=None,\n",
        "                            border_count=None,\n",
        "                            feature_border_type=None,\n",
        "                            per_float_feature_quantization=None,\n",
        "                            input_borders=None,\n",
        "                            output_borders=None,\n",
        "                            fold_permutation_block=None,\n",
        "                            od_pval=None,\n",
        "                            od_wait=None,\n",
        "                            od_type=None,\n",
        "                            nan_mode=None,\n",
        "                            counter_calc_method=None,\n",
        "                            leaf_estimation_iterations=None,\n",
        "                            leaf_estimation_method=None,\n",
        "                            thread_count=None,\n",
        "                            random_seed=None,\n",
        "                            use_best_model=None,\n",
        "                            verbose=None,\n",
        "                            logging_level=None,\n",
        "                            metric_period=None,\n",
        "                            ctr_leaf_count_limit=None,\n",
        "                            store_all_simple_ctr=None,\n",
        "                            max_ctr_complexity=None,\n",
        "                            has_time=None,\n",
        "                            allow_const_label=None,\n",
        "                            classes_count=None,\n",
        "                            class_weights=None,\n",
        "                            auto_class_weights=None,\n",
        "                            one_hot_max_size=None,\n",
        "                            random_strength=None,\n",
        "                            name=None,\n",
        "                            ignored_features=None,\n",
        "                            train_dir=None,\n",
        "                            custom_loss=None,\n",
        "                            custom_metric=None,\n",
        "                            eval_metric=None,\n",
        "                            bagging_temperature=None,\n",
        "                            save_snapshot=None,\n",
        "                            snapshot_file=None,\n",
        "                            snapshot_interval=None,\n",
        "                            fold_len_multiplier=None,\n",
        "                            used_ram_limit=None,\n",
        "                            gpu_ram_part=None,\n",
        "                            allow_writing_files=None,\n",
        "                            final_ctr_computation_mode=None,\n",
        "                            approx_on_full_history=None,\n",
        "                            boosting_type=None,\n",
        "                            simple_ctr=None,\n",
        "                            combinations_ctr=None,\n",
        "                            per_feature_ctr=None,\n",
        "                            task_type=None,\n",
        "                            device_config=None,\n",
        "                            devices=None,\n",
        "                            bootstrap_type=None,\n",
        "                            subsample=None,\n",
        "                            sampling_unit=None,\n",
        "                            dev_score_calc_obj_block_size=None,\n",
        "                            max_depth=None,\n",
        "                            n_estimators=None,\n",
        "                            num_boost_round=None,\n",
        "                            num_trees=None,\n",
        "                            colsample_bylevel=None,\n",
        "                            random_state=None,\n",
        "                            reg_lambda=None,\n",
        "                            objective=None,\n",
        "                            eta=None,\n",
        "                            max_bin=None,\n",
        "                            scale_pos_weight=None,\n",
        "                            gpu_cat_features_storage=None,\n",
        "                            data_partition=None\n",
        "                            metadata=None,\n",
        "                            early_stopping_rounds=None,\n",
        "                            cat_features=None,\n",
        "                            grow_policy=None,\n",
        "                            min_data_in_leaf=None,\n",
        "                            min_child_samples=None,\n",
        "                            max_leaves=None,\n",
        "                            num_leaves=None,\n",
        "                            score_function=None,\n",
        "                            leaf_estimation_backtracking=None,\n",
        "                            ctr_history_unit=None,\n",
        "                            monotone_constraints=None,\n",
        "                            feature_weights=None,\n",
        "                            penalties_coefficient=None,\n",
        "                            first_feature_use_penalties=None,\n",
        "                            model_shrink_rate=None,\n",
        "                            model_shrink_mode=None,\n",
        "                            langevin=None,\n",
        "                            diffusion_temperature=None,\n",
        "                            posterior_sampling=None,\n",
        "                            boost_from_average=None,\n",
        "                            text_features=None,\n",
        "                            tokenizers=None,\n",
        "                            dictionaries=None,\n",
        "                            feature_calcers=None,\n",
        "                            text_processing=None,\n",
        "                            fixed_binary_splits=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvZIUdxLzo-n"
      },
      "outputs": [],
      "source": [
        "cat_prob = {\"objective\":\"RMSE\",\"eval_metric\":\"RMSE\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJvn8jzFQUNr"
      },
      "source": [
        "##### 4.1.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDGGnrUAQUNr"
      },
      "outputs": [],
      "source": [
        "def objective_catboost(trial, X, y, n_splits, n_repeats, model=CatBoostRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=cat_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {\n",
        "        'iterations': 1000,\n",
        "        'learning_rate': 0.025, #trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'depth': trial.suggest_int('depth', 5, 9),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-4, 0.1, log=True),\n",
        "        \"bootstrap_type\": \"Bayesian\",\n",
        "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.5, 1.5, step=0.1),\n",
        "        'random_strength': trial.suggest_float('random_strength', 0.5, 3.5, step=0.25),\n",
        "        #'border_count': trial.suggest_int('border_count', 32, 255),\n",
        "        'cat_features': categorical_features,\n",
        "        'task_type': 'GPU' if use_gpu else 'CPU',\n",
        "        'random_seed':rs,\n",
        "        'verbose': 250,\n",
        "        'objective': metrics[\"objective\"],\n",
        "        'eval_metric': metrics[\"eval_metric\"],\n",
        "        \"od_type\":'EBS', #Early stopping hyperparmeter\n",
        "        \"od_wait\":101,\n",
        "        #\"sampling_frequency\":\"PerTreeLevel\",\n",
        "        \"use_best_model\":True,\n",
        "    }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "      kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "      kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "      kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "      kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1,1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1,1)\n",
        "\n",
        "        if fit_scaling:\n",
        "          scaler = StandardScaler()\n",
        "          X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "          X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create the Pool objects for CatBoost\n",
        "        train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features)\n",
        "        valid_pool = Pool(data=X_valid, label=y_valid, cat_features=categorical_features)\n",
        "\n",
        "        # Create the pipeline\n",
        "        model = model_class(**params)\n",
        "        # Fit the model:\n",
        "        model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=101,\n",
        "                  #callbacks=[optuna.integration.CatBoostPruningCallback(trial, \"RMSE\")]\n",
        "                  )\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "#        y_pred = np.expm1(y_pred)\n",
        "#        y_valid = np.expm1(y_valid)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4q9nTQpQUNs"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_catboost(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMaZd811QUNs"
      },
      "outputs": [],
      "source": [
        "# usage with XGBRegressor\n",
        "cat_study = tune_hyperparameters(X_enc, y, model_class=CatBoostRegressor, n_trials=31, n_splits_ = 5 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(cat_study, CatBoostRegressor, \"CatBoost_ext\")\n",
        "cat_params = cat_study.best_paramsy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 10 finished with value: 38.67948459216966\n",
        "- Parameters: {'depth': 9, 'l2_leaf_reg': 0.004177701145518355, 'bagging_temperature': 0.5, 'random_strength': 0.5}"
      ],
      "metadata": {
        "id": "c8WTOGC1vY-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-aPe-7lMjdY"
      },
      "outputs": [],
      "source": [
        "X_enc.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvqbaMJMxyqI"
      },
      "source": [
        "#### **4.2.1 LGBMRegressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2VbsJlay765"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm_prob = {\"objective\":\"regression\",\"eval_metric\":\"rmse\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOB-zCzMx9Jr"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cHDHOJMiYKK"
      },
      "outputs": [],
      "source": [
        "X_enc.info()\n",
        "t.cat_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_EHAxqPPyrU"
      },
      "outputs": [],
      "source": [
        "def objective_lgbm(trial, X, y, n_splits, n_repeats, model=LGBMRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=lgbm_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 131),\n",
        "        'learning_rate': 0.02, #trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        #'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 60),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 0.95),\n",
        "        'subsample_freq': trial.suggest_int('subsample_freq', 1, 3),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        \"reg_alpha\" :         trial.suggest_float(\"reg_alpha\", 1e-3, 1.0, log=True),\n",
        "        \"reg_lambda\" :        trial.suggest_float(\"reg_lambda\", 1e-3, 1.0, log=True),\n",
        "        \"boosting_type\":      'gbdt',\n",
        "        'n_estimators': 2501,\n",
        "        'objective': metrics[\"objective\"],\n",
        "        'device': 'gpu' if use_gpu else 'cpu',\n",
        "        'verbose': -1,\n",
        "        #'scale_pos_weight': sample_pos_weight,\n",
        "#       'categorical_feature': [2,4,5,9],\n",
        "        'random_state': rs,\n",
        "    }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1, 1)\n",
        "\n",
        "        if fit_scaling:\n",
        "            scaler = StandardScaler()\n",
        "            X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "            X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create the datasets for LightGBM\n",
        "        # d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "        # d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features)\n",
        "\n",
        "        # Create the model\n",
        "        model = model_class(**params)\n",
        "\n",
        "        # Create the early stopping callback\n",
        "        early_stop = early_stopping(stopping_rounds=101)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[early_stop], eval_metric=metrics[\"eval_metric\"], categorical_feature= categorical_features)\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnC-tzJ-PyoE"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_lgbm(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mXmUXUZPyk8"
      },
      "outputs": [],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=LGBMRegressor, n_trials=31, n_splits_ = 5 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(cat_study, LGBMRegressor, \"LGBMBoost_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 28 finished with value: 38.657451168104984\n",
        "- parameters: {'num_leaves': 103, 'min_child_samples': 36, 'subsample': 0.9131771240297577, 'subsample_freq': 2, 'colsample_bytree': 0.6190291906152294, 'reg_alpha': 0.03976551748855951, 'reg_lambda': 0.2576052197300848}"
      ],
      "metadata": {
        "id": "4Ul6eVTJZNC5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md0hNqSwgfV4"
      },
      "source": [
        "#### **4.3.1 XGBClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yoxlSCCgfV5"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "xgb_prob = {'objective': \"reg:squarederror\",'eval_metric': \"rmse\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc[t.cat_features] = X_enc[t.cat_features].astype(\"category\")\n",
        "test_enc[t.cat_features] = test_enc[t.cat_features].astype(\"category\")\n",
        "test_enc.info()"
      ],
      "metadata": {
        "id": "ELHBSxDA4C95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SglmFB9QgfV5"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ROmQq9OgfV6"
      },
      "outputs": [],
      "source": [
        "def objective_xgb(trial, X, y, n_splits, n_repeats, model=XGBRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=xgb_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {\n",
        "              'n_estimators': 1000,\n",
        "              'learning_rate': 0.025, #trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "              'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "              #'max_bin': trial.suggest_int('max_bin', 255, 511),\n",
        "              'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "              'subsample': trial.suggest_float('subsample', 0.7, 0.95),\n",
        "              'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.95),\n",
        "              'gamma': trial.suggest_float('gamma', 0, 1),\n",
        "              'reg_alpha': trial.suggest_float('reg_alpha', 0.00001, 1.0, log=True),\n",
        "              'reg_lambda': trial.suggest_float('reg_lambda', 0.00001, 10, log=True),\n",
        "              'objective':  metrics[\"objective\"],  # For binary classification\n",
        "              'eval_metric': metrics[\"eval_metric\"],\n",
        "              \"early_stopping_rounds\":51,\n",
        "              'tree_method': 'gpu_hist' if use_gpu else 'hist',  # Use GPU if available\n",
        "              'random_state': rs,\n",
        "              'enable_categorical': True,\n",
        "#              'scale_pos_weight': sample_pos_weight,\n",
        "             }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1, 1)\n",
        "\n",
        "        if fit_scaling:\n",
        "            scaler = StandardScaler()\n",
        "            X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "            X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create DMatrix objects for XGBoost\n",
        "        d_train = xgb.DMatrix(X_train, label=y_train,feature_types=[\"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"q\",\"q\",\"q\",\"q\",\n",
        "                                                                    \"c\",\"c\"],enable_categorical=True)\n",
        "        d_valid = xgb.DMatrix(X_valid, label=y_valid,feature_types=[\"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"q\",\"q\",\"q\",\"q\",\n",
        "                                                                    \"c\",\"c\"],enable_categorical=True)\n",
        "\n",
        "        # Create the model\n",
        "        model = model_class(**params)\n",
        "\n",
        "        # Create the early stopping callback\n",
        "        # early_stop = early_stopping(stopping_rounds=101)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train,\n",
        "                  eval_set=[(X_valid, y_valid)],\n",
        "                  verbose=False)\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1QiMlUGgfV6"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_xgb(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57BpBdEqgfV6"
      },
      "outputs": [],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=XGBRegressor, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(cat_study, XGBRegressor, \"XGB_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 18 finished with value: 38.66071701049805\n",
        "- parameters: {'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.9493097301768285, 'colsample_bytree': 0.750026675584575, 'gamma': 0.5511841320880777, 'reg_alpha': 0.006948944983035811, 'reg_lambda': 0.0015661314558322087}."
      ],
      "metadata": {
        "id": "jxXsfZ_hDsYH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbThfscYTcDW"
      },
      "source": [
        "#### **4.4.1 TabNetClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u__JMQhTcDX"
      },
      "outputs": [],
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "tab_prob = {'eval_metric': \"rmse\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFqDr7bBZKoj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "a3f157d3-60d1-4e6a-c578-752ecc625063"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "3782983      5         3     1             7                   1           1   \n",
              "2003343      3         0     1             1                   2           2   \n",
              "3404963      4         3     3             7                   2           2   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "3782983      3      2              1.451171  0.016408  0.483511 -0.935853   \n",
              "2003343      3      0             -0.433108  0.033023 -1.185578 -0.348455   \n",
              "3404963      3      6              1.522432 -1.099254 -0.846883  0.448205   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "3782983           0               0  \n",
              "2003343           0               0  \n",
              "3404963           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24a6a99d-0672-43cf-92dd-bb5402b777ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3782983</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.451171</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>0.483511</td>\n",
              "      <td>-0.935853</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003343</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.433108</td>\n",
              "      <td>0.033023</td>\n",
              "      <td>-1.185578</td>\n",
              "      <td>-0.348455</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3404963</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1.522432</td>\n",
              "      <td>-1.099254</td>\n",
              "      <td>-0.846883</td>\n",
              "      <td>0.448205</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24a6a99d-0672-43cf-92dd-bb5402b777ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24a6a99d-0672-43cf-92dd-bb5402b777ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24a6a99d-0672-43cf-92dd-bb5402b777ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6f397794-e47b-42fc-85e9-13ceccbf56be\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f397794-e47b-42fc-85e9-13ceccbf56be')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6f397794-e47b-42fc-85e9-13ceccbf56be button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 7,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.4511709213256836\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.01640772819519043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4835107922554016\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.9358525276184082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UikYbKG4ZC24"
      },
      "outputs": [],
      "source": [
        "grouped_features = []\n",
        "feature_cols = X_enc.columns.to_list()\n",
        "\n",
        "group_1 = [\"Brand\",\t\"Material\",\t\"Size\",\t\"Compartments\",\t\"Laptop Compartment\",\t\"Waterproof\",\t\"Style\",\t\"Color\"]\n",
        "group_2 = ['cheap_flag',\t'expansive_flag']\n",
        "#group_3 = ['cb_person_default_on_file']\n",
        "\n",
        "# Iterate through each set of related columns (e.g., blood glucose, insulin, etc.)\n",
        "for colset in [group_1,group_2]:\n",
        "    group_idxs = [idx for idx, col in enumerate(feature_cols) if col in colset]\n",
        "    grouped_features.append(group_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh4feROUZoUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b96d80-f3f7-4c82-e67a-f13ee845aef4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 1, 2, 3, 4, 5, 6, 7], [12, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "grouped_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWEKkZi7TcDX"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyVDQ0FjUBUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daae5a4a-771f-488d-fceb-2741fa8fa4da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 13]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "cate_feat = t.cat_features.copy()\n",
        "\n",
        "for colset in [cate_feat]:\n",
        "    cat_index_cols = [idx for idx, col in enumerate(feature_cols) if col in colset]\n",
        "\n",
        "cat_index_cols[-2:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[len(X_enc[col].unique()) for col in t.cat_features][-2:]"
      ],
      "metadata": {
        "id": "X76B9uW6BSVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ca0d6d-d1d7-4d73-824d-4b7f3ff89c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWyN_Q_rTcDY"
      },
      "outputs": [],
      "source": [
        "def objective_tabnet(trial, X, y, n_splits, n_repeats, model=TabNetRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=tab_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'n_d': trial.suggest_categorical('n_d', [4,8,12]),\n",
        "              'n_a': trial.suggest_categorical('n_d', [4,8,12]),\n",
        "              'n_steps': trial.suggest_int('n_steps', 3, 4),\n",
        "              'gamma': trial.suggest_float('gamma', 1.01, 2),\n",
        "              'lambda_sparse':trial.suggest_float('lambda_sparse', 1e-5, 1e-1),\n",
        "              \"grouped_features\":grouped_features,\n",
        "              #'cat_idxs': cat_index_cols,\n",
        "              #'cat_dims': [len(X_enc[col].unique()) for col in categorical_features],\n",
        "              #'cat_emb_dim': [2,2,1,3,1,1,1,2,1,1],\n",
        "              'n_independent': trial.suggest_int('n_independent', 1, 3),\n",
        "              'n_shared': trial.suggest_int('n_shared', 1, 3),\n",
        "              'mask_type': trial.suggest_categorical('mask_type', ['sparsemax']),\n",
        "              'device_name': 'cuda' if use_gpu else 'cpu',\n",
        "              'seed': rs,\n",
        "              \"optimizer_fn\":torch.optim.Adam,\n",
        "              \"optimizer_params\":dict(lr=0.01),\n",
        "              \"scheduler_params\":{\"patience\":3, # how to use learning rate scheduler\n",
        "                                \"factor\":0.5,\n",
        "                                \"min_lr\":0.0001},\n",
        "              \"scheduler_fn\":torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1, 1)\n",
        "\n",
        "        if fit_scaling:\n",
        "            scaler = StandardScaler()\n",
        "            X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "            X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create the model\n",
        "        model = model_class(**params)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train.values, y_train,\n",
        "                  eval_set=[(X_valid.values, y_valid)],\n",
        "                  batch_size=1024,\n",
        "                  max_epochs=11,\n",
        "                  patience=5,\n",
        "                  eval_metric=[tab_prob[\"eval_metric\"]])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid.values)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfvK9Qw-TcDY"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_tabnet(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJwRqrJRTcDY"
      },
      "outputs": [],
      "source": [
        "tab_study = tune_hyperparameters(X_enc, y, model_class=TabNetRegressor, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(tab_study, TabNetRegressor, \"tabnet_ext\")\n",
        "tab_params = tab_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jch2yojjoiZS"
      },
      "source": [
        "#### 4.5.1 Yggdrasil - RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mgl-1wLt_4I"
      },
      "outputs": [],
      "source": [
        "#!pip install ydf -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_YHewPTzMVM"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vavm8Xws3vO"
      },
      "source": [
        "##### 4.5.2 Optuna Optimization:\n",
        "\n",
        "[HyperParameters Link](https://ydf.readthedocs.io/en/stable/hyperparameters/#lambda_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eJuaeqmPyh0"
      },
      "outputs": [],
      "source": [
        "# import ydf\n",
        "\n",
        "# def objective_ydf_gbt(trial, X, y, n_splits, n_repeats, model_ = ydf.GradientBoostedTreesLearner, rs=42, fit_scaling=False, cv_strategy=\"KFold\", use_gpu=False):\n",
        "\n",
        "#     model_class = model_  # Use ydf's GradientBoostedTreesLearner\n",
        "\n",
        "#     categorical_features = t.cat_features.copy()  # Assuming 't' is defined somewhere with categorical features\n",
        "\n",
        "#     num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "#     auc_scores = []\n",
        "\n",
        "#     params = {\n",
        "#         'loss':\"BINOMIAL_LOG_LIKELIHOOD\",\n",
        "#         'num_trees': trial.suggest_int('num_trees', 100, 1000),  # Number of trees in the forest\n",
        "#         'max_depth': trial.suggest_int('max_depth', 3, 10),  # Maximum depth of the trees\n",
        "#         'min_examples': trial.suggest_int('min_examples', 1, 10),  # Minimum number of samples required to split an internal node\n",
        "#         'l1_regularization': trial.suggest_float('l1_regularization', 1e-5, 10.0, log=True),\n",
        "#         'l2_regularization': trial.suggest_float('l2_regularization', 1e-5, 10.0, log=True),\n",
        "# #         'shrinkage': trial.suggest_float('shrinkage', 0.01, 0.3, log=True),  # Similar to learning rate, but applied after each tree is trained\n",
        "#          'subsample': trial.suggest_float('subsample', 0.7, 1.0),  # Fraction of samples used for training each tree\n",
        "# # #        'max_categorical_cardinality': trial.suggest_int('max_categorical_cardinality', 10, 100),  # Maximum number of unique values for categorical features\n",
        "# #         'categorical_algorithm': trial.suggest_categorical('categorical_algorithm', ['CART', 'RANDOM']),  # Algorithm used for categorical splits\n",
        "# #         'split_axis': trial.suggest_categorical('split_axis', ['AXIS_ALIGNED', 'SPARSE_OBLIQUE']),  # How to split numerical features\n",
        "# #         'sparse_oblique_normalization': trial.suggest_categorical('sparse_oblique_normalization', ['NONE', 'STANDARD_DEVIATION', 'MIN_MAX']),  # Normalization for sparse oblique splits\n",
        "# #         'sparse_oblique_num_projections_exponent': trial.suggest_float('sparse_oblique_num_projections_exponent', 0.5, 1.0),  # Exponent for the number of projections in sparse oblique splits\n",
        "#         'random_seed': rs,  # Random seed for reproducibility\n",
        "#         'early_stopping':\"LOSS_INCREASE\",  # Enable early stopping\n",
        "#         'early_stopping_num_trees_look_ahead': 30,  # Number of trees to look ahead for early stopping\n",
        "# #        'weights': {0:1,1:sample_pos_weight},\n",
        "#         \"label\" : \"loan_status\",\n",
        "#         }\n",
        "\n",
        "\n",
        "#     if cv_strategy == 'RepKFold':\n",
        "#         kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "#     elif cv_strategy == 'KFold':\n",
        "#         kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "#     elif cv_strategy == \"StratKFold\":\n",
        "#         kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "#     elif cv_strategy == \"RepStratKFold\":\n",
        "#         kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "#     for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "#         # Split the data\n",
        "#         X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
        "#         X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
        "\n",
        "#         if fit_scaling:\n",
        "#             scaler = StandardScaler()\n",
        "#             X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "#             X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "#         X_train[\"loan_status\"] = y_train.values\n",
        "#         X_valid[\"loan_status\"] = y_valid.values\n",
        "\n",
        "#         # Train the model with early stopping\n",
        "#         model = model_class(task=ydf.Task.CLASSIFICATION, **params)\n",
        "#         model.train(X_train, valid=(X_valid))  # Use ydf's fit method with early stopping\n",
        "\n",
        "#         # Make predictions\n",
        "#         y_pred = model.get_predictions(X_valid)[\"loan_status_probability_1\"]\n",
        "\n",
        "#         # Calculate AUC\n",
        "#         auc_score = roc_auc_score(y_valid, y_pred)\n",
        "#         auc_scores.append(auc_score)\n",
        "\n",
        "#     # Calculate the mean AUC\n",
        "#     key_metric = np.mean(auc_scores)\n",
        "\n",
        "#     return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbunXYe3vquN"
      },
      "outputs": [],
      "source": [
        "# # Step 2: Tuning Hyperparameters with Optuna\n",
        "# def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "#     study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "#     study.optimize(lambda trial: objective_ydf_gbt(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model_=model_class, use_gpu=use_gpu, cv_strategy=\"StratKFold\"), n_trials=n_trials)\n",
        "#     return study  # Return the study object\n",
        "\n",
        "# # Step 3: Saving Best Results and Models\n",
        "# def save_results(study, model_class, model_name):\n",
        "#     best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "#     joblib.dump(study.best_params, best_params_file)\n",
        "#     print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "#     verbose_file = f\"{model_name}_ydf_verbose.log\"\n",
        "#     with open(verbose_file, \"w\") as f:\n",
        "#         f.write(str(study.trials))\n",
        "#     print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6XXbt231ogK"
      },
      "outputs": [],
      "source": [
        "# cat_study = tune_hyperparameters(X_enc, y, model_class=ydf.GradientBoostedTreesLearner, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=False)\n",
        "# save_results(cat_study, TabNetClassifier, \"ydf_ext\")\n",
        "# cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41K1txKVzFhu"
      },
      "source": [
        "#### **4.6.1 NeuralNetwork**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGyFvBNgzFh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "b73f9188-9dae-44c8-91c6-852eeae6104a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "312954       1         4     0             8                   1           1   \n",
              "2703119      4         4     3             1                   1           1   \n",
              "678964       1         3     1             0                   2           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "312954       1      5             -1.865994 -0.753177  1.391704 -0.179771   \n",
              "2703119      3      3              1.110761  0.290271  1.019273  0.544976   \n",
              "678964       3      5              0.293288  0.645984 -0.168518 -0.517618   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "312954            0               0  \n",
              "2703119           0               0  \n",
              "678964            0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c297503-03e3-442d-865d-2bf33e08dcaf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>312954</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-1.865994</td>\n",
              "      <td>-0.753177</td>\n",
              "      <td>1.391704</td>\n",
              "      <td>-0.179771</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2703119</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1.110761</td>\n",
              "      <td>0.290271</td>\n",
              "      <td>1.019273</td>\n",
              "      <td>0.544976</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>678964</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.293288</td>\n",
              "      <td>0.645984</td>\n",
              "      <td>-0.168518</td>\n",
              "      <td>-0.517618</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c297503-03e3-442d-865d-2bf33e08dcaf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c297503-03e3-442d-865d-2bf33e08dcaf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c297503-03e3-442d-865d-2bf33e08dcaf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ced39ac2-1349-4819-9692-db020c3eb47d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ced39ac2-1349-4819-9692-db020c3eb47d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ced39ac2-1349-4819-9692-db020c3eb47d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          8,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.865993857383728\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.7531771659851074\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.391703724861145\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.17977148294448853\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512,last_layer = 1, activation=\"relu\"):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "VF6Qd4eziUd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIf9YCb8Bdcu",
        "outputId": "60892f8c-db2e-4b69-9081-f36f67fb21fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([6, 5, 4, 10, 3, 3, 4, 7, 2, 2],\n",
              " array([3., 3., 2., 4., 2., 2., 2., 3., 2., 2.]),\n",
              " 10)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Efx2uIuzFh4"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maUIASqazFh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c620f1d0-d621-4224-dda6-46500e384546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Dtype\n",
            "---  ------              -----\n",
            " 0   Brand               int64\n",
            " 1   Material            int64\n",
            " 2   Size                int64\n",
            " 3   Compartments        int64\n",
            " 4   Laptop Compartment  int64\n",
            " 5   Waterproof          int64\n",
            " 6   Style               int64\n",
            " 7   Color               int64\n",
            " 8   cheap_flag          int64\n",
            " 9   expansive_flag      int64\n",
            "dtypes: int64(10)\n",
            "memory usage: 335.2 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Weight Capacity (kg)  float32\n",
            " 1   TE_wc                 float32\n",
            " 2   skew_0                float32\n",
            " 3   skew_1                float32\n",
            "dtypes: float32(4)\n",
            "memory usage: 91.4 MB\n"
          ]
        }
      ],
      "source": [
        "categorical_feat = t.cat_features.copy()\n",
        "numerical_feat = t.num_features.copy()\n",
        "\n",
        "X_train_cat = X_enc[categorical_feat]\n",
        "X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "X_test_cat = test_enc[categorical_feat]\n",
        "X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "X_train_cat.info()\n",
        "X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GgRZk52zFh5"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"])}\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num])\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYf3TjJVzFh6"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9KKYtiYzFh7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cdfa05b7-110b-498b-f260-2cb6671285c6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 10:16:44,687] A new study created in memory with name: no-name-d7bfcb07-6dd6-4153-9e33-44f99b8d61aa\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  25s 7ms/step - RMSE: 53.3488 - loss: 2996.9104 - val_RMSE: 38.7680 - val_loss: 1502.9572 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7438 - loss: 1501.0842 - val_RMSE: 38.7101 - val_loss: 1498.4749 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7387 - loss: 1500.6854 - val_RMSE: 38.7810 - val_loss: 1503.9675 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7373 - loss: 1500.5754 - val_RMSE: 38.7479 - val_loss: 1501.4033 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7362 - loss: 1500.4922 - val_RMSE: 38.7172 - val_loss: 1499.0236 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7354 - loss: 1500.4320 - val_RMSE: 38.7088 - val_loss: 1498.3741 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7351 - loss: 1500.4102 - val_RMSE: 38.7098 - val_loss: 1498.4497 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.7346 - loss: 1500.3710 - val_RMSE: 38.7177 - val_loss: 1499.0566 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7342 - loss: 1500.3417 - val_RMSE: 38.7288 - val_loss: 1499.9218 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7336 - loss: 1500.2925 - val_RMSE: 38.7177 - val_loss: 1499.0636 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7327 - loss: 1500.2250 - val_RMSE: 38.7158 - val_loss: 1498.9110 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7253 - loss: 1499.6500 - val_RMSE: 38.6850 - val_loss: 1496.5319 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.7234 - loss: 1499.5046 - val_RMSE: 38.6848 - val_loss: 1496.5164 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7230 - loss: 1499.4679 - val_RMSE: 38.6847 - val_loss: 1496.5052 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7226 - loss: 1499.4371 - val_RMSE: 38.6846 - val_loss: 1496.4973 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7223 - loss: 1499.4138 - val_RMSE: 38.6846 - val_loss: 1496.4966 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7220 - loss: 1499.3964 - val_RMSE: 38.6845 - val_loss: 1496.4885 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7218 - loss: 1499.3761 - val_RMSE: 38.6845 - val_loss: 1496.4889 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7215 - loss: 1499.3588 - val_RMSE: 38.6844 - val_loss: 1496.4838 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7213 - loss: 1499.3431 - val_RMSE: 38.6844 - val_loss: 1496.4847 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7212 - loss: 1499.3291 - val_RMSE: 38.6844 - val_loss: 1496.4850 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7210 - loss: 1499.3151 - val_RMSE: 38.6844 - val_loss: 1496.4835 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7208 - loss: 1499.3029 - val_RMSE: 38.6844 - val_loss: 1496.4854 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7206 - loss: 1499.2888 - val_RMSE: 38.6845 - val_loss: 1496.4905 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7205 - loss: 1499.2792 - val_RMSE: 38.6846 - val_loss: 1496.4954 - learning_rate: 1.0000e-04\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  26s 8ms/step - RMSE: 53.2917 - loss: 2989.8503 - val_RMSE: 38.7388 - val_loss: 1500.6951 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6913 - loss: 1497.0150 - val_RMSE: 38.7415 - val_loss: 1500.9073 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6887 - loss: 1496.8157 - val_RMSE: 38.7421 - val_loss: 1500.9474 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6866 - loss: 1496.6555 - val_RMSE: 38.7345 - val_loss: 1500.3625 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6854 - loss: 1496.5598 - val_RMSE: 38.7245 - val_loss: 1499.5845 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6848 - loss: 1496.5118 - val_RMSE: 38.7420 - val_loss: 1500.9388 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6840 - loss: 1496.4501 - val_RMSE: 38.7205 - val_loss: 1499.2780 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6833 - loss: 1496.4015 - val_RMSE: 38.7363 - val_loss: 1500.4982 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6826 - loss: 1496.3461 - val_RMSE: 38.7169 - val_loss: 1498.9999 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6821 - loss: 1496.3041 - val_RMSE: 38.7238 - val_loss: 1499.5336 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6815 - loss: 1496.2607 - val_RMSE: 38.7342 - val_loss: 1500.3352 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6812 - loss: 1496.2394 - val_RMSE: 38.7526 - val_loss: 1501.7678 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6810 - loss: 1496.2241 - val_RMSE: 38.7539 - val_loss: 1501.8683 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6807 - loss: 1496.1965 - val_RMSE: 38.7435 - val_loss: 1501.0594 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6729 - loss: 1495.5959 - val_RMSE: 38.7156 - val_loss: 1498.9008 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6717 - loss: 1495.5046 - val_RMSE: 38.7157 - val_loss: 1498.9084 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6714 - loss: 1495.4757 - val_RMSE: 38.7157 - val_loss: 1498.9043 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6711 - loss: 1495.4542 - val_RMSE: 38.7156 - val_loss: 1498.8981 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6709 - loss: 1495.4390 - val_RMSE: 38.7156 - val_loss: 1498.8966 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6707 - loss: 1495.4257 - val_RMSE: 38.7154 - val_loss: 1498.8845 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6705 - loss: 1495.4106 - val_RMSE: 38.7154 - val_loss: 1498.8818 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6703 - loss: 1495.3962 - val_RMSE: 38.7153 - val_loss: 1498.8741 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6702 - loss: 1495.3857 - val_RMSE: 38.7152 - val_loss: 1498.8654 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6701 - loss: 1495.3737 - val_RMSE: 38.7152 - val_loss: 1498.8636 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6699 - loss: 1495.3649 - val_RMSE: 38.7151 - val_loss: 1498.8568 - learning_rate: 1.0000e-04\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 8ms/step - RMSE: 53.2472 - loss: 2984.4116 - val_RMSE: 38.7255 - val_loss: 1499.6621 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7226 - loss: 1499.4381 - val_RMSE: 38.7243 - val_loss: 1499.5703 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7181 - loss: 1499.0909 - val_RMSE: 38.7263 - val_loss: 1499.7239 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7163 - loss: 1498.9539 - val_RMSE: 38.9383 - val_loss: 1516.1874 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7141 - loss: 1498.7784 - val_RMSE: 38.8476 - val_loss: 1509.1339 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7127 - loss: 1498.6731 - val_RMSE: 38.9148 - val_loss: 1514.3632 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7118 - loss: 1498.6011 - val_RMSE: 38.8934 - val_loss: 1512.6975 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7026 - loss: 1497.8910 - val_RMSE: 38.7066 - val_loss: 1498.2042 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7010 - loss: 1497.7683 - val_RMSE: 38.7065 - val_loss: 1498.1912 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7004 - loss: 1497.7245 - val_RMSE: 38.7073 - val_loss: 1498.2535 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7001 - loss: 1497.6953 - val_RMSE: 38.7075 - val_loss: 1498.2676 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6998 - loss: 1497.6760 - val_RMSE: 38.7078 - val_loss: 1498.2948 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6996 - loss: 1497.6565 - val_RMSE: 38.7081 - val_loss: 1498.3188 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6993 - loss: 1497.6401 - val_RMSE: 38.7080 - val_loss: 1498.3088 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6982 - loss: 1497.5485 - val_RMSE: 38.7049 - val_loss: 1498.0710 - learning_rate: 1.0000e-05\n",
            "Epoch 16/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6980 - loss: 1497.5325 - val_RMSE: 38.7049 - val_loss: 1498.0703 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6979 - loss: 1497.5300 - val_RMSE: 38.7049 - val_loss: 1498.0695 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6979 - loss: 1497.5261 - val_RMSE: 38.7049 - val_loss: 1498.0696 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6978 - loss: 1497.5223 - val_RMSE: 38.7049 - val_loss: 1498.0685 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6978 - loss: 1497.5204 - val_RMSE: 38.7049 - val_loss: 1498.0692 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6978 - loss: 1497.5181 - val_RMSE: 38.7049 - val_loss: 1498.0693 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6977 - loss: 1497.5155 - val_RMSE: 38.7049 - val_loss: 1498.0699 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6977 - loss: 1497.5131 - val_RMSE: 38.7049 - val_loss: 1498.0698 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6977 - loss: 1497.5107 - val_RMSE: 38.7049 - val_loss: 1498.0698 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6976 - loss: 1497.5073 - val_RMSE: 38.7047 - val_loss: 1498.0518 - learning_rate: 1.0000e-06\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 10:44:22,712] Trial 0 finished with value: 38.701437632242836 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'selu'}. Best is trial 0 with value: 38.701437632242836.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  26s 7ms/step - RMSE: 56.3630 - loss: 3340.2073 - val_RMSE: 38.6960 - val_loss: 1497.3799 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7409 - loss: 1500.8616 - val_RMSE: 38.7025 - val_loss: 1497.8840 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7374 - loss: 1500.5897 - val_RMSE: 38.6956 - val_loss: 1497.3497 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7356 - loss: 1500.4476 - val_RMSE: 38.6931 - val_loss: 1497.1523 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7339 - loss: 1500.3163 - val_RMSE: 38.6863 - val_loss: 1496.6328 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7322 - loss: 1500.1838 - val_RMSE: 38.6855 - val_loss: 1496.5647 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7298 - loss: 1500.0016 - val_RMSE: 38.6847 - val_loss: 1496.5066 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7285 - loss: 1499.8959 - val_RMSE: 38.6859 - val_loss: 1496.6007 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7273 - loss: 1499.8015 - val_RMSE: 38.6860 - val_loss: 1496.6062 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7264 - loss: 1499.7316 - val_RMSE: 38.6849 - val_loss: 1496.5193 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7253 - loss: 1499.6483 - val_RMSE: 38.6838 - val_loss: 1496.4329 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7244 - loss: 1499.5797 - val_RMSE: 38.6838 - val_loss: 1496.4369 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7233 - loss: 1499.4962 - val_RMSE: 38.6837 - val_loss: 1496.4279 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7224 - loss: 1499.4224 - val_RMSE: 38.6831 - val_loss: 1496.3823 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7210 - loss: 1499.3186 - val_RMSE: 38.6832 - val_loss: 1496.3865 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7200 - loss: 1499.2412 - val_RMSE: 38.6840 - val_loss: 1496.4532 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7183 - loss: 1499.1088 - val_RMSE: 38.6836 - val_loss: 1496.4241 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7171 - loss: 1499.0129 - val_RMSE: 38.6846 - val_loss: 1496.4962 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7153 - loss: 1498.8740 - val_RMSE: 38.6851 - val_loss: 1496.5378 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7088 - loss: 1498.3752 - val_RMSE: 38.6788 - val_loss: 1496.0526 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7053 - loss: 1498.1029 - val_RMSE: 38.6791 - val_loss: 1496.0709 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7039 - loss: 1497.9907 - val_RMSE: 38.6793 - val_loss: 1496.0872 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7028 - loss: 1497.9105 - val_RMSE: 38.6795 - val_loss: 1496.1014 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7020 - loss: 1497.8457 - val_RMSE: 38.6796 - val_loss: 1496.1132 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7012 - loss: 1497.7875 - val_RMSE: 38.6798 - val_loss: 1496.1257 - learning_rate: 1.0000e-04\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  26s 8ms/step - RMSE: 56.3323 - loss: 3335.9031 - val_RMSE: 38.7281 - val_loss: 1499.8630 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6939 - loss: 1497.2158 - val_RMSE: 38.7284 - val_loss: 1499.8890 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6886 - loss: 1496.8049 - val_RMSE: 38.7258 - val_loss: 1499.6859 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6856 - loss: 1496.5751 - val_RMSE: 38.7245 - val_loss: 1499.5905 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6833 - loss: 1496.4011 - val_RMSE: 38.7185 - val_loss: 1499.1229 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6815 - loss: 1496.2599 - val_RMSE: 38.7168 - val_loss: 1498.9868 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6798 - loss: 1496.1265 - val_RMSE: 38.7160 - val_loss: 1498.9304 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6786 - loss: 1496.0315 - val_RMSE: 38.7150 - val_loss: 1498.8538 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6778 - loss: 1495.9731 - val_RMSE: 38.7160 - val_loss: 1498.9301 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6771 - loss: 1495.9177 - val_RMSE: 38.7144 - val_loss: 1498.8081 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6757 - loss: 1495.8132 - val_RMSE: 38.7155 - val_loss: 1498.8861 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6744 - loss: 1495.7072 - val_RMSE: 38.7160 - val_loss: 1498.9326 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6731 - loss: 1495.6069 - val_RMSE: 38.7151 - val_loss: 1498.8607 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6716 - loss: 1495.4904 - val_RMSE: 38.7162 - val_loss: 1498.9424 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6700 - loss: 1495.3658 - val_RMSE: 38.7159 - val_loss: 1498.9176 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6627 - loss: 1494.8049 - val_RMSE: 38.7124 - val_loss: 1498.6525 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6601 - loss: 1494.6075 - val_RMSE: 38.7125 - val_loss: 1498.6609 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6590 - loss: 1494.5162 - val_RMSE: 38.7127 - val_loss: 1498.6738 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6581 - loss: 1494.4482 - val_RMSE: 38.7129 - val_loss: 1498.6860 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6573 - loss: 1494.3903 - val_RMSE: 38.7131 - val_loss: 1498.7030 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6567 - loss: 1494.3374 - val_RMSE: 38.7133 - val_loss: 1498.7192 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6559 - loss: 1494.2758 - val_RMSE: 38.7117 - val_loss: 1498.5962 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6552 - loss: 1494.2219 - val_RMSE: 38.7117 - val_loss: 1498.5952 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6550 - loss: 1494.2095 - val_RMSE: 38.7118 - val_loss: 1498.5997 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6549 - loss: 1494.1990 - val_RMSE: 38.7118 - val_loss: 1498.6012 - learning_rate: 1.0000e-05\n",
            "41608/41608  87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 8ms/step - RMSE: 56.2866 - loss: 3330.4028 - val_RMSE: 38.7194 - val_loss: 1499.1917 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7191 - loss: 1499.1687 - val_RMSE: 38.7195 - val_loss: 1499.1991 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7142 - loss: 1498.7919 - val_RMSE: 38.7139 - val_loss: 1498.7687 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7118 - loss: 1498.6016 - val_RMSE: 38.7120 - val_loss: 1498.6177 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7100 - loss: 1498.4615 - val_RMSE: 38.7142 - val_loss: 1498.7891 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7081 - loss: 1498.3149 - val_RMSE: 38.7087 - val_loss: 1498.3608 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7064 - loss: 1498.1826 - val_RMSE: 38.7050 - val_loss: 1498.0765 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7045 - loss: 1498.0358 - val_RMSE: 38.7075 - val_loss: 1498.2732 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7037 - loss: 1497.9795 - val_RMSE: 38.7062 - val_loss: 1498.1704 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7025 - loss: 1497.8868 - val_RMSE: 38.7065 - val_loss: 1498.1903 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7013 - loss: 1497.7908 - val_RMSE: 38.7056 - val_loss: 1498.1235 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7001 - loss: 1497.6946 - val_RMSE: 38.7032 - val_loss: 1497.9351 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6985 - loss: 1497.5751 - val_RMSE: 38.7034 - val_loss: 1497.9548 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6969 - loss: 1497.4501 - val_RMSE: 38.7031 - val_loss: 1497.9271 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6953 - loss: 1497.3273 - val_RMSE: 38.7024 - val_loss: 1497.8740 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6939 - loss: 1497.2183 - val_RMSE: 38.7042 - val_loss: 1498.0181 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6920 - loss: 1497.0734 - val_RMSE: 38.7024 - val_loss: 1497.8735 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6904 - loss: 1496.9506 - val_RMSE: 38.7022 - val_loss: 1497.8588 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6875 - loss: 1496.7223 - val_RMSE: 38.7029 - val_loss: 1497.9164 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6845 - loss: 1496.4904 - val_RMSE: 38.7040 - val_loss: 1498.0021 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6819 - loss: 1496.2886 - val_RMSE: 38.7054 - val_loss: 1498.1082 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6784 - loss: 1496.0183 - val_RMSE: 38.7099 - val_loss: 1498.4568 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6741 - loss: 1495.6871 - val_RMSE: 38.7125 - val_loss: 1498.6545 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6670 - loss: 1495.1350 - val_RMSE: 38.7136 - val_loss: 1498.7467 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6602 - loss: 1494.6125 - val_RMSE: 38.7163 - val_loss: 1498.9550 - learning_rate: 1.0000e-04\n",
            "41608/41608  83s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 11:11:52,706] Trial 1 finished with value: 38.702624003092446 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu'}. Best is trial 0 with value: 38.701437632242836.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  22s 6ms/step - RMSE: 64.5121 - loss: 4325.4487 - val_RMSE: 38.7013 - val_loss: 1497.7886 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7409 - loss: 1500.8591 - val_RMSE: 38.6934 - val_loss: 1497.1771 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7337 - loss: 1500.2982 - val_RMSE: 38.6902 - val_loss: 1496.9346 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7293 - loss: 1499.9631 - val_RMSE: 38.6879 - val_loss: 1496.7524 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7264 - loss: 1499.7333 - val_RMSE: 38.6879 - val_loss: 1496.7570 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7241 - loss: 1499.5571 - val_RMSE: 38.6863 - val_loss: 1496.6329 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7214 - loss: 1499.3507 - val_RMSE: 38.6874 - val_loss: 1496.7142 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7193 - loss: 1499.1849 - val_RMSE: 38.6880 - val_loss: 1496.7595 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7171 - loss: 1499.0181 - val_RMSE: 38.6892 - val_loss: 1496.8555 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7148 - loss: 1498.8333 - val_RMSE: 38.6907 - val_loss: 1496.9700 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7123 - loss: 1498.6447 - val_RMSE: 38.6905 - val_loss: 1496.9526 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7073 - loss: 1498.2531 - val_RMSE: 38.6841 - val_loss: 1496.4625 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7019 - loss: 1497.8368 - val_RMSE: 38.6845 - val_loss: 1496.4872 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6997 - loss: 1497.6676 - val_RMSE: 38.6850 - val_loss: 1496.5302 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6981 - loss: 1497.5465 - val_RMSE: 38.6855 - val_loss: 1496.5707 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6968 - loss: 1497.4463 - val_RMSE: 38.6861 - val_loss: 1496.6144 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6957 - loss: 1497.3588 - val_RMSE: 38.6866 - val_loss: 1496.6509 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6945 - loss: 1497.2655 - val_RMSE: 38.6865 - val_loss: 1496.6454 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6939 - loss: 1497.2188 - val_RMSE: 38.6865 - val_loss: 1496.6481 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6935 - loss: 1497.1914 - val_RMSE: 38.6866 - val_loss: 1496.6519 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6933 - loss: 1497.1696 - val_RMSE: 38.6866 - val_loss: 1496.6562 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6930 - loss: 1497.1525 - val_RMSE: 38.6867 - val_loss: 1496.6594 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6928 - loss: 1497.1340 - val_RMSE: 38.6867 - val_loss: 1496.6600 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6928 - loss: 1497.1316 - val_RMSE: 38.6867 - val_loss: 1496.6613 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6927 - loss: 1497.1293 - val_RMSE: 38.6867 - val_loss: 1496.6617 - learning_rate: 1.0000e-06\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  22s 6ms/step - RMSE: 64.5150 - loss: 4325.3882 - val_RMSE: 38.7267 - val_loss: 1499.7600 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6865 - loss: 1496.6490 - val_RMSE: 38.7232 - val_loss: 1499.4896 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6808 - loss: 1496.2069 - val_RMSE: 38.7217 - val_loss: 1499.3718 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6774 - loss: 1495.9437 - val_RMSE: 38.7216 - val_loss: 1499.3618 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6746 - loss: 1495.7242 - val_RMSE: 38.7198 - val_loss: 1499.2205 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6720 - loss: 1495.5223 - val_RMSE: 38.7208 - val_loss: 1499.3035 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6700 - loss: 1495.3719 - val_RMSE: 38.7202 - val_loss: 1499.2549 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6680 - loss: 1495.2141 - val_RMSE: 38.7199 - val_loss: 1499.2338 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6658 - loss: 1495.0443 - val_RMSE: 38.7208 - val_loss: 1499.2971 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6637 - loss: 1494.8813 - val_RMSE: 38.7213 - val_loss: 1499.3357 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6584 - loss: 1494.4728 - val_RMSE: 38.7114 - val_loss: 1498.5724 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6540 - loss: 1494.1296 - val_RMSE: 38.7118 - val_loss: 1498.6000 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6522 - loss: 1493.9968 - val_RMSE: 38.7122 - val_loss: 1498.6334 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6509 - loss: 1493.8917 - val_RMSE: 38.7127 - val_loss: 1498.6693 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6497 - loss: 1493.8013 - val_RMSE: 38.7131 - val_loss: 1498.7021 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6486 - loss: 1493.7137 - val_RMSE: 38.7135 - val_loss: 1498.7325 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6474 - loss: 1493.6248 - val_RMSE: 38.7130 - val_loss: 1498.6976 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6469 - loss: 1493.5829 - val_RMSE: 38.7129 - val_loss: 1498.6885 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6465 - loss: 1493.5563 - val_RMSE: 38.7129 - val_loss: 1498.6866 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6463 - loss: 1493.5371 - val_RMSE: 38.7129 - val_loss: 1498.6886 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6461 - loss: 1493.5212 - val_RMSE: 38.7129 - val_loss: 1498.6891 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6459 - loss: 1493.5033 - val_RMSE: 38.7129 - val_loss: 1498.6904 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6458 - loss: 1493.5012 - val_RMSE: 38.7129 - val_loss: 1498.6912 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6458 - loss: 1493.4995 - val_RMSE: 38.7129 - val_loss: 1498.6917 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6458 - loss: 1493.4985 - val_RMSE: 38.7129 - val_loss: 1498.6917 - learning_rate: 1.0000e-06\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  21s 6ms/step - RMSE: 64.4789 - loss: 4320.5557 - val_RMSE: 38.7139 - val_loss: 1498.7650 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7158 - loss: 1498.9166 - val_RMSE: 38.7123 - val_loss: 1498.6431 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7107 - loss: 1498.5210 - val_RMSE: 38.7098 - val_loss: 1498.4509 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7072 - loss: 1498.2494 - val_RMSE: 38.7093 - val_loss: 1498.4098 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7047 - loss: 1498.0529 - val_RMSE: 38.7125 - val_loss: 1498.6559 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7015 - loss: 1497.8041 - val_RMSE: 38.7116 - val_loss: 1498.5917 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6990 - loss: 1497.6101 - val_RMSE: 38.7113 - val_loss: 1498.5660 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6965 - loss: 1497.4224 - val_RMSE: 38.7091 - val_loss: 1498.3955 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6942 - loss: 1497.2443 - val_RMSE: 38.7086 - val_loss: 1498.3523 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6920 - loss: 1497.0718 - val_RMSE: 38.7089 - val_loss: 1498.3821 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6902 - loss: 1496.9303 - val_RMSE: 38.7095 - val_loss: 1498.4274 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6880 - loss: 1496.7579 - val_RMSE: 38.7107 - val_loss: 1498.5161 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6860 - loss: 1496.6072 - val_RMSE: 38.7122 - val_loss: 1498.6366 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6835 - loss: 1496.4124 - val_RMSE: 38.7126 - val_loss: 1498.6691 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6770 - loss: 1495.9087 - val_RMSE: 38.7081 - val_loss: 1498.3142 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6727 - loss: 1495.5787 - val_RMSE: 38.7089 - val_loss: 1498.3782 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6707 - loss: 1495.4200 - val_RMSE: 38.7097 - val_loss: 1498.4431 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6690 - loss: 1495.2957 - val_RMSE: 38.7105 - val_loss: 1498.5027 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6676 - loss: 1495.1859 - val_RMSE: 38.7113 - val_loss: 1498.5620 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6664 - loss: 1495.0883 - val_RMSE: 38.7120 - val_loss: 1498.6224 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6651 - loss: 1494.9919 - val_RMSE: 38.7119 - val_loss: 1498.6079 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6644 - loss: 1494.9369 - val_RMSE: 38.7119 - val_loss: 1498.6124 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6640 - loss: 1494.9049 - val_RMSE: 38.7120 - val_loss: 1498.6202 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6637 - loss: 1494.8798 - val_RMSE: 38.7121 - val_loss: 1498.6256 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6634 - loss: 1494.8580 - val_RMSE: 38.7122 - val_loss: 1498.6338 - learning_rate: 1.0000e-05\n",
            "41608/41608  83s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 11:34:24,660] Trial 2 finished with value: 38.703948974609375 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'relu'}. Best is trial 0 with value: 38.701437632242836.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 53.3184 - loss: 2993.5410 - val_RMSE: 38.7082 - val_loss: 1498.3252 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7452 - loss: 1501.1896 - val_RMSE: 38.6995 - val_loss: 1497.6506 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7410 - loss: 1500.8696 - val_RMSE: 38.6889 - val_loss: 1496.8324 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7403 - loss: 1500.8141 - val_RMSE: 38.6914 - val_loss: 1497.0251 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7378 - loss: 1500.6171 - val_RMSE: 38.6944 - val_loss: 1497.2560 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7363 - loss: 1500.4993 - val_RMSE: 38.6877 - val_loss: 1496.7389 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7341 - loss: 1500.3284 - val_RMSE: 38.6887 - val_loss: 1496.8169 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7340 - loss: 1500.3260 - val_RMSE: 38.6899 - val_loss: 1496.9095 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7320 - loss: 1500.1686 - val_RMSE: 38.6838 - val_loss: 1496.4397 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7308 - loss: 1500.0759 - val_RMSE: 38.6849 - val_loss: 1496.5253 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7304 - loss: 1500.0464 - val_RMSE: 38.6823 - val_loss: 1496.3225 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7293 - loss: 1499.9624 - val_RMSE: 38.6849 - val_loss: 1496.5219 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7282 - loss: 1499.8768 - val_RMSE: 38.6849 - val_loss: 1496.5212 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7274 - loss: 1499.8086 - val_RMSE: 38.6847 - val_loss: 1496.5034 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7269 - loss: 1499.7722 - val_RMSE: 38.6852 - val_loss: 1496.5410 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7257 - loss: 1499.6833 - val_RMSE: 38.6826 - val_loss: 1496.3438 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7170 - loss: 1499.0070 - val_RMSE: 38.6794 - val_loss: 1496.0990 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7150 - loss: 1498.8549 - val_RMSE: 38.6794 - val_loss: 1496.0988 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7143 - loss: 1498.7965 - val_RMSE: 38.6794 - val_loss: 1496.0945 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7138 - loss: 1498.7576 - val_RMSE: 38.6794 - val_loss: 1496.0989 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7134 - loss: 1498.7267 - val_RMSE: 38.6794 - val_loss: 1496.0924 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7130 - loss: 1498.7004 - val_RMSE: 38.6794 - val_loss: 1496.0938 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7127 - loss: 1498.6779 - val_RMSE: 38.6795 - val_loss: 1496.1000 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7124 - loss: 1498.6545 - val_RMSE: 38.6795 - val_loss: 1496.1007 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7122 - loss: 1498.6324 - val_RMSE: 38.6796 - val_loss: 1496.1083 - learning_rate: 1.0000e-04\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 53.2740 - loss: 2987.8987 - val_RMSE: 38.7414 - val_loss: 1500.8953 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6953 - loss: 1497.3280 - val_RMSE: 38.7230 - val_loss: 1499.4685 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6902 - loss: 1496.9292 - val_RMSE: 38.7269 - val_loss: 1499.7762 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6885 - loss: 1496.8029 - val_RMSE: 38.7665 - val_loss: 1502.8441 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6865 - loss: 1496.6486 - val_RMSE: 38.7212 - val_loss: 1499.3335 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6857 - loss: 1496.5853 - val_RMSE: 38.7222 - val_loss: 1499.4106 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6834 - loss: 1496.4034 - val_RMSE: 38.7256 - val_loss: 1499.6753 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6819 - loss: 1496.2924 - val_RMSE: 38.7209 - val_loss: 1499.3096 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6811 - loss: 1496.2260 - val_RMSE: 38.7284 - val_loss: 1499.8920 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6798 - loss: 1496.1282 - val_RMSE: 38.7241 - val_loss: 1499.5536 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6792 - loss: 1496.0835 - val_RMSE: 38.7164 - val_loss: 1498.9600 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6782 - loss: 1496.0002 - val_RMSE: 38.7220 - val_loss: 1499.3961 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6770 - loss: 1495.9143 - val_RMSE: 38.7200 - val_loss: 1499.2391 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6761 - loss: 1495.8381 - val_RMSE: 38.7197 - val_loss: 1499.2126 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6757 - loss: 1495.8107 - val_RMSE: 38.7148 - val_loss: 1498.8330 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6751 - loss: 1495.7626 - val_RMSE: 38.7225 - val_loss: 1499.4319 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6732 - loss: 1495.6138 - val_RMSE: 38.7425 - val_loss: 1500.9796 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6719 - loss: 1495.5156 - val_RMSE: 38.7356 - val_loss: 1500.4487 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6707 - loss: 1495.4242 - val_RMSE: 38.7387 - val_loss: 1500.6858 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6699 - loss: 1495.3605 - val_RMSE: 38.7460 - val_loss: 1501.2520 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6613 - loss: 1494.6968 - val_RMSE: 38.7109 - val_loss: 1498.5367 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6599 - loss: 1494.5867 - val_RMSE: 38.7112 - val_loss: 1498.5591 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6593 - loss: 1494.5402 - val_RMSE: 38.7115 - val_loss: 1498.5774 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6588 - loss: 1494.5007 - val_RMSE: 38.7117 - val_loss: 1498.5944 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6583 - loss: 1494.4667 - val_RMSE: 38.7119 - val_loss: 1498.6124 - learning_rate: 1.0000e-04\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  30s 9ms/step - RMSE: 53.2408 - loss: 2983.8542 - val_RMSE: 38.7222 - val_loss: 1499.4124 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7228 - loss: 1499.4531 - val_RMSE: 38.7305 - val_loss: 1500.0483 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7180 - loss: 1499.0820 - val_RMSE: 38.7285 - val_loss: 1499.8940 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7149 - loss: 1498.8420 - val_RMSE: 38.7249 - val_loss: 1499.6216 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7134 - loss: 1498.7308 - val_RMSE: 38.7163 - val_loss: 1498.9521 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7119 - loss: 1498.6118 - val_RMSE: 38.7098 - val_loss: 1498.4474 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7096 - loss: 1498.4325 - val_RMSE: 38.7109 - val_loss: 1498.5358 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7086 - loss: 1498.3595 - val_RMSE: 38.7109 - val_loss: 1498.5375 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7076 - loss: 1498.2781 - val_RMSE: 38.7153 - val_loss: 1498.8751 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7062 - loss: 1498.1741 - val_RMSE: 38.7062 - val_loss: 1498.1671 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7052 - loss: 1498.0902 - val_RMSE: 38.7064 - val_loss: 1498.1857 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7032 - loss: 1497.9396 - val_RMSE: 38.7063 - val_loss: 1498.1785 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7024 - loss: 1497.8737 - val_RMSE: 38.7093 - val_loss: 1498.4108 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7007 - loss: 1497.7412 - val_RMSE: 38.7069 - val_loss: 1498.2214 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6993 - loss: 1497.6368 - val_RMSE: 38.7066 - val_loss: 1498.1970 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6902 - loss: 1496.9292 - val_RMSE: 38.7027 - val_loss: 1497.9022 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6878 - loss: 1496.7491 - val_RMSE: 38.7028 - val_loss: 1497.9097 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6868 - loss: 1496.6720 - val_RMSE: 38.7030 - val_loss: 1497.9215 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6860 - loss: 1496.6078 - val_RMSE: 38.7031 - val_loss: 1497.9320 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6852 - loss: 1496.5492 - val_RMSE: 38.7033 - val_loss: 1497.9432 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6845 - loss: 1496.4929 - val_RMSE: 38.7034 - val_loss: 1497.9561 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6833 - loss: 1496.4015 - val_RMSE: 38.7025 - val_loss: 1497.8856 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6827 - loss: 1496.3500 - val_RMSE: 38.7026 - val_loss: 1497.8876 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6825 - loss: 1496.3358 - val_RMSE: 38.7026 - val_loss: 1497.8879 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6823 - loss: 1496.3239 - val_RMSE: 38.7026 - val_loss: 1497.8889 - learning_rate: 1.0000e-05\n",
            "41608/41608  82s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 12:05:41,185] Trial 3 finished with value: 38.69801712036133 and parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  26s 8ms/step - RMSE: 56.4368 - loss: 3348.9377 - val_RMSE: 38.7139 - val_loss: 1498.7676 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7428 - loss: 1501.0071 - val_RMSE: 38.6941 - val_loss: 1497.2349 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7378 - loss: 1500.6176 - val_RMSE: 38.7297 - val_loss: 1499.9932 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7353 - loss: 1500.4229 - val_RMSE: 38.6935 - val_loss: 1497.1893 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7341 - loss: 1500.3314 - val_RMSE: 38.6938 - val_loss: 1497.2128 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7333 - loss: 1500.2690 - val_RMSE: 38.6879 - val_loss: 1496.7505 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7324 - loss: 1500.2023 - val_RMSE: 38.6880 - val_loss: 1496.7637 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7314 - loss: 1500.1254 - val_RMSE: 38.6899 - val_loss: 1496.9089 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7307 - loss: 1500.0691 - val_RMSE: 38.6873 - val_loss: 1496.7039 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7300 - loss: 1500.0103 - val_RMSE: 38.6891 - val_loss: 1496.8473 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7295 - loss: 1499.9738 - val_RMSE: 38.6850 - val_loss: 1496.5292 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7290 - loss: 1499.9391 - val_RMSE: 38.6874 - val_loss: 1496.7168 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7285 - loss: 1499.8978 - val_RMSE: 38.6858 - val_loss: 1496.5950 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7275 - loss: 1499.8175 - val_RMSE: 38.6856 - val_loss: 1496.5793 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7273 - loss: 1499.8031 - val_RMSE: 38.6863 - val_loss: 1496.6307 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7265 - loss: 1499.7390 - val_RMSE: 38.6850 - val_loss: 1496.5288 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7258 - loss: 1499.6855 - val_RMSE: 38.6868 - val_loss: 1496.6686 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7248 - loss: 1499.6100 - val_RMSE: 38.6854 - val_loss: 1496.5591 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7249 - loss: 1499.6204 - val_RMSE: 38.6858 - val_loss: 1496.5930 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7244 - loss: 1499.5779 - val_RMSE: 38.6863 - val_loss: 1496.6265 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7240 - loss: 1499.5492 - val_RMSE: 38.6850 - val_loss: 1496.5275 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7234 - loss: 1499.5000 - val_RMSE: 38.6847 - val_loss: 1496.5068 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7231 - loss: 1499.4777 - val_RMSE: 38.6844 - val_loss: 1496.4818 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7227 - loss: 1499.4506 - val_RMSE: 38.6840 - val_loss: 1496.4491 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7223 - loss: 1499.4203 - val_RMSE: 38.6830 - val_loss: 1496.3767 - learning_rate: 0.0010\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  25s 8ms/step - RMSE: 56.3855 - loss: 3342.3340 - val_RMSE: 38.7578 - val_loss: 1502.1672 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6942 - loss: 1497.2444 - val_RMSE: 38.7284 - val_loss: 1499.8899 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6904 - loss: 1496.9506 - val_RMSE: 38.7503 - val_loss: 1501.5886 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6872 - loss: 1496.7040 - val_RMSE: 38.7347 - val_loss: 1500.3752 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6850 - loss: 1496.5262 - val_RMSE: 38.7437 - val_loss: 1501.0710 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6837 - loss: 1496.4263 - val_RMSE: 38.7497 - val_loss: 1501.5380 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6817 - loss: 1496.2728 - val_RMSE: 38.7430 - val_loss: 1501.0221 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6755 - loss: 1495.7985 - val_RMSE: 38.7141 - val_loss: 1498.7809 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6720 - loss: 1495.5222 - val_RMSE: 38.7138 - val_loss: 1498.7609 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6714 - loss: 1495.4755 - val_RMSE: 38.7137 - val_loss: 1498.7501 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6710 - loss: 1495.4434 - val_RMSE: 38.7135 - val_loss: 1498.7352 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6707 - loss: 1495.4203 - val_RMSE: 38.7134 - val_loss: 1498.7253 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6704 - loss: 1495.4017 - val_RMSE: 38.7133 - val_loss: 1498.7189 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6702 - loss: 1495.3862 - val_RMSE: 38.7133 - val_loss: 1498.7178 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6700 - loss: 1495.3719 - val_RMSE: 38.7132 - val_loss: 1498.7155 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6699 - loss: 1495.3593 - val_RMSE: 38.7132 - val_loss: 1498.7114 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6697 - loss: 1495.3469 - val_RMSE: 38.7132 - val_loss: 1498.7098 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6696 - loss: 1495.3354 - val_RMSE: 38.7131 - val_loss: 1498.7039 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6695 - loss: 1495.3293 - val_RMSE: 38.7131 - val_loss: 1498.7006 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6693 - loss: 1495.3195 - val_RMSE: 38.7130 - val_loss: 1498.6985 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6693 - loss: 1495.3119 - val_RMSE: 38.7130 - val_loss: 1498.6959 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6691 - loss: 1495.3025 - val_RMSE: 38.7129 - val_loss: 1498.6918 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6690 - loss: 1495.2944 - val_RMSE: 38.7129 - val_loss: 1498.6870 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.6689 - loss: 1495.2872 - val_RMSE: 38.7128 - val_loss: 1498.6826 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6688 - loss: 1495.2778 - val_RMSE: 38.7128 - val_loss: 1498.6803 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  26s 8ms/step - RMSE: 56.3180 - loss: 3334.0947 - val_RMSE: 38.7402 - val_loss: 1500.8021 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7196 - loss: 1499.2118 - val_RMSE: 38.7488 - val_loss: 1501.4677 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7151 - loss: 1498.8605 - val_RMSE: 38.7237 - val_loss: 1499.5269 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7135 - loss: 1498.7323 - val_RMSE: 38.7372 - val_loss: 1500.5731 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7115 - loss: 1498.5814 - val_RMSE: 38.7149 - val_loss: 1498.8451 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.7101 - loss: 1498.4702 - val_RMSE: 38.7406 - val_loss: 1500.8348 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7093 - loss: 1498.4080 - val_RMSE: 38.7288 - val_loss: 1499.9174 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7076 - loss: 1498.2799 - val_RMSE: 38.7546 - val_loss: 1501.9227 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7064 - loss: 1498.1854 - val_RMSE: 38.7389 - val_loss: 1500.7008 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7062 - loss: 1498.1708 - val_RMSE: 38.7295 - val_loss: 1499.9778 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.7002 - loss: 1497.7090 - val_RMSE: 38.7053 - val_loss: 1498.1018 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6986 - loss: 1497.5848 - val_RMSE: 38.7051 - val_loss: 1498.0845 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6981 - loss: 1497.5450 - val_RMSE: 38.7051 - val_loss: 1498.0831 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6978 - loss: 1497.5199 - val_RMSE: 38.7051 - val_loss: 1498.0813 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6976 - loss: 1497.5016 - val_RMSE: 38.7051 - val_loss: 1498.0815 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6973 - loss: 1497.4838 - val_RMSE: 38.7050 - val_loss: 1498.0758 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6971 - loss: 1497.4681 - val_RMSE: 38.7050 - val_loss: 1498.0740 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6969 - loss: 1497.4546 - val_RMSE: 38.7050 - val_loss: 1498.0737 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6968 - loss: 1497.4404 - val_RMSE: 38.7050 - val_loss: 1498.0739 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  17s 6ms/step - RMSE: 38.6966 - loss: 1497.4277 - val_RMSE: 38.7049 - val_loss: 1498.0731 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6964 - loss: 1497.4154 - val_RMSE: 38.7049 - val_loss: 1498.0717 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6962 - loss: 1497.3998 - val_RMSE: 38.7048 - val_loss: 1498.0649 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6961 - loss: 1497.3860 - val_RMSE: 38.7048 - val_loss: 1498.0631 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6959 - loss: 1497.3723 - val_RMSE: 38.7048 - val_loss: 1498.0631 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  17s 7ms/step - RMSE: 38.6958 - loss: 1497.3623 - val_RMSE: 38.7048 - val_loss: 1498.0623 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 12:33:12,824] Trial 4 finished with value: 38.700225830078125 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'selu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 50.8331 - loss: 2718.5952 - val_RMSE: 38.7244 - val_loss: 1499.5774 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7516 - loss: 1501.6853 - val_RMSE: 38.6992 - val_loss: 1497.6318 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7468 - loss: 1501.3146 - val_RMSE: 38.7076 - val_loss: 1498.2753 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7420 - loss: 1500.9408 - val_RMSE: 38.6994 - val_loss: 1497.6414 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7401 - loss: 1500.7953 - val_RMSE: 38.6930 - val_loss: 1497.1493 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7373 - loss: 1500.5830 - val_RMSE: 38.6882 - val_loss: 1496.7804 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  22s 9ms/step - RMSE: 38.7343 - loss: 1500.3457 - val_RMSE: 38.6894 - val_loss: 1496.8679 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7310 - loss: 1500.0947 - val_RMSE: 38.6885 - val_loss: 1496.8007 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7280 - loss: 1499.8595 - val_RMSE: 38.6958 - val_loss: 1497.3621 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7244 - loss: 1499.5829 - val_RMSE: 38.6919 - val_loss: 1497.0640 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7213 - loss: 1499.3397 - val_RMSE: 38.6948 - val_loss: 1497.2843 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7075 - loss: 1498.2739 - val_RMSE: 38.6879 - val_loss: 1496.7563 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7005 - loss: 1497.7330 - val_RMSE: 38.6898 - val_loss: 1496.8967 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6970 - loss: 1497.4572 - val_RMSE: 38.6916 - val_loss: 1497.0433 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6940 - loss: 1497.2266 - val_RMSE: 38.6935 - val_loss: 1497.1903 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6912 - loss: 1497.0066 - val_RMSE: 38.6961 - val_loss: 1497.3857 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6884 - loss: 1496.7943 - val_RMSE: 38.6981 - val_loss: 1497.5446 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6861 - loss: 1496.6121 - val_RMSE: 38.6980 - val_loss: 1497.5358 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6845 - loss: 1496.4889 - val_RMSE: 38.6981 - val_loss: 1497.5413 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6836 - loss: 1496.4188 - val_RMSE: 38.6982 - val_loss: 1497.5543 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6829 - loss: 1496.3660 - val_RMSE: 38.6985 - val_loss: 1497.5715 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6823 - loss: 1496.3213 - val_RMSE: 38.6987 - val_loss: 1497.5894 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6820 - loss: 1496.2947 - val_RMSE: 38.6987 - val_loss: 1497.5884 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6817 - loss: 1496.2764 - val_RMSE: 38.6987 - val_loss: 1497.5885 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6816 - loss: 1496.2676 - val_RMSE: 38.6987 - val_loss: 1497.5900 - learning_rate: 1.0000e-06\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 10ms/step - RMSE: 50.7762 - loss: 2711.9099 - val_RMSE: 38.7446 - val_loss: 1501.1411 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6979 - loss: 1497.5304 - val_RMSE: 38.7504 - val_loss: 1501.5945 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6948 - loss: 1497.2847 - val_RMSE: 38.7520 - val_loss: 1501.7173 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6897 - loss: 1496.8970 - val_RMSE: 38.7326 - val_loss: 1500.2173 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6884 - loss: 1496.7952 - val_RMSE: 38.7685 - val_loss: 1503.0004 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6847 - loss: 1496.5074 - val_RMSE: 38.7424 - val_loss: 1500.9719 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6817 - loss: 1496.2762 - val_RMSE: 38.7456 - val_loss: 1501.2242 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6795 - loss: 1496.1074 - val_RMSE: 38.7454 - val_loss: 1501.2048 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6761 - loss: 1495.8431 - val_RMSE: 38.7467 - val_loss: 1501.3079 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6614 - loss: 1494.7084 - val_RMSE: 38.7118 - val_loss: 1498.6062 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6555 - loss: 1494.2458 - val_RMSE: 38.7124 - val_loss: 1498.6514 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6527 - loss: 1494.0336 - val_RMSE: 38.7133 - val_loss: 1498.7213 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6504 - loss: 1493.8557 - val_RMSE: 38.7144 - val_loss: 1498.8011 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  22s 9ms/step - RMSE: 38.6483 - loss: 1493.6926 - val_RMSE: 38.7154 - val_loss: 1498.8795 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6461 - loss: 1493.5245 - val_RMSE: 38.7164 - val_loss: 1498.9612 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  22s 9ms/step - RMSE: 38.6436 - loss: 1493.3247 - val_RMSE: 38.7140 - val_loss: 1498.7756 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601  22s 9ms/step - RMSE: 38.6420 - loss: 1493.2084 - val_RMSE: 38.7140 - val_loss: 1498.7704 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6414 - loss: 1493.1559 - val_RMSE: 38.7140 - val_loss: 1498.7742 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6408 - loss: 1493.1136 - val_RMSE: 38.7141 - val_loss: 1498.7838 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6404 - loss: 1493.0782 - val_RMSE: 38.7143 - val_loss: 1498.7943 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6401 - loss: 1493.0575 - val_RMSE: 38.7136 - val_loss: 1498.7463 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6398 - loss: 1493.0321 - val_RMSE: 38.7136 - val_loss: 1498.7415 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6397 - loss: 1493.0244 - val_RMSE: 38.7136 - val_loss: 1498.7402 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6396 - loss: 1493.0199 - val_RMSE: 38.7136 - val_loss: 1498.7410 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6396 - loss: 1493.0160 - val_RMSE: 38.7136 - val_loss: 1498.7418 - learning_rate: 1.0000e-06\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 10ms/step - RMSE: 50.7367 - loss: 2707.1123 - val_RMSE: 38.7286 - val_loss: 1499.9026 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7279 - loss: 1499.8497 - val_RMSE: 38.7156 - val_loss: 1498.8960 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7221 - loss: 1499.4038 - val_RMSE: 38.7098 - val_loss: 1498.4447 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7184 - loss: 1499.1116 - val_RMSE: 38.7103 - val_loss: 1498.4866 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7155 - loss: 1498.8915 - val_RMSE: 38.7079 - val_loss: 1498.2988 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7129 - loss: 1498.6921 - val_RMSE: 38.7208 - val_loss: 1499.3019 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7105 - loss: 1498.5066 - val_RMSE: 38.7156 - val_loss: 1498.9008 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7070 - loss: 1498.2313 - val_RMSE: 38.7098 - val_loss: 1498.4500 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7037 - loss: 1497.9785 - val_RMSE: 38.7078 - val_loss: 1498.2975 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7002 - loss: 1497.7042 - val_RMSE: 38.7165 - val_loss: 1498.9672 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6964 - loss: 1497.4080 - val_RMSE: 38.7104 - val_loss: 1498.4926 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  22s 9ms/step - RMSE: 38.6910 - loss: 1496.9972 - val_RMSE: 38.7123 - val_loss: 1498.6415 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6833 - loss: 1496.3998 - val_RMSE: 38.7199 - val_loss: 1499.2311 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6749 - loss: 1495.7496 - val_RMSE: 38.7286 - val_loss: 1499.9014 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6637 - loss: 1494.8832 - val_RMSE: 38.7278 - val_loss: 1499.8394 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  22s 9ms/step - RMSE: 38.6510 - loss: 1493.9038 - val_RMSE: 38.7368 - val_loss: 1500.5371 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6431 - loss: 1493.2928 - val_RMSE: 38.7439 - val_loss: 1501.0931 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6362 - loss: 1492.7578 - val_RMSE: 38.7501 - val_loss: 1501.5710 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6298 - loss: 1492.2589 - val_RMSE: 38.7569 - val_loss: 1502.1005 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6286 - loss: 1492.1696 - val_RMSE: 38.7519 - val_loss: 1501.7096 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6252 - loss: 1491.9100 - val_RMSE: 38.7521 - val_loss: 1501.7274 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6234 - loss: 1491.7700 - val_RMSE: 38.7526 - val_loss: 1501.7649 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6220 - loss: 1491.6581 - val_RMSE: 38.7532 - val_loss: 1501.8105 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6207 - loss: 1491.5568 - val_RMSE: 38.7539 - val_loss: 1501.8651 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6199 - loss: 1491.5012 - val_RMSE: 38.7536 - val_loss: 1501.8447 - learning_rate: 1.0000e-06\n",
            "41608/41608  82s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 13:07:52,360] Trial 5 finished with value: 38.72197469075521 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'relu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 50.8548 - loss: 2721.0947 - val_RMSE: 38.7073 - val_loss: 1498.2577 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7491 - loss: 1501.4927 - val_RMSE: 38.7116 - val_loss: 1498.5873 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7482 - loss: 1501.4208 - val_RMSE: 38.6942 - val_loss: 1497.2389 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7431 - loss: 1501.0256 - val_RMSE: 38.7144 - val_loss: 1498.8073 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7419 - loss: 1500.9354 - val_RMSE: 38.7164 - val_loss: 1498.9589 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7403 - loss: 1500.8125 - val_RMSE: 38.7189 - val_loss: 1499.1520 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7393 - loss: 1500.7322 - val_RMSE: 38.6936 - val_loss: 1497.1985 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7375 - loss: 1500.5920 - val_RMSE: 38.7034 - val_loss: 1497.9497 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7368 - loss: 1500.5392 - val_RMSE: 38.6955 - val_loss: 1497.3387 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7355 - loss: 1500.4360 - val_RMSE: 38.6957 - val_loss: 1497.3535 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7337 - loss: 1500.2979 - val_RMSE: 38.6943 - val_loss: 1497.2463 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7329 - loss: 1500.2395 - val_RMSE: 38.6977 - val_loss: 1497.5128 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7223 - loss: 1499.4171 - val_RMSE: 38.6816 - val_loss: 1496.2692 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7195 - loss: 1499.2035 - val_RMSE: 38.6817 - val_loss: 1496.2723 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7187 - loss: 1499.1382 - val_RMSE: 38.6818 - val_loss: 1496.2803 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7182 - loss: 1499.0978 - val_RMSE: 38.6818 - val_loss: 1496.2789 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7177 - loss: 1499.0638 - val_RMSE: 38.6817 - val_loss: 1496.2726 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7173 - loss: 1499.0295 - val_RMSE: 38.6817 - val_loss: 1496.2776 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7153 - loss: 1498.8789 - val_RMSE: 38.6813 - val_loss: 1496.2446 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7151 - loss: 1498.8602 - val_RMSE: 38.6813 - val_loss: 1496.2444 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7150 - loss: 1498.8485 - val_RMSE: 38.6813 - val_loss: 1496.2460 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7148 - loss: 1498.8392 - val_RMSE: 38.6813 - val_loss: 1496.2461 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7147 - loss: 1498.8323 - val_RMSE: 38.6813 - val_loss: 1496.2466 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7146 - loss: 1498.8247 - val_RMSE: 38.6814 - val_loss: 1496.2476 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7146 - loss: 1498.8186 - val_RMSE: 38.6814 - val_loss: 1496.2485 - learning_rate: 1.0000e-05\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 50.8159 - loss: 2716.2915 - val_RMSE: 38.7432 - val_loss: 1501.0382 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6999 - loss: 1497.6820 - val_RMSE: 38.7426 - val_loss: 1500.9867 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6957 - loss: 1497.3590 - val_RMSE: 38.7340 - val_loss: 1500.3257 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6916 - loss: 1497.0389 - val_RMSE: 38.7224 - val_loss: 1499.4238 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6886 - loss: 1496.8103 - val_RMSE: 38.7329 - val_loss: 1500.2352 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6869 - loss: 1496.6776 - val_RMSE: 38.7299 - val_loss: 1500.0061 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6856 - loss: 1496.5781 - val_RMSE: 38.7501 - val_loss: 1501.5703 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6842 - loss: 1496.4658 - val_RMSE: 38.7208 - val_loss: 1499.3007 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6819 - loss: 1496.2933 - val_RMSE: 38.7616 - val_loss: 1502.4611 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6806 - loss: 1496.1869 - val_RMSE: 38.7299 - val_loss: 1500.0055 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6796 - loss: 1496.1089 - val_RMSE: 38.7310 - val_loss: 1500.0929 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6782 - loss: 1496.0045 - val_RMSE: 38.7345 - val_loss: 1500.3624 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6759 - loss: 1495.8285 - val_RMSE: 38.7335 - val_loss: 1500.2847 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6639 - loss: 1494.8961 - val_RMSE: 38.7117 - val_loss: 1498.5925 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6612 - loss: 1494.6926 - val_RMSE: 38.7118 - val_loss: 1498.6062 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6601 - loss: 1494.6040 - val_RMSE: 38.7121 - val_loss: 1498.6250 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6592 - loss: 1494.5315 - val_RMSE: 38.7123 - val_loss: 1498.6400 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6583 - loss: 1494.4672 - val_RMSE: 38.7125 - val_loss: 1498.6592 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6576 - loss: 1494.4102 - val_RMSE: 38.7128 - val_loss: 1498.6819 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6561 - loss: 1494.2955 - val_RMSE: 38.7105 - val_loss: 1498.5006 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6554 - loss: 1494.2385 - val_RMSE: 38.7105 - val_loss: 1498.5020 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6551 - loss: 1494.2205 - val_RMSE: 38.7105 - val_loss: 1498.5059 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6549 - loss: 1494.2047 - val_RMSE: 38.7106 - val_loss: 1498.5110 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.6548 - loss: 1494.1921 - val_RMSE: 38.7107 - val_loss: 1498.5156 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6546 - loss: 1494.1805 - val_RMSE: 38.7107 - val_loss: 1498.5209 - learning_rate: 1.0000e-05\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 50.7556 - loss: 2709.2288 - val_RMSE: 38.7215 - val_loss: 1499.3531 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7264 - loss: 1499.7343 - val_RMSE: 38.7223 - val_loss: 1499.4198 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7220 - loss: 1499.3903 - val_RMSE: 38.7120 - val_loss: 1498.6226 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7202 - loss: 1499.2543 - val_RMSE: 38.7196 - val_loss: 1499.2111 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7183 - loss: 1499.1062 - val_RMSE: 38.7097 - val_loss: 1498.4419 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7170 - loss: 1499.0045 - val_RMSE: 38.7121 - val_loss: 1498.6268 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7148 - loss: 1498.8376 - val_RMSE: 38.7073 - val_loss: 1498.2518 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7128 - loss: 1498.6788 - val_RMSE: 38.7192 - val_loss: 1499.1736 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7113 - loss: 1498.5619 - val_RMSE: 38.7085 - val_loss: 1498.3450 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7096 - loss: 1498.4318 - val_RMSE: 38.7048 - val_loss: 1498.0632 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7080 - loss: 1498.3113 - val_RMSE: 38.7038 - val_loss: 1497.9880 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7057 - loss: 1498.1285 - val_RMSE: 38.7083 - val_loss: 1498.3315 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7033 - loss: 1497.9475 - val_RMSE: 38.7060 - val_loss: 1498.1506 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.7012 - loss: 1497.7826 - val_RMSE: 38.7064 - val_loss: 1498.1843 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6985 - loss: 1497.5731 - val_RMSE: 38.7075 - val_loss: 1498.2677 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6952 - loss: 1497.3198 - val_RMSE: 38.7084 - val_loss: 1498.3430 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6825 - loss: 1496.3365 - val_RMSE: 38.7047 - val_loss: 1498.0509 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6793 - loss: 1496.0919 - val_RMSE: 38.7053 - val_loss: 1498.0966 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6775 - loss: 1495.9491 - val_RMSE: 38.7058 - val_loss: 1498.1420 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6760 - loss: 1495.8317 - val_RMSE: 38.7065 - val_loss: 1498.1893 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6746 - loss: 1495.7233 - val_RMSE: 38.7071 - val_loss: 1498.2417 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6731 - loss: 1495.6104 - val_RMSE: 38.7062 - val_loss: 1498.1721 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6722 - loss: 1495.5402 - val_RMSE: 38.7065 - val_loss: 1498.1925 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6718 - loss: 1495.5088 - val_RMSE: 38.7067 - val_loss: 1498.2070 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  23s 9ms/step - RMSE: 38.6715 - loss: 1495.4846 - val_RMSE: 38.7069 - val_loss: 1498.2203 - learning_rate: 1.0000e-05\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 13:43:14,659] Trial 6 finished with value: 38.69965489705404 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'gelu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  22s 6ms/step - RMSE: 64.5167 - loss: 4326.0571 - val_RMSE: 38.6998 - val_loss: 1497.6766 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7397 - loss: 1500.7635 - val_RMSE: 38.6922 - val_loss: 1497.0889 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7331 - loss: 1500.2554 - val_RMSE: 38.6905 - val_loss: 1496.9545 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7294 - loss: 1499.9686 - val_RMSE: 38.6890 - val_loss: 1496.8379 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7267 - loss: 1499.7560 - val_RMSE: 38.6887 - val_loss: 1496.8121 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7246 - loss: 1499.5919 - val_RMSE: 38.6869 - val_loss: 1496.6796 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7223 - loss: 1499.4199 - val_RMSE: 38.6860 - val_loss: 1496.6075 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7204 - loss: 1499.2717 - val_RMSE: 38.6860 - val_loss: 1496.6072 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7186 - loss: 1499.1293 - val_RMSE: 38.6862 - val_loss: 1496.6184 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7167 - loss: 1498.9807 - val_RMSE: 38.6868 - val_loss: 1496.6718 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7146 - loss: 1498.8195 - val_RMSE: 38.6875 - val_loss: 1496.7207 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7123 - loss: 1498.6395 - val_RMSE: 38.6886 - val_loss: 1496.8044 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7099 - loss: 1498.4536 - val_RMSE: 38.6896 - val_loss: 1496.8857 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7052 - loss: 1498.0973 - val_RMSE: 38.6840 - val_loss: 1496.4507 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6997 - loss: 1497.6710 - val_RMSE: 38.6841 - val_loss: 1496.4604 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6976 - loss: 1497.5018 - val_RMSE: 38.6846 - val_loss: 1496.4976 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6960 - loss: 1497.3812 - val_RMSE: 38.6852 - val_loss: 1496.5416 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6947 - loss: 1497.2786 - val_RMSE: 38.6858 - val_loss: 1496.5889 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6935 - loss: 1497.1869 - val_RMSE: 38.6863 - val_loss: 1496.6333 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6923 - loss: 1497.0963 - val_RMSE: 38.6862 - val_loss: 1496.6251 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6917 - loss: 1497.0496 - val_RMSE: 38.6862 - val_loss: 1496.6248 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6913 - loss: 1497.0206 - val_RMSE: 38.6862 - val_loss: 1496.6259 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6910 - loss: 1496.9980 - val_RMSE: 38.6863 - val_loss: 1496.6279 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6908 - loss: 1496.9799 - val_RMSE: 38.6863 - val_loss: 1496.6318 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6906 - loss: 1496.9611 - val_RMSE: 38.6863 - val_loss: 1496.6328 - learning_rate: 1.0000e-06\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  23s 6ms/step - RMSE: 64.5209 - loss: 4326.1514 - val_RMSE: 38.7310 - val_loss: 1500.0906 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6875 - loss: 1496.7255 - val_RMSE: 38.7239 - val_loss: 1499.5428 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6829 - loss: 1496.3674 - val_RMSE: 38.7220 - val_loss: 1499.3896 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6796 - loss: 1496.1140 - val_RMSE: 38.7196 - val_loss: 1499.2107 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6771 - loss: 1495.9210 - val_RMSE: 38.7184 - val_loss: 1499.1115 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6749 - loss: 1495.7512 - val_RMSE: 38.7175 - val_loss: 1499.0411 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6727 - loss: 1495.5812 - val_RMSE: 38.7171 - val_loss: 1499.0106 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6708 - loss: 1495.4329 - val_RMSE: 38.7179 - val_loss: 1499.0736 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6687 - loss: 1495.2682 - val_RMSE: 38.7183 - val_loss: 1499.1074 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6667 - loss: 1495.1161 - val_RMSE: 38.7196 - val_loss: 1499.2053 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6644 - loss: 1494.9368 - val_RMSE: 38.7202 - val_loss: 1499.2515 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6619 - loss: 1494.7406 - val_RMSE: 38.7212 - val_loss: 1499.3282 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6567 - loss: 1494.3383 - val_RMSE: 38.7138 - val_loss: 1498.7599 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6518 - loss: 1493.9613 - val_RMSE: 38.7143 - val_loss: 1498.7976 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6499 - loss: 1493.8162 - val_RMSE: 38.7149 - val_loss: 1498.8464 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6485 - loss: 1493.7086 - val_RMSE: 38.7155 - val_loss: 1498.8939 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6474 - loss: 1493.6196 - val_RMSE: 38.7161 - val_loss: 1498.9375 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6463 - loss: 1493.5380 - val_RMSE: 38.7167 - val_loss: 1498.9822 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6451 - loss: 1493.4484 - val_RMSE: 38.7162 - val_loss: 1498.9456 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6447 - loss: 1493.4100 - val_RMSE: 38.7162 - val_loss: 1498.9406 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6444 - loss: 1493.3871 - val_RMSE: 38.7162 - val_loss: 1498.9438 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6441 - loss: 1493.3690 - val_RMSE: 38.7162 - val_loss: 1498.9474 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6439 - loss: 1493.3535 - val_RMSE: 38.7163 - val_loss: 1498.9518 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6437 - loss: 1493.3356 - val_RMSE: 38.7163 - val_loss: 1498.9535 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6437 - loss: 1493.3337 - val_RMSE: 38.7163 - val_loss: 1498.9543 - learning_rate: 1.0000e-06\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  22s 6ms/step - RMSE: 64.4752 - loss: 4320.0786 - val_RMSE: 38.7167 - val_loss: 1498.9790 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7149 - loss: 1498.8474 - val_RMSE: 38.7163 - val_loss: 1498.9487 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7108 - loss: 1498.5229 - val_RMSE: 38.7128 - val_loss: 1498.6794 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7076 - loss: 1498.2799 - val_RMSE: 38.7112 - val_loss: 1498.5579 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7054 - loss: 1498.1057 - val_RMSE: 38.7108 - val_loss: 1498.5284 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7031 - loss: 1497.9335 - val_RMSE: 38.7102 - val_loss: 1498.4799 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7012 - loss: 1497.7812 - val_RMSE: 38.7101 - val_loss: 1498.4750 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6994 - loss: 1497.6471 - val_RMSE: 38.7099 - val_loss: 1498.4536 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6975 - loss: 1497.4999 - val_RMSE: 38.7090 - val_loss: 1498.3870 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6954 - loss: 1497.3309 - val_RMSE: 38.7086 - val_loss: 1498.3552 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6931 - loss: 1497.1576 - val_RMSE: 38.7088 - val_loss: 1498.3699 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6907 - loss: 1496.9701 - val_RMSE: 38.7091 - val_loss: 1498.3942 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6883 - loss: 1496.7844 - val_RMSE: 38.7094 - val_loss: 1498.4147 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6854 - loss: 1496.5642 - val_RMSE: 38.7099 - val_loss: 1498.4557 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6824 - loss: 1496.3311 - val_RMSE: 38.7108 - val_loss: 1498.5288 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6774 - loss: 1495.9399 - val_RMSE: 38.7079 - val_loss: 1498.3007 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6713 - loss: 1495.4668 - val_RMSE: 38.7087 - val_loss: 1498.3658 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6690 - loss: 1495.2920 - val_RMSE: 38.7096 - val_loss: 1498.4351 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6672 - loss: 1495.1554 - val_RMSE: 38.7105 - val_loss: 1498.5001 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6657 - loss: 1495.0396 - val_RMSE: 38.7113 - val_loss: 1498.5619 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6643 - loss: 1494.9321 - val_RMSE: 38.7121 - val_loss: 1498.6233 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6628 - loss: 1494.8140 - val_RMSE: 38.7121 - val_loss: 1498.6232 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6623 - loss: 1494.7714 - val_RMSE: 38.7121 - val_loss: 1498.6290 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6619 - loss: 1494.7441 - val_RMSE: 38.7123 - val_loss: 1498.6387 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6617 - loss: 1494.7245 - val_RMSE: 38.7124 - val_loss: 1498.6477 - learning_rate: 1.0000e-05\n",
            "41608/41608  86s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 14:06:30,056] Trial 7 finished with value: 38.70501963297526 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'gelu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  25s 7ms/step - RMSE: 56.3068 - loss: 3333.3040 - val_RMSE: 38.7013 - val_loss: 1497.7872 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7423 - loss: 1500.9646 - val_RMSE: 38.6976 - val_loss: 1497.5044 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7369 - loss: 1500.5446 - val_RMSE: 38.6902 - val_loss: 1496.9320 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7329 - loss: 1500.2416 - val_RMSE: 38.6915 - val_loss: 1497.0309 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7307 - loss: 1500.0662 - val_RMSE: 38.6894 - val_loss: 1496.8667 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7285 - loss: 1499.8958 - val_RMSE: 38.6879 - val_loss: 1496.7532 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7261 - loss: 1499.7104 - val_RMSE: 38.6892 - val_loss: 1496.8555 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7239 - loss: 1499.5438 - val_RMSE: 38.6899 - val_loss: 1496.9084 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7215 - loss: 1499.3575 - val_RMSE: 38.6887 - val_loss: 1496.8184 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7189 - loss: 1499.1566 - val_RMSE: 38.6879 - val_loss: 1496.7538 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7162 - loss: 1498.9487 - val_RMSE: 38.6890 - val_loss: 1496.8369 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7077 - loss: 1498.2899 - val_RMSE: 38.6840 - val_loss: 1496.4543 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7022 - loss: 1497.8649 - val_RMSE: 38.6846 - val_loss: 1496.4967 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6997 - loss: 1497.6714 - val_RMSE: 38.6855 - val_loss: 1496.5662 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6978 - loss: 1497.5209 - val_RMSE: 38.6863 - val_loss: 1496.6324 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6961 - loss: 1497.3879 - val_RMSE: 38.6873 - val_loss: 1496.7043 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6945 - loss: 1497.2648 - val_RMSE: 38.6881 - val_loss: 1496.7728 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6929 - loss: 1497.1410 - val_RMSE: 38.6885 - val_loss: 1496.7979 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6920 - loss: 1497.0713 - val_RMSE: 38.6887 - val_loss: 1496.8143 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6915 - loss: 1497.0337 - val_RMSE: 38.6888 - val_loss: 1496.8254 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.6911 - loss: 1497.0037 - val_RMSE: 38.6890 - val_loss: 1496.8365 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.6908 - loss: 1496.9783 - val_RMSE: 38.6891 - val_loss: 1496.8483 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6905 - loss: 1496.9562 - val_RMSE: 38.6892 - val_loss: 1496.8539 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.6904 - loss: 1496.9493 - val_RMSE: 38.6892 - val_loss: 1496.8572 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6904 - loss: 1496.9442 - val_RMSE: 38.6893 - val_loss: 1496.8594 - learning_rate: 1.0000e-06\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  24s 7ms/step - RMSE: 56.2791 - loss: 3329.3713 - val_RMSE: 38.7458 - val_loss: 1501.2393 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6939 - loss: 1497.2213 - val_RMSE: 38.7272 - val_loss: 1499.7983 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6870 - loss: 1496.6836 - val_RMSE: 38.7249 - val_loss: 1499.6165 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6825 - loss: 1496.3372 - val_RMSE: 38.7218 - val_loss: 1499.3798 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6793 - loss: 1496.0886 - val_RMSE: 38.7233 - val_loss: 1499.4954 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6763 - loss: 1495.8604 - val_RMSE: 38.7220 - val_loss: 1499.3940 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6738 - loss: 1495.6648 - val_RMSE: 38.7179 - val_loss: 1499.0771 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6710 - loss: 1495.4453 - val_RMSE: 38.7182 - val_loss: 1499.0984 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6686 - loss: 1495.2600 - val_RMSE: 38.7191 - val_loss: 1499.1699 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6651 - loss: 1494.9916 - val_RMSE: 38.7215 - val_loss: 1499.3510 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6614 - loss: 1494.7074 - val_RMSE: 38.7253 - val_loss: 1499.6528 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6571 - loss: 1494.3741 - val_RMSE: 38.7288 - val_loss: 1499.9171 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6491 - loss: 1493.7557 - val_RMSE: 38.7186 - val_loss: 1499.1322 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6412 - loss: 1493.1453 - val_RMSE: 38.7207 - val_loss: 1499.2931 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6371 - loss: 1492.8293 - val_RMSE: 38.7228 - val_loss: 1499.4573 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6339 - loss: 1492.5811 - val_RMSE: 38.7248 - val_loss: 1499.6067 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6310 - loss: 1492.3550 - val_RMSE: 38.7265 - val_loss: 1499.7454 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6291 - loss: 1492.2118 - val_RMSE: 38.7245 - val_loss: 1499.5883 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6275 - loss: 1492.0876 - val_RMSE: 38.7245 - val_loss: 1499.5858 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6268 - loss: 1492.0310 - val_RMSE: 38.7246 - val_loss: 1499.5967 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6263 - loss: 1491.9882 - val_RMSE: 38.7248 - val_loss: 1499.6111 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6258 - loss: 1491.9509 - val_RMSE: 38.7250 - val_loss: 1499.6287 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6253 - loss: 1491.9175 - val_RMSE: 38.7251 - val_loss: 1499.6320 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6253 - loss: 1491.9116 - val_RMSE: 38.7251 - val_loss: 1499.6320 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6252 - loss: 1491.9061 - val_RMSE: 38.7251 - val_loss: 1499.6320 - learning_rate: 1.0000e-06\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  23s 7ms/step - RMSE: 56.2757 - loss: 3328.9019 - val_RMSE: 38.7203 - val_loss: 1499.2627 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7203 - loss: 1499.2638 - val_RMSE: 38.7155 - val_loss: 1498.8885 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7144 - loss: 1498.8069 - val_RMSE: 38.7117 - val_loss: 1498.5968 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7107 - loss: 1498.5164 - val_RMSE: 38.7116 - val_loss: 1498.5848 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7073 - loss: 1498.2544 - val_RMSE: 38.7111 - val_loss: 1498.5508 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.7042 - loss: 1498.0142 - val_RMSE: 38.7141 - val_loss: 1498.7821 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.7020 - loss: 1497.8428 - val_RMSE: 38.7085 - val_loss: 1498.3462 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6990 - loss: 1497.6136 - val_RMSE: 38.7111 - val_loss: 1498.5498 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6965 - loss: 1497.4165 - val_RMSE: 38.7120 - val_loss: 1498.6190 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6929 - loss: 1497.1406 - val_RMSE: 38.7123 - val_loss: 1498.6420 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6900 - loss: 1496.9147 - val_RMSE: 38.7161 - val_loss: 1498.9366 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6864 - loss: 1496.6355 - val_RMSE: 38.7168 - val_loss: 1498.9889 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6794 - loss: 1496.0945 - val_RMSE: 38.7097 - val_loss: 1498.4427 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6720 - loss: 1495.5265 - val_RMSE: 38.7116 - val_loss: 1498.5875 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6685 - loss: 1495.2556 - val_RMSE: 38.7135 - val_loss: 1498.7386 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6658 - loss: 1495.0413 - val_RMSE: 38.7153 - val_loss: 1498.8745 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.6633 - loss: 1494.8488 - val_RMSE: 38.7171 - val_loss: 1499.0142 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6614 - loss: 1494.7063 - val_RMSE: 38.7170 - val_loss: 1499.0067 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6604 - loss: 1494.6240 - val_RMSE: 38.7171 - val_loss: 1499.0115 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.6597 - loss: 1494.5747 - val_RMSE: 38.7172 - val_loss: 1499.0221 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6592 - loss: 1494.5363 - val_RMSE: 38.7174 - val_loss: 1499.0366 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6588 - loss: 1494.5020 - val_RMSE: 38.7176 - val_loss: 1499.0520 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6584 - loss: 1494.4727 - val_RMSE: 38.7177 - val_loss: 1499.0580 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6583 - loss: 1494.4662 - val_RMSE: 38.7177 - val_loss: 1499.0599 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.6582 - loss: 1494.4594 - val_RMSE: 38.7177 - val_loss: 1499.0623 - learning_rate: 1.0000e-06\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 14:31:06,153] Trial 8 finished with value: 38.71068827311198 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'relu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  23s 6ms/step - RMSE: 64.5205 - loss: 4326.5693 - val_RMSE: 38.7010 - val_loss: 1497.7694 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7391 - loss: 1500.7207 - val_RMSE: 38.6970 - val_loss: 1497.4564 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7344 - loss: 1500.3568 - val_RMSE: 38.6930 - val_loss: 1497.1469 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7314 - loss: 1500.1243 - val_RMSE: 38.6910 - val_loss: 1496.9957 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7288 - loss: 1499.9196 - val_RMSE: 38.6897 - val_loss: 1496.8904 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7266 - loss: 1499.7516 - val_RMSE: 38.6882 - val_loss: 1496.7745 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7249 - loss: 1499.6224 - val_RMSE: 38.6872 - val_loss: 1496.6969 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7234 - loss: 1499.5006 - val_RMSE: 38.6861 - val_loss: 1496.6150 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7219 - loss: 1499.3895 - val_RMSE: 38.6858 - val_loss: 1496.5894 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7204 - loss: 1499.2673 - val_RMSE: 38.6853 - val_loss: 1496.5532 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7187 - loss: 1499.1367 - val_RMSE: 38.6848 - val_loss: 1496.5103 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7167 - loss: 1498.9874 - val_RMSE: 38.6846 - val_loss: 1496.4966 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7148 - loss: 1498.8378 - val_RMSE: 38.6846 - val_loss: 1496.4971 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7128 - loss: 1498.6786 - val_RMSE: 38.6851 - val_loss: 1496.5380 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7107 - loss: 1498.5225 - val_RMSE: 38.6856 - val_loss: 1496.5720 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7086 - loss: 1498.3586 - val_RMSE: 38.6860 - val_loss: 1496.6034 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7063 - loss: 1498.1823 - val_RMSE: 38.6865 - val_loss: 1496.6479 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7012 - loss: 1497.7820 - val_RMSE: 38.6829 - val_loss: 1496.3678 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6969 - loss: 1497.4495 - val_RMSE: 38.6835 - val_loss: 1496.4165 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6951 - loss: 1497.3085 - val_RMSE: 38.6842 - val_loss: 1496.4655 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6938 - loss: 1497.2075 - val_RMSE: 38.6847 - val_loss: 1496.5099 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6927 - loss: 1497.1262 - val_RMSE: 38.6853 - val_loss: 1496.5541 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6918 - loss: 1497.0538 - val_RMSE: 38.6859 - val_loss: 1496.5953 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6909 - loss: 1496.9841 - val_RMSE: 38.6858 - val_loss: 1496.5902 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6903 - loss: 1496.9395 - val_RMSE: 38.6858 - val_loss: 1496.5895 - learning_rate: 1.0000e-05\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  23s 6ms/step - RMSE: 64.5233 - loss: 4326.4814 - val_RMSE: 38.7250 - val_loss: 1499.6293 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6861 - loss: 1496.6162 - val_RMSE: 38.7194 - val_loss: 1499.1913 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6820 - loss: 1496.2994 - val_RMSE: 38.7185 - val_loss: 1499.1201 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6794 - loss: 1496.0997 - val_RMSE: 38.7188 - val_loss: 1499.1487 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6774 - loss: 1495.9409 - val_RMSE: 38.7171 - val_loss: 1499.0175 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6754 - loss: 1495.7887 - val_RMSE: 38.7164 - val_loss: 1498.9617 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6739 - loss: 1495.6697 - val_RMSE: 38.7163 - val_loss: 1498.9539 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6725 - loss: 1495.5641 - val_RMSE: 38.7163 - val_loss: 1498.9489 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6712 - loss: 1495.4602 - val_RMSE: 38.7161 - val_loss: 1498.9370 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6699 - loss: 1495.3630 - val_RMSE: 38.7156 - val_loss: 1498.9000 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6685 - loss: 1495.2506 - val_RMSE: 38.7153 - val_loss: 1498.8735 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6669 - loss: 1495.1290 - val_RMSE: 38.7154 - val_loss: 1498.8793 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6653 - loss: 1495.0042 - val_RMSE: 38.7155 - val_loss: 1498.8884 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6635 - loss: 1494.8691 - val_RMSE: 38.7158 - val_loss: 1498.9111 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6617 - loss: 1494.7300 - val_RMSE: 38.7163 - val_loss: 1498.9529 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6598 - loss: 1494.5820 - val_RMSE: 38.7171 - val_loss: 1499.0170 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6554 - loss: 1494.2438 - val_RMSE: 38.7120 - val_loss: 1498.6179 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6512 - loss: 1493.9165 - val_RMSE: 38.7126 - val_loss: 1498.6636 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6497 - loss: 1493.7963 - val_RMSE: 38.7131 - val_loss: 1498.7043 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.6486 - loss: 1493.7142 - val_RMSE: 38.7136 - val_loss: 1498.7401 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6477 - loss: 1493.6433 - val_RMSE: 38.7140 - val_loss: 1498.7709 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6468 - loss: 1493.5774 - val_RMSE: 38.7144 - val_loss: 1498.8016 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6458 - loss: 1493.5024 - val_RMSE: 38.7142 - val_loss: 1498.7925 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6455 - loss: 1493.4731 - val_RMSE: 38.7143 - val_loss: 1498.7937 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6452 - loss: 1493.4558 - val_RMSE: 38.7143 - val_loss: 1498.7971 - learning_rate: 1.0000e-05\n",
            "41608/41608  87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  22s 6ms/step - RMSE: 64.4759 - loss: 4320.1543 - val_RMSE: 38.7162 - val_loss: 1498.9456 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7165 - loss: 1498.9674 - val_RMSE: 38.7140 - val_loss: 1498.7737 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7127 - loss: 1498.6738 - val_RMSE: 38.7125 - val_loss: 1498.6570 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7098 - loss: 1498.4526 - val_RMSE: 38.7114 - val_loss: 1498.5735 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  13s 5ms/step - RMSE: 38.7078 - loss: 1498.2924 - val_RMSE: 38.7101 - val_loss: 1498.4744 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7060 - loss: 1498.1528 - val_RMSE: 38.7095 - val_loss: 1498.4247 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7043 - loss: 1498.0205 - val_RMSE: 38.7086 - val_loss: 1498.3577 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7023 - loss: 1497.8685 - val_RMSE: 38.7080 - val_loss: 1498.3123 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.7005 - loss: 1497.7267 - val_RMSE: 38.7076 - val_loss: 1498.2804 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6988 - loss: 1497.5990 - val_RMSE: 38.7071 - val_loss: 1498.2391 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6972 - loss: 1497.4728 - val_RMSE: 38.7071 - val_loss: 1498.2383 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6956 - loss: 1497.3521 - val_RMSE: 38.7070 - val_loss: 1498.2296 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6938 - loss: 1497.2139 - val_RMSE: 38.7071 - val_loss: 1498.2358 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6919 - loss: 1497.0665 - val_RMSE: 38.7072 - val_loss: 1498.2505 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6900 - loss: 1496.9147 - val_RMSE: 38.7074 - val_loss: 1498.2626 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6879 - loss: 1496.7509 - val_RMSE: 38.7082 - val_loss: 1498.3237 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6856 - loss: 1496.5726 - val_RMSE: 38.7093 - val_loss: 1498.4061 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6794 - loss: 1496.0999 - val_RMSE: 38.7069 - val_loss: 1498.2238 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6759 - loss: 1495.8245 - val_RMSE: 38.7076 - val_loss: 1498.2793 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6742 - loss: 1495.6978 - val_RMSE: 38.7083 - val_loss: 1498.3309 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6730 - loss: 1495.6025 - val_RMSE: 38.7088 - val_loss: 1498.3740 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6720 - loss: 1495.5222 - val_RMSE: 38.7094 - val_loss: 1498.4156 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6710 - loss: 1495.4490 - val_RMSE: 38.7099 - val_loss: 1498.4529 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6698 - loss: 1495.3577 - val_RMSE: 38.7096 - val_loss: 1498.4365 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.6694 - loss: 1495.3240 - val_RMSE: 38.7097 - val_loss: 1498.4375 - learning_rate: 1.0000e-05\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 14:54:24,546] Trial 9 finished with value: 38.7032470703125 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'silu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 53.3186 - loss: 2993.5579 - val_RMSE: 38.7071 - val_loss: 1498.2429 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7440 - loss: 1501.0951 - val_RMSE: 38.6936 - val_loss: 1497.1932 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7411 - loss: 1500.8773 - val_RMSE: 38.7064 - val_loss: 1498.1865 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7388 - loss: 1500.6960 - val_RMSE: 38.7052 - val_loss: 1498.0944 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7383 - loss: 1500.6528 - val_RMSE: 38.6929 - val_loss: 1497.1420 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7352 - loss: 1500.4137 - val_RMSE: 38.6895 - val_loss: 1496.8790 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7336 - loss: 1500.2933 - val_RMSE: 38.6901 - val_loss: 1496.9241 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7323 - loss: 1500.1934 - val_RMSE: 38.6950 - val_loss: 1497.3043 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7310 - loss: 1500.0924 - val_RMSE: 38.7181 - val_loss: 1499.0892 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7309 - loss: 1500.0822 - val_RMSE: 38.6837 - val_loss: 1496.4310 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7291 - loss: 1499.9446 - val_RMSE: 38.6840 - val_loss: 1496.4510 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7285 - loss: 1499.8938 - val_RMSE: 38.6892 - val_loss: 1496.8503 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7278 - loss: 1499.8427 - val_RMSE: 38.6833 - val_loss: 1496.4011 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7268 - loss: 1499.7677 - val_RMSE: 38.6881 - val_loss: 1496.7714 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7270 - loss: 1499.7819 - val_RMSE: 38.6866 - val_loss: 1496.6532 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7260 - loss: 1499.7024 - val_RMSE: 38.6826 - val_loss: 1496.3412 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7251 - loss: 1499.6340 - val_RMSE: 38.6842 - val_loss: 1496.4639 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7239 - loss: 1499.5372 - val_RMSE: 38.6835 - val_loss: 1496.4156 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7226 - loss: 1499.4398 - val_RMSE: 38.6868 - val_loss: 1496.6671 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7213 - loss: 1499.3378 - val_RMSE: 38.6846 - val_loss: 1496.4976 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7206 - loss: 1499.2872 - val_RMSE: 38.6838 - val_loss: 1496.4332 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7126 - loss: 1498.6664 - val_RMSE: 38.6784 - val_loss: 1496.0188 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7093 - loss: 1498.4097 - val_RMSE: 38.6784 - val_loss: 1496.0154 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7083 - loss: 1498.3367 - val_RMSE: 38.6784 - val_loss: 1496.0164 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7077 - loss: 1498.2845 - val_RMSE: 38.6784 - val_loss: 1496.0192 - learning_rate: 1.0000e-04\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  30s 9ms/step - RMSE: 53.2733 - loss: 2987.8425 - val_RMSE: 38.7349 - val_loss: 1500.3892 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6927 - loss: 1497.1256 - val_RMSE: 38.7316 - val_loss: 1500.1345 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6904 - loss: 1496.9482 - val_RMSE: 38.7230 - val_loss: 1499.4745 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6877 - loss: 1496.7358 - val_RMSE: 38.7330 - val_loss: 1500.2435 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6855 - loss: 1496.5675 - val_RMSE: 38.7185 - val_loss: 1499.1194 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6842 - loss: 1496.4686 - val_RMSE: 38.7223 - val_loss: 1499.4143 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6830 - loss: 1496.3733 - val_RMSE: 38.7225 - val_loss: 1499.4310 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6820 - loss: 1496.2997 - val_RMSE: 38.7210 - val_loss: 1499.3123 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6813 - loss: 1496.2462 - val_RMSE: 38.7397 - val_loss: 1500.7631 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6796 - loss: 1496.1113 - val_RMSE: 38.7297 - val_loss: 1499.9910 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6713 - loss: 1495.4679 - val_RMSE: 38.7140 - val_loss: 1498.7743 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6696 - loss: 1495.3373 - val_RMSE: 38.7140 - val_loss: 1498.7760 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6690 - loss: 1495.2925 - val_RMSE: 38.7140 - val_loss: 1498.7715 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6686 - loss: 1495.2627 - val_RMSE: 38.7139 - val_loss: 1498.7659 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6682 - loss: 1495.2341 - val_RMSE: 38.7138 - val_loss: 1498.7554 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6680 - loss: 1495.2122 - val_RMSE: 38.7136 - val_loss: 1498.7467 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6677 - loss: 1495.1919 - val_RMSE: 38.7136 - val_loss: 1498.7390 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6674 - loss: 1495.1726 - val_RMSE: 38.7135 - val_loss: 1498.7358 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6672 - loss: 1495.1550 - val_RMSE: 38.7135 - val_loss: 1498.7365 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6670 - loss: 1495.1384 - val_RMSE: 38.7135 - val_loss: 1498.7358 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6668 - loss: 1495.1208 - val_RMSE: 38.7135 - val_loss: 1498.7355 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6665 - loss: 1495.1028 - val_RMSE: 38.7135 - val_loss: 1498.7336 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6663 - loss: 1495.0834 - val_RMSE: 38.7135 - val_loss: 1498.7355 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6661 - loss: 1495.0645 - val_RMSE: 38.7136 - val_loss: 1498.7396 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6658 - loss: 1495.0465 - val_RMSE: 38.7136 - val_loss: 1498.7426 - learning_rate: 1.0000e-04\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  30s 9ms/step - RMSE: 53.2412 - loss: 2983.8818 - val_RMSE: 38.7205 - val_loss: 1499.2791 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7226 - loss: 1499.4396 - val_RMSE: 38.7290 - val_loss: 1499.9385 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7183 - loss: 1499.1036 - val_RMSE: 38.7236 - val_loss: 1499.5153 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7176 - loss: 1499.0568 - val_RMSE: 38.7117 - val_loss: 1498.5991 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7150 - loss: 1498.8558 - val_RMSE: 38.7164 - val_loss: 1498.9625 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7139 - loss: 1498.7700 - val_RMSE: 38.7172 - val_loss: 1499.0209 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7123 - loss: 1498.6466 - val_RMSE: 38.7147 - val_loss: 1498.8301 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7106 - loss: 1498.5078 - val_RMSE: 38.7078 - val_loss: 1498.2960 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7096 - loss: 1498.4333 - val_RMSE: 38.7104 - val_loss: 1498.4935 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7077 - loss: 1498.2885 - val_RMSE: 38.7070 - val_loss: 1498.2352 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7066 - loss: 1498.2037 - val_RMSE: 38.7039 - val_loss: 1497.9928 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7049 - loss: 1498.0693 - val_RMSE: 38.7053 - val_loss: 1498.1021 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7041 - loss: 1498.0063 - val_RMSE: 38.7070 - val_loss: 1498.2332 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7027 - loss: 1497.8972 - val_RMSE: 38.7058 - val_loss: 1498.1415 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7011 - loss: 1497.7777 - val_RMSE: 38.7051 - val_loss: 1498.0840 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6996 - loss: 1497.6604 - val_RMSE: 38.7040 - val_loss: 1498.0010 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6902 - loss: 1496.9329 - val_RMSE: 38.7026 - val_loss: 1497.8904 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6883 - loss: 1496.7838 - val_RMSE: 38.7029 - val_loss: 1497.9113 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6871 - loss: 1496.6962 - val_RMSE: 38.7031 - val_loss: 1497.9337 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6862 - loss: 1496.6212 - val_RMSE: 38.7034 - val_loss: 1497.9550 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6853 - loss: 1496.5498 - val_RMSE: 38.7037 - val_loss: 1497.9750 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6844 - loss: 1496.4833 - val_RMSE: 38.7039 - val_loss: 1497.9956 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6830 - loss: 1496.3727 - val_RMSE: 38.7032 - val_loss: 1497.9346 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6824 - loss: 1496.3285 - val_RMSE: 38.7031 - val_loss: 1497.9335 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6822 - loss: 1496.3137 - val_RMSE: 38.7032 - val_loss: 1497.9363 - learning_rate: 1.0000e-05\n",
            "41608/41608  83s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 15:26:28,049] Trial 10 finished with value: 38.69839350382487 and parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  30s 9ms/step - RMSE: 53.3182 - loss: 2993.5210 - val_RMSE: 38.7001 - val_loss: 1497.7002 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7444 - loss: 1501.1309 - val_RMSE: 38.6928 - val_loss: 1497.1317 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7420 - loss: 1500.9396 - val_RMSE: 38.6988 - val_loss: 1497.5938 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7382 - loss: 1500.6515 - val_RMSE: 38.6949 - val_loss: 1497.2966 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7358 - loss: 1500.4611 - val_RMSE: 38.6896 - val_loss: 1496.8850 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7345 - loss: 1500.3644 - val_RMSE: 38.6875 - val_loss: 1496.7209 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7341 - loss: 1500.3313 - val_RMSE: 38.6879 - val_loss: 1496.7523 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7324 - loss: 1500.2007 - val_RMSE: 38.6879 - val_loss: 1496.7507 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7328 - loss: 1500.2299 - val_RMSE: 38.6865 - val_loss: 1496.6488 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7315 - loss: 1500.1271 - val_RMSE: 38.6874 - val_loss: 1496.7111 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7302 - loss: 1500.0288 - val_RMSE: 38.6841 - val_loss: 1496.4628 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7289 - loss: 1499.9288 - val_RMSE: 38.6831 - val_loss: 1496.3828 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7297 - loss: 1499.9921 - val_RMSE: 38.6843 - val_loss: 1496.4767 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7278 - loss: 1499.8407 - val_RMSE: 38.6880 - val_loss: 1496.7582 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7272 - loss: 1499.7946 - val_RMSE: 38.6922 - val_loss: 1497.0842 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7268 - loss: 1499.7622 - val_RMSE: 38.6893 - val_loss: 1496.8632 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7256 - loss: 1499.6724 - val_RMSE: 38.6871 - val_loss: 1496.6946 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7173 - loss: 1499.0298 - val_RMSE: 38.6801 - val_loss: 1496.1477 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7151 - loss: 1498.8584 - val_RMSE: 38.6799 - val_loss: 1496.1340 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7143 - loss: 1498.7955 - val_RMSE: 38.6798 - val_loss: 1496.1288 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7137 - loss: 1498.7512 - val_RMSE: 38.6798 - val_loss: 1496.1282 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7132 - loss: 1498.7134 - val_RMSE: 38.6799 - val_loss: 1496.1337 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7128 - loss: 1498.6805 - val_RMSE: 38.6800 - val_loss: 1496.1442 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7124 - loss: 1498.6511 - val_RMSE: 38.6802 - val_loss: 1496.1571 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7120 - loss: 1498.6233 - val_RMSE: 38.6803 - val_loss: 1496.1666 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  30s 9ms/step - RMSE: 53.2736 - loss: 2987.8662 - val_RMSE: 38.7326 - val_loss: 1500.2148 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6918 - loss: 1497.0553 - val_RMSE: 38.7311 - val_loss: 1500.0999 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6885 - loss: 1496.8013 - val_RMSE: 38.7335 - val_loss: 1500.2803 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6870 - loss: 1496.6886 - val_RMSE: 38.7253 - val_loss: 1499.6528 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6855 - loss: 1496.5717 - val_RMSE: 38.7232 - val_loss: 1499.4899 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6842 - loss: 1496.4673 - val_RMSE: 38.7206 - val_loss: 1499.2819 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6829 - loss: 1496.3699 - val_RMSE: 38.7182 - val_loss: 1499.1023 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6829 - loss: 1496.3691 - val_RMSE: 38.7186 - val_loss: 1499.1332 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6808 - loss: 1496.2013 - val_RMSE: 38.7166 - val_loss: 1498.9739 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6795 - loss: 1496.1061 - val_RMSE: 38.7164 - val_loss: 1498.9606 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6790 - loss: 1496.0621 - val_RMSE: 38.7168 - val_loss: 1498.9935 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6780 - loss: 1495.9908 - val_RMSE: 38.7177 - val_loss: 1499.0569 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6777 - loss: 1495.9657 - val_RMSE: 38.7158 - val_loss: 1498.9131 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6766 - loss: 1495.8829 - val_RMSE: 38.7384 - val_loss: 1500.6659 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6758 - loss: 1495.8217 - val_RMSE: 38.7163 - val_loss: 1498.9550 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6754 - loss: 1495.7870 - val_RMSE: 38.7183 - val_loss: 1499.1045 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6746 - loss: 1495.7249 - val_RMSE: 38.7184 - val_loss: 1499.1160 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6741 - loss: 1495.6904 - val_RMSE: 38.7191 - val_loss: 1499.1710 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6654 - loss: 1495.0125 - val_RMSE: 38.7128 - val_loss: 1498.6842 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6639 - loss: 1494.9015 - val_RMSE: 38.7127 - val_loss: 1498.6755 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6632 - loss: 1494.8478 - val_RMSE: 38.7129 - val_loss: 1498.6915 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6628 - loss: 1494.8092 - val_RMSE: 38.7131 - val_loss: 1498.7061 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6623 - loss: 1494.7767 - val_RMSE: 38.7133 - val_loss: 1498.7195 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6620 - loss: 1494.7484 - val_RMSE: 38.7134 - val_loss: 1498.7306 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6617 - loss: 1494.7253 - val_RMSE: 38.7136 - val_loss: 1498.7423 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  30s 9ms/step - RMSE: 53.2410 - loss: 2983.8684 - val_RMSE: 38.7145 - val_loss: 1498.8087 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7225 - loss: 1499.4362 - val_RMSE: 38.7203 - val_loss: 1499.2604 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7191 - loss: 1499.1697 - val_RMSE: 38.7137 - val_loss: 1498.7532 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7175 - loss: 1499.0491 - val_RMSE: 38.7234 - val_loss: 1499.4988 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7151 - loss: 1498.8591 - val_RMSE: 38.7104 - val_loss: 1498.4938 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7136 - loss: 1498.7418 - val_RMSE: 38.7083 - val_loss: 1498.3307 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7118 - loss: 1498.6025 - val_RMSE: 38.7109 - val_loss: 1498.5364 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7109 - loss: 1498.5325 - val_RMSE: 38.7095 - val_loss: 1498.4218 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7094 - loss: 1498.4159 - val_RMSE: 38.7065 - val_loss: 1498.1924 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7080 - loss: 1498.3065 - val_RMSE: 38.7076 - val_loss: 1498.2800 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7067 - loss: 1498.2075 - val_RMSE: 38.7049 - val_loss: 1498.0663 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7058 - loss: 1498.1422 - val_RMSE: 38.7054 - val_loss: 1498.1062 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7040 - loss: 1497.9983 - val_RMSE: 38.7055 - val_loss: 1498.1162 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7027 - loss: 1497.8960 - val_RMSE: 38.7037 - val_loss: 1497.9774 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7012 - loss: 1497.7806 - val_RMSE: 38.7030 - val_loss: 1497.9189 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6996 - loss: 1497.6615 - val_RMSE: 38.7034 - val_loss: 1497.9550 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6982 - loss: 1497.5549 - val_RMSE: 38.7037 - val_loss: 1497.9731 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6963 - loss: 1497.4067 - val_RMSE: 38.7039 - val_loss: 1497.9911 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6945 - loss: 1497.2668 - val_RMSE: 38.7046 - val_loss: 1498.0454 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6927 - loss: 1497.1243 - val_RMSE: 38.7046 - val_loss: 1498.0496 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.6842 - loss: 1496.4664 - val_RMSE: 38.7037 - val_loss: 1497.9731 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6811 - loss: 1496.2301 - val_RMSE: 38.7044 - val_loss: 1498.0341 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6796 - loss: 1496.1124 - val_RMSE: 38.7052 - val_loss: 1498.0941 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6783 - loss: 1496.0143 - val_RMSE: 38.7055 - val_loss: 1498.1146 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.6773 - loss: 1495.9305 - val_RMSE: 38.7058 - val_loss: 1498.1394 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 15:58:20,303] Trial 11 finished with value: 38.69990666707357 and parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            " 249/2601  16s 7ms/step - RMSE: 84.7905 - loss: 7211.5386"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-15 15:58:29,727] Trial 12 failed with parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-43-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-42-096f0de6de51>\", line 48, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n",
            "    if not opt_outputs.has_value():\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\", line 176, in has_value\n",
            "    return gen_optional_ops.optional_has_value(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\", line 172, in optional_has_value\n",
            "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-02-15 15:58:29,729] Trial 12 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-096f0de6de51>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train, \n\u001b[0m\u001b[1;32m     49\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtsmIR6j1aLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 3 finished with value: 38.69801712036133\n",
        "* parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'}"
      ],
      "metadata": {
        "id": "egD1bR4BVai7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kz6gSSAQlNe"
      },
      "source": [
        "#### **4.6.1 NeuralNetwork v1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "2b34d4ed-ae4f-436a-d522-17da4a9b5279",
        "id": "Vu4pcEHqQlNf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "56569        3         4     3             8                   2           1   \n",
              "1183394      1         3     3             7                   2           1   \n",
              "855407       4         1     1             9                   2           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "56569        0      0              0.727470 -0.518096 -0.606337 -0.709550   \n",
              "1183394      0      6              1.159509  0.950699 -0.606337 -0.613824   \n",
              "855407       0      6             -0.842369 -0.192151 -0.168518 -0.018634   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "56569             0               0  \n",
              "1183394           0               0  \n",
              "855407            0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87fca73f-247b-4e84-89db-75fc1afb8d2c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>56569</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.727470</td>\n",
              "      <td>-0.518096</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>-0.709550</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1183394</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1.159509</td>\n",
              "      <td>0.950699</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>-0.613824</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855407</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.842369</td>\n",
              "      <td>-0.192151</td>\n",
              "      <td>-0.168518</td>\n",
              "      <td>-0.018634</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87fca73f-247b-4e84-89db-75fc1afb8d2c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-87fca73f-247b-4e84-89db-75fc1afb8d2c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-87fca73f-247b-4e84-89db-75fc1afb8d2c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5bbe5fca-40db-4be7-888f-af61eba3a9fd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5bbe5fca-40db-4be7-888f-af61eba3a9fd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5bbe5fca-40db-4be7-888f-af61eba3a9fd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 7,\n        \"max\": 9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          8,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7274695038795471\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.5180957317352295\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.16851840913295746\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.7095502018928528\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512,last_layer = 1, activation=\"relu\", reg=0.001, dropout_rate=0.33):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    reshaped_features = layers.Reshape((1, -1))(x)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x, flattened_attention])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "J50ErSTRQlNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model()\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "71HFsxAZVnCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bdaf28-50f6-4c7b-9818-75aa19191dd3",
        "id": "9tzW7MmLQlNg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([6, 5, 4, 10, 3, 3, 4, 7, 2, 2],\n",
              " array([3., 3., 2., 4., 2., 2., 2., 3., 2., 2.]),\n",
              " 10)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ceVtzBwQlNg"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a87fac-94f6-4023-c880-b2082a669485",
        "id": "iQ9CoxtaQlNg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Dtype\n",
            "---  ------              -----\n",
            " 0   Brand               int64\n",
            " 1   Material            int64\n",
            " 2   Size                int64\n",
            " 3   Compartments        int64\n",
            " 4   Laptop Compartment  int64\n",
            " 5   Waterproof          int64\n",
            " 6   Style               int64\n",
            " 7   Color               int64\n",
            " 8   cheap_flag          int64\n",
            " 9   expansive_flag      int64\n",
            "dtypes: int64(10)\n",
            "memory usage: 335.2 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Weight Capacity (kg)  float32\n",
            " 1   TE_wc                 float32\n",
            " 2   skew_0                float32\n",
            " 3   skew_1                float32\n",
            "dtypes: float32(4)\n",
            "memory usage: 91.4 MB\n"
          ]
        }
      ],
      "source": [
        "categorical_feat = t.cat_features.copy()\n",
        "numerical_feat = t.num_features.copy()\n",
        "\n",
        "X_train_cat = X_enc[categorical_feat]\n",
        "X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "X_test_cat = test_enc[categorical_feat]\n",
        "X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "X_train_cat.info()\n",
        "X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tObj5kq1QlNg"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': trial.suggest_float('reg', 1e-4, 0.1, log=True),\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.50)\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num])\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sux1Xc6ZQlNh"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7529e2b3-d69b-4ddb-a44c-2f80dbe52ca4",
        "id": "gW_aoIIBQlNh"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 16:03:45,993] A new study created in memory with name: no-name-36849c32-d360-4d35-8d2a-109bfaa1d4aa\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  39s 12ms/step - RMSE: 51.3745 - loss: 2832.1123 - val_RMSE: 38.7568 - val_loss: 1519.5251 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9979 - loss: 1536.6611 - val_RMSE: 38.7666 - val_loss: 1515.6813 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8919 - loss: 1525.1195 - val_RMSE: 38.7090 - val_loss: 1509.4550 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8501 - loss: 1519.8094 - val_RMSE: 38.6949 - val_loss: 1506.4019 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8228 - loss: 1516.0771 - val_RMSE: 38.6996 - val_loss: 1505.6942 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8137 - loss: 1514.2294 - val_RMSE: 38.7109 - val_loss: 1505.5392 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8025 - loss: 1512.2858 - val_RMSE: 38.6927 - val_loss: 1503.1927 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7950 - loss: 1510.7366 - val_RMSE: 38.6923 - val_loss: 1502.1176 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7867 - loss: 1509.0934 - val_RMSE: 38.6909 - val_loss: 1500.8621 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7805 - loss: 1507.7684 - val_RMSE: 38.6896 - val_loss: 1500.1909 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7806 - loss: 1507.3898 - val_RMSE: 38.6902 - val_loss: 1500.0416 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7818 - loss: 1507.2737 - val_RMSE: 38.6917 - val_loss: 1500.2687 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7816 - loss: 1507.1475 - val_RMSE: 38.6891 - val_loss: 1499.9069 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7808 - loss: 1506.9562 - val_RMSE: 38.6876 - val_loss: 1499.5712 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7792 - loss: 1506.7211 - val_RMSE: 38.6887 - val_loss: 1499.6896 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7803 - loss: 1506.7301 - val_RMSE: 38.6895 - val_loss: 1499.5400 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7802 - loss: 1506.6393 - val_RMSE: 38.6883 - val_loss: 1499.5293 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7802 - loss: 1506.5903 - val_RMSE: 38.6864 - val_loss: 1499.0499 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7780 - loss: 1506.3221 - val_RMSE: 38.6881 - val_loss: 1499.2017 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7787 - loss: 1506.2954 - val_RMSE: 38.6896 - val_loss: 1499.3910 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7775 - loss: 1506.1781 - val_RMSE: 38.6892 - val_loss: 1499.2687 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7766 - loss: 1506.0671 - val_RMSE: 38.6860 - val_loss: 1499.0186 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7808 - loss: 1506.2799 - val_RMSE: 38.6865 - val_loss: 1498.8927 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7795 - loss: 1506.1647 - val_RMSE: 38.6855 - val_loss: 1498.7249 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7756 - loss: 1505.7499 - val_RMSE: 38.6862 - val_loss: 1498.7686 - learning_rate: 0.0010\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.3021 - loss: 2823.8208 - val_RMSE: 38.7634 - val_loss: 1520.7347 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9422 - loss: 1532.8909 - val_RMSE: 38.7831 - val_loss: 1516.9619 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8378 - loss: 1520.9048 - val_RMSE: 38.7349 - val_loss: 1511.1633 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7928 - loss: 1515.2720 - val_RMSE: 38.7321 - val_loss: 1509.1328 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7676 - loss: 1511.5394 - val_RMSE: 38.7262 - val_loss: 1507.4763 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7551 - loss: 1509.4453 - val_RMSE: 38.7316 - val_loss: 1506.8464 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7485 - loss: 1507.9105 - val_RMSE: 38.7240 - val_loss: 1505.2249 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7398 - loss: 1506.3206 - val_RMSE: 38.7215 - val_loss: 1504.0413 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7341 - loss: 1504.8588 - val_RMSE: 38.7214 - val_loss: 1503.0850 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7336 - loss: 1504.0422 - val_RMSE: 38.7209 - val_loss: 1502.7485 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7323 - loss: 1503.5555 - val_RMSE: 38.7200 - val_loss: 1502.3033 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7319 - loss: 1503.3514 - val_RMSE: 38.7201 - val_loss: 1502.2601 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7316 - loss: 1503.2905 - val_RMSE: 38.7225 - val_loss: 1502.4164 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7313 - loss: 1503.1978 - val_RMSE: 38.7182 - val_loss: 1501.9128 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7306 - loss: 1502.9703 - val_RMSE: 38.7212 - val_loss: 1502.1805 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7295 - loss: 1502.8280 - val_RMSE: 38.7195 - val_loss: 1501.8370 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7306 - loss: 1502.7605 - val_RMSE: 38.7213 - val_loss: 1501.8317 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7265 - loss: 1502.3130 - val_RMSE: 38.7208 - val_loss: 1501.7612 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7292 - loss: 1502.4355 - val_RMSE: 38.7230 - val_loss: 1501.8983 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7263 - loss: 1502.2406 - val_RMSE: 38.7213 - val_loss: 1501.7244 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7275 - loss: 1502.2611 - val_RMSE: 38.7168 - val_loss: 1501.2633 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7272 - loss: 1502.1406 - val_RMSE: 38.7202 - val_loss: 1501.5856 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7272 - loss: 1502.1337 - val_RMSE: 38.7190 - val_loss: 1501.3167 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7260 - loss: 1501.9783 - val_RMSE: 38.7203 - val_loss: 1501.3640 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7227 - loss: 1501.6710 - val_RMSE: 38.7188 - val_loss: 1501.1597 - learning_rate: 0.0010\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.3149 - loss: 2824.5806 - val_RMSE: 38.7822 - val_loss: 1522.3011 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9822 - loss: 1536.1847 - val_RMSE: 38.7577 - val_loss: 1515.6809 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8762 - loss: 1524.1930 - val_RMSE: 38.7271 - val_loss: 1510.6638 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8197 - loss: 1517.1710 - val_RMSE: 38.7419 - val_loss: 1509.5366 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8001 - loss: 1513.8381 - val_RMSE: 38.7225 - val_loss: 1507.0442 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7900 - loss: 1512.0044 - val_RMSE: 38.7213 - val_loss: 1505.7921 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7782 - loss: 1510.1273 - val_RMSE: 38.7158 - val_loss: 1504.5847 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7653 - loss: 1508.2162 - val_RMSE: 38.7130 - val_loss: 1503.3190 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7611 - loss: 1507.0109 - val_RMSE: 38.7166 - val_loss: 1503.0569 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7581 - loss: 1506.0563 - val_RMSE: 38.7155 - val_loss: 1502.3295 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7562 - loss: 1505.5942 - val_RMSE: 38.7131 - val_loss: 1502.0338 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7568 - loss: 1505.4513 - val_RMSE: 38.7114 - val_loss: 1501.6659 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7535 - loss: 1505.0535 - val_RMSE: 38.7111 - val_loss: 1501.5841 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7596 - loss: 1505.3881 - val_RMSE: 38.7163 - val_loss: 1502.1027 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7549 - loss: 1505.0312 - val_RMSE: 38.7110 - val_loss: 1501.3414 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7559 - loss: 1504.9349 - val_RMSE: 38.7133 - val_loss: 1501.6398 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7529 - loss: 1504.6545 - val_RMSE: 38.7095 - val_loss: 1501.4309 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7559 - loss: 1504.7786 - val_RMSE: 38.7117 - val_loss: 1501.1122 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7561 - loss: 1504.7273 - val_RMSE: 38.7122 - val_loss: 1501.1674 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7546 - loss: 1504.4834 - val_RMSE: 38.7121 - val_loss: 1501.1199 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7566 - loss: 1504.6086 - val_RMSE: 38.7148 - val_loss: 1501.3589 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7537 - loss: 1504.3334 - val_RMSE: 38.7112 - val_loss: 1500.9198 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7567 - loss: 1504.4890 - val_RMSE: 38.7107 - val_loss: 1500.9138 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7536 - loss: 1504.1897 - val_RMSE: 38.7084 - val_loss: 1500.6278 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7551 - loss: 1504.3167 - val_RMSE: 38.7082 - val_loss: 1500.4899 - learning_rate: 0.0010\n",
            "41608/41608  82s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 16:40:07,636] Trial 0 finished with value: 38.70439020792643 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'relu', 'reg': 0.03532288502832786, 'dropout_rate': 0.31396524201979953}. Best is trial 0 with value: 38.70439020792643.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.1264 - loss: 3079.4102 - val_RMSE: 38.7246 - val_loss: 1500.0896 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0518 - loss: 1525.5660 - val_RMSE: 38.7018 - val_loss: 1498.4287 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9228 - loss: 1515.6116 - val_RMSE: 38.6893 - val_loss: 1497.5673 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8847 - loss: 1512.7523 - val_RMSE: 38.6864 - val_loss: 1497.4307 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8597 - loss: 1510.8882 - val_RMSE: 38.6857 - val_loss: 1497.4148 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8535 - loss: 1510.4323 - val_RMSE: 38.6840 - val_loss: 1497.2799 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8469 - loss: 1509.9010 - val_RMSE: 38.6838 - val_loss: 1497.2241 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8435 - loss: 1509.5848 - val_RMSE: 38.6836 - val_loss: 1497.1499 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8411 - loss: 1509.3501 - val_RMSE: 38.6828 - val_loss: 1497.0570 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8424 - loss: 1509.4250 - val_RMSE: 38.6823 - val_loss: 1497.0051 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8404 - loss: 1509.2593 - val_RMSE: 38.6821 - val_loss: 1496.9814 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8350 - loss: 1508.8384 - val_RMSE: 38.6813 - val_loss: 1496.9292 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8436 - loss: 1509.5121 - val_RMSE: 38.6818 - val_loss: 1496.9677 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8348 - loss: 1508.8245 - val_RMSE: 38.6814 - val_loss: 1496.9409 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8363 - loss: 1508.9521 - val_RMSE: 38.6803 - val_loss: 1496.8596 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8300 - loss: 1508.4655 - val_RMSE: 38.6812 - val_loss: 1496.9338 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8321 - loss: 1508.6306 - val_RMSE: 38.6806 - val_loss: 1496.8925 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8286 - loss: 1508.3646 - val_RMSE: 38.6802 - val_loss: 1496.8665 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8292 - loss: 1508.4248 - val_RMSE: 38.6799 - val_loss: 1496.8623 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8313 - loss: 1508.6008 - val_RMSE: 38.6797 - val_loss: 1496.8492 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8260 - loss: 1508.1886 - val_RMSE: 38.6804 - val_loss: 1496.9110 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8287 - loss: 1508.4119 - val_RMSE: 38.6807 - val_loss: 1496.9382 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8261 - loss: 1508.2118 - val_RMSE: 38.6808 - val_loss: 1496.9559 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8216 - loss: 1507.8792 - val_RMSE: 38.6808 - val_loss: 1496.9628 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8245 - loss: 1508.1134 - val_RMSE: 38.6798 - val_loss: 1496.9000 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.0879 - loss: 3074.5679 - val_RMSE: 38.7391 - val_loss: 1501.2137 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 7ms/step - RMSE: 39.0029 - loss: 1521.7534 - val_RMSE: 38.7260 - val_loss: 1500.3019 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8782 - loss: 1512.1405 - val_RMSE: 38.7228 - val_loss: 1500.1617 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8270 - loss: 1508.2686 - val_RMSE: 38.7208 - val_loss: 1500.1039 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8100 - loss: 1507.0386 - val_RMSE: 38.7170 - val_loss: 1499.8690 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8010 - loss: 1506.3768 - val_RMSE: 38.7162 - val_loss: 1499.7908 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7953 - loss: 1505.9154 - val_RMSE: 38.7160 - val_loss: 1499.7277 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7952 - loss: 1505.8512 - val_RMSE: 38.7157 - val_loss: 1499.6515 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7937 - loss: 1505.6903 - val_RMSE: 38.7156 - val_loss: 1499.6105 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7909 - loss: 1505.4437 - val_RMSE: 38.7152 - val_loss: 1499.5591 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7871 - loss: 1505.1288 - val_RMSE: 38.7149 - val_loss: 1499.5374 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7876 - loss: 1505.1685 - val_RMSE: 38.7153 - val_loss: 1499.5674 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7842 - loss: 1504.9037 - val_RMSE: 38.7139 - val_loss: 1499.4578 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7823 - loss: 1504.7612 - val_RMSE: 38.7151 - val_loss: 1499.5588 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7884 - loss: 1505.2439 - val_RMSE: 38.7147 - val_loss: 1499.5333 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7809 - loss: 1504.6642 - val_RMSE: 38.7139 - val_loss: 1499.4816 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7882 - loss: 1505.2380 - val_RMSE: 38.7140 - val_loss: 1499.5017 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7826 - loss: 1504.8125 - val_RMSE: 38.7144 - val_loss: 1499.5380 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7756 - loss: 1504.2653 - val_RMSE: 38.7076 - val_loss: 1498.9706 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7759 - loss: 1504.2529 - val_RMSE: 38.7076 - val_loss: 1498.9368 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7686 - loss: 1503.6591 - val_RMSE: 38.7078 - val_loss: 1498.9263 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7749 - loss: 1504.1180 - val_RMSE: 38.7079 - val_loss: 1498.9117 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7652 - loss: 1503.3452 - val_RMSE: 38.7076 - val_loss: 1498.8704 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7685 - loss: 1503.5879 - val_RMSE: 38.7077 - val_loss: 1498.8667 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7652 - loss: 1503.3159 - val_RMSE: 38.7079 - val_loss: 1498.8663 - learning_rate: 1.0000e-04\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.0607 - loss: 3071.0916 - val_RMSE: 38.7247 - val_loss: 1500.0953 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0263 - loss: 1523.5748 - val_RMSE: 38.7151 - val_loss: 1499.4620 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8984 - loss: 1513.7181 - val_RMSE: 38.7100 - val_loss: 1499.1707 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8556 - loss: 1510.4905 - val_RMSE: 38.7084 - val_loss: 1499.1277 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8397 - loss: 1509.3270 - val_RMSE: 38.7072 - val_loss: 1499.0754 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8244 - loss: 1508.1620 - val_RMSE: 38.7072 - val_loss: 1499.0577 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8248 - loss: 1508.1646 - val_RMSE: 38.7059 - val_loss: 1498.9071 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8213 - loss: 1507.8408 - val_RMSE: 38.7053 - val_loss: 1498.8085 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8185 - loss: 1507.5780 - val_RMSE: 38.7059 - val_loss: 1498.8350 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8207 - loss: 1507.7319 - val_RMSE: 38.7045 - val_loss: 1498.7238 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8156 - loss: 1507.3353 - val_RMSE: 38.7049 - val_loss: 1498.7467 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8149 - loss: 1507.2789 - val_RMSE: 38.7043 - val_loss: 1498.7028 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8107 - loss: 1506.9546 - val_RMSE: 38.7050 - val_loss: 1498.7563 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8134 - loss: 1507.1660 - val_RMSE: 38.7049 - val_loss: 1498.7622 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8092 - loss: 1506.8434 - val_RMSE: 38.7037 - val_loss: 1498.6705 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8119 - loss: 1507.0581 - val_RMSE: 38.7040 - val_loss: 1498.7018 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8108 - loss: 1506.9803 - val_RMSE: 38.7032 - val_loss: 1498.6395 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8082 - loss: 1506.7808 - val_RMSE: 38.7036 - val_loss: 1498.6838 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8077 - loss: 1506.7535 - val_RMSE: 38.7032 - val_loss: 1498.6501 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8067 - loss: 1506.6863 - val_RMSE: 38.7030 - val_loss: 1498.6527 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7974 - loss: 1505.9706 - val_RMSE: 38.7024 - val_loss: 1498.6149 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8040 - loss: 1506.4856 - val_RMSE: 38.7032 - val_loss: 1498.6814 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8049 - loss: 1506.5691 - val_RMSE: 38.7030 - val_loss: 1498.6797 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8021 - loss: 1506.3671 - val_RMSE: 38.7018 - val_loss: 1498.5995 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8007 - loss: 1506.2587 - val_RMSE: 38.7027 - val_loss: 1498.6749 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 17:10:07,902] Trial 1 finished with value: 38.696816762288414 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00027561698595006656, 'dropout_rate': 0.4105017873627729}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  39s 12ms/step - RMSE: 51.4811 - loss: 2785.5352 - val_RMSE: 38.7200 - val_loss: 1501.3983 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 39.0050 - loss: 1523.6791 - val_RMSE: 38.7155 - val_loss: 1501.4891 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.8815 - loss: 1514.4375 - val_RMSE: 38.7056 - val_loss: 1500.8835 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8411 - loss: 1511.3909 - val_RMSE: 38.7018 - val_loss: 1500.5566 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8134 - loss: 1509.1803 - val_RMSE: 38.6988 - val_loss: 1500.1974 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8055 - loss: 1508.4283 - val_RMSE: 38.6927 - val_loss: 1499.5613 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7993 - loss: 1507.7716 - val_RMSE: 38.6914 - val_loss: 1499.2638 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7926 - loss: 1507.0441 - val_RMSE: 38.6929 - val_loss: 1499.1389 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7870 - loss: 1506.3828 - val_RMSE: 38.6878 - val_loss: 1498.5454 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7828 - loss: 1505.8733 - val_RMSE: 38.6885 - val_loss: 1498.4587 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7820 - loss: 1505.6763 - val_RMSE: 38.6873 - val_loss: 1498.2343 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7822 - loss: 1505.5815 - val_RMSE: 38.6873 - val_loss: 1498.1597 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7804 - loss: 1505.3665 - val_RMSE: 38.6853 - val_loss: 1497.9257 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7785 - loss: 1505.1427 - val_RMSE: 38.6838 - val_loss: 1497.7542 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7766 - loss: 1504.9423 - val_RMSE: 38.6827 - val_loss: 1497.6527 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7774 - loss: 1504.9865 - val_RMSE: 38.6820 - val_loss: 1497.5724 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7744 - loss: 1504.7397 - val_RMSE: 38.6823 - val_loss: 1497.5912 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7753 - loss: 1504.8101 - val_RMSE: 38.6818 - val_loss: 1497.5594 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7720 - loss: 1504.5520 - val_RMSE: 38.6817 - val_loss: 1497.5492 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7737 - loss: 1504.6752 - val_RMSE: 38.6824 - val_loss: 1497.6013 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7715 - loss: 1504.5157 - val_RMSE: 38.6819 - val_loss: 1497.5652 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7703 - loss: 1504.4159 - val_RMSE: 38.6815 - val_loss: 1497.5253 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7748 - loss: 1504.7688 - val_RMSE: 38.6811 - val_loss: 1497.5056 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7722 - loss: 1504.5754 - val_RMSE: 38.6815 - val_loss: 1497.5227 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7673 - loss: 1504.1896 - val_RMSE: 38.6817 - val_loss: 1497.5555 - learning_rate: 0.0010\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4232 - loss: 2778.6873 - val_RMSE: 38.7358 - val_loss: 1502.5631 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9488 - loss: 1519.2096 - val_RMSE: 38.7286 - val_loss: 1502.3649 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8307 - loss: 1510.3438 - val_RMSE: 38.7201 - val_loss: 1501.8761 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7850 - loss: 1506.9270 - val_RMSE: 38.7249 - val_loss: 1502.3204 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7640 - loss: 1505.3335 - val_RMSE: 38.7204 - val_loss: 1501.8672 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7538 - loss: 1504.4165 - val_RMSE: 38.7161 - val_loss: 1501.3892 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7451 - loss: 1503.5719 - val_RMSE: 38.7156 - val_loss: 1501.1223 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7413 - loss: 1503.0411 - val_RMSE: 38.7135 - val_loss: 1500.7371 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7359 - loss: 1502.4141 - val_RMSE: 38.7136 - val_loss: 1500.5466 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7355 - loss: 1502.1884 - val_RMSE: 38.7144 - val_loss: 1500.4609 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7333 - loss: 1501.8695 - val_RMSE: 38.7143 - val_loss: 1500.2827 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7309 - loss: 1501.5409 - val_RMSE: 38.7140 - val_loss: 1500.1484 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7289 - loss: 1501.2842 - val_RMSE: 38.7154 - val_loss: 1500.1740 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7294 - loss: 1501.2535 - val_RMSE: 38.7144 - val_loss: 1500.0696 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7274 - loss: 1501.0635 - val_RMSE: 38.7159 - val_loss: 1500.1901 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7267 - loss: 1501.0211 - val_RMSE: 38.7144 - val_loss: 1500.0526 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7277 - loss: 1501.0828 - val_RMSE: 38.7163 - val_loss: 1500.2013 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7225 - loss: 1500.6899 - val_RMSE: 38.7157 - val_loss: 1500.1570 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7254 - loss: 1500.9139 - val_RMSE: 38.7153 - val_loss: 1500.1416 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7226 - loss: 1500.6957 - val_RMSE: 38.7161 - val_loss: 1500.1908 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7224 - loss: 1500.6945 - val_RMSE: 38.7145 - val_loss: 1500.0940 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7145 - loss: 1500.0428 - val_RMSE: 38.7088 - val_loss: 1499.4592 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7091 - loss: 1499.4463 - val_RMSE: 38.7091 - val_loss: 1499.3541 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7079 - loss: 1499.2405 - val_RMSE: 38.7091 - val_loss: 1499.2749 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7049 - loss: 1498.9391 - val_RMSE: 38.7090 - val_loss: 1499.2125 - learning_rate: 1.0000e-04\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4282 - loss: 2778.9778 - val_RMSE: 38.7325 - val_loss: 1502.2775 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9723 - loss: 1521.0208 - val_RMSE: 38.7222 - val_loss: 1501.8488 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8661 - loss: 1513.0850 - val_RMSE: 38.7160 - val_loss: 1501.5682 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8211 - loss: 1509.7300 - val_RMSE: 38.7168 - val_loss: 1501.6185 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7951 - loss: 1507.6799 - val_RMSE: 38.7103 - val_loss: 1501.0483 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7825 - loss: 1506.6033 - val_RMSE: 38.7076 - val_loss: 1500.6799 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7775 - loss: 1506.0386 - val_RMSE: 38.7064 - val_loss: 1500.3843 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7653 - loss: 1504.9027 - val_RMSE: 38.7060 - val_loss: 1500.1735 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7628 - loss: 1504.5100 - val_RMSE: 38.7045 - val_loss: 1499.8124 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7596 - loss: 1504.0413 - val_RMSE: 38.7045 - val_loss: 1499.6505 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7552 - loss: 1503.5354 - val_RMSE: 38.7041 - val_loss: 1499.4872 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7555 - loss: 1503.4480 - val_RMSE: 38.7037 - val_loss: 1499.3448 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7506 - loss: 1502.9628 - val_RMSE: 38.7035 - val_loss: 1499.2722 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7563 - loss: 1503.3506 - val_RMSE: 38.7031 - val_loss: 1499.1765 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7515 - loss: 1502.9250 - val_RMSE: 38.7031 - val_loss: 1499.1622 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7523 - loss: 1502.9747 - val_RMSE: 38.7021 - val_loss: 1499.0925 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7504 - loss: 1502.8300 - val_RMSE: 38.7021 - val_loss: 1499.0677 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7518 - loss: 1502.9227 - val_RMSE: 38.7015 - val_loss: 1499.0411 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7540 - loss: 1503.1014 - val_RMSE: 38.7025 - val_loss: 1499.1080 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7495 - loss: 1502.7600 - val_RMSE: 38.7018 - val_loss: 1499.0720 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7519 - loss: 1502.9512 - val_RMSE: 38.7024 - val_loss: 1499.1102 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7488 - loss: 1502.7002 - val_RMSE: 38.7018 - val_loss: 1499.0699 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7511 - loss: 1502.8854 - val_RMSE: 38.7018 - val_loss: 1499.0673 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7425 - loss: 1502.1704 - val_RMSE: 38.7008 - val_loss: 1498.8035 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7384 - loss: 1501.6848 - val_RMSE: 38.7008 - val_loss: 1498.6813 - learning_rate: 1.0000e-04\n",
            "41608/41608  83s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 17:46:47,489] Trial 2 finished with value: 38.697156270345054 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0005402789192665301, 'dropout_rate': 0.36286329232617837}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  28s 7ms/step - RMSE: 61.1220 - loss: 3901.1499 - val_RMSE: 38.7016 - val_loss: 1498.0620 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.4299 - loss: 1554.9777 - val_RMSE: 38.6945 - val_loss: 1497.5388 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.3545 - loss: 1549.0581 - val_RMSE: 38.6946 - val_loss: 1497.5609 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3389 - loss: 1547.8391 - val_RMSE: 38.6933 - val_loss: 1497.4711 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3170 - loss: 1546.1323 - val_RMSE: 38.6943 - val_loss: 1497.5634 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3075 - loss: 1545.3971 - val_RMSE: 38.6924 - val_loss: 1497.4204 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2967 - loss: 1544.5590 - val_RMSE: 38.6915 - val_loss: 1497.3578 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2819 - loss: 1543.3956 - val_RMSE: 38.6921 - val_loss: 1497.4036 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2615 - loss: 1541.7911 - val_RMSE: 38.6909 - val_loss: 1497.3091 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2421 - loss: 1540.2733 - val_RMSE: 38.6904 - val_loss: 1497.2765 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2259 - loss: 1538.9979 - val_RMSE: 38.6894 - val_loss: 1497.1974 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2156 - loss: 1538.1879 - val_RMSE: 38.6893 - val_loss: 1497.1865 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2061 - loss: 1537.4473 - val_RMSE: 38.6895 - val_loss: 1497.1979 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1827 - loss: 1535.6063 - val_RMSE: 38.6887 - val_loss: 1497.1351 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1827 - loss: 1535.6050 - val_RMSE: 38.6882 - val_loss: 1497.0979 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1665 - loss: 1534.3417 - val_RMSE: 38.6879 - val_loss: 1497.0800 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1591 - loss: 1533.7563 - val_RMSE: 38.6883 - val_loss: 1497.1042 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1412 - loss: 1532.3553 - val_RMSE: 38.6870 - val_loss: 1497.0095 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1302 - loss: 1531.4967 - val_RMSE: 38.6870 - val_loss: 1497.0079 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1210 - loss: 1530.7767 - val_RMSE: 38.6866 - val_loss: 1496.9836 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1172 - loss: 1530.4889 - val_RMSE: 38.6873 - val_loss: 1497.0319 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0976 - loss: 1528.9534 - val_RMSE: 38.6856 - val_loss: 1496.9064 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0885 - loss: 1528.2468 - val_RMSE: 38.6866 - val_loss: 1496.9849 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0750 - loss: 1527.1884 - val_RMSE: 38.6865 - val_loss: 1496.9753 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.0714 - loss: 1526.9084 - val_RMSE: 38.6863 - val_loss: 1496.9641 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  26s 7ms/step - RMSE: 61.1121 - loss: 3899.5061 - val_RMSE: 38.7294 - val_loss: 1500.2212 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3797 - loss: 1551.0195 - val_RMSE: 38.7242 - val_loss: 1499.8402 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3071 - loss: 1545.3280 - val_RMSE: 38.7235 - val_loss: 1499.7985 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2922 - loss: 1544.1672 - val_RMSE: 38.7232 - val_loss: 1499.7891 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2675 - loss: 1542.2424 - val_RMSE: 38.7241 - val_loss: 1499.8722 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2493 - loss: 1540.8242 - val_RMSE: 38.7234 - val_loss: 1499.8176 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2371 - loss: 1539.8723 - val_RMSE: 38.7225 - val_loss: 1499.7538 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2248 - loss: 1538.9126 - val_RMSE: 38.7214 - val_loss: 1499.6703 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2051 - loss: 1537.3649 - val_RMSE: 38.7217 - val_loss: 1499.6891 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1998 - loss: 1536.9500 - val_RMSE: 38.7198 - val_loss: 1499.5421 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1839 - loss: 1535.6959 - val_RMSE: 38.7200 - val_loss: 1499.5596 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1752 - loss: 1535.0199 - val_RMSE: 38.7201 - val_loss: 1499.5636 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1506 - loss: 1533.0902 - val_RMSE: 38.7186 - val_loss: 1499.4491 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1460 - loss: 1532.7303 - val_RMSE: 38.7186 - val_loss: 1499.4446 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1252 - loss: 1531.0986 - val_RMSE: 38.7181 - val_loss: 1499.4125 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1251 - loss: 1531.0919 - val_RMSE: 38.7181 - val_loss: 1499.4121 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1055 - loss: 1529.5645 - val_RMSE: 38.7177 - val_loss: 1499.3754 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1024 - loss: 1529.3180 - val_RMSE: 38.7171 - val_loss: 1499.3369 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0829 - loss: 1527.7985 - val_RMSE: 38.7170 - val_loss: 1499.3302 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0718 - loss: 1526.9307 - val_RMSE: 38.7177 - val_loss: 1499.3884 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0634 - loss: 1526.2772 - val_RMSE: 38.7171 - val_loss: 1499.3406 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0478 - loss: 1525.0576 - val_RMSE: 38.7162 - val_loss: 1499.2717 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0444 - loss: 1524.7906 - val_RMSE: 38.7165 - val_loss: 1499.2999 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0418 - loss: 1524.5929 - val_RMSE: 38.7172 - val_loss: 1499.3510 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0247 - loss: 1523.2585 - val_RMSE: 38.7163 - val_loss: 1499.2823 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  28s 7ms/step - RMSE: 61.0899 - loss: 3896.7007 - val_RMSE: 38.7189 - val_loss: 1499.4044 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3946 - loss: 1552.1942 - val_RMSE: 38.7210 - val_loss: 1499.5898 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3287 - loss: 1547.0219 - val_RMSE: 38.7218 - val_loss: 1499.6602 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3118 - loss: 1545.7069 - val_RMSE: 38.7199 - val_loss: 1499.5288 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2914 - loss: 1544.1165 - val_RMSE: 38.7237 - val_loss: 1499.8373 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2798 - loss: 1543.2173 - val_RMSE: 38.7212 - val_loss: 1499.6534 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2628 - loss: 1541.8882 - val_RMSE: 38.7151 - val_loss: 1499.1798 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2652 - loss: 1542.0763 - val_RMSE: 38.7151 - val_loss: 1499.1746 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2605 - loss: 1541.6981 - val_RMSE: 38.7142 - val_loss: 1499.0953 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2621 - loss: 1541.8195 - val_RMSE: 38.7142 - val_loss: 1499.0936 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2583 - loss: 1541.5154 - val_RMSE: 38.7140 - val_loss: 1499.0720 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2597 - loss: 1541.6219 - val_RMSE: 38.7138 - val_loss: 1499.0522 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.2608 - loss: 1541.7036 - val_RMSE: 38.7140 - val_loss: 1499.0601 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.2528 - loss: 1541.0702 - val_RMSE: 38.7140 - val_loss: 1499.0582 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2437 - loss: 1540.3527 - val_RMSE: 38.7136 - val_loss: 1499.0215 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2506 - loss: 1540.8850 - val_RMSE: 38.7131 - val_loss: 1498.9785 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2565 - loss: 1541.3456 - val_RMSE: 38.7131 - val_loss: 1498.9720 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2475 - loss: 1540.6411 - val_RMSE: 38.7129 - val_loss: 1498.9581 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2401 - loss: 1540.0510 - val_RMSE: 38.7133 - val_loss: 1498.9821 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2514 - loss: 1540.9315 - val_RMSE: 38.7132 - val_loss: 1498.9702 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2488 - loss: 1540.7273 - val_RMSE: 38.7132 - val_loss: 1498.9698 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.2469 - loss: 1540.5726 - val_RMSE: 38.7129 - val_loss: 1498.9382 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.2395 - loss: 1539.9875 - val_RMSE: 38.7130 - val_loss: 1498.9432 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2509 - loss: 1540.8840 - val_RMSE: 38.7126 - val_loss: 1498.9132 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2343 - loss: 1539.5750 - val_RMSE: 38.7129 - val_loss: 1498.9302 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 18:10:55,657] Trial 3 finished with value: 38.70516586303711 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'selu', 'reg': 0.0006168819899394228, 'dropout_rate': 0.48350841958817187}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 60.7275 - loss: 3860.7412 - val_RMSE: 38.7025 - val_loss: 1503.1106 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1660 - loss: 1538.8193 - val_RMSE: 38.6939 - val_loss: 1500.7462 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0876 - loss: 1530.9313 - val_RMSE: 38.6927 - val_loss: 1499.0325 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0727 - loss: 1528.3319 - val_RMSE: 38.6935 - val_loss: 1498.2562 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0608 - loss: 1526.7576 - val_RMSE: 38.6941 - val_loss: 1498.1229 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0604 - loss: 1526.5984 - val_RMSE: 38.6947 - val_loss: 1498.0813 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0617 - loss: 1526.6384 - val_RMSE: 38.6930 - val_loss: 1497.9167 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0472 - loss: 1525.4810 - val_RMSE: 38.6966 - val_loss: 1498.2043 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0436 - loss: 1525.1986 - val_RMSE: 38.6924 - val_loss: 1497.8715 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0335 - loss: 1524.4050 - val_RMSE: 38.6930 - val_loss: 1497.9349 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0296 - loss: 1524.1052 - val_RMSE: 38.6930 - val_loss: 1497.9077 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0229 - loss: 1523.5594 - val_RMSE: 38.6927 - val_loss: 1497.9104 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0233 - loss: 1523.6105 - val_RMSE: 38.6926 - val_loss: 1497.8959 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0091 - loss: 1522.5095 - val_RMSE: 38.6918 - val_loss: 1497.8387 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0185 - loss: 1523.2445 - val_RMSE: 38.6916 - val_loss: 1497.8258 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0086 - loss: 1522.4824 - val_RMSE: 38.6933 - val_loss: 1497.9510 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0157 - loss: 1523.0261 - val_RMSE: 38.6940 - val_loss: 1498.0188 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9957 - loss: 1521.4740 - val_RMSE: 38.6899 - val_loss: 1497.7015 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9942 - loss: 1521.3615 - val_RMSE: 38.6917 - val_loss: 1497.8342 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9895 - loss: 1520.9830 - val_RMSE: 38.6923 - val_loss: 1497.9017 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9922 - loss: 1521.2125 - val_RMSE: 38.6909 - val_loss: 1497.7697 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9860 - loss: 1520.7096 - val_RMSE: 38.6904 - val_loss: 1497.7242 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9783 - loss: 1520.1158 - val_RMSE: 38.6915 - val_loss: 1497.8085 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9729 - loss: 1519.6266 - val_RMSE: 38.6862 - val_loss: 1497.2625 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9711 - loss: 1519.3628 - val_RMSE: 38.6859 - val_loss: 1497.1476 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 60.7271 - loss: 3860.0928 - val_RMSE: 38.7317 - val_loss: 1505.3287 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1190 - loss: 1535.1005 - val_RMSE: 38.7229 - val_loss: 1502.9666 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0407 - loss: 1527.2592 - val_RMSE: 38.7253 - val_loss: 1501.5563 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0226 - loss: 1524.4153 - val_RMSE: 38.7230 - val_loss: 1500.5271 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0112 - loss: 1522.8704 - val_RMSE: 38.7265 - val_loss: 1500.5741 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0022 - loss: 1522.0120 - val_RMSE: 38.7238 - val_loss: 1500.3422 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9977 - loss: 1521.6361 - val_RMSE: 38.7243 - val_loss: 1500.3431 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9938 - loss: 1521.3024 - val_RMSE: 38.7228 - val_loss: 1500.2301 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9847 - loss: 1520.5884 - val_RMSE: 38.7241 - val_loss: 1500.3109 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9884 - loss: 1520.8776 - val_RMSE: 38.7239 - val_loss: 1500.3004 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9819 - loss: 1520.3705 - val_RMSE: 38.7220 - val_loss: 1500.1422 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9818 - loss: 1520.3409 - val_RMSE: 38.7224 - val_loss: 1500.1669 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9704 - loss: 1519.4506 - val_RMSE: 38.7217 - val_loss: 1500.1550 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9708 - loss: 1519.5157 - val_RMSE: 38.7195 - val_loss: 1499.9707 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9610 - loss: 1518.7417 - val_RMSE: 38.7219 - val_loss: 1500.1582 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9624 - loss: 1518.8473 - val_RMSE: 38.7227 - val_loss: 1500.2067 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9570 - loss: 1518.4193 - val_RMSE: 38.7220 - val_loss: 1500.1650 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9550 - loss: 1518.2831 - val_RMSE: 38.7208 - val_loss: 1500.0781 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9483 - loss: 1517.7571 - val_RMSE: 38.7223 - val_loss: 1500.2198 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9406 - loss: 1517.1356 - val_RMSE: 38.7152 - val_loss: 1499.5250 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.9350 - loss: 1516.5638 - val_RMSE: 38.7150 - val_loss: 1499.4087 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9327 - loss: 1516.2960 - val_RMSE: 38.7147 - val_loss: 1499.3195 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9325 - loss: 1516.2141 - val_RMSE: 38.7146 - val_loss: 1499.2625 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9412 - loss: 1516.8430 - val_RMSE: 38.7141 - val_loss: 1499.1801 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9320 - loss: 1516.0990 - val_RMSE: 38.7147 - val_loss: 1499.2025 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 60.7024 - loss: 3856.9219 - val_RMSE: 38.7191 - val_loss: 1504.3219 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1440 - loss: 1537.0243 - val_RMSE: 38.7173 - val_loss: 1502.4677 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0631 - loss: 1528.9379 - val_RMSE: 38.7180 - val_loss: 1500.9121 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0463 - loss: 1526.1890 - val_RMSE: 38.7190 - val_loss: 1500.1782 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0397 - loss: 1525.0461 - val_RMSE: 38.7199 - val_loss: 1500.0298 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0356 - loss: 1524.5862 - val_RMSE: 38.7168 - val_loss: 1499.7716 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0341 - loss: 1524.4611 - val_RMSE: 38.7176 - val_loss: 1499.8107 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0218 - loss: 1523.4763 - val_RMSE: 38.7176 - val_loss: 1499.7853 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.0164 - loss: 1523.0387 - val_RMSE: 38.7149 - val_loss: 1499.5848 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0189 - loss: 1523.2378 - val_RMSE: 38.7157 - val_loss: 1499.6538 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0105 - loss: 1522.5756 - val_RMSE: 38.7169 - val_loss: 1499.7473 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0035 - loss: 1522.0476 - val_RMSE: 38.7141 - val_loss: 1499.5364 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0007 - loss: 1521.8361 - val_RMSE: 38.7135 - val_loss: 1499.4797 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9916 - loss: 1521.1102 - val_RMSE: 38.7138 - val_loss: 1499.5151 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9875 - loss: 1520.7906 - val_RMSE: 38.7129 - val_loss: 1499.4524 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.9831 - loss: 1520.4603 - val_RMSE: 38.7127 - val_loss: 1499.4526 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.9856 - loss: 1520.6663 - val_RMSE: 38.7125 - val_loss: 1499.4307 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9780 - loss: 1520.0653 - val_RMSE: 38.7113 - val_loss: 1499.3521 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9685 - loss: 1519.3345 - val_RMSE: 38.7128 - val_loss: 1499.4357 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9715 - loss: 1519.5533 - val_RMSE: 38.7142 - val_loss: 1499.5631 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9706 - loss: 1519.5002 - val_RMSE: 38.7133 - val_loss: 1499.4885 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9655 - loss: 1519.0929 - val_RMSE: 38.7113 - val_loss: 1499.3341 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9578 - loss: 1518.5057 - val_RMSE: 38.7128 - val_loss: 1499.4435 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9586 - loss: 1518.5446 - val_RMSE: 38.7120 - val_loss: 1499.4027 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9454 - loss: 1517.5496 - val_RMSE: 38.7118 - val_loss: 1499.3850 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 18:35:07,326] Trial 4 finished with value: 38.70415115356445 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'selu', 'reg': 0.019199229166119592, 'dropout_rate': 0.3341626635520698}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  29s 8ms/step - RMSE: 57.1019 - loss: 3422.7141 - val_RMSE: 38.7026 - val_loss: 1498.1483 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0765 - loss: 1527.2397 - val_RMSE: 38.6906 - val_loss: 1497.2533 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9670 - loss: 1518.7303 - val_RMSE: 38.6889 - val_loss: 1497.1610 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9329 - loss: 1516.1066 - val_RMSE: 38.6860 - val_loss: 1496.9572 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9313 - loss: 1516.0071 - val_RMSE: 38.6864 - val_loss: 1497.0135 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9215 - loss: 1515.2563 - val_RMSE: 38.6849 - val_loss: 1496.9028 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9178 - loss: 1514.9745 - val_RMSE: 38.6852 - val_loss: 1496.9235 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9195 - loss: 1515.1115 - val_RMSE: 38.6845 - val_loss: 1496.8754 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9177 - loss: 1514.9722 - val_RMSE: 38.6844 - val_loss: 1496.8666 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9107 - loss: 1514.4308 - val_RMSE: 38.6840 - val_loss: 1496.8417 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9166 - loss: 1514.8933 - val_RMSE: 38.6827 - val_loss: 1496.7443 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9068 - loss: 1514.1381 - val_RMSE: 38.6840 - val_loss: 1496.8505 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9096 - loss: 1514.3577 - val_RMSE: 38.6829 - val_loss: 1496.7690 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9073 - loss: 1514.1802 - val_RMSE: 38.6824 - val_loss: 1496.7325 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8974 - loss: 1513.4181 - val_RMSE: 38.6822 - val_loss: 1496.7258 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8920 - loss: 1512.9998 - val_RMSE: 38.6818 - val_loss: 1496.6971 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8913 - loss: 1512.9496 - val_RMSE: 38.6827 - val_loss: 1496.7736 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8928 - loss: 1513.0688 - val_RMSE: 38.6816 - val_loss: 1496.6895 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8956 - loss: 1513.2979 - val_RMSE: 38.6812 - val_loss: 1496.6664 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8945 - loss: 1513.2184 - val_RMSE: 38.6820 - val_loss: 1496.7319 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8896 - loss: 1512.8368 - val_RMSE: 38.6813 - val_loss: 1496.6841 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8804 - loss: 1512.1338 - val_RMSE: 38.6815 - val_loss: 1496.7079 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8909 - loss: 1512.9537 - val_RMSE: 38.6813 - val_loss: 1496.7025 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8837 - loss: 1512.4005 - val_RMSE: 38.6808 - val_loss: 1496.6716 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8773 - loss: 1511.9095 - val_RMSE: 38.6795 - val_loss: 1496.5598 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  28s 8ms/step - RMSE: 57.0798 - loss: 3419.6045 - val_RMSE: 38.7378 - val_loss: 1500.8750 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0223 - loss: 1523.0117 - val_RMSE: 38.7196 - val_loss: 1499.5078 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9216 - loss: 1515.1971 - val_RMSE: 38.7167 - val_loss: 1499.3125 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8893 - loss: 1512.7181 - val_RMSE: 38.7177 - val_loss: 1499.4166 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8841 - loss: 1512.3414 - val_RMSE: 38.7161 - val_loss: 1499.3121 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8756 - loss: 1511.6926 - val_RMSE: 38.7160 - val_loss: 1499.3086 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8671 - loss: 1511.0376 - val_RMSE: 38.7160 - val_loss: 1499.3159 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8607 - loss: 1510.5386 - val_RMSE: 38.7155 - val_loss: 1499.2704 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8628 - loss: 1510.6973 - val_RMSE: 38.7143 - val_loss: 1499.1808 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8610 - loss: 1510.5599 - val_RMSE: 38.7147 - val_loss: 1499.2192 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8595 - loss: 1510.4478 - val_RMSE: 38.7150 - val_loss: 1499.2386 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8596 - loss: 1510.4637 - val_RMSE: 38.7143 - val_loss: 1499.1863 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8578 - loss: 1510.3207 - val_RMSE: 38.7141 - val_loss: 1499.1774 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8555 - loss: 1510.1471 - val_RMSE: 38.7145 - val_loss: 1499.2074 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8551 - loss: 1510.1213 - val_RMSE: 38.7144 - val_loss: 1499.2085 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8446 - loss: 1509.3044 - val_RMSE: 38.7138 - val_loss: 1499.1660 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8508 - loss: 1509.7930 - val_RMSE: 38.7140 - val_loss: 1499.1906 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8441 - loss: 1509.2769 - val_RMSE: 38.7136 - val_loss: 1499.1621 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8407 - loss: 1509.0232 - val_RMSE: 38.7133 - val_loss: 1499.1454 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8394 - loss: 1508.9252 - val_RMSE: 38.7122 - val_loss: 1499.0658 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8354 - loss: 1508.6171 - val_RMSE: 38.7126 - val_loss: 1499.1012 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8336 - loss: 1508.4851 - val_RMSE: 38.7137 - val_loss: 1499.1908 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8339 - loss: 1508.5144 - val_RMSE: 38.7134 - val_loss: 1499.1732 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8360 - loss: 1508.6816 - val_RMSE: 38.7133 - val_loss: 1499.1696 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8341 - loss: 1508.5431 - val_RMSE: 38.7138 - val_loss: 1499.2173 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  30s 8ms/step - RMSE: 57.0575 - loss: 3416.6711 - val_RMSE: 38.7173 - val_loss: 1499.2837 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0492 - loss: 1525.1086 - val_RMSE: 38.7125 - val_loss: 1498.9554 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9459 - loss: 1517.0900 - val_RMSE: 38.7113 - val_loss: 1498.8917 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9137 - loss: 1514.6139 - val_RMSE: 38.7098 - val_loss: 1498.8010 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9009 - loss: 1513.6399 - val_RMSE: 38.7103 - val_loss: 1498.8569 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9018 - loss: 1513.7206 - val_RMSE: 38.7085 - val_loss: 1498.7189 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8966 - loss: 1513.3174 - val_RMSE: 38.7087 - val_loss: 1498.7360 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8899 - loss: 1512.7970 - val_RMSE: 38.7084 - val_loss: 1498.7219 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8896 - loss: 1512.7843 - val_RMSE: 38.7071 - val_loss: 1498.6261 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8919 - loss: 1512.9684 - val_RMSE: 38.7069 - val_loss: 1498.6093 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8831 - loss: 1512.2891 - val_RMSE: 38.7073 - val_loss: 1498.6445 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8795 - loss: 1512.0112 - val_RMSE: 38.7059 - val_loss: 1498.5452 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8859 - loss: 1512.5170 - val_RMSE: 38.7068 - val_loss: 1498.6179 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8774 - loss: 1511.8562 - val_RMSE: 38.7053 - val_loss: 1498.5110 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8771 - loss: 1511.8365 - val_RMSE: 38.7070 - val_loss: 1498.6456 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8774 - loss: 1511.8636 - val_RMSE: 38.7069 - val_loss: 1498.6376 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8718 - loss: 1511.4375 - val_RMSE: 38.7057 - val_loss: 1498.5535 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8679 - loss: 1511.1353 - val_RMSE: 38.7062 - val_loss: 1498.5947 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8715 - loss: 1511.4213 - val_RMSE: 38.7051 - val_loss: 1498.5200 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8597 - loss: 1510.5067 - val_RMSE: 38.7030 - val_loss: 1498.3457 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8619 - loss: 1510.6716 - val_RMSE: 38.7028 - val_loss: 1498.3179 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8558 - loss: 1510.1830 - val_RMSE: 38.7027 - val_loss: 1498.3052 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8602 - loss: 1510.5157 - val_RMSE: 38.7024 - val_loss: 1498.2743 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8590 - loss: 1510.4141 - val_RMSE: 38.7024 - val_loss: 1498.2650 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8598 - loss: 1510.4696 - val_RMSE: 38.7024 - val_loss: 1498.2576 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 19:01:01,290] Trial 5 finished with value: 38.698568979899086 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0003050914095911242, 'dropout_rate': 0.3691098556240708}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 53.8575 - loss: 3059.3425 - val_RMSE: 38.7152 - val_loss: 1507.5746 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9927 - loss: 1528.3422 - val_RMSE: 38.7115 - val_loss: 1504.0999 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8892 - loss: 1517.3347 - val_RMSE: 38.6953 - val_loss: 1500.9766 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8553 - loss: 1513.1832 - val_RMSE: 38.6935 - val_loss: 1500.1469 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8347 - loss: 1510.9808 - val_RMSE: 38.6920 - val_loss: 1499.5760 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8232 - loss: 1509.6844 - val_RMSE: 38.6922 - val_loss: 1499.2373 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8176 - loss: 1508.8828 - val_RMSE: 38.6911 - val_loss: 1498.8298 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8125 - loss: 1508.2145 - val_RMSE: 38.6886 - val_loss: 1498.3792 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8081 - loss: 1507.6248 - val_RMSE: 38.6897 - val_loss: 1498.3115 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8099 - loss: 1507.6558 - val_RMSE: 38.6882 - val_loss: 1498.1571 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8085 - loss: 1507.5332 - val_RMSE: 38.6884 - val_loss: 1498.1942 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8061 - loss: 1507.3561 - val_RMSE: 38.6905 - val_loss: 1498.3774 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8115 - loss: 1507.7490 - val_RMSE: 38.6881 - val_loss: 1498.1760 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8065 - loss: 1507.3190 - val_RMSE: 38.6868 - val_loss: 1497.9678 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8087 - loss: 1507.4465 - val_RMSE: 38.6867 - val_loss: 1497.9766 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8052 - loss: 1507.1985 - val_RMSE: 38.6868 - val_loss: 1497.9678 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8047 - loss: 1507.1356 - val_RMSE: 38.6862 - val_loss: 1497.9454 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8029 - loss: 1507.0101 - val_RMSE: 38.6856 - val_loss: 1497.8439 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8067 - loss: 1507.2838 - val_RMSE: 38.6853 - val_loss: 1497.8370 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8067 - loss: 1507.2637 - val_RMSE: 38.6855 - val_loss: 1497.8129 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8023 - loss: 1506.9001 - val_RMSE: 38.6869 - val_loss: 1497.9243 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8065 - loss: 1507.2102 - val_RMSE: 38.6866 - val_loss: 1497.8113 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8034 - loss: 1506.9292 - val_RMSE: 38.6845 - val_loss: 1497.6653 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8026 - loss: 1506.8529 - val_RMSE: 38.6852 - val_loss: 1497.7114 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8042 - loss: 1506.9939 - val_RMSE: 38.6852 - val_loss: 1497.6940 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 53.8064 - loss: 3052.9604 - val_RMSE: 38.7544 - val_loss: 1510.3811 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9437 - loss: 1524.3280 - val_RMSE: 38.7247 - val_loss: 1504.9838 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8440 - loss: 1513.6884 - val_RMSE: 38.7227 - val_loss: 1503.0750 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8007 - loss: 1508.8926 - val_RMSE: 38.7240 - val_loss: 1502.5308 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7808 - loss: 1506.8301 - val_RMSE: 38.7199 - val_loss: 1501.8140 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7740 - loss: 1505.9149 - val_RMSE: 38.7200 - val_loss: 1501.4786 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7600 - loss: 1504.4626 - val_RMSE: 38.7191 - val_loss: 1500.9484 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7632 - loss: 1504.3308 - val_RMSE: 38.7216 - val_loss: 1500.9006 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7595 - loss: 1503.8173 - val_RMSE: 38.7205 - val_loss: 1500.6738 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7589 - loss: 1503.6686 - val_RMSE: 38.7216 - val_loss: 1500.7496 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7594 - loss: 1503.6960 - val_RMSE: 38.7215 - val_loss: 1500.7327 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7553 - loss: 1503.3489 - val_RMSE: 38.7217 - val_loss: 1500.7219 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7560 - loss: 1503.3950 - val_RMSE: 38.7235 - val_loss: 1500.8422 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7553 - loss: 1503.3322 - val_RMSE: 38.7235 - val_loss: 1500.8344 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7506 - loss: 1502.7841 - val_RMSE: 38.7132 - val_loss: 1499.5673 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7451 - loss: 1501.9807 - val_RMSE: 38.7126 - val_loss: 1499.3402 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7482 - loss: 1502.0668 - val_RMSE: 38.7125 - val_loss: 1499.2322 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7474 - loss: 1501.9209 - val_RMSE: 38.7125 - val_loss: 1499.1783 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7433 - loss: 1501.5502 - val_RMSE: 38.7120 - val_loss: 1499.1029 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7472 - loss: 1501.8118 - val_RMSE: 38.7122 - val_loss: 1499.0840 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7429 - loss: 1501.4557 - val_RMSE: 38.7126 - val_loss: 1499.0936 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7471 - loss: 1501.7670 - val_RMSE: 38.7117 - val_loss: 1499.0170 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7415 - loss: 1501.3187 - val_RMSE: 38.7115 - val_loss: 1498.9858 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7465 - loss: 1501.6902 - val_RMSE: 38.7115 - val_loss: 1498.9705 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7439 - loss: 1501.4828 - val_RMSE: 38.7117 - val_loss: 1498.9807 - learning_rate: 1.0000e-04\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 53.7987 - loss: 3051.8386 - val_RMSE: 38.7279 - val_loss: 1508.4514 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9722 - loss: 1526.7030 - val_RMSE: 38.7203 - val_loss: 1504.9979 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8738 - loss: 1516.3914 - val_RMSE: 38.7164 - val_loss: 1502.9056 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8325 - loss: 1511.6307 - val_RMSE: 38.7183 - val_loss: 1502.1237 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8120 - loss: 1509.2515 - val_RMSE: 38.7140 - val_loss: 1501.3054 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7954 - loss: 1507.5232 - val_RMSE: 38.7117 - val_loss: 1500.7601 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7919 - loss: 1506.9127 - val_RMSE: 38.7106 - val_loss: 1500.2843 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7884 - loss: 1506.2535 - val_RMSE: 38.7103 - val_loss: 1500.0157 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7853 - loss: 1505.7999 - val_RMSE: 38.7094 - val_loss: 1499.8024 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7876 - loss: 1505.8838 - val_RMSE: 38.7098 - val_loss: 1499.8402 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7848 - loss: 1505.6862 - val_RMSE: 38.7084 - val_loss: 1499.7268 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7852 - loss: 1505.6782 - val_RMSE: 38.7083 - val_loss: 1499.6956 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7850 - loss: 1505.6604 - val_RMSE: 38.7080 - val_loss: 1499.6895 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7833 - loss: 1505.5394 - val_RMSE: 38.7077 - val_loss: 1499.6508 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7832 - loss: 1505.4836 - val_RMSE: 38.7086 - val_loss: 1499.6971 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7820 - loss: 1505.3712 - val_RMSE: 38.7103 - val_loss: 1499.8469 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7840 - loss: 1505.5438 - val_RMSE: 38.7079 - val_loss: 1499.5906 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7810 - loss: 1505.2521 - val_RMSE: 38.7072 - val_loss: 1499.5189 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7804 - loss: 1505.2148 - val_RMSE: 38.7066 - val_loss: 1499.4785 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7805 - loss: 1505.2145 - val_RMSE: 38.7088 - val_loss: 1499.5863 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7753 - loss: 1504.7609 - val_RMSE: 38.7076 - val_loss: 1499.5492 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7810 - loss: 1505.2401 - val_RMSE: 38.7068 - val_loss: 1499.4810 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7782 - loss: 1505.0005 - val_RMSE: 38.7074 - val_loss: 1499.4962 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7807 - loss: 1505.1736 - val_RMSE: 38.7080 - val_loss: 1499.5042 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7725 - loss: 1504.3693 - val_RMSE: 38.7038 - val_loss: 1498.7878 - learning_rate: 1.0000e-04\n",
            "41608/41608  86s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 19:31:47,179] Trial 6 finished with value: 38.700243631998696 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'reg': 0.007472176524756258, 'dropout_rate': 0.30743712269143775}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  39s 12ms/step - RMSE: 51.5286 - loss: 2852.2712 - val_RMSE: 38.7185 - val_loss: 1518.3163 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  25s 10ms/step - RMSE: 39.0282 - loss: 1540.8573 - val_RMSE: 38.7080 - val_loss: 1513.8391 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.9052 - loss: 1528.6173 - val_RMSE: 38.7091 - val_loss: 1511.3335 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8609 - loss: 1523.0334 - val_RMSE: 38.7366 - val_loss: 1511.8179 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8331 - loss: 1518.8773 - val_RMSE: 38.7156 - val_loss: 1508.7775 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8215 - loss: 1516.3958 - val_RMSE: 38.6994 - val_loss: 1505.7628 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8113 - loss: 1514.0636 - val_RMSE: 38.6978 - val_loss: 1503.7344 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8048 - loss: 1511.7982 - val_RMSE: 38.6993 - val_loss: 1502.6901 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7997 - loss: 1510.1787 - val_RMSE: 38.6985 - val_loss: 1502.1903 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7967 - loss: 1509.6781 - val_RMSE: 38.6974 - val_loss: 1502.1272 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7966 - loss: 1509.6090 - val_RMSE: 38.6967 - val_loss: 1501.5555 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7998 - loss: 1509.6238 - val_RMSE: 38.6971 - val_loss: 1501.6257 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7970 - loss: 1509.3464 - val_RMSE: 38.6937 - val_loss: 1501.2955 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7967 - loss: 1509.2089 - val_RMSE: 38.6942 - val_loss: 1501.3264 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7950 - loss: 1509.0024 - val_RMSE: 38.6969 - val_loss: 1501.2295 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7964 - loss: 1508.9362 - val_RMSE: 38.6936 - val_loss: 1501.1515 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7949 - loss: 1508.8281 - val_RMSE: 38.6933 - val_loss: 1500.7833 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7960 - loss: 1508.8257 - val_RMSE: 38.6913 - val_loss: 1500.4706 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7932 - loss: 1508.4797 - val_RMSE: 38.6929 - val_loss: 1500.7047 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7957 - loss: 1508.7478 - val_RMSE: 38.6936 - val_loss: 1500.6428 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7914 - loss: 1508.3126 - val_RMSE: 38.6910 - val_loss: 1500.4664 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7905 - loss: 1508.2111 - val_RMSE: 38.6927 - val_loss: 1500.4425 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7962 - loss: 1508.5892 - val_RMSE: 38.6912 - val_loss: 1500.2487 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7959 - loss: 1508.5150 - val_RMSE: 38.6902 - val_loss: 1500.0854 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7910 - loss: 1508.0262 - val_RMSE: 38.6916 - val_loss: 1500.1575 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  40s 12ms/step - RMSE: 51.4598 - loss: 2845.1838 - val_RMSE: 38.7767 - val_loss: 1525.3945 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.9769 - loss: 1538.4990 - val_RMSE: 38.7701 - val_loss: 1519.0621 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.8559 - loss: 1524.8116 - val_RMSE: 38.7531 - val_loss: 1514.3655 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8036 - loss: 1517.7728 - val_RMSE: 38.7422 - val_loss: 1511.1298 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7790 - loss: 1513.6190 - val_RMSE: 38.7304 - val_loss: 1508.8058 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7692 - loss: 1511.3303 - val_RMSE: 38.7288 - val_loss: 1507.1094 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7558 - loss: 1508.8678 - val_RMSE: 38.7251 - val_loss: 1505.3943 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7500 - loss: 1507.0267 - val_RMSE: 38.7246 - val_loss: 1503.9808 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7463 - loss: 1505.6868 - val_RMSE: 38.7226 - val_loss: 1503.6935 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7475 - loss: 1505.5291 - val_RMSE: 38.7233 - val_loss: 1503.5381 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7466 - loss: 1505.4153 - val_RMSE: 38.7257 - val_loss: 1503.6134 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7472 - loss: 1505.2638 - val_RMSE: 38.7216 - val_loss: 1502.9609 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7447 - loss: 1504.9309 - val_RMSE: 38.7237 - val_loss: 1502.9507 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7468 - loss: 1504.9750 - val_RMSE: 38.7254 - val_loss: 1503.3693 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7451 - loss: 1504.7506 - val_RMSE: 38.7264 - val_loss: 1503.3263 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7434 - loss: 1504.7736 - val_RMSE: 38.7219 - val_loss: 1502.7815 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7466 - loss: 1504.7899 - val_RMSE: 38.7261 - val_loss: 1502.9895 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7403 - loss: 1504.2426 - val_RMSE: 38.7247 - val_loss: 1502.9430 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7440 - loss: 1504.5675 - val_RMSE: 38.7308 - val_loss: 1503.5018 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7422 - loss: 1504.3862 - val_RMSE: 38.7229 - val_loss: 1502.6877 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7412 - loss: 1504.2197 - val_RMSE: 38.7232 - val_loss: 1502.7212 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7417 - loss: 1504.2346 - val_RMSE: 38.7234 - val_loss: 1502.6967 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7410 - loss: 1504.0951 - val_RMSE: 38.7241 - val_loss: 1502.6064 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7413 - loss: 1504.0226 - val_RMSE: 38.7300 - val_loss: 1503.1766 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7385 - loss: 1503.8560 - val_RMSE: 38.7266 - val_loss: 1502.8417 - learning_rate: 0.0010\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4708 - loss: 2845.7400 - val_RMSE: 38.7702 - val_loss: 1526.5149 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  25s 9ms/step - RMSE: 39.0170 - loss: 1543.0101 - val_RMSE: 38.7751 - val_loss: 1519.9900 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.8921 - loss: 1528.1085 - val_RMSE: 38.7474 - val_loss: 1514.3889 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8314 - loss: 1520.3462 - val_RMSE: 38.7392 - val_loss: 1512.0295 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.8094 - loss: 1517.1302 - val_RMSE: 38.7303 - val_loss: 1509.6882 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7958 - loss: 1514.3269 - val_RMSE: 38.7269 - val_loss: 1507.7281 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7864 - loss: 1511.8748 - val_RMSE: 38.7228 - val_loss: 1505.3118 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7760 - loss: 1509.1525 - val_RMSE: 38.7221 - val_loss: 1504.1284 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7733 - loss: 1508.0520 - val_RMSE: 38.7229 - val_loss: 1503.9752 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7738 - loss: 1508.0132 - val_RMSE: 38.7251 - val_loss: 1504.2043 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7701 - loss: 1507.5835 - val_RMSE: 38.7185 - val_loss: 1503.3129 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7716 - loss: 1507.5648 - val_RMSE: 38.7164 - val_loss: 1503.1106 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7676 - loss: 1507.1177 - val_RMSE: 38.7166 - val_loss: 1502.8873 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7738 - loss: 1507.5548 - val_RMSE: 38.7151 - val_loss: 1502.6533 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7697 - loss: 1507.0375 - val_RMSE: 38.7156 - val_loss: 1502.8741 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7717 - loss: 1507.1680 - val_RMSE: 38.7158 - val_loss: 1502.8152 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7686 - loss: 1506.8170 - val_RMSE: 38.7149 - val_loss: 1502.3252 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7709 - loss: 1506.8314 - val_RMSE: 38.7137 - val_loss: 1502.1184 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7728 - loss: 1506.8254 - val_RMSE: 38.7131 - val_loss: 1501.9169 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7694 - loss: 1506.5668 - val_RMSE: 38.7127 - val_loss: 1501.9113 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7718 - loss: 1506.6975 - val_RMSE: 38.7137 - val_loss: 1502.3365 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7687 - loss: 1506.4891 - val_RMSE: 38.7136 - val_loss: 1502.0699 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7701 - loss: 1506.5562 - val_RMSE: 38.7130 - val_loss: 1501.8527 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7700 - loss: 1506.3962 - val_RMSE: 38.7123 - val_loss: 1501.7814 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7699 - loss: 1506.3728 - val_RMSE: 38.7127 - val_loss: 1501.7281 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 20:09:20,682] Trial 7 finished with value: 38.710269927978516 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'silu', 'reg': 0.03866576727475806, 'dropout_rate': 0.37177793999597164}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 61.1209 - loss: 3902.0278 - val_RMSE: 38.6977 - val_loss: 1498.7308 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.4198 - loss: 1555.1478 - val_RMSE: 38.6921 - val_loss: 1498.2742 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.3436 - loss: 1549.0868 - val_RMSE: 38.6923 - val_loss: 1498.1431 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3325 - loss: 1548.0499 - val_RMSE: 38.6910 - val_loss: 1497.8624 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3114 - loss: 1546.2186 - val_RMSE: 38.6920 - val_loss: 1497.8009 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3005 - loss: 1545.2455 - val_RMSE: 38.6901 - val_loss: 1497.5774 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2899 - loss: 1544.3424 - val_RMSE: 38.6894 - val_loss: 1497.4891 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2768 - loss: 1543.2798 - val_RMSE: 38.6902 - val_loss: 1497.5203 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2560 - loss: 1541.6190 - val_RMSE: 38.6900 - val_loss: 1497.4908 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2385 - loss: 1540.2305 - val_RMSE: 38.6893 - val_loss: 1497.4287 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2215 - loss: 1538.8915 - val_RMSE: 38.6894 - val_loss: 1497.4237 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2103 - loss: 1538.0042 - val_RMSE: 38.6880 - val_loss: 1497.3157 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2023 - loss: 1537.3796 - val_RMSE: 38.6885 - val_loss: 1497.3528 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1786 - loss: 1535.5190 - val_RMSE: 38.6880 - val_loss: 1497.3240 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1777 - loss: 1535.4551 - val_RMSE: 38.6873 - val_loss: 1497.2700 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1629 - loss: 1534.2987 - val_RMSE: 38.6869 - val_loss: 1497.2450 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1550 - loss: 1533.6946 - val_RMSE: 38.6869 - val_loss: 1497.2510 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1376 - loss: 1532.3247 - val_RMSE: 38.6865 - val_loss: 1497.2130 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1267 - loss: 1531.4701 - val_RMSE: 38.6856 - val_loss: 1497.1497 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1183 - loss: 1530.8130 - val_RMSE: 38.6859 - val_loss: 1497.1729 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1132 - loss: 1530.4182 - val_RMSE: 38.6866 - val_loss: 1497.2325 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0954 - loss: 1529.0322 - val_RMSE: 38.6846 - val_loss: 1497.0836 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0857 - loss: 1528.2788 - val_RMSE: 38.6863 - val_loss: 1497.2079 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0722 - loss: 1527.2186 - val_RMSE: 38.6859 - val_loss: 1497.1895 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.0694 - loss: 1527.0153 - val_RMSE: 38.6851 - val_loss: 1497.1338 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 61.1169 - loss: 3901.1719 - val_RMSE: 38.7299 - val_loss: 1501.2250 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.3685 - loss: 1551.0978 - val_RMSE: 38.7243 - val_loss: 1500.7673 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.3001 - loss: 1545.6677 - val_RMSE: 38.7220 - val_loss: 1500.4536 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.2836 - loss: 1544.2148 - val_RMSE: 38.7220 - val_loss: 1500.2710 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2606 - loss: 1542.2256 - val_RMSE: 38.7228 - val_loss: 1500.1848 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2435 - loss: 1540.7621 - val_RMSE: 38.7218 - val_loss: 1500.0316 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2310 - loss: 1539.7133 - val_RMSE: 38.7199 - val_loss: 1499.8373 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2181 - loss: 1538.6545 - val_RMSE: 38.7215 - val_loss: 1499.9443 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1986 - loss: 1537.1063 - val_RMSE: 38.7201 - val_loss: 1499.8127 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1931 - loss: 1536.6681 - val_RMSE: 38.7186 - val_loss: 1499.6941 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1786 - loss: 1535.5302 - val_RMSE: 38.7184 - val_loss: 1499.6852 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1704 - loss: 1534.8855 - val_RMSE: 38.7184 - val_loss: 1499.6836 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1468 - loss: 1533.0361 - val_RMSE: 38.7167 - val_loss: 1499.5507 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1420 - loss: 1532.6622 - val_RMSE: 38.7175 - val_loss: 1499.6100 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1194 - loss: 1530.8885 - val_RMSE: 38.7176 - val_loss: 1499.6178 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1203 - loss: 1530.9589 - val_RMSE: 38.7181 - val_loss: 1499.6541 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1024 - loss: 1529.5548 - val_RMSE: 38.7178 - val_loss: 1499.6268 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0984 - loss: 1529.2516 - val_RMSE: 38.7168 - val_loss: 1499.5583 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0806 - loss: 1527.8538 - val_RMSE: 38.7145 - val_loss: 1499.3529 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0761 - loss: 1527.4706 - val_RMSE: 38.7143 - val_loss: 1499.3093 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0769 - loss: 1527.5074 - val_RMSE: 38.7143 - val_loss: 1499.2874 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0698 - loss: 1526.9324 - val_RMSE: 38.7139 - val_loss: 1499.2341 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0740 - loss: 1527.2351 - val_RMSE: 38.7137 - val_loss: 1499.1998 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0821 - loss: 1527.8506 - val_RMSE: 38.7136 - val_loss: 1499.1775 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.0721 - loss: 1527.0569 - val_RMSE: 38.7139 - val_loss: 1499.1780 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 61.0877 - loss: 3897.3528 - val_RMSE: 38.7163 - val_loss: 1500.1722 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3868 - loss: 1552.5505 - val_RMSE: 38.7195 - val_loss: 1500.3879 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3191 - loss: 1547.1432 - val_RMSE: 38.7178 - val_loss: 1500.1010 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3036 - loss: 1545.7640 - val_RMSE: 38.7175 - val_loss: 1499.9011 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2857 - loss: 1544.1846 - val_RMSE: 38.7180 - val_loss: 1499.7913 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2750 - loss: 1543.2133 - val_RMSE: 38.7164 - val_loss: 1499.5973 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2562 - loss: 1541.6792 - val_RMSE: 38.7169 - val_loss: 1499.5927 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2457 - loss: 1540.8234 - val_RMSE: 38.7190 - val_loss: 1499.7355 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2291 - loss: 1539.4998 - val_RMSE: 38.7172 - val_loss: 1499.5797 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.2189 - loss: 1538.6761 - val_RMSE: 38.7179 - val_loss: 1499.6318 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.2034 - loss: 1537.4597 - val_RMSE: 38.7166 - val_loss: 1499.5310 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.1938 - loss: 1536.7150 - val_RMSE: 38.7150 - val_loss: 1499.4080 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1823 - loss: 1535.8115 - val_RMSE: 38.7155 - val_loss: 1499.4434 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1630 - loss: 1534.2987 - val_RMSE: 38.7165 - val_loss: 1499.5266 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1458 - loss: 1532.9509 - val_RMSE: 38.7150 - val_loss: 1499.4058 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1392 - loss: 1532.4392 - val_RMSE: 38.7137 - val_loss: 1499.3174 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1346 - loss: 1532.0847 - val_RMSE: 38.7145 - val_loss: 1499.3782 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1174 - loss: 1530.7379 - val_RMSE: 38.7114 - val_loss: 1499.1495 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1018 - loss: 1529.5291 - val_RMSE: 38.7135 - val_loss: 1499.3093 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0977 - loss: 1529.2113 - val_RMSE: 38.7136 - val_loss: 1499.3247 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0894 - loss: 1528.5648 - val_RMSE: 38.7141 - val_loss: 1499.3652 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0776 - loss: 1527.6477 - val_RMSE: 38.7120 - val_loss: 1499.2026 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0636 - loss: 1526.5555 - val_RMSE: 38.7128 - val_loss: 1499.2717 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0640 - loss: 1526.5757 - val_RMSE: 38.7080 - val_loss: 1498.8640 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0491 - loss: 1525.3817 - val_RMSE: 38.7079 - val_loss: 1498.8315 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 20:33:31,551] Trial 8 finished with value: 38.70230611165365 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'silu', 'reg': 0.0032489164477933394, 'dropout_rate': 0.4814024944816824}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4691 - loss: 2813.0303 - val_RMSE: 38.7235 - val_loss: 1517.4996 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 39.0040 - loss: 1536.6215 - val_RMSE: 38.7503 - val_loss: 1512.3701 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8962 - loss: 1523.0516 - val_RMSE: 38.7205 - val_loss: 1507.7740 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8496 - loss: 1517.4974 - val_RMSE: 38.6963 - val_loss: 1504.5991 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8223 - loss: 1514.1731 - val_RMSE: 38.7003 - val_loss: 1503.9467 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8110 - loss: 1512.2694 - val_RMSE: 38.6946 - val_loss: 1502.6302 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8040 - loss: 1510.9678 - val_RMSE: 38.6917 - val_loss: 1501.7863 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7975 - loss: 1509.7816 - val_RMSE: 38.6893 - val_loss: 1500.8617 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7912 - loss: 1508.5957 - val_RMSE: 38.6903 - val_loss: 1500.2661 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7859 - loss: 1507.5677 - val_RMSE: 38.6894 - val_loss: 1499.7230 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7856 - loss: 1507.2086 - val_RMSE: 38.6885 - val_loss: 1499.5718 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7874 - loss: 1507.2902 - val_RMSE: 38.6893 - val_loss: 1499.5602 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7864 - loss: 1507.1146 - val_RMSE: 38.6900 - val_loss: 1499.6432 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7859 - loss: 1507.0447 - val_RMSE: 38.6875 - val_loss: 1499.2913 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7856 - loss: 1506.9264 - val_RMSE: 38.6874 - val_loss: 1499.1938 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7859 - loss: 1506.8579 - val_RMSE: 38.6902 - val_loss: 1499.4567 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7851 - loss: 1506.7717 - val_RMSE: 38.6876 - val_loss: 1499.1912 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7860 - loss: 1506.8245 - val_RMSE: 38.6871 - val_loss: 1498.9998 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7827 - loss: 1506.4755 - val_RMSE: 38.6865 - val_loss: 1499.0103 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7840 - loss: 1506.6088 - val_RMSE: 38.6871 - val_loss: 1498.9474 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7831 - loss: 1506.4626 - val_RMSE: 38.6861 - val_loss: 1498.9922 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7809 - loss: 1506.2688 - val_RMSE: 38.6863 - val_loss: 1498.8827 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7857 - loss: 1506.5446 - val_RMSE: 38.6860 - val_loss: 1498.8080 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7847 - loss: 1506.4434 - val_RMSE: 38.6857 - val_loss: 1498.7134 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7805 - loss: 1506.0798 - val_RMSE: 38.6848 - val_loss: 1498.5958 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4111 - loss: 2806.1509 - val_RMSE: 38.7396 - val_loss: 1519.3008 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9547 - loss: 1533.0541 - val_RMSE: 38.7394 - val_loss: 1511.6337 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8421 - loss: 1519.1315 - val_RMSE: 38.7240 - val_loss: 1508.8429 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7945 - loss: 1513.7356 - val_RMSE: 38.7302 - val_loss: 1507.1317 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7689 - loss: 1510.0515 - val_RMSE: 38.7255 - val_loss: 1506.1013 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7578 - loss: 1508.3307 - val_RMSE: 38.7252 - val_loss: 1505.1339 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7468 - loss: 1506.6418 - val_RMSE: 38.7224 - val_loss: 1504.1670 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7428 - loss: 1505.5044 - val_RMSE: 38.7185 - val_loss: 1502.8888 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7390 - loss: 1504.3309 - val_RMSE: 38.7176 - val_loss: 1502.1860 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7386 - loss: 1503.7543 - val_RMSE: 38.7167 - val_loss: 1501.8513 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7377 - loss: 1503.4658 - val_RMSE: 38.7186 - val_loss: 1501.9520 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7370 - loss: 1503.3743 - val_RMSE: 38.7165 - val_loss: 1501.5751 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7363 - loss: 1503.2263 - val_RMSE: 38.7191 - val_loss: 1501.6782 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7369 - loss: 1503.2773 - val_RMSE: 38.7176 - val_loss: 1501.6855 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7351 - loss: 1503.0836 - val_RMSE: 38.7167 - val_loss: 1501.6775 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7352 - loss: 1503.0073 - val_RMSE: 38.7156 - val_loss: 1501.3649 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7375 - loss: 1503.0364 - val_RMSE: 38.7189 - val_loss: 1501.4536 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7302 - loss: 1502.4960 - val_RMSE: 38.7179 - val_loss: 1501.4189 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7340 - loss: 1502.7595 - val_RMSE: 38.7202 - val_loss: 1501.5583 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7334 - loss: 1502.6182 - val_RMSE: 38.7164 - val_loss: 1501.3289 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7323 - loss: 1502.5583 - val_RMSE: 38.7170 - val_loss: 1501.1899 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7317 - loss: 1502.4124 - val_RMSE: 38.7171 - val_loss: 1501.1547 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7316 - loss: 1502.3152 - val_RMSE: 38.7164 - val_loss: 1501.0695 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7316 - loss: 1502.2948 - val_RMSE: 38.7190 - val_loss: 1501.3092 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7293 - loss: 1502.1042 - val_RMSE: 38.7187 - val_loss: 1501.1826 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 11ms/step - RMSE: 51.4084 - loss: 2805.2761 - val_RMSE: 38.7452 - val_loss: 1519.3495 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9788 - loss: 1534.6334 - val_RMSE: 38.7431 - val_loss: 1511.3177 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8782 - loss: 1521.4064 - val_RMSE: 38.7219 - val_loss: 1508.1727 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8237 - loss: 1515.6943 - val_RMSE: 38.7163 - val_loss: 1506.0165 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8020 - loss: 1512.4397 - val_RMSE: 38.7247 - val_loss: 1505.6578 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7893 - loss: 1510.4745 - val_RMSE: 38.7166 - val_loss: 1504.1571 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7827 - loss: 1509.2094 - val_RMSE: 38.7275 - val_loss: 1504.4454 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7702 - loss: 1507.6050 - val_RMSE: 38.7200 - val_loss: 1503.1456 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7669 - loss: 1506.6617 - val_RMSE: 38.7115 - val_loss: 1501.7999 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7634 - loss: 1505.8108 - val_RMSE: 38.7134 - val_loss: 1501.6399 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7605 - loss: 1505.3065 - val_RMSE: 38.7114 - val_loss: 1501.3373 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7616 - loss: 1505.2423 - val_RMSE: 38.7121 - val_loss: 1501.3223 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7588 - loss: 1504.9919 - val_RMSE: 38.7111 - val_loss: 1501.2677 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7652 - loss: 1505.4694 - val_RMSE: 38.7097 - val_loss: 1501.0770 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7603 - loss: 1505.0127 - val_RMSE: 38.7105 - val_loss: 1501.0117 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7616 - loss: 1505.0355 - val_RMSE: 38.7092 - val_loss: 1500.8599 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7586 - loss: 1504.7434 - val_RMSE: 38.7085 - val_loss: 1500.8516 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7612 - loss: 1504.9330 - val_RMSE: 38.7078 - val_loss: 1500.6390 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7636 - loss: 1505.0391 - val_RMSE: 38.7111 - val_loss: 1500.8312 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7594 - loss: 1504.6670 - val_RMSE: 38.7079 - val_loss: 1500.6687 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7616 - loss: 1504.7908 - val_RMSE: 38.7088 - val_loss: 1500.7115 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7595 - loss: 1504.6287 - val_RMSE: 38.7071 - val_loss: 1500.4790 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7611 - loss: 1504.7207 - val_RMSE: 38.7093 - val_loss: 1500.6246 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7601 - loss: 1504.5687 - val_RMSE: 38.7085 - val_loss: 1500.5195 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7592 - loss: 1504.5013 - val_RMSE: 38.7053 - val_loss: 1500.2549 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 21:10:13,385] Trial 9 finished with value: 38.702955881754555 and parameters: {'units': 1024, 'last_layer': 2, 'activation': 'relu', 'reg': 0.013130181575743027, 'dropout_rate': 0.3554543298268401}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.1922 - loss: 3086.4292 - val_RMSE: 38.7158 - val_loss: 1499.1244 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0593 - loss: 1525.8591 - val_RMSE: 38.6948 - val_loss: 1497.5507 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9279 - loss: 1515.6602 - val_RMSE: 38.6919 - val_loss: 1497.3785 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8905 - loss: 1512.8024 - val_RMSE: 38.6877 - val_loss: 1497.1183 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8687 - loss: 1511.1748 - val_RMSE: 38.6868 - val_loss: 1497.1005 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8609 - loss: 1510.6158 - val_RMSE: 38.6852 - val_loss: 1497.0114 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8567 - loss: 1510.3160 - val_RMSE: 38.6847 - val_loss: 1496.9912 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8530 - loss: 1510.0344 - val_RMSE: 38.6842 - val_loss: 1496.9512 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8508 - loss: 1509.8704 - val_RMSE: 38.6842 - val_loss: 1496.9491 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8524 - loss: 1509.9918 - val_RMSE: 38.6838 - val_loss: 1496.9176 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8510 - loss: 1509.8839 - val_RMSE: 38.6831 - val_loss: 1496.8740 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8456 - loss: 1509.4684 - val_RMSE: 38.6831 - val_loss: 1496.8713 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8522 - loss: 1509.9856 - val_RMSE: 38.6828 - val_loss: 1496.8525 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8437 - loss: 1509.3285 - val_RMSE: 38.6823 - val_loss: 1496.8154 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8455 - loss: 1509.4724 - val_RMSE: 38.6819 - val_loss: 1496.7974 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8390 - loss: 1508.9779 - val_RMSE: 38.6824 - val_loss: 1496.8373 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8423 - loss: 1509.2363 - val_RMSE: 38.6816 - val_loss: 1496.7876 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8362 - loss: 1508.7734 - val_RMSE: 38.6815 - val_loss: 1496.7871 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8393 - loss: 1509.0171 - val_RMSE: 38.6809 - val_loss: 1496.7478 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8401 - loss: 1509.0957 - val_RMSE: 38.6809 - val_loss: 1496.7489 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8337 - loss: 1508.6005 - val_RMSE: 38.6807 - val_loss: 1496.7427 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8368 - loss: 1508.8475 - val_RMSE: 38.6807 - val_loss: 1496.7518 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8340 - loss: 1508.6398 - val_RMSE: 38.6811 - val_loss: 1496.7921 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8302 - loss: 1508.3550 - val_RMSE: 38.6803 - val_loss: 1496.7354 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8317 - loss: 1508.4766 - val_RMSE: 38.6804 - val_loss: 1496.7581 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  35s 10ms/step - RMSE: 54.1436 - loss: 3080.3943 - val_RMSE: 38.7303 - val_loss: 1500.2467 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0116 - loss: 1522.1278 - val_RMSE: 38.7203 - val_loss: 1499.5223 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8858 - loss: 1512.3817 - val_RMSE: 38.7207 - val_loss: 1499.6080 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8341 - loss: 1508.4161 - val_RMSE: 38.7191 - val_loss: 1499.5466 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8198 - loss: 1507.3671 - val_RMSE: 38.7180 - val_loss: 1499.5077 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8091 - loss: 1506.5831 - val_RMSE: 38.7178 - val_loss: 1499.5234 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8052 - loss: 1506.3085 - val_RMSE: 38.7166 - val_loss: 1499.4503 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8063 - loss: 1506.4032 - val_RMSE: 38.7163 - val_loss: 1499.4252 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8051 - loss: 1506.3087 - val_RMSE: 38.7159 - val_loss: 1499.3994 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8004 - loss: 1505.9484 - val_RMSE: 38.7166 - val_loss: 1499.4600 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7960 - loss: 1505.6138 - val_RMSE: 38.7155 - val_loss: 1499.3750 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7963 - loss: 1505.6345 - val_RMSE: 38.7158 - val_loss: 1499.4049 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7946 - loss: 1505.5087 - val_RMSE: 38.7156 - val_loss: 1499.3896 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7929 - loss: 1505.3820 - val_RMSE: 38.7163 - val_loss: 1499.4552 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7978 - loss: 1505.7725 - val_RMSE: 38.7165 - val_loss: 1499.4700 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7894 - loss: 1505.1289 - val_RMSE: 38.7151 - val_loss: 1499.3717 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7985 - loss: 1505.8342 - val_RMSE: 38.7150 - val_loss: 1499.3671 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7911 - loss: 1505.2671 - val_RMSE: 38.7168 - val_loss: 1499.5156 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7890 - loss: 1505.1118 - val_RMSE: 38.7155 - val_loss: 1499.4302 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7938 - loss: 1505.4945 - val_RMSE: 38.7148 - val_loss: 1499.3796 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7852 - loss: 1504.8323 - val_RMSE: 38.7155 - val_loss: 1499.4423 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7886 - loss: 1505.1125 - val_RMSE: 38.7149 - val_loss: 1499.4056 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7758 - loss: 1504.1195 - val_RMSE: 38.7083 - val_loss: 1498.8820 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7787 - loss: 1504.3354 - val_RMSE: 38.7083 - val_loss: 1498.8636 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7756 - loss: 1504.0765 - val_RMSE: 38.7084 - val_loss: 1498.8582 - learning_rate: 1.0000e-04\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.1229 - loss: 3077.8313 - val_RMSE: 38.7183 - val_loss: 1499.3210 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0365 - loss: 1524.0742 - val_RMSE: 38.7155 - val_loss: 1499.1550 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9029 - loss: 1513.7166 - val_RMSE: 38.7126 - val_loss: 1498.9885 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8642 - loss: 1510.7618 - val_RMSE: 38.7109 - val_loss: 1498.9165 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8493 - loss: 1509.6608 - val_RMSE: 38.7089 - val_loss: 1498.8103 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8330 - loss: 1508.4442 - val_RMSE: 38.7079 - val_loss: 1498.7695 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8338 - loss: 1508.5392 - val_RMSE: 38.7067 - val_loss: 1498.6917 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8303 - loss: 1508.2820 - val_RMSE: 38.7054 - val_loss: 1498.5994 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8276 - loss: 1508.0770 - val_RMSE: 38.7066 - val_loss: 1498.6935 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8292 - loss: 1508.2025 - val_RMSE: 38.7056 - val_loss: 1498.6188 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8237 - loss: 1507.7791 - val_RMSE: 38.7057 - val_loss: 1498.6217 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8252 - loss: 1507.8901 - val_RMSE: 38.7043 - val_loss: 1498.5242 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8211 - loss: 1507.5760 - val_RMSE: 38.7046 - val_loss: 1498.5520 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8231 - loss: 1507.7375 - val_RMSE: 38.7047 - val_loss: 1498.5641 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8180 - loss: 1507.3539 - val_RMSE: 38.7037 - val_loss: 1498.4885 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8185 - loss: 1507.3956 - val_RMSE: 38.7051 - val_loss: 1498.6013 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8159 - loss: 1507.1997 - val_RMSE: 38.7038 - val_loss: 1498.5093 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8164 - loss: 1507.2449 - val_RMSE: 38.7041 - val_loss: 1498.5470 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8155 - loss: 1507.1851 - val_RMSE: 38.7036 - val_loss: 1498.5156 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8159 - loss: 1507.2233 - val_RMSE: 38.7038 - val_loss: 1498.5430 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8013 - loss: 1506.0947 - val_RMSE: 38.7021 - val_loss: 1498.3947 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8046 - loss: 1506.3385 - val_RMSE: 38.7021 - val_loss: 1498.3796 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8066 - loss: 1506.4810 - val_RMSE: 38.7019 - val_loss: 1498.3558 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8034 - loss: 1506.2188 - val_RMSE: 38.7019 - val_loss: 1498.3466 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8010 - loss: 1506.0227 - val_RMSE: 38.7022 - val_loss: 1498.3558 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 21:41:02,537] Trial 10 finished with value: 38.69698842366537 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.00011431075362835225, 'dropout_rate': 0.42901303537903984}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.1763 - loss: 3084.7754 - val_RMSE: 38.7122 - val_loss: 1498.8872 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0579 - loss: 1525.7869 - val_RMSE: 38.6963 - val_loss: 1497.7144 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9301 - loss: 1515.8787 - val_RMSE: 38.6895 - val_loss: 1497.2484 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8883 - loss: 1512.6915 - val_RMSE: 38.6871 - val_loss: 1497.1299 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8667 - loss: 1511.0721 - val_RMSE: 38.6863 - val_loss: 1497.1219 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8596 - loss: 1510.5698 - val_RMSE: 38.6845 - val_loss: 1497.0142 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8551 - loss: 1510.2463 - val_RMSE: 38.6841 - val_loss: 1496.9896 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8522 - loss: 1510.0212 - val_RMSE: 38.6837 - val_loss: 1496.9536 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8491 - loss: 1509.7754 - val_RMSE: 38.6842 - val_loss: 1496.9885 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8509 - loss: 1509.9172 - val_RMSE: 38.6830 - val_loss: 1496.8949 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8494 - loss: 1509.7966 - val_RMSE: 38.6829 - val_loss: 1496.8917 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8449 - loss: 1509.4520 - val_RMSE: 38.6821 - val_loss: 1496.8319 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8514 - loss: 1509.9597 - val_RMSE: 38.6825 - val_loss: 1496.8707 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8418 - loss: 1509.2164 - val_RMSE: 38.6818 - val_loss: 1496.8170 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8447 - loss: 1509.4473 - val_RMSE: 38.6820 - val_loss: 1496.8372 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8360 - loss: 1508.7780 - val_RMSE: 38.6815 - val_loss: 1496.8143 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8408 - loss: 1509.1644 - val_RMSE: 38.6812 - val_loss: 1496.7994 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8347 - loss: 1508.6938 - val_RMSE: 38.6811 - val_loss: 1496.7985 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8380 - loss: 1508.9614 - val_RMSE: 38.6806 - val_loss: 1496.7693 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8388 - loss: 1509.0323 - val_RMSE: 38.6807 - val_loss: 1496.7883 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8322 - loss: 1508.5310 - val_RMSE: 38.6801 - val_loss: 1496.7471 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8353 - loss: 1508.7817 - val_RMSE: 38.6809 - val_loss: 1496.8159 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8327 - loss: 1508.5896 - val_RMSE: 38.6803 - val_loss: 1496.7786 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8292 - loss: 1508.3246 - val_RMSE: 38.6802 - val_loss: 1496.7853 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8297 - loss: 1508.3719 - val_RMSE: 38.6802 - val_loss: 1496.7921 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.1315 - loss: 3079.1267 - val_RMSE: 38.7343 - val_loss: 1500.5990 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0113 - loss: 1522.1523 - val_RMSE: 38.7212 - val_loss: 1499.6415 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8836 - loss: 1512.2633 - val_RMSE: 38.7194 - val_loss: 1499.5706 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8322 - loss: 1508.3304 - val_RMSE: 38.7190 - val_loss: 1499.5975 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8196 - loss: 1507.4176 - val_RMSE: 38.7179 - val_loss: 1499.5709 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8081 - loss: 1506.5762 - val_RMSE: 38.7179 - val_loss: 1499.6046 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8043 - loss: 1506.3021 - val_RMSE: 38.7167 - val_loss: 1499.5193 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8049 - loss: 1506.3502 - val_RMSE: 38.7165 - val_loss: 1499.4906 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8040 - loss: 1506.2754 - val_RMSE: 38.7158 - val_loss: 1499.4307 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7997 - loss: 1505.9366 - val_RMSE: 38.7163 - val_loss: 1499.4681 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7959 - loss: 1505.6387 - val_RMSE: 38.7157 - val_loss: 1499.4236 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7956 - loss: 1505.6202 - val_RMSE: 38.7166 - val_loss: 1499.4957 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7939 - loss: 1505.4872 - val_RMSE: 38.7155 - val_loss: 1499.4136 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7920 - loss: 1505.3468 - val_RMSE: 38.7160 - val_loss: 1499.4601 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7968 - loss: 1505.7231 - val_RMSE: 38.7159 - val_loss: 1499.4611 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7884 - loss: 1505.0842 - val_RMSE: 38.7145 - val_loss: 1499.3617 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7973 - loss: 1505.7848 - val_RMSE: 38.7144 - val_loss: 1499.3713 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7897 - loss: 1505.2036 - val_RMSE: 38.7165 - val_loss: 1499.5397 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7874 - loss: 1505.0391 - val_RMSE: 38.7155 - val_loss: 1499.4724 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7922 - loss: 1505.4133 - val_RMSE: 38.7143 - val_loss: 1499.3831 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7838 - loss: 1504.7717 - val_RMSE: 38.7160 - val_loss: 1499.5247 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7827 - loss: 1504.6860 - val_RMSE: 38.7090 - val_loss: 1498.9630 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7739 - loss: 1503.9841 - val_RMSE: 38.7090 - val_loss: 1498.9510 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7759 - loss: 1504.1246 - val_RMSE: 38.7091 - val_loss: 1498.9390 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7736 - loss: 1503.9362 - val_RMSE: 38.7090 - val_loss: 1498.9218 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.1161 - loss: 3077.0850 - val_RMSE: 38.7296 - val_loss: 1500.2301 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0351 - loss: 1524.0101 - val_RMSE: 38.7174 - val_loss: 1499.3510 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.9040 - loss: 1513.8521 - val_RMSE: 38.7120 - val_loss: 1499.0028 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8647 - loss: 1510.8666 - val_RMSE: 38.7098 - val_loss: 1498.9033 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8472 - loss: 1509.5715 - val_RMSE: 38.7080 - val_loss: 1498.8170 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8318 - loss: 1508.4305 - val_RMSE: 38.7075 - val_loss: 1498.8112 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8333 - loss: 1508.5674 - val_RMSE: 38.7064 - val_loss: 1498.7351 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8289 - loss: 1508.2319 - val_RMSE: 38.7056 - val_loss: 1498.6703 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8265 - loss: 1508.0457 - val_RMSE: 38.7068 - val_loss: 1498.7614 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8273 - loss: 1508.0999 - val_RMSE: 38.7049 - val_loss: 1498.6068 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8230 - loss: 1507.7583 - val_RMSE: 38.7052 - val_loss: 1498.6263 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8242 - loss: 1507.8521 - val_RMSE: 38.7044 - val_loss: 1498.5568 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8205 - loss: 1507.5604 - val_RMSE: 38.7046 - val_loss: 1498.5757 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8217 - loss: 1507.6604 - val_RMSE: 38.7041 - val_loss: 1498.5426 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8172 - loss: 1507.3151 - val_RMSE: 38.7041 - val_loss: 1498.5443 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8179 - loss: 1507.3704 - val_RMSE: 38.7048 - val_loss: 1498.6072 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8159 - loss: 1507.2201 - val_RMSE: 38.7037 - val_loss: 1498.5337 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8161 - loss: 1507.2452 - val_RMSE: 38.7041 - val_loss: 1498.5703 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8152 - loss: 1507.1847 - val_RMSE: 38.7041 - val_loss: 1498.5776 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8151 - loss: 1507.1897 - val_RMSE: 38.7039 - val_loss: 1498.5745 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8042 - loss: 1506.3496 - val_RMSE: 38.7029 - val_loss: 1498.5033 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8093 - loss: 1506.7581 - val_RMSE: 38.7029 - val_loss: 1498.5149 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8109 - loss: 1506.8901 - val_RMSE: 38.7049 - val_loss: 1498.6849 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8066 - loss: 1506.5692 - val_RMSE: 38.7028 - val_loss: 1498.5244 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8050 - loss: 1506.4512 - val_RMSE: 38.7046 - val_loss: 1498.6814 - learning_rate: 0.0010\n",
            "41608/41608  87s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 22:11:45,903] Trial 11 finished with value: 38.69793955485026 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.00013595283372657457, 'dropout_rate': 0.42664745909121726}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.1909 - loss: 3086.3071 - val_RMSE: 38.7219 - val_loss: 1499.5839 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0627 - loss: 1526.1074 - val_RMSE: 38.7008 - val_loss: 1497.9996 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9299 - loss: 1515.8036 - val_RMSE: 38.6926 - val_loss: 1497.4127 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8894 - loss: 1512.7048 - val_RMSE: 38.6882 - val_loss: 1497.1348 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8687 - loss: 1511.1492 - val_RMSE: 38.6869 - val_loss: 1497.0865 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8607 - loss: 1510.5780 - val_RMSE: 38.6852 - val_loss: 1496.9894 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8561 - loss: 1510.2482 - val_RMSE: 38.6846 - val_loss: 1496.9614 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8528 - loss: 1510.0039 - val_RMSE: 38.6841 - val_loss: 1496.9238 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8510 - loss: 1509.8691 - val_RMSE: 38.6844 - val_loss: 1496.9553 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8513 - loss: 1509.8944 - val_RMSE: 38.6832 - val_loss: 1496.8602 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8503 - loss: 1509.8154 - val_RMSE: 38.6829 - val_loss: 1496.8400 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8455 - loss: 1509.4471 - val_RMSE: 38.6823 - val_loss: 1496.8037 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8515 - loss: 1509.9181 - val_RMSE: 38.6827 - val_loss: 1496.8345 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8427 - loss: 1509.2408 - val_RMSE: 38.6825 - val_loss: 1496.8235 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8455 - loss: 1509.4712 - val_RMSE: 38.6821 - val_loss: 1496.8068 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8378 - loss: 1508.8770 - val_RMSE: 38.6818 - val_loss: 1496.7957 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8417 - loss: 1509.1940 - val_RMSE: 38.6819 - val_loss: 1496.8142 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8355 - loss: 1508.7183 - val_RMSE: 38.6814 - val_loss: 1496.7753 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8384 - loss: 1508.9492 - val_RMSE: 38.6812 - val_loss: 1496.7728 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8389 - loss: 1508.9973 - val_RMSE: 38.6809 - val_loss: 1496.7520 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8337 - loss: 1508.5984 - val_RMSE: 38.6815 - val_loss: 1496.8038 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8367 - loss: 1508.8386 - val_RMSE: 38.6809 - val_loss: 1496.7734 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8330 - loss: 1508.5625 - val_RMSE: 38.6815 - val_loss: 1496.8279 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8283 - loss: 1508.2123 - val_RMSE: 38.6804 - val_loss: 1496.7528 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8310 - loss: 1508.4293 - val_RMSE: 38.6806 - val_loss: 1496.7800 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.1367 - loss: 3079.6467 - val_RMSE: 38.7284 - val_loss: 1500.0900 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0146 - loss: 1522.3511 - val_RMSE: 38.7226 - val_loss: 1499.6888 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8868 - loss: 1512.4427 - val_RMSE: 38.7224 - val_loss: 1499.7280 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8360 - loss: 1508.5499 - val_RMSE: 38.7198 - val_loss: 1499.5858 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8210 - loss: 1507.4440 - val_RMSE: 38.7187 - val_loss: 1499.5490 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8095 - loss: 1506.6018 - val_RMSE: 38.7178 - val_loss: 1499.5184 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8057 - loss: 1506.3345 - val_RMSE: 38.7168 - val_loss: 1499.4583 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8066 - loss: 1506.4176 - val_RMSE: 38.7169 - val_loss: 1499.4635 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8046 - loss: 1506.2675 - val_RMSE: 38.7159 - val_loss: 1499.3970 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8005 - loss: 1505.9498 - val_RMSE: 38.7159 - val_loss: 1499.3976 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7961 - loss: 1505.6115 - val_RMSE: 38.7156 - val_loss: 1499.3722 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7963 - loss: 1505.6316 - val_RMSE: 38.7160 - val_loss: 1499.4141 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7947 - loss: 1505.5127 - val_RMSE: 38.7152 - val_loss: 1499.3586 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7917 - loss: 1505.2908 - val_RMSE: 38.7159 - val_loss: 1499.4211 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7969 - loss: 1505.6969 - val_RMSE: 38.7156 - val_loss: 1499.3998 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7891 - loss: 1505.0994 - val_RMSE: 38.7148 - val_loss: 1499.3491 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7971 - loss: 1505.7301 - val_RMSE: 38.7149 - val_loss: 1499.3639 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7901 - loss: 1505.1948 - val_RMSE: 38.7159 - val_loss: 1499.4548 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7880 - loss: 1505.0436 - val_RMSE: 38.7151 - val_loss: 1499.4009 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7919 - loss: 1505.3596 - val_RMSE: 38.7139 - val_loss: 1499.3234 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7841 - loss: 1504.7611 - val_RMSE: 38.7154 - val_loss: 1499.4457 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7879 - loss: 1505.0723 - val_RMSE: 38.7150 - val_loss: 1499.4261 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7787 - loss: 1504.3616 - val_RMSE: 38.7134 - val_loss: 1499.3108 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7829 - loss: 1504.6980 - val_RMSE: 38.7155 - val_loss: 1499.4840 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7800 - loss: 1504.4830 - val_RMSE: 38.7150 - val_loss: 1499.4509 - learning_rate: 0.0010\n",
            "41608/41608  88s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  35s 10ms/step - RMSE: 54.1216 - loss: 3077.6299 - val_RMSE: 38.7213 - val_loss: 1499.5381 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0330 - loss: 1523.7867 - val_RMSE: 38.7167 - val_loss: 1499.2307 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9011 - loss: 1513.5540 - val_RMSE: 38.7164 - val_loss: 1499.2573 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8663 - loss: 1510.9023 - val_RMSE: 38.7096 - val_loss: 1498.7858 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8485 - loss: 1509.5791 - val_RMSE: 38.7089 - val_loss: 1498.7866 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8327 - loss: 1508.3979 - val_RMSE: 38.7080 - val_loss: 1498.7520 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8338 - loss: 1508.5079 - val_RMSE: 38.7068 - val_loss: 1498.6699 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8295 - loss: 1508.1863 - val_RMSE: 38.7056 - val_loss: 1498.5801 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8265 - loss: 1507.9617 - val_RMSE: 38.7069 - val_loss: 1498.6877 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8288 - loss: 1508.1416 - val_RMSE: 38.7049 - val_loss: 1498.5419 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8235 - loss: 1507.7312 - val_RMSE: 38.7052 - val_loss: 1498.5679 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8241 - loss: 1507.7897 - val_RMSE: 38.7044 - val_loss: 1498.5088 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8202 - loss: 1507.4939 - val_RMSE: 38.7045 - val_loss: 1498.5292 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8221 - loss: 1507.6453 - val_RMSE: 38.7043 - val_loss: 1498.5215 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8177 - loss: 1507.3112 - val_RMSE: 38.7035 - val_loss: 1498.4585 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8180 - loss: 1507.3434 - val_RMSE: 38.7045 - val_loss: 1498.5477 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8152 - loss: 1507.1343 - val_RMSE: 38.7039 - val_loss: 1498.5127 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8156 - loss: 1507.1738 - val_RMSE: 38.7040 - val_loss: 1498.5319 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8153 - loss: 1507.1606 - val_RMSE: 38.7039 - val_loss: 1498.5289 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8149 - loss: 1507.1396 - val_RMSE: 38.7036 - val_loss: 1498.5215 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8005 - loss: 1506.0233 - val_RMSE: 38.7016 - val_loss: 1498.3496 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8047 - loss: 1506.3372 - val_RMSE: 38.7015 - val_loss: 1498.3258 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8054 - loss: 1506.3799 - val_RMSE: 38.7018 - val_loss: 1498.3412 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8020 - loss: 1506.1045 - val_RMSE: 38.7020 - val_loss: 1498.3456 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8001 - loss: 1505.9463 - val_RMSE: 38.7021 - val_loss: 1498.3409 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 22:42:27,728] Trial 12 finished with value: 38.6991933186849 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.00010782612784245901, 'dropout_rate': 0.4285140854773215}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.1502 - loss: 3083.9399 - val_RMSE: 38.7238 - val_loss: 1502.3201 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0558 - loss: 1528.2102 - val_RMSE: 38.7006 - val_loss: 1500.6951 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9289 - loss: 1518.4215 - val_RMSE: 38.6945 - val_loss: 1500.1030 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8872 - loss: 1514.9609 - val_RMSE: 38.6906 - val_loss: 1499.3574 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8636 - loss: 1512.6492 - val_RMSE: 38.6900 - val_loss: 1498.7699 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8584 - loss: 1511.7175 - val_RMSE: 38.6868 - val_loss: 1498.0895 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8532 - loss: 1510.9292 - val_RMSE: 38.6867 - val_loss: 1497.8519 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8509 - loss: 1510.5573 - val_RMSE: 38.6867 - val_loss: 1497.7715 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8484 - loss: 1510.3207 - val_RMSE: 38.6872 - val_loss: 1497.8181 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8505 - loss: 1510.4972 - val_RMSE: 38.6856 - val_loss: 1497.6705 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8495 - loss: 1510.3989 - val_RMSE: 38.6861 - val_loss: 1497.7012 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8441 - loss: 1509.9744 - val_RMSE: 38.6853 - val_loss: 1497.6494 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8522 - loss: 1510.5988 - val_RMSE: 38.6851 - val_loss: 1497.6385 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8436 - loss: 1509.9357 - val_RMSE: 38.6844 - val_loss: 1497.5726 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8468 - loss: 1510.1835 - val_RMSE: 38.6846 - val_loss: 1497.6105 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8382 - loss: 1509.5328 - val_RMSE: 38.6849 - val_loss: 1497.6105 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8420 - loss: 1509.8026 - val_RMSE: 38.6848 - val_loss: 1497.5953 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8376 - loss: 1509.4633 - val_RMSE: 38.6846 - val_loss: 1497.5886 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8404 - loss: 1509.6797 - val_RMSE: 38.6843 - val_loss: 1497.5726 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8381 - loss: 1509.4515 - val_RMSE: 38.6804 - val_loss: 1497.1021 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8307 - loss: 1508.7223 - val_RMSE: 38.6801 - val_loss: 1496.9619 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8349 - loss: 1508.9396 - val_RMSE: 38.6800 - val_loss: 1496.8700 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8315 - loss: 1508.6008 - val_RMSE: 38.6799 - val_loss: 1496.8066 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8273 - loss: 1508.2185 - val_RMSE: 38.6799 - val_loss: 1496.7548 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8304 - loss: 1508.4109 - val_RMSE: 38.6798 - val_loss: 1496.7188 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.1101 - loss: 3078.8408 - val_RMSE: 38.7350 - val_loss: 1503.1906 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0073 - loss: 1524.4459 - val_RMSE: 38.7260 - val_loss: 1502.7195 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8818 - loss: 1514.7937 - val_RMSE: 38.7244 - val_loss: 1502.4152 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8312 - loss: 1510.6010 - val_RMSE: 38.7204 - val_loss: 1501.6827 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8173 - loss: 1509.0682 - val_RMSE: 38.7194 - val_loss: 1501.1160 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8059 - loss: 1507.6998 - val_RMSE: 38.7174 - val_loss: 1500.5309 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8009 - loss: 1506.9116 - val_RMSE: 38.7179 - val_loss: 1500.3008 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8024 - loss: 1506.8149 - val_RMSE: 38.7184 - val_loss: 1500.2615 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8020 - loss: 1506.7490 - val_RMSE: 38.7167 - val_loss: 1500.1276 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7985 - loss: 1506.4796 - val_RMSE: 38.7174 - val_loss: 1500.1863 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7952 - loss: 1506.2120 - val_RMSE: 38.7168 - val_loss: 1500.1390 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7960 - loss: 1506.2762 - val_RMSE: 38.7179 - val_loss: 1500.2194 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7935 - loss: 1506.0697 - val_RMSE: 38.7177 - val_loss: 1500.1886 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7912 - loss: 1505.8981 - val_RMSE: 38.7173 - val_loss: 1500.1777 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7929 - loss: 1505.9857 - val_RMSE: 38.7108 - val_loss: 1499.4858 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7831 - loss: 1505.0587 - val_RMSE: 38.7106 - val_loss: 1499.3463 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7908 - loss: 1505.5411 - val_RMSE: 38.7104 - val_loss: 1499.2404 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7851 - loss: 1505.0199 - val_RMSE: 38.7103 - val_loss: 1499.1709 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7834 - loss: 1504.8270 - val_RMSE: 38.7102 - val_loss: 1499.1156 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7876 - loss: 1505.1053 - val_RMSE: 38.7101 - val_loss: 1499.0645 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7805 - loss: 1504.5116 - val_RMSE: 38.7100 - val_loss: 1499.0281 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7879 - loss: 1505.0587 - val_RMSE: 38.7100 - val_loss: 1499.0044 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7777 - loss: 1504.2455 - val_RMSE: 38.7097 - val_loss: 1498.9623 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7810 - loss: 1504.4775 - val_RMSE: 38.7098 - val_loss: 1498.9447 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7811 - loss: 1504.4702 - val_RMSE: 38.7099 - val_loss: 1498.9431 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 9ms/step - RMSE: 54.0903 - loss: 3076.3555 - val_RMSE: 38.7258 - val_loss: 1502.4470 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0317 - loss: 1526.3021 - val_RMSE: 38.7141 - val_loss: 1501.6918 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8995 - loss: 1516.0549 - val_RMSE: 38.7132 - val_loss: 1501.3813 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8609 - loss: 1512.7275 - val_RMSE: 38.7093 - val_loss: 1500.5996 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8458 - loss: 1511.0698 - val_RMSE: 38.7084 - val_loss: 1500.0768 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8304 - loss: 1509.4550 - val_RMSE: 38.7083 - val_loss: 1499.7263 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8311 - loss: 1509.1948 - val_RMSE: 38.7080 - val_loss: 1499.5142 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8273 - loss: 1508.7548 - val_RMSE: 38.7072 - val_loss: 1499.4109 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8249 - loss: 1508.5374 - val_RMSE: 38.7077 - val_loss: 1499.4397 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8273 - loss: 1508.7076 - val_RMSE: 38.7063 - val_loss: 1499.3219 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8222 - loss: 1508.2985 - val_RMSE: 38.7066 - val_loss: 1499.3531 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8228 - loss: 1508.3646 - val_RMSE: 38.7061 - val_loss: 1499.3060 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8187 - loss: 1508.0330 - val_RMSE: 38.7056 - val_loss: 1499.2594 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8215 - loss: 1508.2578 - val_RMSE: 38.7055 - val_loss: 1499.2800 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8172 - loss: 1507.9354 - val_RMSE: 38.7065 - val_loss: 1499.3347 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8195 - loss: 1508.0999 - val_RMSE: 38.7066 - val_loss: 1499.3313 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8181 - loss: 1507.9783 - val_RMSE: 38.7055 - val_loss: 1499.2605 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8172 - loss: 1507.9121 - val_RMSE: 38.7051 - val_loss: 1499.2234 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8162 - loss: 1507.8428 - val_RMSE: 38.7050 - val_loss: 1499.2141 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8162 - loss: 1507.8494 - val_RMSE: 38.7051 - val_loss: 1499.2327 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8076 - loss: 1507.1770 - val_RMSE: 38.7049 - val_loss: 1499.1859 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8135 - loss: 1507.6168 - val_RMSE: 38.7052 - val_loss: 1499.2134 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8145 - loss: 1507.6886 - val_RMSE: 38.7053 - val_loss: 1499.2268 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8116 - loss: 1507.4708 - val_RMSE: 38.7042 - val_loss: 1499.1322 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8108 - loss: 1507.3915 - val_RMSE: 38.7045 - val_loss: 1499.1766 - learning_rate: 0.0010\n",
            "41608/41608  88s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 23:12:45,140] Trial 13 finished with value: 38.698099772135414 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.001629847097661793, 'dropout_rate': 0.41742826609863065}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.2372 - loss: 3091.4924 - val_RMSE: 38.7170 - val_loss: 1499.5424 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0687 - loss: 1526.9362 - val_RMSE: 38.6961 - val_loss: 1498.0338 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9385 - loss: 1516.8846 - val_RMSE: 38.6910 - val_loss: 1497.7511 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8992 - loss: 1513.9355 - val_RMSE: 38.6866 - val_loss: 1497.4983 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8761 - loss: 1512.2119 - val_RMSE: 38.6863 - val_loss: 1497.5071 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8708 - loss: 1511.8153 - val_RMSE: 38.6842 - val_loss: 1497.3075 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8647 - loss: 1511.2953 - val_RMSE: 38.6851 - val_loss: 1497.3179 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8628 - loss: 1511.0874 - val_RMSE: 38.6839 - val_loss: 1497.1840 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8612 - loss: 1510.9324 - val_RMSE: 38.6839 - val_loss: 1497.1731 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8610 - loss: 1510.9070 - val_RMSE: 38.6827 - val_loss: 1497.0681 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8606 - loss: 1510.8702 - val_RMSE: 38.6826 - val_loss: 1497.0814 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8537 - loss: 1510.3533 - val_RMSE: 38.6827 - val_loss: 1497.0885 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8609 - loss: 1510.9081 - val_RMSE: 38.6823 - val_loss: 1497.0564 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8497 - loss: 1510.0367 - val_RMSE: 38.6825 - val_loss: 1497.0775 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8548 - loss: 1510.4388 - val_RMSE: 38.6821 - val_loss: 1497.0465 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8466 - loss: 1509.8090 - val_RMSE: 38.6821 - val_loss: 1497.0631 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8509 - loss: 1510.1534 - val_RMSE: 38.6814 - val_loss: 1497.0200 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8459 - loss: 1509.7778 - val_RMSE: 38.6812 - val_loss: 1497.0015 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8472 - loss: 1509.8832 - val_RMSE: 38.6807 - val_loss: 1496.9803 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8497 - loss: 1510.0814 - val_RMSE: 38.6802 - val_loss: 1496.9426 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8437 - loss: 1509.6224 - val_RMSE: 38.6799 - val_loss: 1496.9320 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8465 - loss: 1509.8555 - val_RMSE: 38.6801 - val_loss: 1496.9612 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8435 - loss: 1509.6307 - val_RMSE: 38.6798 - val_loss: 1496.9349 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8385 - loss: 1509.2467 - val_RMSE: 38.6803 - val_loss: 1496.9849 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8394 - loss: 1509.3248 - val_RMSE: 38.6805 - val_loss: 1497.0117 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.1874 - loss: 3085.2871 - val_RMSE: 38.7435 - val_loss: 1501.5917 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0258 - loss: 1523.5741 - val_RMSE: 38.7222 - val_loss: 1500.0571 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8945 - loss: 1513.4518 - val_RMSE: 38.7207 - val_loss: 1500.0442 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8429 - loss: 1509.5470 - val_RMSE: 38.7189 - val_loss: 1499.9951 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8265 - loss: 1508.3622 - val_RMSE: 38.7181 - val_loss: 1499.9818 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8174 - loss: 1507.6851 - val_RMSE: 38.7175 - val_loss: 1499.9148 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8162 - loss: 1507.5602 - val_RMSE: 38.7162 - val_loss: 1499.7758 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8160 - loss: 1507.5010 - val_RMSE: 38.7161 - val_loss: 1499.7245 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8146 - loss: 1507.3451 - val_RMSE: 38.7155 - val_loss: 1499.6410 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8090 - loss: 1506.8844 - val_RMSE: 38.7154 - val_loss: 1499.6195 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8048 - loss: 1506.5414 - val_RMSE: 38.7142 - val_loss: 1499.5243 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8069 - loss: 1506.7064 - val_RMSE: 38.7149 - val_loss: 1499.5750 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8034 - loss: 1506.4352 - val_RMSE: 38.7136 - val_loss: 1499.4709 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8015 - loss: 1506.2897 - val_RMSE: 38.7140 - val_loss: 1499.5062 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8063 - loss: 1506.6606 - val_RMSE: 38.7149 - val_loss: 1499.5800 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7974 - loss: 1505.9839 - val_RMSE: 38.7136 - val_loss: 1499.4976 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8075 - loss: 1506.7770 - val_RMSE: 38.7134 - val_loss: 1499.4849 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8002 - loss: 1506.2155 - val_RMSE: 38.7139 - val_loss: 1499.5319 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7929 - loss: 1505.6433 - val_RMSE: 38.7074 - val_loss: 1498.9861 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7955 - loss: 1505.8116 - val_RMSE: 38.7071 - val_loss: 1498.9315 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7869 - loss: 1505.1101 - val_RMSE: 38.7070 - val_loss: 1498.9012 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7926 - loss: 1505.5201 - val_RMSE: 38.7071 - val_loss: 1498.8779 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7827 - loss: 1504.7317 - val_RMSE: 38.7071 - val_loss: 1498.8577 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7861 - loss: 1504.9731 - val_RMSE: 38.7070 - val_loss: 1498.8345 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7846 - loss: 1504.8422 - val_RMSE: 38.7071 - val_loss: 1498.8226 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.1822 - loss: 3084.4521 - val_RMSE: 38.7287 - val_loss: 1500.4498 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0468 - loss: 1525.2246 - val_RMSE: 38.7150 - val_loss: 1499.4998 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9117 - loss: 1514.7979 - val_RMSE: 38.7124 - val_loss: 1499.4175 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8732 - loss: 1511.9169 - val_RMSE: 38.7109 - val_loss: 1499.3944 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8579 - loss: 1510.8069 - val_RMSE: 38.7093 - val_loss: 1499.3021 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8406 - loss: 1509.4832 - val_RMSE: 38.7079 - val_loss: 1499.1726 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8435 - loss: 1509.6738 - val_RMSE: 38.7063 - val_loss: 1498.9808 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8394 - loss: 1509.2943 - val_RMSE: 38.7061 - val_loss: 1498.9274 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8361 - loss: 1509.0034 - val_RMSE: 38.7069 - val_loss: 1498.9799 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8384 - loss: 1509.1742 - val_RMSE: 38.7051 - val_loss: 1498.8281 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8324 - loss: 1508.6997 - val_RMSE: 38.7050 - val_loss: 1498.8247 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8331 - loss: 1508.7588 - val_RMSE: 38.7041 - val_loss: 1498.7686 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8314 - loss: 1508.6329 - val_RMSE: 38.7047 - val_loss: 1498.8119 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8320 - loss: 1508.6835 - val_RMSE: 38.7041 - val_loss: 1498.7722 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8266 - loss: 1508.2732 - val_RMSE: 38.7042 - val_loss: 1498.7761 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8270 - loss: 1508.3079 - val_RMSE: 38.7041 - val_loss: 1498.7855 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8237 - loss: 1508.0518 - val_RMSE: 38.7029 - val_loss: 1498.6909 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8243 - loss: 1508.1083 - val_RMSE: 38.7026 - val_loss: 1498.6742 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8245 - loss: 1508.1329 - val_RMSE: 38.7028 - val_loss: 1498.6964 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8236 - loss: 1508.0685 - val_RMSE: 38.7029 - val_loss: 1498.7174 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8144 - loss: 1507.3624 - val_RMSE: 38.7021 - val_loss: 1498.6572 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8181 - loss: 1507.6537 - val_RMSE: 38.7016 - val_loss: 1498.6260 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8195 - loss: 1507.7714 - val_RMSE: 38.7024 - val_loss: 1498.7084 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8168 - loss: 1507.5803 - val_RMSE: 38.7018 - val_loss: 1498.6704 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8151 - loss: 1507.4535 - val_RMSE: 38.7036 - val_loss: 1498.8120 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 23:43:24,680] Trial 14 finished with value: 38.69705327351888 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002984783533919333, 'dropout_rate': 0.44795270691730493}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  29s 8ms/step - RMSE: 57.1659 - loss: 3430.8306 - val_RMSE: 38.6983 - val_loss: 1498.8441 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0924 - loss: 1529.5322 - val_RMSE: 38.6906 - val_loss: 1498.3064 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9839 - loss: 1521.0792 - val_RMSE: 38.6887 - val_loss: 1498.0994 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9557 - loss: 1518.7845 - val_RMSE: 38.6870 - val_loss: 1497.7704 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9515 - loss: 1518.2507 - val_RMSE: 38.6875 - val_loss: 1497.5935 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9452 - loss: 1517.5596 - val_RMSE: 38.6861 - val_loss: 1497.3618 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9426 - loss: 1517.2594 - val_RMSE: 38.6857 - val_loss: 1497.2886 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9435 - loss: 1517.3000 - val_RMSE: 38.6861 - val_loss: 1497.2948 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9410 - loss: 1517.0885 - val_RMSE: 38.6850 - val_loss: 1497.2120 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9345 - loss: 1516.5826 - val_RMSE: 38.6854 - val_loss: 1497.2504 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9393 - loss: 1516.9591 - val_RMSE: 38.6850 - val_loss: 1497.2163 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9306 - loss: 1516.2841 - val_RMSE: 38.6846 - val_loss: 1497.1846 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9304 - loss: 1516.2693 - val_RMSE: 38.6846 - val_loss: 1497.1831 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9299 - loss: 1516.2289 - val_RMSE: 38.6845 - val_loss: 1497.1815 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9198 - loss: 1515.4485 - val_RMSE: 38.6844 - val_loss: 1497.1842 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9143 - loss: 1515.0242 - val_RMSE: 38.6838 - val_loss: 1497.1359 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9117 - loss: 1514.8230 - val_RMSE: 38.6841 - val_loss: 1497.1598 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9161 - loss: 1515.1650 - val_RMSE: 38.6838 - val_loss: 1497.1333 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9151 - loss: 1515.0920 - val_RMSE: 38.6829 - val_loss: 1497.0654 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9146 - loss: 1515.0533 - val_RMSE: 38.6834 - val_loss: 1497.1141 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9108 - loss: 1514.7668 - val_RMSE: 38.6832 - val_loss: 1497.1071 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9013 - loss: 1514.0417 - val_RMSE: 38.6830 - val_loss: 1497.0922 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9106 - loss: 1514.7650 - val_RMSE: 38.6829 - val_loss: 1497.0931 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9035 - loss: 1514.2114 - val_RMSE: 38.6821 - val_loss: 1497.0345 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9028 - loss: 1514.1614 - val_RMSE: 38.6825 - val_loss: 1497.0570 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  29s 8ms/step - RMSE: 57.1443 - loss: 3427.8079 - val_RMSE: 38.7307 - val_loss: 1501.3450 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0459 - loss: 1525.8882 - val_RMSE: 38.7199 - val_loss: 1500.5787 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9423 - loss: 1517.8477 - val_RMSE: 38.7193 - val_loss: 1500.4846 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9128 - loss: 1515.4645 - val_RMSE: 38.7176 - val_loss: 1500.1639 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9063 - loss: 1514.7551 - val_RMSE: 38.7175 - val_loss: 1499.9242 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8980 - loss: 1513.8986 - val_RMSE: 38.7176 - val_loss: 1499.8086 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8881 - loss: 1513.0250 - val_RMSE: 38.7172 - val_loss: 1499.7290 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8806 - loss: 1512.4054 - val_RMSE: 38.7163 - val_loss: 1499.6388 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8878 - loss: 1512.9453 - val_RMSE: 38.7157 - val_loss: 1499.5846 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8858 - loss: 1512.7778 - val_RMSE: 38.7162 - val_loss: 1499.6216 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8813 - loss: 1512.4236 - val_RMSE: 38.7162 - val_loss: 1499.6226 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8831 - loss: 1512.5712 - val_RMSE: 38.7156 - val_loss: 1499.5880 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8788 - loss: 1512.2426 - val_RMSE: 38.7153 - val_loss: 1499.5529 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8746 - loss: 1511.9155 - val_RMSE: 38.7152 - val_loss: 1499.5577 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8779 - loss: 1512.1763 - val_RMSE: 38.7160 - val_loss: 1499.6110 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8675 - loss: 1511.3635 - val_RMSE: 38.7155 - val_loss: 1499.5750 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8725 - loss: 1511.7579 - val_RMSE: 38.7157 - val_loss: 1499.5924 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8656 - loss: 1511.2235 - val_RMSE: 38.7159 - val_loss: 1499.6100 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8563 - loss: 1510.4905 - val_RMSE: 38.7099 - val_loss: 1499.0966 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8563 - loss: 1510.4412 - val_RMSE: 38.7098 - val_loss: 1499.0450 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8536 - loss: 1510.1877 - val_RMSE: 38.7096 - val_loss: 1498.9941 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8530 - loss: 1510.1133 - val_RMSE: 38.7095 - val_loss: 1498.9586 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8553 - loss: 1510.2628 - val_RMSE: 38.7095 - val_loss: 1498.9364 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8559 - loss: 1510.2888 - val_RMSE: 38.7094 - val_loss: 1498.9080 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8575 - loss: 1510.3870 - val_RMSE: 38.7093 - val_loss: 1498.8811 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  28s 8ms/step - RMSE: 57.1254 - loss: 3425.2498 - val_RMSE: 38.7194 - val_loss: 1500.4795 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0774 - loss: 1528.3647 - val_RMSE: 38.7150 - val_loss: 1500.2128 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9620 - loss: 1519.3999 - val_RMSE: 38.7112 - val_loss: 1499.8650 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9355 - loss: 1517.2517 - val_RMSE: 38.7109 - val_loss: 1499.6720 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9240 - loss: 1516.1539 - val_RMSE: 38.7116 - val_loss: 1499.4846 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9239 - loss: 1515.9294 - val_RMSE: 38.7100 - val_loss: 1499.2283 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9173 - loss: 1515.3032 - val_RMSE: 38.7097 - val_loss: 1499.1447 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9120 - loss: 1514.8485 - val_RMSE: 38.7105 - val_loss: 1499.2045 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9127 - loss: 1514.8962 - val_RMSE: 38.7092 - val_loss: 1499.0884 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9147 - loss: 1515.0438 - val_RMSE: 38.7078 - val_loss: 1498.9818 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9059 - loss: 1514.3590 - val_RMSE: 38.7077 - val_loss: 1498.9750 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9007 - loss: 1513.9552 - val_RMSE: 38.7064 - val_loss: 1498.8713 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9057 - loss: 1514.3368 - val_RMSE: 38.7081 - val_loss: 1499.0177 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8994 - loss: 1513.8552 - val_RMSE: 38.7072 - val_loss: 1498.9406 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8990 - loss: 1513.8284 - val_RMSE: 38.7070 - val_loss: 1498.9335 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8983 - loss: 1513.7806 - val_RMSE: 38.7086 - val_loss: 1499.0530 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8926 - loss: 1513.3330 - val_RMSE: 38.7070 - val_loss: 1498.9297 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8849 - loss: 1512.7251 - val_RMSE: 38.7036 - val_loss: 1498.6143 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8889 - loss: 1512.9840 - val_RMSE: 38.7033 - val_loss: 1498.5455 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8807 - loss: 1512.3025 - val_RMSE: 38.7033 - val_loss: 1498.5125 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8824 - loss: 1512.4025 - val_RMSE: 38.7030 - val_loss: 1498.4615 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8788 - loss: 1512.0936 - val_RMSE: 38.7029 - val_loss: 1498.4274 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8841 - loss: 1512.4811 - val_RMSE: 38.7028 - val_loss: 1498.3962 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8813 - loss: 1512.2443 - val_RMSE: 38.7027 - val_loss: 1498.3696 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8824 - loss: 1512.3110 - val_RMSE: 38.7028 - val_loss: 1498.3608 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 00:09:14,427] Trial 15 finished with value: 38.698187510172524 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0016250296084432089, 'dropout_rate': 0.39540101245549514}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.2782 - loss: 3095.8782 - val_RMSE: 38.7169 - val_loss: 1499.3932 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0832 - loss: 1527.9094 - val_RMSE: 38.6975 - val_loss: 1497.9794 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.9419 - loss: 1516.9744 - val_RMSE: 38.6904 - val_loss: 1497.5193 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9033 - loss: 1514.0696 - val_RMSE: 38.6883 - val_loss: 1497.4456 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8823 - loss: 1512.5110 - val_RMSE: 38.6860 - val_loss: 1497.3236 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8766 - loss: 1512.1173 - val_RMSE: 38.6845 - val_loss: 1497.2179 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8708 - loss: 1511.6632 - val_RMSE: 38.6846 - val_loss: 1497.2097 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8680 - loss: 1511.4290 - val_RMSE: 38.6840 - val_loss: 1497.1393 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8663 - loss: 1511.2745 - val_RMSE: 38.6831 - val_loss: 1497.0627 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8658 - loss: 1511.2291 - val_RMSE: 38.6831 - val_loss: 1497.0541 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8670 - loss: 1511.3171 - val_RMSE: 38.6825 - val_loss: 1497.0161 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8589 - loss: 1510.6921 - val_RMSE: 38.6816 - val_loss: 1496.9396 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8655 - loss: 1511.1991 - val_RMSE: 38.6816 - val_loss: 1496.9424 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8557 - loss: 1510.4449 - val_RMSE: 38.6815 - val_loss: 1496.9385 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8595 - loss: 1510.7480 - val_RMSE: 38.6817 - val_loss: 1496.9653 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8524 - loss: 1510.2029 - val_RMSE: 38.6811 - val_loss: 1496.9233 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8558 - loss: 1510.4786 - val_RMSE: 38.6810 - val_loss: 1496.9232 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8501 - loss: 1510.0358 - val_RMSE: 38.6805 - val_loss: 1496.8965 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8526 - loss: 1510.2360 - val_RMSE: 38.6808 - val_loss: 1496.9279 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8541 - loss: 1510.3625 - val_RMSE: 38.6808 - val_loss: 1496.9269 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8481 - loss: 1509.9030 - val_RMSE: 38.6810 - val_loss: 1496.9478 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8498 - loss: 1510.0400 - val_RMSE: 38.6798 - val_loss: 1496.8708 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8489 - loss: 1509.9806 - val_RMSE: 38.6802 - val_loss: 1496.9056 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8435 - loss: 1509.5713 - val_RMSE: 38.6800 - val_loss: 1496.8999 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8443 - loss: 1509.6378 - val_RMSE: 38.6797 - val_loss: 1496.8862 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.2253 - loss: 3089.3152 - val_RMSE: 38.7393 - val_loss: 1501.1278 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0370 - loss: 1524.3060 - val_RMSE: 38.7239 - val_loss: 1500.0225 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8952 - loss: 1513.3470 - val_RMSE: 38.7231 - val_loss: 1500.0548 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8459 - loss: 1509.6072 - val_RMSE: 38.7212 - val_loss: 1499.9995 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8323 - loss: 1508.6357 - val_RMSE: 38.7192 - val_loss: 1499.8962 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8250 - loss: 1508.1107 - val_RMSE: 38.7164 - val_loss: 1499.6849 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8238 - loss: 1508.0110 - val_RMSE: 38.7161 - val_loss: 1499.6388 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8235 - loss: 1507.9568 - val_RMSE: 38.7160 - val_loss: 1499.6080 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8220 - loss: 1507.8206 - val_RMSE: 38.7148 - val_loss: 1499.5004 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8152 - loss: 1507.2802 - val_RMSE: 38.7156 - val_loss: 1499.5515 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8102 - loss: 1506.8842 - val_RMSE: 38.7149 - val_loss: 1499.5077 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8129 - loss: 1507.0994 - val_RMSE: 38.7151 - val_loss: 1499.5249 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8108 - loss: 1506.9407 - val_RMSE: 38.7147 - val_loss: 1499.4971 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8068 - loss: 1506.6337 - val_RMSE: 38.7154 - val_loss: 1499.5533 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8133 - loss: 1507.1470 - val_RMSE: 38.7147 - val_loss: 1499.5066 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8031 - loss: 1506.3556 - val_RMSE: 38.7146 - val_loss: 1499.5016 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8121 - loss: 1507.0630 - val_RMSE: 38.7143 - val_loss: 1499.4923 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8042 - loss: 1506.4629 - val_RMSE: 38.7149 - val_loss: 1499.5441 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8039 - loss: 1506.4456 - val_RMSE: 38.7144 - val_loss: 1499.5140 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8069 - loss: 1506.6926 - val_RMSE: 38.7124 - val_loss: 1499.3735 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8012 - loss: 1506.2574 - val_RMSE: 38.7141 - val_loss: 1499.5081 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8050 - loss: 1506.5593 - val_RMSE: 38.7130 - val_loss: 1499.4319 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7932 - loss: 1505.6488 - val_RMSE: 38.7122 - val_loss: 1499.3729 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7973 - loss: 1505.9739 - val_RMSE: 38.7138 - val_loss: 1499.5082 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7952 - loss: 1505.8251 - val_RMSE: 38.7139 - val_loss: 1499.5271 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.2234 - loss: 3088.7781 - val_RMSE: 38.7237 - val_loss: 1499.9156 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0555 - loss: 1525.7467 - val_RMSE: 38.7137 - val_loss: 1499.2261 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9162 - loss: 1514.9709 - val_RMSE: 38.7104 - val_loss: 1499.0614 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8789 - loss: 1512.1599 - val_RMSE: 38.7106 - val_loss: 1499.1636 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8631 - loss: 1511.0133 - val_RMSE: 38.7078 - val_loss: 1499.0028 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8446 - loss: 1509.6190 - val_RMSE: 38.7086 - val_loss: 1499.0797 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8489 - loss: 1509.9587 - val_RMSE: 38.7080 - val_loss: 1499.0126 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8443 - loss: 1509.5802 - val_RMSE: 38.7058 - val_loss: 1498.8291 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8415 - loss: 1509.3489 - val_RMSE: 38.7067 - val_loss: 1498.8939 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8449 - loss: 1509.6073 - val_RMSE: 38.7060 - val_loss: 1498.8324 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8396 - loss: 1509.1887 - val_RMSE: 38.7053 - val_loss: 1498.7786 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8383 - loss: 1509.0919 - val_RMSE: 38.7043 - val_loss: 1498.7078 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8376 - loss: 1509.0490 - val_RMSE: 38.7045 - val_loss: 1498.7247 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8368 - loss: 1508.9845 - val_RMSE: 38.7033 - val_loss: 1498.6464 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8324 - loss: 1508.6527 - val_RMSE: 38.7039 - val_loss: 1498.6877 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8318 - loss: 1508.6100 - val_RMSE: 38.7045 - val_loss: 1498.7450 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8299 - loss: 1508.4669 - val_RMSE: 38.7028 - val_loss: 1498.6188 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8290 - loss: 1508.3986 - val_RMSE: 38.7031 - val_loss: 1498.6448 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8292 - loss: 1508.4265 - val_RMSE: 38.7026 - val_loss: 1498.6185 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8287 - loss: 1508.3964 - val_RMSE: 38.7026 - val_loss: 1498.6252 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8188 - loss: 1507.6318 - val_RMSE: 38.7022 - val_loss: 1498.6003 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8250 - loss: 1508.1219 - val_RMSE: 38.7025 - val_loss: 1498.6249 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8240 - loss: 1508.0503 - val_RMSE: 38.7038 - val_loss: 1498.7491 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8210 - loss: 1507.8329 - val_RMSE: 38.7033 - val_loss: 1498.7197 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8191 - loss: 1507.6973 - val_RMSE: 38.7045 - val_loss: 1498.8224 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 00:39:38,632] Trial 16 finished with value: 38.69933573404948 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00021575974293938053, 'dropout_rate': 0.45990474878515086}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 9ms/step - RMSE: 54.1107 - loss: 3078.6514 - val_RMSE: 38.7144 - val_loss: 1500.0889 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0504 - loss: 1526.2701 - val_RMSE: 38.6957 - val_loss: 1498.8293 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.9274 - loss: 1516.8407 - val_RMSE: 38.6920 - val_loss: 1498.6339 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8814 - loss: 1513.3367 - val_RMSE: 38.6914 - val_loss: 1498.5654 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8604 - loss: 1511.6453 - val_RMSE: 38.6913 - val_loss: 1498.4082 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8519 - loss: 1510.8185 - val_RMSE: 38.6897 - val_loss: 1498.0935 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8459 - loss: 1510.1533 - val_RMSE: 38.6900 - val_loss: 1497.9260 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8447 - loss: 1509.8809 - val_RMSE: 38.6898 - val_loss: 1497.7896 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8418 - loss: 1509.5629 - val_RMSE: 38.6898 - val_loss: 1497.7340 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8426 - loss: 1509.5859 - val_RMSE: 38.6886 - val_loss: 1497.6273 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8438 - loss: 1509.6635 - val_RMSE: 38.6879 - val_loss: 1497.5635 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8385 - loss: 1509.2489 - val_RMSE: 38.6880 - val_loss: 1497.5776 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8459 - loss: 1509.8237 - val_RMSE: 38.6875 - val_loss: 1497.5248 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8377 - loss: 1509.1807 - val_RMSE: 38.6866 - val_loss: 1497.4628 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8398 - loss: 1509.3450 - val_RMSE: 38.6866 - val_loss: 1497.4578 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8321 - loss: 1508.7526 - val_RMSE: 38.6872 - val_loss: 1497.5121 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8345 - loss: 1508.9409 - val_RMSE: 38.6865 - val_loss: 1497.4546 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8314 - loss: 1508.6998 - val_RMSE: 38.6860 - val_loss: 1497.4100 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8344 - loss: 1508.9286 - val_RMSE: 38.6860 - val_loss: 1497.4125 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8368 - loss: 1509.1191 - val_RMSE: 38.6865 - val_loss: 1497.4607 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8315 - loss: 1508.7134 - val_RMSE: 38.6851 - val_loss: 1497.3623 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8340 - loss: 1508.9172 - val_RMSE: 38.6855 - val_loss: 1497.3853 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8323 - loss: 1508.7795 - val_RMSE: 38.6854 - val_loss: 1497.3750 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8294 - loss: 1508.5457 - val_RMSE: 38.6843 - val_loss: 1497.2958 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8321 - loss: 1508.7620 - val_RMSE: 38.6847 - val_loss: 1497.3243 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.0633 - loss: 3072.5645 - val_RMSE: 38.7467 - val_loss: 1502.5869 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0063 - loss: 1522.8361 - val_RMSE: 38.7270 - val_loss: 1501.2798 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8792 - loss: 1513.1250 - val_RMSE: 38.7233 - val_loss: 1501.0939 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8261 - loss: 1509.0717 - val_RMSE: 38.7261 - val_loss: 1501.2937 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8118 - loss: 1507.9106 - val_RMSE: 38.7216 - val_loss: 1500.8225 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8011 - loss: 1506.9298 - val_RMSE: 38.7217 - val_loss: 1500.6202 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7958 - loss: 1506.3091 - val_RMSE: 38.7217 - val_loss: 1500.4171 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7957 - loss: 1506.1190 - val_RMSE: 38.7217 - val_loss: 1500.2946 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7959 - loss: 1506.0269 - val_RMSE: 38.7207 - val_loss: 1500.1533 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7920 - loss: 1505.6758 - val_RMSE: 38.7215 - val_loss: 1500.1971 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7896 - loss: 1505.4692 - val_RMSE: 38.7210 - val_loss: 1500.1378 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7907 - loss: 1505.5469 - val_RMSE: 38.7209 - val_loss: 1500.1379 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7869 - loss: 1505.2430 - val_RMSE: 38.7204 - val_loss: 1500.0864 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7865 - loss: 1505.2190 - val_RMSE: 38.7201 - val_loss: 1500.0696 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7896 - loss: 1505.4656 - val_RMSE: 38.7204 - val_loss: 1500.0969 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7845 - loss: 1505.0621 - val_RMSE: 38.7193 - val_loss: 1500.0079 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7900 - loss: 1505.4958 - val_RMSE: 38.7196 - val_loss: 1500.0474 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7857 - loss: 1505.1703 - val_RMSE: 38.7194 - val_loss: 1500.0381 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7837 - loss: 1505.0179 - val_RMSE: 38.7198 - val_loss: 1500.0641 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7879 - loss: 1505.3464 - val_RMSE: 38.7195 - val_loss: 1500.0469 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7807 - loss: 1504.7926 - val_RMSE: 38.7185 - val_loss: 1499.9778 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7869 - loss: 1505.2788 - val_RMSE: 38.7182 - val_loss: 1499.9569 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7769 - loss: 1504.5190 - val_RMSE: 38.7170 - val_loss: 1499.8768 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7826 - loss: 1504.9630 - val_RMSE: 38.7189 - val_loss: 1500.0370 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7790 - loss: 1504.6909 - val_RMSE: 38.7182 - val_loss: 1499.9637 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.0401 - loss: 3069.6545 - val_RMSE: 38.7322 - val_loss: 1501.4784 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0275 - loss: 1524.5115 - val_RMSE: 38.7156 - val_loss: 1500.4202 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8948 - loss: 1514.3752 - val_RMSE: 38.7129 - val_loss: 1500.3600 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8564 - loss: 1511.5105 - val_RMSE: 38.7133 - val_loss: 1500.3911 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8421 - loss: 1510.3601 - val_RMSE: 38.7132 - val_loss: 1500.2396 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8246 - loss: 1508.8284 - val_RMSE: 38.7114 - val_loss: 1499.8651 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8245 - loss: 1508.5780 - val_RMSE: 38.7111 - val_loss: 1499.6149 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8208 - loss: 1508.0903 - val_RMSE: 38.7102 - val_loss: 1499.4279 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8183 - loss: 1507.7866 - val_RMSE: 38.7104 - val_loss: 1499.3872 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8217 - loss: 1508.0056 - val_RMSE: 38.7099 - val_loss: 1499.3264 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8168 - loss: 1507.6061 - val_RMSE: 38.7093 - val_loss: 1499.2693 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8163 - loss: 1507.5669 - val_RMSE: 38.7083 - val_loss: 1499.1895 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8126 - loss: 1507.2797 - val_RMSE: 38.7077 - val_loss: 1499.1591 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8144 - loss: 1507.4257 - val_RMSE: 38.7079 - val_loss: 1499.1654 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8109 - loss: 1507.1516 - val_RMSE: 38.7080 - val_loss: 1499.1681 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8133 - loss: 1507.3391 - val_RMSE: 38.7086 - val_loss: 1499.2253 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8124 - loss: 1507.2797 - val_RMSE: 38.7071 - val_loss: 1499.1300 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8103 - loss: 1507.1340 - val_RMSE: 38.7064 - val_loss: 1499.0674 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8095 - loss: 1507.0614 - val_RMSE: 38.7062 - val_loss: 1499.0457 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8099 - loss: 1507.0938 - val_RMSE: 38.7066 - val_loss: 1499.0726 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8032 - loss: 1506.5662 - val_RMSE: 38.7069 - val_loss: 1499.0944 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8087 - loss: 1506.9945 - val_RMSE: 38.7069 - val_loss: 1499.1034 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8084 - loss: 1506.9818 - val_RMSE: 38.7070 - val_loss: 1499.1201 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8069 - loss: 1506.8638 - val_RMSE: 38.7055 - val_loss: 1498.9998 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8038 - loss: 1506.6199 - val_RMSE: 38.7055 - val_loss: 1498.9918 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 01:10:24,832] Trial 17 finished with value: 38.70278676350912 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'selu', 'reg': 0.0007138024821391923, 'dropout_rate': 0.3957879794222315}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.2398 - loss: 3091.4988 - val_RMSE: 38.7193 - val_loss: 1499.3783 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0753 - loss: 1527.0886 - val_RMSE: 38.6997 - val_loss: 1497.9049 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9403 - loss: 1516.5986 - val_RMSE: 38.6907 - val_loss: 1497.2632 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8992 - loss: 1513.4532 - val_RMSE: 38.6871 - val_loss: 1497.0455 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8772 - loss: 1511.8029 - val_RMSE: 38.6865 - val_loss: 1497.0535 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8716 - loss: 1511.4158 - val_RMSE: 38.6844 - val_loss: 1496.9219 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8660 - loss: 1511.0156 - val_RMSE: 38.6848 - val_loss: 1496.9785 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8637 - loss: 1510.8527 - val_RMSE: 38.6832 - val_loss: 1496.8630 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8611 - loss: 1510.6633 - val_RMSE: 38.6837 - val_loss: 1496.9169 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8606 - loss: 1510.6301 - val_RMSE: 38.6834 - val_loss: 1496.8917 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8621 - loss: 1510.7542 - val_RMSE: 38.6827 - val_loss: 1496.8439 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8531 - loss: 1510.0630 - val_RMSE: 38.6815 - val_loss: 1496.7584 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8593 - loss: 1510.5491 - val_RMSE: 38.6813 - val_loss: 1496.7513 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8500 - loss: 1509.8315 - val_RMSE: 38.6814 - val_loss: 1496.7617 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8534 - loss: 1510.1022 - val_RMSE: 38.6808 - val_loss: 1496.7251 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8466 - loss: 1509.5798 - val_RMSE: 38.6808 - val_loss: 1496.7343 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8502 - loss: 1509.8713 - val_RMSE: 38.6806 - val_loss: 1496.7273 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8449 - loss: 1509.4708 - val_RMSE: 38.6802 - val_loss: 1496.7064 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8474 - loss: 1509.6674 - val_RMSE: 38.6800 - val_loss: 1496.6989 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8484 - loss: 1509.7550 - val_RMSE: 38.6805 - val_loss: 1496.7426 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8421 - loss: 1509.2701 - val_RMSE: 38.6798 - val_loss: 1496.6989 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8444 - loss: 1509.4558 - val_RMSE: 38.6793 - val_loss: 1496.6677 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8428 - loss: 1509.3433 - val_RMSE: 38.6793 - val_loss: 1496.6768 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8369 - loss: 1508.8928 - val_RMSE: 38.6798 - val_loss: 1496.7220 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8367 - loss: 1508.8910 - val_RMSE: 38.6808 - val_loss: 1496.8113 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.1941 - loss: 3085.7605 - val_RMSE: 38.7357 - val_loss: 1500.6461 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0281 - loss: 1523.3992 - val_RMSE: 38.7228 - val_loss: 1499.6948 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8910 - loss: 1512.7607 - val_RMSE: 38.7218 - val_loss: 1499.6647 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8400 - loss: 1508.8491 - val_RMSE: 38.7194 - val_loss: 1499.5392 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8278 - loss: 1507.9591 - val_RMSE: 38.7175 - val_loss: 1499.4386 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8202 - loss: 1507.4189 - val_RMSE: 38.7162 - val_loss: 1499.3790 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8173 - loss: 1507.2258 - val_RMSE: 38.7161 - val_loss: 1499.3939 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8177 - loss: 1507.2728 - val_RMSE: 38.7159 - val_loss: 1499.3864 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8163 - loss: 1507.1754 - val_RMSE: 38.7154 - val_loss: 1499.3591 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8100 - loss: 1506.6975 - val_RMSE: 38.7156 - val_loss: 1499.3827 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8045 - loss: 1506.2773 - val_RMSE: 38.7145 - val_loss: 1499.3005 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8075 - loss: 1506.5176 - val_RMSE: 38.7149 - val_loss: 1499.3445 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8045 - loss: 1506.2889 - val_RMSE: 38.7140 - val_loss: 1499.2806 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8017 - loss: 1506.0815 - val_RMSE: 38.7149 - val_loss: 1499.3608 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8077 - loss: 1506.5504 - val_RMSE: 38.7143 - val_loss: 1499.3213 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7976 - loss: 1505.7821 - val_RMSE: 38.7140 - val_loss: 1499.3085 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8077 - loss: 1506.5778 - val_RMSE: 38.7136 - val_loss: 1499.2871 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7984 - loss: 1505.8611 - val_RMSE: 38.7150 - val_loss: 1499.3993 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7949 - loss: 1505.5833 - val_RMSE: 38.7088 - val_loss: 1498.9016 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7973 - loss: 1505.7582 - val_RMSE: 38.7086 - val_loss: 1498.8751 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7887 - loss: 1505.0756 - val_RMSE: 38.7084 - val_loss: 1498.8481 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7929 - loss: 1505.3882 - val_RMSE: 38.7085 - val_loss: 1498.8446 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7830 - loss: 1504.6169 - val_RMSE: 38.7085 - val_loss: 1498.8344 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7883 - loss: 1505.0145 - val_RMSE: 38.7087 - val_loss: 1498.8383 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7853 - loss: 1504.7732 - val_RMSE: 38.7086 - val_loss: 1498.8279 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.1967 - loss: 3085.7881 - val_RMSE: 38.7236 - val_loss: 1499.7102 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0499 - loss: 1525.0978 - val_RMSE: 38.7167 - val_loss: 1499.2147 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9143 - loss: 1514.5691 - val_RMSE: 38.7121 - val_loss: 1498.9109 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8742 - loss: 1511.5024 - val_RMSE: 38.7099 - val_loss: 1498.7930 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8603 - loss: 1510.4794 - val_RMSE: 38.7098 - val_loss: 1498.8401 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8412 - loss: 1509.0469 - val_RMSE: 38.7078 - val_loss: 1498.7269 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8449 - loss: 1509.3674 - val_RMSE: 38.7070 - val_loss: 1498.6874 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8395 - loss: 1508.9677 - val_RMSE: 38.7060 - val_loss: 1498.6287 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8370 - loss: 1508.7920 - val_RMSE: 38.7064 - val_loss: 1498.6707 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8387 - loss: 1508.9312 - val_RMSE: 38.7050 - val_loss: 1498.5713 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8339 - loss: 1508.5682 - val_RMSE: 38.7044 - val_loss: 1498.5265 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8343 - loss: 1508.6080 - val_RMSE: 38.7036 - val_loss: 1498.4749 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8323 - loss: 1508.4562 - val_RMSE: 38.7049 - val_loss: 1498.5798 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8315 - loss: 1508.3954 - val_RMSE: 38.7042 - val_loss: 1498.5349 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8267 - loss: 1508.0354 - val_RMSE: 38.7035 - val_loss: 1498.4894 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8270 - loss: 1508.0687 - val_RMSE: 38.7038 - val_loss: 1498.5228 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8238 - loss: 1507.8303 - val_RMSE: 38.7027 - val_loss: 1498.4489 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8239 - loss: 1507.8403 - val_RMSE: 38.7034 - val_loss: 1498.5072 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8246 - loss: 1507.9081 - val_RMSE: 38.7026 - val_loss: 1498.4523 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8228 - loss: 1507.7750 - val_RMSE: 38.7033 - val_loss: 1498.5188 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8126 - loss: 1506.9957 - val_RMSE: 38.7020 - val_loss: 1498.4283 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8176 - loss: 1507.3906 - val_RMSE: 38.7019 - val_loss: 1498.4310 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8181 - loss: 1507.4351 - val_RMSE: 38.7036 - val_loss: 1498.5718 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8156 - loss: 1507.2567 - val_RMSE: 38.7021 - val_loss: 1498.4636 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8133 - loss: 1507.0874 - val_RMSE: 38.7045 - val_loss: 1498.6572 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 01:40:46,962] Trial 18 finished with value: 38.69798787434896 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00010652411608589064, 'dropout_rate': 0.4509993110355638}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  28s 8ms/step - RMSE: 57.1954 - loss: 3473.9302 - val_RMSE: 38.7020 - val_loss: 1515.2155 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.1167 - loss: 1543.0260 - val_RMSE: 38.6971 - val_loss: 1502.5275 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9993 - loss: 1525.3372 - val_RMSE: 38.6965 - val_loss: 1500.6035 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9703 - loss: 1521.5928 - val_RMSE: 38.6932 - val_loss: 1499.4594 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9657 - loss: 1520.6129 - val_RMSE: 38.6957 - val_loss: 1499.6681 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9617 - loss: 1520.3317 - val_RMSE: 38.6932 - val_loss: 1499.3479 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9611 - loss: 1520.2102 - val_RMSE: 38.6935 - val_loss: 1499.3765 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9617 - loss: 1520.2311 - val_RMSE: 38.6924 - val_loss: 1499.2362 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9583 - loss: 1519.9635 - val_RMSE: 38.6926 - val_loss: 1499.3032 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9523 - loss: 1519.4067 - val_RMSE: 38.6930 - val_loss: 1499.1931 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9564 - loss: 1519.7504 - val_RMSE: 38.6916 - val_loss: 1499.0542 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9477 - loss: 1519.0076 - val_RMSE: 38.6935 - val_loss: 1499.2684 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9493 - loss: 1519.1442 - val_RMSE: 38.6910 - val_loss: 1499.0090 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9479 - loss: 1519.0330 - val_RMSE: 38.6913 - val_loss: 1498.9174 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9377 - loss: 1518.1302 - val_RMSE: 38.6906 - val_loss: 1498.8359 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9322 - loss: 1517.6932 - val_RMSE: 38.6895 - val_loss: 1498.8109 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9295 - loss: 1517.5000 - val_RMSE: 38.6908 - val_loss: 1498.8977 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9335 - loss: 1517.8086 - val_RMSE: 38.6901 - val_loss: 1498.8118 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9331 - loss: 1517.7023 - val_RMSE: 38.6900 - val_loss: 1498.9238 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9332 - loss: 1517.8079 - val_RMSE: 38.6907 - val_loss: 1498.9600 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9290 - loss: 1517.4254 - val_RMSE: 38.6906 - val_loss: 1498.8538 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9144 - loss: 1515.8588 - val_RMSE: 38.6861 - val_loss: 1497.5668 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9223 - loss: 1515.8263 - val_RMSE: 38.6860 - val_loss: 1497.3218 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9161 - loss: 1515.1467 - val_RMSE: 38.6856 - val_loss: 1497.1844 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9166 - loss: 1515.0980 - val_RMSE: 38.6852 - val_loss: 1497.1039 - learning_rate: 1.0000e-04\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  30s 8ms/step - RMSE: 57.1779 - loss: 3471.2241 - val_RMSE: 38.7458 - val_loss: 1518.5850 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0603 - loss: 1538.6108 - val_RMSE: 38.7352 - val_loss: 1505.8210 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9562 - loss: 1522.1332 - val_RMSE: 38.7293 - val_loss: 1503.0566 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9251 - loss: 1518.0636 - val_RMSE: 38.7242 - val_loss: 1501.8026 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9206 - loss: 1517.1541 - val_RMSE: 38.7234 - val_loss: 1501.9583 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9142 - loss: 1516.7419 - val_RMSE: 38.7213 - val_loss: 1501.7061 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9061 - loss: 1516.0527 - val_RMSE: 38.7223 - val_loss: 1501.6149 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8986 - loss: 1515.3788 - val_RMSE: 38.7211 - val_loss: 1501.5238 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9041 - loss: 1515.7919 - val_RMSE: 38.7224 - val_loss: 1501.5063 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9031 - loss: 1515.6868 - val_RMSE: 38.7253 - val_loss: 1501.7135 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8989 - loss: 1515.2574 - val_RMSE: 38.7205 - val_loss: 1501.5234 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9019 - loss: 1515.5226 - val_RMSE: 38.7202 - val_loss: 1501.2913 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8955 - loss: 1514.9628 - val_RMSE: 38.7188 - val_loss: 1501.3521 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8919 - loss: 1514.7085 - val_RMSE: 38.7213 - val_loss: 1501.3600 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8957 - loss: 1514.9402 - val_RMSE: 38.7224 - val_loss: 1501.4587 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8848 - loss: 1514.0964 - val_RMSE: 38.7210 - val_loss: 1501.2278 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8892 - loss: 1514.3898 - val_RMSE: 38.7200 - val_loss: 1501.2106 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8819 - loss: 1513.8618 - val_RMSE: 38.7201 - val_loss: 1501.2452 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8764 - loss: 1513.3997 - val_RMSE: 38.7203 - val_loss: 1501.2402 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8771 - loss: 1513.4261 - val_RMSE: 38.7196 - val_loss: 1501.1266 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8731 - loss: 1513.1027 - val_RMSE: 38.7211 - val_loss: 1501.2502 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8713 - loss: 1512.9431 - val_RMSE: 38.7221 - val_loss: 1501.3231 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8722 - loss: 1512.9980 - val_RMSE: 38.7187 - val_loss: 1501.0776 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8708 - loss: 1512.8766 - val_RMSE: 38.7192 - val_loss: 1501.1128 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8710 - loss: 1512.8568 - val_RMSE: 38.7192 - val_loss: 1501.0691 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  28s 8ms/step - RMSE: 57.1586 - loss: 3468.8442 - val_RMSE: 38.7308 - val_loss: 1518.0854 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0940 - loss: 1542.0193 - val_RMSE: 38.7218 - val_loss: 1504.8708 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9738 - loss: 1523.6556 - val_RMSE: 38.7192 - val_loss: 1502.2158 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9475 - loss: 1519.7352 - val_RMSE: 38.7162 - val_loss: 1501.1461 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9395 - loss: 1518.5706 - val_RMSE: 38.7153 - val_loss: 1501.1970 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9397 - loss: 1518.6503 - val_RMSE: 38.7170 - val_loss: 1501.2604 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9350 - loss: 1518.2765 - val_RMSE: 38.7143 - val_loss: 1500.9779 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9308 - loss: 1517.8375 - val_RMSE: 38.7154 - val_loss: 1501.0334 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9315 - loss: 1517.8890 - val_RMSE: 38.7131 - val_loss: 1500.8451 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9334 - loss: 1517.9983 - val_RMSE: 38.7131 - val_loss: 1500.8263 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9250 - loss: 1517.3690 - val_RMSE: 38.7133 - val_loss: 1500.6941 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9186 - loss: 1516.7828 - val_RMSE: 38.7143 - val_loss: 1500.9939 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9247 - loss: 1517.2903 - val_RMSE: 38.7143 - val_loss: 1500.7407 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9185 - loss: 1516.7043 - val_RMSE: 38.7149 - val_loss: 1501.0273 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9173 - loss: 1516.6989 - val_RMSE: 38.7129 - val_loss: 1500.6477 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9170 - loss: 1516.5720 - val_RMSE: 38.7133 - val_loss: 1500.6489 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9109 - loss: 1516.0662 - val_RMSE: 38.7128 - val_loss: 1500.7288 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9071 - loss: 1515.8297 - val_RMSE: 38.7115 - val_loss: 1500.4503 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9111 - loss: 1516.0710 - val_RMSE: 38.7126 - val_loss: 1500.6029 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9004 - loss: 1515.2291 - val_RMSE: 38.7126 - val_loss: 1500.6469 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9030 - loss: 1515.4309 - val_RMSE: 38.7118 - val_loss: 1500.5255 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8961 - loss: 1514.8849 - val_RMSE: 38.7118 - val_loss: 1500.3914 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9016 - loss: 1515.2128 - val_RMSE: 38.7141 - val_loss: 1500.5745 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8976 - loss: 1514.9122 - val_RMSE: 38.7143 - val_loss: 1500.6284 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8964 - loss: 1514.8342 - val_RMSE: 38.7157 - val_loss: 1500.7217 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 02:07:06,390] Trial 19 finished with value: 38.706722259521484 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.08811299737907308, 'dropout_rate': 0.4052062072468139}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.0384 - loss: 3071.1411 - val_RMSE: 38.7142 - val_loss: 1500.9333 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0361 - loss: 1526.0258 - val_RMSE: 38.6967 - val_loss: 1499.7330 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9130 - loss: 1516.5133 - val_RMSE: 38.6891 - val_loss: 1499.0563 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8749 - loss: 1513.4211 - val_RMSE: 38.6889 - val_loss: 1498.7920 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8519 - loss: 1511.3660 - val_RMSE: 38.6881 - val_loss: 1498.4183 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8443 - loss: 1510.4503 - val_RMSE: 38.6856 - val_loss: 1497.9375 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8365 - loss: 1509.5820 - val_RMSE: 38.6860 - val_loss: 1497.7517 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8354 - loss: 1509.2983 - val_RMSE: 38.6856 - val_loss: 1497.6060 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8336 - loss: 1509.0789 - val_RMSE: 38.6861 - val_loss: 1497.6204 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8335 - loss: 1509.0549 - val_RMSE: 38.6848 - val_loss: 1497.5051 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8340 - loss: 1509.0795 - val_RMSE: 38.6846 - val_loss: 1497.4790 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8298 - loss: 1508.7493 - val_RMSE: 38.6844 - val_loss: 1497.4545 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8362 - loss: 1509.2292 - val_RMSE: 38.6840 - val_loss: 1497.4305 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8289 - loss: 1508.6766 - val_RMSE: 38.6843 - val_loss: 1497.4526 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8312 - loss: 1508.8406 - val_RMSE: 38.6832 - val_loss: 1497.3586 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8247 - loss: 1508.3398 - val_RMSE: 38.6833 - val_loss: 1497.3785 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8261 - loss: 1508.4633 - val_RMSE: 38.6837 - val_loss: 1497.4025 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8226 - loss: 1508.1925 - val_RMSE: 38.6833 - val_loss: 1497.3867 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8254 - loss: 1508.4210 - val_RMSE: 38.6833 - val_loss: 1497.3918 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8276 - loss: 1508.5923 - val_RMSE: 38.6823 - val_loss: 1497.3270 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8223 - loss: 1508.1713 - val_RMSE: 38.6819 - val_loss: 1497.2715 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8249 - loss: 1508.3746 - val_RMSE: 38.6822 - val_loss: 1497.2911 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8235 - loss: 1508.2632 - val_RMSE: 38.6819 - val_loss: 1497.2640 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8207 - loss: 1508.0426 - val_RMSE: 38.6826 - val_loss: 1497.3433 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8220 - loss: 1508.1719 - val_RMSE: 38.6823 - val_loss: 1497.3188 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.0019 - loss: 3066.4866 - val_RMSE: 38.7354 - val_loss: 1502.5933 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9869 - loss: 1522.1981 - val_RMSE: 38.7256 - val_loss: 1502.0167 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8696 - loss: 1513.2081 - val_RMSE: 38.7239 - val_loss: 1501.8569 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8212 - loss: 1509.3451 - val_RMSE: 38.7208 - val_loss: 1501.3527 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8023 - loss: 1507.5884 - val_RMSE: 38.7185 - val_loss: 1500.8442 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7922 - loss: 1506.4725 - val_RMSE: 38.7183 - val_loss: 1500.5333 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7862 - loss: 1505.7325 - val_RMSE: 38.7167 - val_loss: 1500.1764 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7869 - loss: 1505.5648 - val_RMSE: 38.7172 - val_loss: 1500.0515 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7854 - loss: 1505.3170 - val_RMSE: 38.7173 - val_loss: 1500.0198 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7815 - loss: 1504.9998 - val_RMSE: 38.7173 - val_loss: 1500.0129 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7798 - loss: 1504.8539 - val_RMSE: 38.7163 - val_loss: 1499.9261 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7812 - loss: 1504.9542 - val_RMSE: 38.7168 - val_loss: 1499.9580 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7781 - loss: 1504.7061 - val_RMSE: 38.7181 - val_loss: 1500.0417 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7781 - loss: 1504.7037 - val_RMSE: 38.7178 - val_loss: 1500.0302 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7808 - loss: 1504.9240 - val_RMSE: 38.7188 - val_loss: 1500.1210 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7756 - loss: 1504.5253 - val_RMSE: 38.7173 - val_loss: 1500.0052 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7756 - loss: 1504.4865 - val_RMSE: 38.7097 - val_loss: 1499.2786 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7702 - loss: 1503.9390 - val_RMSE: 38.7095 - val_loss: 1499.1625 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7666 - loss: 1503.5729 - val_RMSE: 38.7092 - val_loss: 1499.0795 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7697 - loss: 1503.7505 - val_RMSE: 38.7092 - val_loss: 1499.0214 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7644 - loss: 1503.2910 - val_RMSE: 38.7092 - val_loss: 1498.9847 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7707 - loss: 1503.7433 - val_RMSE: 38.7091 - val_loss: 1498.9496 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7616 - loss: 1503.0100 - val_RMSE: 38.7090 - val_loss: 1498.9152 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7664 - loss: 1503.3531 - val_RMSE: 38.7090 - val_loss: 1498.8895 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7642 - loss: 1503.1687 - val_RMSE: 38.7090 - val_loss: 1498.8744 - learning_rate: 1.0000e-04\n",
            "41608/41608  87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 53.9871 - loss: 3064.5459 - val_RMSE: 38.7325 - val_loss: 1502.3867 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0128 - loss: 1524.2484 - val_RMSE: 38.7143 - val_loss: 1501.1544 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8919 - loss: 1514.9514 - val_RMSE: 38.7147 - val_loss: 1501.1276 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8487 - loss: 1511.4740 - val_RMSE: 38.7109 - val_loss: 1500.5569 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8346 - loss: 1510.0759 - val_RMSE: 38.7098 - val_loss: 1500.1423 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8176 - loss: 1508.4363 - val_RMSE: 38.7084 - val_loss: 1499.7786 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8156 - loss: 1508.0280 - val_RMSE: 38.7078 - val_loss: 1499.5171 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8111 - loss: 1507.4880 - val_RMSE: 38.7059 - val_loss: 1499.2349 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8094 - loss: 1507.2377 - val_RMSE: 38.7074 - val_loss: 1499.2845 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8119 - loss: 1507.3927 - val_RMSE: 38.7058 - val_loss: 1499.1450 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8076 - loss: 1507.0417 - val_RMSE: 38.7066 - val_loss: 1499.1943 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8069 - loss: 1506.9802 - val_RMSE: 38.7049 - val_loss: 1499.0840 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8039 - loss: 1506.7445 - val_RMSE: 38.7058 - val_loss: 1499.1261 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8066 - loss: 1506.9498 - val_RMSE: 38.7050 - val_loss: 1499.0845 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8027 - loss: 1506.6494 - val_RMSE: 38.7054 - val_loss: 1499.1024 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8048 - loss: 1506.8075 - val_RMSE: 38.7062 - val_loss: 1499.1653 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8049 - loss: 1506.8127 - val_RMSE: 38.7051 - val_loss: 1499.0726 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8022 - loss: 1506.6107 - val_RMSE: 38.7046 - val_loss: 1499.0479 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8015 - loss: 1506.5698 - val_RMSE: 38.7045 - val_loss: 1499.0398 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8015 - loss: 1506.5397 - val_RMSE: 38.7049 - val_loss: 1499.0450 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7939 - loss: 1505.9456 - val_RMSE: 38.7041 - val_loss: 1498.9883 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8000 - loss: 1506.4276 - val_RMSE: 38.7045 - val_loss: 1499.0416 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8001 - loss: 1506.4540 - val_RMSE: 38.7044 - val_loss: 1499.0538 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7984 - loss: 1506.3348 - val_RMSE: 38.7046 - val_loss: 1499.0408 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7958 - loss: 1506.1191 - val_RMSE: 38.7032 - val_loss: 1498.9443 - learning_rate: 0.0010\n",
            "41608/41608  87s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 02:38:07,287] Trial 20 finished with value: 38.698177337646484 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'gelu', 'reg': 0.001253520376146537, 'dropout_rate': 0.383273235469179}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.2278 - loss: 3090.4089 - val_RMSE: 38.7251 - val_loss: 1500.1178 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0667 - loss: 1526.7211 - val_RMSE: 38.6989 - val_loss: 1498.1919 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9381 - loss: 1516.7971 - val_RMSE: 38.6926 - val_loss: 1497.8243 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8963 - loss: 1513.6533 - val_RMSE: 38.6882 - val_loss: 1497.5773 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8748 - loss: 1512.0690 - val_RMSE: 38.6857 - val_loss: 1497.4275 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8683 - loss: 1511.5898 - val_RMSE: 38.6845 - val_loss: 1497.3188 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8640 - loss: 1511.2229 - val_RMSE: 38.6848 - val_loss: 1497.2889 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8605 - loss: 1510.9011 - val_RMSE: 38.6833 - val_loss: 1497.1279 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8586 - loss: 1510.7120 - val_RMSE: 38.6836 - val_loss: 1497.1306 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8591 - loss: 1510.7355 - val_RMSE: 38.6826 - val_loss: 1497.0344 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8588 - loss: 1510.6996 - val_RMSE: 38.6817 - val_loss: 1496.9689 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8521 - loss: 1510.1796 - val_RMSE: 38.6818 - val_loss: 1496.9781 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8587 - loss: 1510.7000 - val_RMSE: 38.6816 - val_loss: 1496.9597 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8497 - loss: 1509.9961 - val_RMSE: 38.6817 - val_loss: 1496.9636 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8527 - loss: 1510.2360 - val_RMSE: 38.6812 - val_loss: 1496.9463 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8459 - loss: 1509.7155 - val_RMSE: 38.6816 - val_loss: 1496.9846 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8490 - loss: 1509.9684 - val_RMSE: 38.6807 - val_loss: 1496.9202 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8438 - loss: 1509.5717 - val_RMSE: 38.6810 - val_loss: 1496.9476 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8468 - loss: 1509.8046 - val_RMSE: 38.6806 - val_loss: 1496.9320 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8482 - loss: 1509.9277 - val_RMSE: 38.6807 - val_loss: 1496.9377 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8413 - loss: 1509.3867 - val_RMSE: 38.6804 - val_loss: 1496.9171 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8460 - loss: 1509.7573 - val_RMSE: 38.6805 - val_loss: 1496.9319 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8430 - loss: 1509.5291 - val_RMSE: 38.6805 - val_loss: 1496.9321 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8369 - loss: 1509.0646 - val_RMSE: 38.6803 - val_loss: 1496.9240 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8379 - loss: 1509.1442 - val_RMSE: 38.6798 - val_loss: 1496.9020 - learning_rate: 0.0010\n",
            "41608/41608  87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.1753 - loss: 3083.8904 - val_RMSE: 38.7344 - val_loss: 1500.8442 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0260 - loss: 1523.5448 - val_RMSE: 38.7225 - val_loss: 1500.0184 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8898 - loss: 1513.0251 - val_RMSE: 38.7197 - val_loss: 1499.8984 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8396 - loss: 1509.2179 - val_RMSE: 38.7181 - val_loss: 1499.8555 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8256 - loss: 1508.2124 - val_RMSE: 38.7181 - val_loss: 1499.9015 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8166 - loss: 1507.5430 - val_RMSE: 38.7169 - val_loss: 1499.8066 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8138 - loss: 1507.3083 - val_RMSE: 38.7160 - val_loss: 1499.6951 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8150 - loss: 1507.3573 - val_RMSE: 38.7165 - val_loss: 1499.6984 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8134 - loss: 1507.2026 - val_RMSE: 38.7148 - val_loss: 1499.5537 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8068 - loss: 1506.6768 - val_RMSE: 38.7147 - val_loss: 1499.5270 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8034 - loss: 1506.4026 - val_RMSE: 38.7146 - val_loss: 1499.5198 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8047 - loss: 1506.5060 - val_RMSE: 38.7145 - val_loss: 1499.5148 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8013 - loss: 1506.2509 - val_RMSE: 38.7147 - val_loss: 1499.5470 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8000 - loss: 1506.1589 - val_RMSE: 38.7144 - val_loss: 1499.5309 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8043 - loss: 1506.4999 - val_RMSE: 38.7144 - val_loss: 1499.5460 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7966 - loss: 1505.9175 - val_RMSE: 38.7138 - val_loss: 1499.5031 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8050 - loss: 1506.5771 - val_RMSE: 38.7140 - val_loss: 1499.5243 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7977 - loss: 1506.0137 - val_RMSE: 38.7148 - val_loss: 1499.5895 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7950 - loss: 1505.8059 - val_RMSE: 38.7138 - val_loss: 1499.5175 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8011 - loss: 1506.2845 - val_RMSE: 38.7130 - val_loss: 1499.4683 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7932 - loss: 1505.6808 - val_RMSE: 38.7146 - val_loss: 1499.5977 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7970 - loss: 1505.9814 - val_RMSE: 38.7146 - val_loss: 1499.5991 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7873 - loss: 1505.2322 - val_RMSE: 38.7132 - val_loss: 1499.4913 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7893 - loss: 1505.3969 - val_RMSE: 38.7138 - val_loss: 1499.5492 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7899 - loss: 1505.4431 - val_RMSE: 38.7137 - val_loss: 1499.5426 - learning_rate: 0.0010\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.1695 - loss: 3083.0068 - val_RMSE: 38.7236 - val_loss: 1500.0024 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0481 - loss: 1525.2650 - val_RMSE: 38.7175 - val_loss: 1499.6311 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9107 - loss: 1514.6559 - val_RMSE: 38.7133 - val_loss: 1499.4006 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8713 - loss: 1511.6849 - val_RMSE: 38.7095 - val_loss: 1499.1990 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8561 - loss: 1510.5833 - val_RMSE: 38.7087 - val_loss: 1499.1779 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8389 - loss: 1509.2760 - val_RMSE: 38.7076 - val_loss: 1499.0699 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8417 - loss: 1509.4633 - val_RMSE: 38.7064 - val_loss: 1498.9381 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8383 - loss: 1509.1564 - val_RMSE: 38.7057 - val_loss: 1498.8528 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8345 - loss: 1508.8416 - val_RMSE: 38.7069 - val_loss: 1498.9324 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8368 - loss: 1509.0072 - val_RMSE: 38.7053 - val_loss: 1498.8063 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8312 - loss: 1508.5721 - val_RMSE: 38.7051 - val_loss: 1498.7968 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8313 - loss: 1508.5808 - val_RMSE: 38.7041 - val_loss: 1498.7208 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8297 - loss: 1508.4636 - val_RMSE: 38.7046 - val_loss: 1498.7661 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8301 - loss: 1508.4952 - val_RMSE: 38.7042 - val_loss: 1498.7338 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8247 - loss: 1508.0795 - val_RMSE: 38.7038 - val_loss: 1498.7101 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8252 - loss: 1508.1283 - val_RMSE: 38.7034 - val_loss: 1498.6868 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8228 - loss: 1507.9518 - val_RMSE: 38.7039 - val_loss: 1498.7324 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8224 - loss: 1507.9238 - val_RMSE: 38.7029 - val_loss: 1498.6663 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8237 - loss: 1508.0353 - val_RMSE: 38.7023 - val_loss: 1498.6287 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8227 - loss: 1507.9691 - val_RMSE: 38.7027 - val_loss: 1498.6659 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8133 - loss: 1507.2393 - val_RMSE: 38.7020 - val_loss: 1498.6255 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8179 - loss: 1507.6034 - val_RMSE: 38.7018 - val_loss: 1498.6021 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8183 - loss: 1507.6455 - val_RMSE: 38.7034 - val_loss: 1498.7487 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8161 - loss: 1507.4789 - val_RMSE: 38.7021 - val_loss: 1498.6378 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8130 - loss: 1507.2428 - val_RMSE: 38.7036 - val_loss: 1498.7642 - learning_rate: 0.0010\n",
            "41608/41608  90s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 03:10:03,649] Trial 21 finished with value: 38.69902038574219 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00026956737552824854, 'dropout_rate': 0.4450751108802483}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.3208 - loss: 3100.5127 - val_RMSE: 38.7158 - val_loss: 1499.3030 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0905 - loss: 1528.4830 - val_RMSE: 38.6945 - val_loss: 1497.7473 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9464 - loss: 1517.3270 - val_RMSE: 38.6909 - val_loss: 1497.5587 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9080 - loss: 1514.4309 - val_RMSE: 38.6876 - val_loss: 1497.3884 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8864 - loss: 1512.8289 - val_RMSE: 38.6866 - val_loss: 1497.3728 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8823 - loss: 1512.5542 - val_RMSE: 38.6843 - val_loss: 1497.2012 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8777 - loss: 1512.1973 - val_RMSE: 38.6838 - val_loss: 1497.1503 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8734 - loss: 1511.8530 - val_RMSE: 38.6831 - val_loss: 1497.0818 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8727 - loss: 1511.7786 - val_RMSE: 38.6836 - val_loss: 1497.1160 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8721 - loss: 1511.7355 - val_RMSE: 38.6828 - val_loss: 1497.0492 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8731 - loss: 1511.8104 - val_RMSE: 38.6819 - val_loss: 1496.9794 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8648 - loss: 1511.1669 - val_RMSE: 38.6822 - val_loss: 1497.0123 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8720 - loss: 1511.7264 - val_RMSE: 38.6823 - val_loss: 1497.0066 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8605 - loss: 1510.8334 - val_RMSE: 38.6821 - val_loss: 1497.0029 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8672 - loss: 1511.3625 - val_RMSE: 38.6815 - val_loss: 1496.9563 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8572 - loss: 1510.5892 - val_RMSE: 38.6814 - val_loss: 1496.9607 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8613 - loss: 1510.9116 - val_RMSE: 38.6809 - val_loss: 1496.9263 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8557 - loss: 1510.4866 - val_RMSE: 38.6811 - val_loss: 1496.9486 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8564 - loss: 1510.5496 - val_RMSE: 38.6810 - val_loss: 1496.9546 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8597 - loss: 1510.8116 - val_RMSE: 38.6814 - val_loss: 1496.9923 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8535 - loss: 1510.3367 - val_RMSE: 38.6811 - val_loss: 1496.9855 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8554 - loss: 1510.5006 - val_RMSE: 38.6810 - val_loss: 1496.9847 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8487 - loss: 1509.9783 - val_RMSE: 38.6790 - val_loss: 1496.7957 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8426 - loss: 1509.4738 - val_RMSE: 38.6788 - val_loss: 1496.7542 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8437 - loss: 1509.5366 - val_RMSE: 38.6787 - val_loss: 1496.7229 - learning_rate: 1.0000e-04\n",
            "41608/41608  91s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.2604 - loss: 3093.1023 - val_RMSE: 38.7338 - val_loss: 1500.6934 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0424 - loss: 1524.7195 - val_RMSE: 38.7224 - val_loss: 1499.8884 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8997 - loss: 1513.6796 - val_RMSE: 38.7198 - val_loss: 1499.7743 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8526 - loss: 1510.0967 - val_RMSE: 38.7192 - val_loss: 1499.8121 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8367 - loss: 1508.9478 - val_RMSE: 38.7175 - val_loss: 1499.7349 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8296 - loss: 1508.4362 - val_RMSE: 38.7173 - val_loss: 1499.7438 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8281 - loss: 1508.3344 - val_RMSE: 38.7166 - val_loss: 1499.6716 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8283 - loss: 1508.3250 - val_RMSE: 38.7163 - val_loss: 1499.6329 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8280 - loss: 1508.2902 - val_RMSE: 38.7150 - val_loss: 1499.5267 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8199 - loss: 1507.6559 - val_RMSE: 38.7147 - val_loss: 1499.4972 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8151 - loss: 1507.2803 - val_RMSE: 38.7138 - val_loss: 1499.4309 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8192 - loss: 1507.6010 - val_RMSE: 38.7140 - val_loss: 1499.4523 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8156 - loss: 1507.3221 - val_RMSE: 38.7140 - val_loss: 1499.4518 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8118 - loss: 1507.0330 - val_RMSE: 38.7149 - val_loss: 1499.5189 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8178 - loss: 1507.5011 - val_RMSE: 38.7144 - val_loss: 1499.4840 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8098 - loss: 1506.8849 - val_RMSE: 38.7141 - val_loss: 1499.4712 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8145 - loss: 1507.2468 - val_RMSE: 38.7084 - val_loss: 1498.9991 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8040 - loss: 1506.4032 - val_RMSE: 38.7081 - val_loss: 1498.9456 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8022 - loss: 1506.2330 - val_RMSE: 38.7079 - val_loss: 1498.9111 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8066 - loss: 1506.5564 - val_RMSE: 38.7078 - val_loss: 1498.8834 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8003 - loss: 1506.0463 - val_RMSE: 38.7078 - val_loss: 1498.8658 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8064 - loss: 1506.5037 - val_RMSE: 38.7078 - val_loss: 1498.8503 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7963 - loss: 1505.7012 - val_RMSE: 38.7078 - val_loss: 1498.8289 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7990 - loss: 1505.8971 - val_RMSE: 38.7077 - val_loss: 1498.8143 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7989 - loss: 1505.8805 - val_RMSE: 38.7078 - val_loss: 1498.8075 - learning_rate: 1.0000e-04\n",
            "41608/41608  90s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  36s 10ms/step - RMSE: 54.2571 - loss: 3092.4207 - val_RMSE: 38.7283 - val_loss: 1500.2661 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0618 - loss: 1526.2323 - val_RMSE: 38.7137 - val_loss: 1499.2183 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9238 - loss: 1515.5464 - val_RMSE: 38.7104 - val_loss: 1499.0369 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8829 - loss: 1512.4448 - val_RMSE: 38.7091 - val_loss: 1499.0220 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8683 - loss: 1511.3890 - val_RMSE: 38.7083 - val_loss: 1499.0106 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8521 - loss: 1510.1748 - val_RMSE: 38.7077 - val_loss: 1498.9742 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8557 - loss: 1510.4541 - val_RMSE: 38.7066 - val_loss: 1498.8805 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8502 - loss: 1510.0190 - val_RMSE: 38.7059 - val_loss: 1498.8292 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8468 - loss: 1509.7466 - val_RMSE: 38.7065 - val_loss: 1498.8591 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8500 - loss: 1509.9894 - val_RMSE: 38.7048 - val_loss: 1498.7224 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8453 - loss: 1509.6240 - val_RMSE: 38.7048 - val_loss: 1498.7218 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8445 - loss: 1509.5668 - val_RMSE: 38.7035 - val_loss: 1498.6393 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8431 - loss: 1509.4688 - val_RMSE: 38.7042 - val_loss: 1498.6980 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8429 - loss: 1509.4601 - val_RMSE: 38.7042 - val_loss: 1498.7042 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8383 - loss: 1509.1041 - val_RMSE: 38.7028 - val_loss: 1498.6055 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8362 - loss: 1508.9556 - val_RMSE: 38.7037 - val_loss: 1498.6831 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8330 - loss: 1508.7144 - val_RMSE: 38.7025 - val_loss: 1498.5972 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8339 - loss: 1508.7906 - val_RMSE: 38.7024 - val_loss: 1498.5980 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8357 - loss: 1508.9348 - val_RMSE: 38.7022 - val_loss: 1498.5911 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8339 - loss: 1508.8019 - val_RMSE: 38.7027 - val_loss: 1498.6316 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8238 - loss: 1508.0271 - val_RMSE: 38.7012 - val_loss: 1498.5250 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8275 - loss: 1508.3156 - val_RMSE: 38.7011 - val_loss: 1498.5302 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8277 - loss: 1508.3470 - val_RMSE: 38.7032 - val_loss: 1498.7075 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8252 - loss: 1508.1738 - val_RMSE: 38.7020 - val_loss: 1498.6212 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8236 - loss: 1508.0454 - val_RMSE: 38.7028 - val_loss: 1498.6865 - learning_rate: 0.0010\n",
            "41608/41608  88s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 03:42:28,918] Trial 22 finished with value: 38.69640350341797 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002133354415296107, 'dropout_rate': 0.47090232042440616}. Best is trial 22 with value: 38.69640350341797.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.3926 - loss: 3108.2227 - val_RMSE: 38.7105 - val_loss: 1498.8347 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.1030 - loss: 1529.3966 - val_RMSE: 38.6939 - val_loss: 1497.6128 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9579 - loss: 1518.1373 - val_RMSE: 38.6908 - val_loss: 1497.4556 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9221 - loss: 1515.4268 - val_RMSE: 38.6872 - val_loss: 1497.2570 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9022 - loss: 1513.9501 - val_RMSE: 38.6860 - val_loss: 1497.2203 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8956 - loss: 1513.4907 - val_RMSE: 38.6842 - val_loss: 1497.1089 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8931 - loss: 1513.3197 - val_RMSE: 38.6840 - val_loss: 1497.1019 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8883 - loss: 1512.9509 - val_RMSE: 38.6836 - val_loss: 1497.0714 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8847 - loss: 1512.6777 - val_RMSE: 38.6841 - val_loss: 1497.1112 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8871 - loss: 1512.8662 - val_RMSE: 38.6830 - val_loss: 1497.0245 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8866 - loss: 1512.8291 - val_RMSE: 38.6824 - val_loss: 1496.9918 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8792 - loss: 1512.2565 - val_RMSE: 38.6822 - val_loss: 1496.9862 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8865 - loss: 1512.8339 - val_RMSE: 38.6813 - val_loss: 1496.9243 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8736 - loss: 1511.8363 - val_RMSE: 38.6814 - val_loss: 1496.9330 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8801 - loss: 1512.3489 - val_RMSE: 38.6810 - val_loss: 1496.9075 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8719 - loss: 1511.7194 - val_RMSE: 38.6807 - val_loss: 1496.8986 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8725 - loss: 1511.7711 - val_RMSE: 38.6801 - val_loss: 1496.8505 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8668 - loss: 1511.3331 - val_RMSE: 38.6802 - val_loss: 1496.8661 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8687 - loss: 1511.4895 - val_RMSE: 38.6802 - val_loss: 1496.8788 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8725 - loss: 1511.7947 - val_RMSE: 38.6797 - val_loss: 1496.8425 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8640 - loss: 1511.1376 - val_RMSE: 38.6804 - val_loss: 1496.9043 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8661 - loss: 1511.3091 - val_RMSE: 38.6798 - val_loss: 1496.8618 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8657 - loss: 1511.2834 - val_RMSE: 38.6800 - val_loss: 1496.8942 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8597 - loss: 1510.8302 - val_RMSE: 38.6799 - val_loss: 1496.8971 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8606 - loss: 1510.9110 - val_RMSE: 38.6797 - val_loss: 1496.8875 - learning_rate: 0.0010\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.3526 - loss: 3103.1470 - val_RMSE: 38.7315 - val_loss: 1500.4688 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0578 - loss: 1525.8635 - val_RMSE: 38.7208 - val_loss: 1499.7095 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9140 - loss: 1514.7240 - val_RMSE: 38.7194 - val_loss: 1499.6794 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8650 - loss: 1510.9910 - val_RMSE: 38.7180 - val_loss: 1499.6410 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8501 - loss: 1509.9091 - val_RMSE: 38.7170 - val_loss: 1499.6246 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8459 - loss: 1509.6292 - val_RMSE: 38.7166 - val_loss: 1499.6102 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8440 - loss: 1509.4971 - val_RMSE: 38.7168 - val_loss: 1499.6326 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8422 - loss: 1509.3621 - val_RMSE: 38.7165 - val_loss: 1499.6129 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8419 - loss: 1509.3339 - val_RMSE: 38.7151 - val_loss: 1499.5034 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8336 - loss: 1508.6904 - val_RMSE: 38.7153 - val_loss: 1499.5222 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8306 - loss: 1508.4633 - val_RMSE: 38.7148 - val_loss: 1499.4825 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8329 - loss: 1508.6409 - val_RMSE: 38.7149 - val_loss: 1499.4979 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8289 - loss: 1508.3391 - val_RMSE: 38.7149 - val_loss: 1499.5079 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8249 - loss: 1508.0361 - val_RMSE: 38.7153 - val_loss: 1499.5428 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8320 - loss: 1508.5900 - val_RMSE: 38.7147 - val_loss: 1499.5044 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8220 - loss: 1507.8259 - val_RMSE: 38.7139 - val_loss: 1499.4518 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8304 - loss: 1508.4857 - val_RMSE: 38.7136 - val_loss: 1499.4414 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8219 - loss: 1507.8411 - val_RMSE: 38.7153 - val_loss: 1499.5741 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8208 - loss: 1507.7557 - val_RMSE: 38.7149 - val_loss: 1499.5541 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8251 - loss: 1508.1027 - val_RMSE: 38.7133 - val_loss: 1499.4435 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8171 - loss: 1507.4922 - val_RMSE: 38.7144 - val_loss: 1499.5375 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8227 - loss: 1507.9366 - val_RMSE: 38.7145 - val_loss: 1499.5503 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8077 - loss: 1506.7759 - val_RMSE: 38.7085 - val_loss: 1499.0688 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8092 - loss: 1506.8654 - val_RMSE: 38.7085 - val_loss: 1499.0408 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8088 - loss: 1506.8107 - val_RMSE: 38.7085 - val_loss: 1499.0178 - learning_rate: 1.0000e-04\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.3420 - loss: 3101.5876 - val_RMSE: 38.7273 - val_loss: 1500.1343 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0831 - loss: 1527.8464 - val_RMSE: 38.7149 - val_loss: 1499.2526 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9375 - loss: 1516.5575 - val_RMSE: 38.7110 - val_loss: 1499.0278 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8972 - loss: 1513.5001 - val_RMSE: 38.7095 - val_loss: 1498.9880 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8800 - loss: 1512.2302 - val_RMSE: 38.7090 - val_loss: 1499.0065 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8669 - loss: 1511.2573 - val_RMSE: 38.7081 - val_loss: 1498.9573 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8720 - loss: 1511.6740 - val_RMSE: 38.7076 - val_loss: 1498.9209 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8660 - loss: 1511.2126 - val_RMSE: 38.7062 - val_loss: 1498.8158 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8617 - loss: 1510.8754 - val_RMSE: 38.7073 - val_loss: 1498.8992 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8653 - loss: 1511.1545 - val_RMSE: 38.7055 - val_loss: 1498.7643 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8594 - loss: 1510.7045 - val_RMSE: 38.7061 - val_loss: 1498.8151 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8592 - loss: 1510.6851 - val_RMSE: 38.7047 - val_loss: 1498.7052 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8546 - loss: 1510.3394 - val_RMSE: 38.7050 - val_loss: 1498.7295 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8552 - loss: 1510.3861 - val_RMSE: 38.7035 - val_loss: 1498.6238 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8519 - loss: 1510.1377 - val_RMSE: 38.7040 - val_loss: 1498.6747 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8495 - loss: 1509.9532 - val_RMSE: 38.7043 - val_loss: 1498.6893 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8494 - loss: 1509.9414 - val_RMSE: 38.7031 - val_loss: 1498.6071 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8474 - loss: 1509.7963 - val_RMSE: 38.7037 - val_loss: 1498.6588 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8482 - loss: 1509.8666 - val_RMSE: 38.7033 - val_loss: 1498.6365 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8471 - loss: 1509.7942 - val_RMSE: 38.7037 - val_loss: 1498.6721 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8384 - loss: 1509.1168 - val_RMSE: 38.7041 - val_loss: 1498.7097 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8426 - loss: 1509.4498 - val_RMSE: 38.7027 - val_loss: 1498.6167 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8352 - loss: 1508.8816 - val_RMSE: 38.7016 - val_loss: 1498.5007 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8333 - loss: 1508.7089 - val_RMSE: 38.7016 - val_loss: 1498.4830 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8312 - loss: 1508.5260 - val_RMSE: 38.7016 - val_loss: 1498.4552 - learning_rate: 1.0000e-04\n",
            "41608/41608  92s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 04:14:41,087] Trial 23 finished with value: 38.69656880696615 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00018285344196860296, 'dropout_rate': 0.4958852821505541}. Best is trial 22 with value: 38.69640350341797.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  35s 10ms/step - RMSE: 54.3742 - loss: 3106.7458 - val_RMSE: 38.7131 - val_loss: 1499.6084 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.1008 - loss: 1529.8162 - val_RMSE: 38.6954 - val_loss: 1498.3958 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9560 - loss: 1518.6693 - val_RMSE: 38.6901 - val_loss: 1498.1415 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9177 - loss: 1515.8304 - val_RMSE: 38.6869 - val_loss: 1497.9764 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8996 - loss: 1514.4752 - val_RMSE: 38.6867 - val_loss: 1497.9011 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8926 - loss: 1513.8458 - val_RMSE: 38.6844 - val_loss: 1497.5876 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8902 - loss: 1513.5194 - val_RMSE: 38.6845 - val_loss: 1497.4938 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8858 - loss: 1513.0938 - val_RMSE: 38.6845 - val_loss: 1497.4396 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8831 - loss: 1512.8446 - val_RMSE: 38.6848 - val_loss: 1497.4486 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8842 - loss: 1512.9211 - val_RMSE: 38.6838 - val_loss: 1497.3635 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8854 - loss: 1513.0110 - val_RMSE: 38.6829 - val_loss: 1497.3047 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8781 - loss: 1512.4506 - val_RMSE: 38.6824 - val_loss: 1497.2651 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8855 - loss: 1513.0215 - val_RMSE: 38.6829 - val_loss: 1497.3037 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8733 - loss: 1512.0736 - val_RMSE: 38.6833 - val_loss: 1497.3385 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8791 - loss: 1512.5287 - val_RMSE: 38.6826 - val_loss: 1497.2965 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8707 - loss: 1511.8906 - val_RMSE: 38.6824 - val_loss: 1497.2834 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8723 - loss: 1512.0159 - val_RMSE: 38.6818 - val_loss: 1497.2421 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8676 - loss: 1511.6558 - val_RMSE: 38.6817 - val_loss: 1497.2469 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8686 - loss: 1511.7430 - val_RMSE: 38.6813 - val_loss: 1497.2260 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8722 - loss: 1512.0331 - val_RMSE: 38.6811 - val_loss: 1497.2048 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8653 - loss: 1511.4901 - val_RMSE: 38.6810 - val_loss: 1497.2039 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8659 - loss: 1511.5535 - val_RMSE: 38.6806 - val_loss: 1497.1833 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8669 - loss: 1511.6404 - val_RMSE: 38.6802 - val_loss: 1497.1604 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8607 - loss: 1511.1592 - val_RMSE: 38.6815 - val_loss: 1497.2582 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8617 - loss: 1511.2428 - val_RMSE: 38.6810 - val_loss: 1497.2234 - learning_rate: 0.0010\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.3294 - loss: 3101.0984 - val_RMSE: 38.7362 - val_loss: 1501.3876 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0573 - loss: 1526.4099 - val_RMSE: 38.7201 - val_loss: 1500.3046 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9122 - loss: 1515.2542 - val_RMSE: 38.7197 - val_loss: 1500.4049 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8611 - loss: 1511.3992 - val_RMSE: 38.7180 - val_loss: 1500.3362 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8472 - loss: 1510.3560 - val_RMSE: 38.7171 - val_loss: 1500.2311 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8411 - loss: 1509.8203 - val_RMSE: 38.7167 - val_loss: 1500.0773 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8396 - loss: 1509.5865 - val_RMSE: 38.7171 - val_loss: 1500.0164 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8389 - loss: 1509.4502 - val_RMSE: 38.7165 - val_loss: 1499.9355 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8384 - loss: 1509.3826 - val_RMSE: 38.7153 - val_loss: 1499.8303 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8317 - loss: 1508.8485 - val_RMSE: 38.7155 - val_loss: 1499.8304 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8283 - loss: 1508.5773 - val_RMSE: 38.7146 - val_loss: 1499.7714 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8303 - loss: 1508.7450 - val_RMSE: 38.7148 - val_loss: 1499.7863 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8264 - loss: 1508.4324 - val_RMSE: 38.7155 - val_loss: 1499.8446 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8240 - loss: 1508.2527 - val_RMSE: 38.7147 - val_loss: 1499.7885 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8291 - loss: 1508.6553 - val_RMSE: 38.7144 - val_loss: 1499.7731 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8208 - loss: 1508.0199 - val_RMSE: 38.7144 - val_loss: 1499.7777 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8248 - loss: 1508.3191 - val_RMSE: 38.7086 - val_loss: 1499.2599 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8143 - loss: 1507.4417 - val_RMSE: 38.7083 - val_loss: 1499.1759 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8137 - loss: 1507.3374 - val_RMSE: 38.7080 - val_loss: 1499.1064 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8183 - loss: 1507.6465 - val_RMSE: 38.7079 - val_loss: 1499.0591 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8108 - loss: 1507.0297 - val_RMSE: 38.7079 - val_loss: 1499.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8181 - loss: 1507.5647 - val_RMSE: 38.7078 - val_loss: 1498.9885 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8066 - loss: 1506.6410 - val_RMSE: 38.7078 - val_loss: 1498.9668 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8099 - loss: 1506.8774 - val_RMSE: 38.7076 - val_loss: 1498.9280 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8096 - loss: 1506.8282 - val_RMSE: 38.7078 - val_loss: 1498.9246 - learning_rate: 1.0000e-04\n",
            "41608/41608  95s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.3186 - loss: 3099.4873 - val_RMSE: 38.7199 - val_loss: 1500.1370 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0769 - loss: 1527.9541 - val_RMSE: 38.7163 - val_loss: 1500.0200 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9302 - loss: 1516.6616 - val_RMSE: 38.7110 - val_loss: 1499.7379 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8936 - loss: 1513.9352 - val_RMSE: 38.7093 - val_loss: 1499.6638 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8770 - loss: 1512.6691 - val_RMSE: 38.7089 - val_loss: 1499.5791 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8645 - loss: 1511.6184 - val_RMSE: 38.7080 - val_loss: 1499.3773 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8692 - loss: 1511.8556 - val_RMSE: 38.7076 - val_loss: 1499.2562 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8627 - loss: 1511.2736 - val_RMSE: 38.7066 - val_loss: 1499.1486 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8591 - loss: 1510.9698 - val_RMSE: 38.7068 - val_loss: 1499.1526 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8616 - loss: 1511.1550 - val_RMSE: 38.7061 - val_loss: 1499.0885 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8573 - loss: 1510.8187 - val_RMSE: 38.7054 - val_loss: 1499.0260 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8564 - loss: 1510.7313 - val_RMSE: 38.7052 - val_loss: 1499.0048 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8546 - loss: 1510.5918 - val_RMSE: 38.7054 - val_loss: 1499.0148 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8536 - loss: 1510.5154 - val_RMSE: 38.7049 - val_loss: 1498.9822 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8512 - loss: 1510.3356 - val_RMSE: 38.7045 - val_loss: 1498.9658 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8495 - loss: 1510.2122 - val_RMSE: 38.7049 - val_loss: 1498.9963 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8469 - loss: 1510.0051 - val_RMSE: 38.7044 - val_loss: 1498.9785 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8467 - loss: 1510.0103 - val_RMSE: 38.7047 - val_loss: 1498.9928 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8482 - loss: 1510.1261 - val_RMSE: 38.7041 - val_loss: 1498.9618 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8469 - loss: 1510.0453 - val_RMSE: 38.7041 - val_loss: 1498.9664 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8389 - loss: 1509.4177 - val_RMSE: 38.7027 - val_loss: 1498.8628 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8409 - loss: 1509.5818 - val_RMSE: 38.7025 - val_loss: 1498.8490 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8395 - loss: 1509.4774 - val_RMSE: 38.7042 - val_loss: 1498.9983 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8377 - loss: 1509.3455 - val_RMSE: 38.7028 - val_loss: 1498.8794 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8365 - loss: 1509.2500 - val_RMSE: 38.7032 - val_loss: 1498.9156 - learning_rate: 0.0010\n",
            "41608/41608  88s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 04:47:00,751] Trial 24 finished with value: 38.69732538859049 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0005025556166064429, 'dropout_rate': 0.48984751751659333}. Best is trial 22 with value: 38.69640350341797.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  36s 10ms/step - RMSE: 54.4057 - loss: 3109.7021 - val_RMSE: 38.7136 - val_loss: 1499.1035 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.1025 - loss: 1529.3859 - val_RMSE: 38.6958 - val_loss: 1497.7939 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9640 - loss: 1518.6431 - val_RMSE: 38.6897 - val_loss: 1497.4073 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9243 - loss: 1515.6382 - val_RMSE: 38.6868 - val_loss: 1497.2644 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9051 - loss: 1514.2286 - val_RMSE: 38.6861 - val_loss: 1497.2644 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8989 - loss: 1513.7876 - val_RMSE: 38.6842 - val_loss: 1497.1451 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8961 - loss: 1513.5814 - val_RMSE: 38.6843 - val_loss: 1497.1528 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8911 - loss: 1513.1978 - val_RMSE: 38.6838 - val_loss: 1497.1125 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8882 - loss: 1512.9745 - val_RMSE: 38.6836 - val_loss: 1497.1016 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8899 - loss: 1513.1068 - val_RMSE: 38.6831 - val_loss: 1497.0591 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8898 - loss: 1513.0931 - val_RMSE: 38.6827 - val_loss: 1497.0402 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8824 - loss: 1512.5287 - val_RMSE: 38.6820 - val_loss: 1496.9803 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8896 - loss: 1513.0913 - val_RMSE: 38.6819 - val_loss: 1496.9784 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8778 - loss: 1512.1780 - val_RMSE: 38.6817 - val_loss: 1496.9678 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8827 - loss: 1512.5575 - val_RMSE: 38.6816 - val_loss: 1496.9596 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8761 - loss: 1512.0461 - val_RMSE: 38.6818 - val_loss: 1496.9812 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8759 - loss: 1512.0347 - val_RMSE: 38.6814 - val_loss: 1496.9561 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8700 - loss: 1511.5802 - val_RMSE: 38.6813 - val_loss: 1496.9491 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8735 - loss: 1511.8597 - val_RMSE: 38.6813 - val_loss: 1496.9619 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8761 - loss: 1512.0709 - val_RMSE: 38.6808 - val_loss: 1496.9270 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8669 - loss: 1511.3622 - val_RMSE: 38.6802 - val_loss: 1496.8854 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8698 - loss: 1511.5920 - val_RMSE: 38.6809 - val_loss: 1496.9432 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8692 - loss: 1511.5511 - val_RMSE: 38.6814 - val_loss: 1496.9891 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8635 - loss: 1511.1118 - val_RMSE: 38.6807 - val_loss: 1496.9469 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8641 - loss: 1511.1655 - val_RMSE: 38.6801 - val_loss: 1496.9037 - learning_rate: 0.0010\n",
            "41608/41608  87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.3674 - loss: 3104.8064 - val_RMSE: 38.7339 - val_loss: 1500.6754 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0589 - loss: 1525.9745 - val_RMSE: 38.7232 - val_loss: 1499.9159 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9153 - loss: 1514.8567 - val_RMSE: 38.7201 - val_loss: 1499.7589 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "1923/2601  4s 7ms/step - RMSE: 38.8621 - loss: 1510.7944"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-16 04:59:10,475] Trial 25 failed with parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00019662830318865977, 'dropout_rate': 0.49978399797646733} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-54-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-53-bcff8c4fc905>\", line 51, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 132, in call_function\n",
            "    function = trace_function(\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n",
            "    concrete_function = _maybe_define_function(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 239, in _maybe_define_function\n",
            "    concrete_function = tracing_options.function_cache.lookup(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\", line 50, in lookup\n",
            "    return self._primary[(context, dispatch_type)]\n",
            "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 456, in __hash__\n",
            "    return hash((tuple(self.parameters.items()), tuple(self.captures.items())))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 893, in __hash__\n",
            "    def __hash__(self):\n",
            "    \n",
            "KeyboardInterrupt\n",
            "[W 2025-02-16 04:59:10,478] Trial 25 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-bcff8c4fc905>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     52\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   function = trace_function(\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     concrete_function = tracing_options.function_cache.lookup(\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_func_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, function_type, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mdispatch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdispatch_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_primary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m         type(self).__name__, self.shape, repr(self.dtype), repr(self.name))\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 22 finished with value: 38.69640350341797\n",
        "* parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002133354415296107, 'dropout_rate': 0.47090232042440616}. Best is trial 22 with value: 38.69640350341797."
      ],
      "metadata": {
        "id": "dwbBHeboznPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9",
        "id": "ayypCUhQQlNh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QaE2SucWPmo"
      },
      "source": [
        "#### **4.6.3 NeuralNetwork v2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "188ce703-686f-421a-9f60-16c93ddd74f5",
        "id": "A9WMnacSWPms"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "3817382      5         4     3             8                   1           1   \n",
              "3390957      3         1     0             4                   1           2   \n",
              "2083310      0         0     1             2                   1           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "3817382      3      3              0.844471  0.725961  1.019273 -1.055655   \n",
              "3390957      3      2              0.618669 -0.149067  0.044250  0.216236   \n",
              "2083310      0      1             -0.588908 -0.824154  0.483511  0.987293   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "3817382           0               0  \n",
              "3390957           0               0  \n",
              "2083310           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2eda4f68-2010-473b-88f1-48a6eae0b4b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3817382</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.844471</td>\n",
              "      <td>0.725961</td>\n",
              "      <td>1.019273</td>\n",
              "      <td>-1.055655</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3390957</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.618669</td>\n",
              "      <td>-0.149067</td>\n",
              "      <td>0.044250</td>\n",
              "      <td>0.216236</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2083310</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.588908</td>\n",
              "      <td>-0.824154</td>\n",
              "      <td>0.483511</td>\n",
              "      <td>0.987293</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2eda4f68-2010-473b-88f1-48a6eae0b4b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2eda4f68-2010-473b-88f1-48a6eae0b4b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2eda4f68-2010-473b-88f1-48a6eae0b4b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7029e872-3655-4aa8-99b1-95e158a855a2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7029e872-3655-4aa8-99b1-95e158a855a2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7029e872-3655-4aa8-99b1-95e158a855a2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 8,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          8,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.8444705605506897\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.725960910320282\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0192734003067017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.0556546449661255\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512, last_layer=1, activation=\"relu\",repeat_att=2,dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001): # Reduced transformer_units\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    transformer_outputs = [] # List to store transformer outputs for each categorical feature\n",
        "\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:, j])\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        embs.append(x)\n",
        "\n",
        "        # Reshape for Transformer (batch_size, 1, features) - Crucial!\n",
        "        reshaped_embedding = layers.Reshape((1, -1))(x)\n",
        "\n",
        "        # Transformer Layer for each categorical feature\n",
        "        for q in list(range(repeat_att)):\n",
        "          if q == 0:\n",
        "            attention_output = reshaped_embedding\n",
        "\n",
        "          attention_output_ = layers.MultiHeadAttention(num_heads=num_transformer_heads, key_dim=transformer_units,name=f\"mh_{j}_{q}\")(attention_output, attention_output)\n",
        "          attention_output_ = layers.LayerNormalization(name=f\"mh_ln1_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "          attention_output_ = layers.Dense(reshaped_embedding.shape[-1], activation=activation,name=f\"mh_dense_{j}_{q}\")(attention_output_)\n",
        "          attention_output = layers.LayerNormalization(name=f\"mh_ln2_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "\n",
        "        transformer_outputs.append(layers.Flatten()(attention_output)) # Store flattened transformer output\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    x_orig = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    reshaped_features = layers.Reshape((1, -1))(x_orig)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x_orig, flattened_attention])\n",
        "\n",
        "    # Concatenate Transformer outputs and numerical features\n",
        "    all_features = layers.Concatenate(axis=-1)(transformer_outputs + [x])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(all_features)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    #x = layers.Concatenate(axis=-1)([x_orig, x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "T7x-clb6WPms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in range(1):\n",
        "  print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX6xpQsF-vsl",
        "outputId": "325859c0-bfb5-43c8-bf8a-d3592d731d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model()\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "g0C5K7thWPmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(mod_test, show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"TB\")"
      ],
      "metadata": {
        "id": "xsUPc7xAb4zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "id": "8cmiDeT2WPmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D61nTPd2WPmu"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69Nrn1WWPmu"
      },
      "outputs": [],
      "source": [
        "# categorical_feat = t.cat_features.copy()\n",
        "# numerical_feat = t.num_features.copy()\n",
        "\n",
        "# X_train_cat = X_enc[categorical_feat]\n",
        "# X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "# X_test_cat = test_enc[categorical_feat]\n",
        "# X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "# X_train_cat.info()\n",
        "# X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsrSslecWPmu"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "#(units=512, last_layer=1, activation=\"relu\", dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001)\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': 0.0001, #trial.suggest_float('reg', 1e-4, 0.1, log=True),\n",
        "              \"num_transformer_heads\": trial.suggest_int(\"num_transformer_heads\", 2, 4),\n",
        "              \"transformer_units\": trial.suggest_int(\"transformer_units\", 32, 128,step=32),\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.51,step=0.03),\n",
        "              'repeat_att': trial.suggest_categorical('repeat_att', [1,2]),\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num], batch_size=1024)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHngrYKwWPmu"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57a94819-3390-4ad0-a394-174ce31ca217",
        "id": "ghy_gPSMWPmv"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 00:46:45,620] A new study created in memory with name: no-name-dc44f225-28b0-4ec0-8c91-f67686b2cd34\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  149s 35ms/step - RMSE: 42.5423 - loss: 1853.1946 - val_RMSE: 38.7149 - val_loss: 1499.9546 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9247 - loss: 1516.3540 - val_RMSE: 38.7003 - val_loss: 1499.3855 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9184 - loss: 1516.5168 - val_RMSE: 38.7284 - val_loss: 1502.1870 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9076 - loss: 1516.2740 - val_RMSE: 38.7073 - val_loss: 1500.9421 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8804 - loss: 1514.1544 - val_RMSE: 38.6908 - val_loss: 1498.8295 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8641 - loss: 1512.1082 - val_RMSE: 38.6891 - val_loss: 1498.1650 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8571 - loss: 1511.1117 - val_RMSE: 38.6884 - val_loss: 1497.8333 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8573 - loss: 1510.8917 - val_RMSE: 38.6869 - val_loss: 1497.5610 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8491 - loss: 1510.1136 - val_RMSE: 38.6872 - val_loss: 1497.4873 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8484 - loss: 1509.9669 - val_RMSE: 38.6857 - val_loss: 1497.3024 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8528 - loss: 1510.2468 - val_RMSE: 38.6860 - val_loss: 1497.2677 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8579 - loss: 1510.5892 - val_RMSE: 38.6851 - val_loss: 1497.1603 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8491 - loss: 1509.8719 - val_RMSE: 38.6859 - val_loss: 1497.1989 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8485 - loss: 1509.7949 - val_RMSE: 38.6871 - val_loss: 1497.2744 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8413 - loss: 1509.2239 - val_RMSE: 38.6854 - val_loss: 1497.1283 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8417 - loss: 1509.2438 - val_RMSE: 38.6848 - val_loss: 1497.0713 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8416 - loss: 1509.2209 - val_RMSE: 38.6846 - val_loss: 1497.0367 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8447 - loss: 1509.4489 - val_RMSE: 38.6842 - val_loss: 1496.9976 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8436 - loss: 1509.3550 - val_RMSE: 38.6838 - val_loss: 1496.9580 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8409 - loss: 1509.1313 - val_RMSE: 38.6839 - val_loss: 1496.9517 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8394 - loss: 1509.0048 - val_RMSE: 38.6835 - val_loss: 1496.9149 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8418 - loss: 1509.1865 - val_RMSE: 38.6834 - val_loss: 1496.9009 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8338 - loss: 1508.5527 - val_RMSE: 38.6832 - val_loss: 1496.8790 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8419 - loss: 1509.1719 - val_RMSE: 38.6832 - val_loss: 1496.8655 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8380 - loss: 1508.8633 - val_RMSE: 38.6831 - val_loss: 1496.8566 - learning_rate: 1.0000e-04\n",
            "1301/1301  18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  152s 35ms/step - RMSE: 42.4635 - loss: 1845.5852 - val_RMSE: 38.7378 - val_loss: 1501.7828 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8727 - loss: 1512.3889 - val_RMSE: 38.7483 - val_loss: 1503.2280 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8710 - loss: 1512.9287 - val_RMSE: 38.7473 - val_loss: 1503.9238 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8331 - loss: 1510.4143 - val_RMSE: 38.7256 - val_loss: 1501.5660 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8221 - loss: 1508.8901 - val_RMSE: 38.7289 - val_loss: 1501.3015 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8085 - loss: 1507.3918 - val_RMSE: 38.7245 - val_loss: 1500.6874 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8097 - loss: 1507.2500 - val_RMSE: 38.7236 - val_loss: 1500.4556 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8165 - loss: 1507.6310 - val_RMSE: 38.7238 - val_loss: 1500.3698 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8052 - loss: 1506.6687 - val_RMSE: 38.7224 - val_loss: 1500.1976 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8041 - loss: 1506.5164 - val_RMSE: 38.7234 - val_loss: 1500.2178 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8052 - loss: 1506.5483 - val_RMSE: 38.7254 - val_loss: 1500.3353 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8051 - loss: 1506.5110 - val_RMSE: 38.7175 - val_loss: 1499.7087 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8017 - loss: 1506.2274 - val_RMSE: 38.7176 - val_loss: 1499.6948 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.7980 - loss: 1505.9227 - val_RMSE: 38.7168 - val_loss: 1499.6138 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.7999 - loss: 1506.0510 - val_RMSE: 38.7167 - val_loss: 1499.5980 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8006 - loss: 1506.0966 - val_RMSE: 38.7167 - val_loss: 1499.5845 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.7930 - loss: 1505.4945 - val_RMSE: 38.7163 - val_loss: 1499.5337 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7964 - loss: 1505.7448 - val_RMSE: 38.7163 - val_loss: 1499.5311 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8012 - loss: 1506.1034 - val_RMSE: 38.7164 - val_loss: 1499.5283 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8004 - loss: 1506.0343 - val_RMSE: 38.7161 - val_loss: 1499.4877 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7983 - loss: 1505.8585 - val_RMSE: 38.7167 - val_loss: 1499.5251 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.7916 - loss: 1505.3295 - val_RMSE: 38.7166 - val_loss: 1499.5121 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7955 - loss: 1505.6284 - val_RMSE: 38.7167 - val_loss: 1499.5171 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7963 - loss: 1505.6843 - val_RMSE: 38.7165 - val_loss: 1499.4994 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7963 - loss: 1505.6914 - val_RMSE: 38.7164 - val_loss: 1499.4944 - learning_rate: 1.0000e-06\n",
            "1301/1301  16s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  149s 35ms/step - RMSE: 42.4441 - loss: 1843.2596 - val_RMSE: 38.7304 - val_loss: 1501.2000 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.9038 - loss: 1514.7972 - val_RMSE: 38.7187 - val_loss: 1500.8505 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8969 - loss: 1514.8219 - val_RMSE: 38.7294 - val_loss: 1502.3921 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8973 - loss: 1515.5651 - val_RMSE: 38.7309 - val_loss: 1503.0609 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8497 - loss: 1512.0414 - val_RMSE: 38.7132 - val_loss: 1500.7770 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8386 - loss: 1510.3381 - val_RMSE: 38.7125 - val_loss: 1500.1444 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8348 - loss: 1509.5366 - val_RMSE: 38.7120 - val_loss: 1499.7932 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8308 - loss: 1508.9564 - val_RMSE: 38.7135 - val_loss: 1499.7236 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8265 - loss: 1508.4512 - val_RMSE: 38.7110 - val_loss: 1499.4092 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  57s 22ms/step - RMSE: 38.8278 - loss: 1508.4470 - val_RMSE: 38.7111 - val_loss: 1499.3374 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8293 - loss: 1508.4926 - val_RMSE: 38.7108 - val_loss: 1499.2607 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8343 - loss: 1508.8291 - val_RMSE: 38.7101 - val_loss: 1499.1632 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8220 - loss: 1507.8356 - val_RMSE: 38.7096 - val_loss: 1499.0898 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8270 - loss: 1508.1875 - val_RMSE: 38.7110 - val_loss: 1499.1808 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8256 - loss: 1508.0579 - val_RMSE: 38.7101 - val_loss: 1499.0874 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8245 - loss: 1507.9469 - val_RMSE: 38.7112 - val_loss: 1499.1582 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  57s 22ms/step - RMSE: 38.8216 - loss: 1507.7120 - val_RMSE: 38.7095 - val_loss: 1499.0039 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8235 - loss: 1507.8420 - val_RMSE: 38.7105 - val_loss: 1499.0677 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8223 - loss: 1507.7360 - val_RMSE: 38.7091 - val_loss: 1498.9581 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8203 - loss: 1507.5741 - val_RMSE: 38.7125 - val_loss: 1499.2124 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8198 - loss: 1507.5265 - val_RMSE: 38.7109 - val_loss: 1499.0803 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8171 - loss: 1507.3075 - val_RMSE: 38.7096 - val_loss: 1498.9631 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8114 - loss: 1506.8531 - val_RMSE: 38.7094 - val_loss: 1498.9340 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  57s 22ms/step - RMSE: 38.8078 - loss: 1506.5636 - val_RMSE: 38.7092 - val_loss: 1498.9062 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8137 - loss: 1507.0103 - val_RMSE: 38.7090 - val_loss: 1498.8834 - learning_rate: 1.0000e-04\n",
            "1301/1301  16s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 02:06:26,496] Trial 0 finished with value: 38.70286560058594 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.44999999999999996, 'repeat_att': 1}. Best is trial 0 with value: 38.70286560058594.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  154s 35ms/step - RMSE: 43.3852 - loss: 1934.7306 - val_RMSE: 38.7027 - val_loss: 1498.9362 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  60s 23ms/step - RMSE: 39.0693 - loss: 1527.6146 - val_RMSE: 38.6950 - val_loss: 1498.9910 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 39.0576 - loss: 1527.3376 - val_RMSE: 38.6957 - val_loss: 1499.5364 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9966 - loss: 1522.7792 - val_RMSE: 38.6897 - val_loss: 1498.5286 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9863 - loss: 1521.4486 - val_RMSE: 38.6876 - val_loss: 1497.9673 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9687 - loss: 1519.7284 - val_RMSE: 38.6857 - val_loss: 1497.5868 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9707 - loss: 1519.6841 - val_RMSE: 38.6867 - val_loss: 1497.5294 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9633 - loss: 1518.9814 - val_RMSE: 38.6871 - val_loss: 1497.4590 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9621 - loss: 1518.7979 - val_RMSE: 38.6859 - val_loss: 1497.2992 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9610 - loss: 1518.6493 - val_RMSE: 38.6857 - val_loss: 1497.2311 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9632 - loss: 1518.7689 - val_RMSE: 38.6860 - val_loss: 1497.2183 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9496 - loss: 1517.6786 - val_RMSE: 38.6855 - val_loss: 1497.1537 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9574 - loss: 1518.2554 - val_RMSE: 38.6853 - val_loss: 1497.1129 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9451 - loss: 1517.2749 - val_RMSE: 38.6860 - val_loss: 1497.1459 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9440 - loss: 1517.1774 - val_RMSE: 38.6850 - val_loss: 1497.0625 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9448 - loss: 1517.2255 - val_RMSE: 38.6860 - val_loss: 1497.1222 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9347 - loss: 1516.4252 - val_RMSE: 38.6840 - val_loss: 1496.9670 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9362 - loss: 1516.5424 - val_RMSE: 38.6845 - val_loss: 1496.9954 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9342 - loss: 1516.3783 - val_RMSE: 38.6845 - val_loss: 1496.9844 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9261 - loss: 1515.7404 - val_RMSE: 38.6839 - val_loss: 1496.9280 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9264 - loss: 1515.7483 - val_RMSE: 38.6836 - val_loss: 1496.8975 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9230 - loss: 1515.4722 - val_RMSE: 38.6835 - val_loss: 1496.8818 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9199 - loss: 1515.2224 - val_RMSE: 38.6836 - val_loss: 1496.8789 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9283 - loss: 1515.8676 - val_RMSE: 38.6840 - val_loss: 1496.9043 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9230 - loss: 1515.4517 - val_RMSE: 38.6840 - val_loss: 1496.8937 - learning_rate: 1.0000e-04\n",
            "1301/1301  16s 9ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  147s 35ms/step - RMSE: 43.3502 - loss: 1931.5552 - val_RMSE: 38.7316 - val_loss: 1501.1777 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  60s 23ms/step - RMSE: 39.0198 - loss: 1523.7283 - val_RMSE: 38.7399 - val_loss: 1502.3658 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9995 - loss: 1522.7172 - val_RMSE: 38.7387 - val_loss: 1502.7998 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9452 - loss: 1518.7039 - val_RMSE: 38.7208 - val_loss: 1500.8723 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9316 - loss: 1517.1335 - val_RMSE: 38.7213 - val_loss: 1500.5249 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9314 - loss: 1516.7751 - val_RMSE: 38.7199 - val_loss: 1500.1968 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9249 - loss: 1516.0735 - val_RMSE: 38.7192 - val_loss: 1500.0155 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9205 - loss: 1515.6187 - val_RMSE: 38.7187 - val_loss: 1499.8903 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9211 - loss: 1515.5809 - val_RMSE: 38.7194 - val_loss: 1499.8732 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9138 - loss: 1514.9574 - val_RMSE: 38.7185 - val_loss: 1499.7616 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9080 - loss: 1514.4657 - val_RMSE: 38.7178 - val_loss: 1499.6732 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9072 - loss: 1514.3712 - val_RMSE: 38.7176 - val_loss: 1499.6364 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.9051 - loss: 1514.1798 - val_RMSE: 38.7188 - val_loss: 1499.7069 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8932 - loss: 1513.2394 - val_RMSE: 38.7182 - val_loss: 1499.6433 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8962 - loss: 1513.4537 - val_RMSE: 38.7157 - val_loss: 1499.4407 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8876 - loss: 1512.7773 - val_RMSE: 38.7159 - val_loss: 1499.4401 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8815 - loss: 1512.2913 - val_RMSE: 38.7154 - val_loss: 1499.3939 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8888 - loss: 1512.8477 - val_RMSE: 38.7155 - val_loss: 1499.3929 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8924 - loss: 1513.1184 - val_RMSE: 38.7154 - val_loss: 1499.3776 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8854 - loss: 1512.5703 - val_RMSE: 38.7153 - val_loss: 1499.3597 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8868 - loss: 1512.6654 - val_RMSE: 38.7155 - val_loss: 1499.3667 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8918 - loss: 1513.0470 - val_RMSE: 38.7155 - val_loss: 1499.3618 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8939 - loss: 1513.2028 - val_RMSE: 38.7155 - val_loss: 1499.3582 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8927 - loss: 1513.1119 - val_RMSE: 38.7155 - val_loss: 1499.3569 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8863 - loss: 1512.6150 - val_RMSE: 38.7154 - val_loss: 1499.3484 - learning_rate: 1.0000e-05\n",
            "1301/1301  17s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  157s 36ms/step - RMSE: 43.2625 - loss: 1922.0507 - val_RMSE: 38.7210 - val_loss: 1500.3311 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  62s 24ms/step - RMSE: 39.0474 - loss: 1525.8640 - val_RMSE: 38.7199 - val_loss: 1500.7574 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  61s 23ms/step - RMSE: 39.0306 - loss: 1525.0643 - val_RMSE: 38.7252 - val_loss: 1501.7371 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9748 - loss: 1520.9982 - val_RMSE: 38.7165 - val_loss: 1500.5239 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9541 - loss: 1518.8676 - val_RMSE: 38.7149 - val_loss: 1500.0076 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9517 - loss: 1518.3409 - val_RMSE: 38.7143 - val_loss: 1499.7439 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9435 - loss: 1517.5121 - val_RMSE: 38.7116 - val_loss: 1499.4058 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9468 - loss: 1517.6493 - val_RMSE: 38.7124 - val_loss: 1499.3759 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9462 - loss: 1517.5144 - val_RMSE: 38.7118 - val_loss: 1499.2648 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9401 - loss: 1516.9845 - val_RMSE: 38.7118 - val_loss: 1499.2213 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9376 - loss: 1516.7467 - val_RMSE: 38.7111 - val_loss: 1499.1396 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  64s 24ms/step - RMSE: 38.9317 - loss: 1516.2589 - val_RMSE: 38.7106 - val_loss: 1499.0748 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9274 - loss: 1515.9056 - val_RMSE: 38.7104 - val_loss: 1499.0415 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9280 - loss: 1515.9312 - val_RMSE: 38.7108 - val_loss: 1499.0625 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9181 - loss: 1515.1509 - val_RMSE: 38.7099 - val_loss: 1498.9727 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9195 - loss: 1515.2476 - val_RMSE: 38.7110 - val_loss: 1499.0500 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  65s 25ms/step - RMSE: 38.9204 - loss: 1515.3063 - val_RMSE: 38.7099 - val_loss: 1498.9590 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9077 - loss: 1514.3134 - val_RMSE: 38.7093 - val_loss: 1498.9106 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9087 - loss: 1514.3798 - val_RMSE: 38.7105 - val_loss: 1498.9954 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9068 - loss: 1514.2285 - val_RMSE: 38.7107 - val_loss: 1499.0084 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  64s 24ms/step - RMSE: 38.8995 - loss: 1513.6593 - val_RMSE: 38.7079 - val_loss: 1498.7753 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8973 - loss: 1513.4734 - val_RMSE: 38.7077 - val_loss: 1498.7532 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  64s 24ms/step - RMSE: 38.8961 - loss: 1513.3704 - val_RMSE: 38.7082 - val_loss: 1498.7817 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  64s 25ms/step - RMSE: 38.9010 - loss: 1513.7474 - val_RMSE: 38.7080 - val_loss: 1498.7625 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  64s 25ms/step - RMSE: 38.9025 - loss: 1513.8590 - val_RMSE: 38.7075 - val_loss: 1498.7212 - learning_rate: 1.0000e-05\n",
            "1301/1301  17s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 03:28:09,029] Trial 1 finished with value: 38.70228703816732 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'selu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.48, 'repeat_att': 1}. Best is trial 1 with value: 38.70228703816732.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  147s 35ms/step - RMSE: 43.2199 - loss: 1919.5537 - val_RMSE: 38.7000 - val_loss: 1498.5961 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9782 - loss: 1520.3369 - val_RMSE: 38.6960 - val_loss: 1498.7506 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9793 - loss: 1520.8539 - val_RMSE: 38.7000 - val_loss: 1499.3563 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9302 - loss: 1517.1078 - val_RMSE: 38.6875 - val_loss: 1497.9254 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9197 - loss: 1515.8606 - val_RMSE: 38.6853 - val_loss: 1497.4617 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9056 - loss: 1514.5089 - val_RMSE: 38.6849 - val_loss: 1497.2765 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9104 - loss: 1514.7513 - val_RMSE: 38.6849 - val_loss: 1497.1888 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9049 - loss: 1514.2415 - val_RMSE: 38.6845 - val_loss: 1497.0953 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9060 - loss: 1514.2686 - val_RMSE: 38.6845 - val_loss: 1497.0533 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9000 - loss: 1513.7655 - val_RMSE: 38.6841 - val_loss: 1496.9913 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9058 - loss: 1514.1903 - val_RMSE: 38.6838 - val_loss: 1496.9438 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8927 - loss: 1513.1487 - val_RMSE: 38.6834 - val_loss: 1496.9041 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9035 - loss: 1513.9727 - val_RMSE: 38.6831 - val_loss: 1496.8627 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8964 - loss: 1513.4114 - val_RMSE: 38.6833 - val_loss: 1496.8708 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8967 - loss: 1513.4277 - val_RMSE: 38.6825 - val_loss: 1496.8010 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8926 - loss: 1513.1035 - val_RMSE: 38.6832 - val_loss: 1496.8527 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8866 - loss: 1512.6248 - val_RMSE: 38.6826 - val_loss: 1496.8025 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8882 - loss: 1512.7504 - val_RMSE: 38.6817 - val_loss: 1496.7234 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8858 - loss: 1512.5521 - val_RMSE: 38.6815 - val_loss: 1496.6926 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8798 - loss: 1512.0736 - val_RMSE: 38.6814 - val_loss: 1496.6783 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8808 - loss: 1512.1464 - val_RMSE: 38.6811 - val_loss: 1496.6537 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8835 - loss: 1512.3491 - val_RMSE: 38.6811 - val_loss: 1496.6418 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8788 - loss: 1511.9792 - val_RMSE: 38.6811 - val_loss: 1496.6355 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8838 - loss: 1512.3633 - val_RMSE: 38.6809 - val_loss: 1496.6188 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8828 - loss: 1512.2748 - val_RMSE: 38.6809 - val_loss: 1496.6119 - learning_rate: 1.0000e-04\n",
            "1301/1301  18s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  148s 35ms/step - RMSE: 43.1828 - loss: 1916.1990 - val_RMSE: 38.7341 - val_loss: 1501.2308 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9260 - loss: 1516.2416 - val_RMSE: 38.7522 - val_loss: 1503.1017 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9187 - loss: 1516.1676 - val_RMSE: 38.7418 - val_loss: 1502.6028 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8751 - loss: 1512.8329 - val_RMSE: 38.7197 - val_loss: 1500.4335 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8671 - loss: 1511.7841 - val_RMSE: 38.7194 - val_loss: 1500.1112 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8683 - loss: 1511.6200 - val_RMSE: 38.7181 - val_loss: 1499.8542 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8590 - loss: 1510.7631 - val_RMSE: 38.7174 - val_loss: 1499.7125 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8560 - loss: 1510.4453 - val_RMSE: 38.7172 - val_loss: 1499.6349 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8590 - loss: 1510.6244 - val_RMSE: 38.7175 - val_loss: 1499.6127 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8564 - loss: 1510.3749 - val_RMSE: 38.7168 - val_loss: 1499.5217 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8506 - loss: 1509.9030 - val_RMSE: 38.7158 - val_loss: 1499.4307 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8516 - loss: 1509.9583 - val_RMSE: 38.7153 - val_loss: 1499.3761 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8479 - loss: 1509.6581 - val_RMSE: 38.7153 - val_loss: 1499.3641 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8450 - loss: 1509.4153 - val_RMSE: 38.7172 - val_loss: 1499.5005 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8449 - loss: 1509.4033 - val_RMSE: 38.7163 - val_loss: 1499.4207 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8356 - loss: 1508.6681 - val_RMSE: 38.7119 - val_loss: 1499.0619 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8357 - loss: 1508.6619 - val_RMSE: 38.7116 - val_loss: 1499.0386 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8349 - loss: 1508.5945 - val_RMSE: 38.7117 - val_loss: 1499.0326 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8381 - loss: 1508.8324 - val_RMSE: 38.7116 - val_loss: 1499.0187 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8330 - loss: 1508.4298 - val_RMSE: 38.7115 - val_loss: 1499.0021 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8333 - loss: 1508.4491 - val_RMSE: 38.7115 - val_loss: 1498.9976 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8369 - loss: 1508.7228 - val_RMSE: 38.7116 - val_loss: 1498.9965 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8395 - loss: 1508.9120 - val_RMSE: 38.7117 - val_loss: 1499.0038 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8379 - loss: 1508.7859 - val_RMSE: 38.7117 - val_loss: 1498.9905 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8321 - loss: 1508.3302 - val_RMSE: 38.7114 - val_loss: 1498.9688 - learning_rate: 1.0000e-04\n",
            "1301/1301  17s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  155s 36ms/step - RMSE: 43.1270 - loss: 1909.9421 - val_RMSE: 38.7296 - val_loss: 1500.8849 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9558 - loss: 1518.5751 - val_RMSE: 38.7394 - val_loss: 1502.0955 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9556 - loss: 1518.9839 - val_RMSE: 38.7361 - val_loss: 1502.3036 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9071 - loss: 1515.4553 - val_RMSE: 38.7132 - val_loss: 1500.0302 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8936 - loss: 1513.9342 - val_RMSE: 38.7108 - val_loss: 1499.5002 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8915 - loss: 1513.4712 - val_RMSE: 38.7099 - val_loss: 1499.2487 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8826 - loss: 1512.6215 - val_RMSE: 38.7114 - val_loss: 1499.2659 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8900 - loss: 1513.1058 - val_RMSE: 38.7099 - val_loss: 1499.0817 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8863 - loss: 1512.7578 - val_RMSE: 38.7094 - val_loss: 1498.9949 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8821 - loss: 1512.3795 - val_RMSE: 38.7107 - val_loss: 1499.0641 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8840 - loss: 1512.5049 - val_RMSE: 38.7088 - val_loss: 1498.8883 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8795 - loss: 1512.1324 - val_RMSE: 38.7078 - val_loss: 1498.7916 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8736 - loss: 1511.6577 - val_RMSE: 38.7090 - val_loss: 1498.8766 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8795 - loss: 1512.0961 - val_RMSE: 38.7102 - val_loss: 1498.9519 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8671 - loss: 1511.1216 - val_RMSE: 38.7067 - val_loss: 1498.6674 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8667 - loss: 1511.0752 - val_RMSE: 38.7062 - val_loss: 1498.6179 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8728 - loss: 1511.5441 - val_RMSE: 38.7059 - val_loss: 1498.5858 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8655 - loss: 1510.9685 - val_RMSE: 38.7058 - val_loss: 1498.5698 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8659 - loss: 1510.9923 - val_RMSE: 38.7056 - val_loss: 1498.5522 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8658 - loss: 1510.9742 - val_RMSE: 38.7058 - val_loss: 1498.5548 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  70s 27ms/step - RMSE: 38.8660 - loss: 1510.9816 - val_RMSE: 38.7055 - val_loss: 1498.5292 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  69s 27ms/step - RMSE: 38.8629 - loss: 1510.7365 - val_RMSE: 38.7055 - val_loss: 1498.5220 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  67s 26ms/step - RMSE: 38.8638 - loss: 1510.8021 - val_RMSE: 38.7057 - val_loss: 1498.5306 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8657 - loss: 1510.9388 - val_RMSE: 38.7055 - val_loss: 1498.5111 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  67s 26ms/step - RMSE: 38.8657 - loss: 1510.9323 - val_RMSE: 38.7055 - val_loss: 1498.5066 - learning_rate: 1.0000e-04\n",
            "1301/1301  17s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 04:50:47,418] Trial 2 finished with value: 38.699286142985024 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  153s 37ms/step - RMSE: 43.1467 - loss: 1912.6401 - val_RMSE: 38.6983 - val_loss: 1497.9614 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9335 - loss: 1516.2772 - val_RMSE: 38.6935 - val_loss: 1497.8076 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9336 - loss: 1516.4974 - val_RMSE: 38.6955 - val_loss: 1498.1663 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9154 - loss: 1515.2963 - val_RMSE: 38.6973 - val_loss: 1498.4552 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8831 - loss: 1512.8254 - val_RMSE: 38.6868 - val_loss: 1497.4468 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8683 - loss: 1511.4871 - val_RMSE: 38.6863 - val_loss: 1497.2582 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8694 - loss: 1511.4333 - val_RMSE: 38.6862 - val_loss: 1497.1595 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8638 - loss: 1510.9166 - val_RMSE: 38.6854 - val_loss: 1497.0293 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8688 - loss: 1511.2439 - val_RMSE: 38.6858 - val_loss: 1497.0173 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8612 - loss: 1510.6085 - val_RMSE: 38.6854 - val_loss: 1496.9486 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8672 - loss: 1511.0455 - val_RMSE: 38.6846 - val_loss: 1496.8655 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8593 - loss: 1510.4016 - val_RMSE: 38.6850 - val_loss: 1496.8708 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8674 - loss: 1511.0186 - val_RMSE: 38.6845 - val_loss: 1496.8170 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8612 - loss: 1510.5210 - val_RMSE: 38.6855 - val_loss: 1496.8867 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8607 - loss: 1510.4718 - val_RMSE: 38.6845 - val_loss: 1496.7950 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8577 - loss: 1510.2253 - val_RMSE: 38.6849 - val_loss: 1496.8239 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8546 - loss: 1509.9801 - val_RMSE: 38.6836 - val_loss: 1496.7120 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8552 - loss: 1510.0190 - val_RMSE: 38.6834 - val_loss: 1496.6898 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8520 - loss: 1509.7625 - val_RMSE: 38.6835 - val_loss: 1496.6901 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8515 - loss: 1509.7184 - val_RMSE: 38.6841 - val_loss: 1496.7323 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8479 - loss: 1509.4292 - val_RMSE: 38.6834 - val_loss: 1496.6722 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8515 - loss: 1509.7061 - val_RMSE: 38.6829 - val_loss: 1496.6322 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  64s 24ms/step - RMSE: 38.8445 - loss: 1509.1569 - val_RMSE: 38.6829 - val_loss: 1496.6284 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8512 - loss: 1509.6818 - val_RMSE: 38.6827 - val_loss: 1496.6064 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8499 - loss: 1509.5737 - val_RMSE: 38.6826 - val_loss: 1496.6011 - learning_rate: 1.0000e-04\n",
            "1301/1301  15s 9ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  159s 38ms/step - RMSE: 43.0882 - loss: 1907.0781 - val_RMSE: 38.7436 - val_loss: 1501.4581 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8796 - loss: 1512.0743 - val_RMSE: 38.7295 - val_loss: 1500.6298 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8777 - loss: 1512.2151 - val_RMSE: 38.7367 - val_loss: 1501.4878 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  65s 25ms/step - RMSE: 38.8659 - loss: 1511.5642 - val_RMSE: 38.7309 - val_loss: 1501.2053 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8290 - loss: 1508.7550 - val_RMSE: 38.7219 - val_loss: 1500.2830 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  65s 25ms/step - RMSE: 38.8282 - loss: 1508.4794 - val_RMSE: 38.7212 - val_loss: 1500.0447 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8215 - loss: 1507.7941 - val_RMSE: 38.7204 - val_loss: 1499.8717 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  66s 26ms/step - RMSE: 38.8174 - loss: 1507.3712 - val_RMSE: 38.7206 - val_loss: 1499.8170 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  68s 26ms/step - RMSE: 38.8207 - loss: 1507.5630 - val_RMSE: 38.7213 - val_loss: 1499.8226 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  68s 26ms/step - RMSE: 38.8193 - loss: 1507.4075 - val_RMSE: 38.7201 - val_loss: 1499.6876 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8152 - loss: 1507.0485 - val_RMSE: 38.7198 - val_loss: 1499.6343 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8172 - loss: 1507.1785 - val_RMSE: 38.7192 - val_loss: 1499.5641 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8114 - loss: 1506.7085 - val_RMSE: 38.7183 - val_loss: 1499.4696 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  66s 26ms/step - RMSE: 38.8119 - loss: 1506.7212 - val_RMSE: 38.7187 - val_loss: 1499.4884 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8116 - loss: 1506.6865 - val_RMSE: 38.7199 - val_loss: 1499.5673 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8046 - loss: 1506.1307 - val_RMSE: 38.7150 - val_loss: 1499.1841 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  64s 25ms/step - RMSE: 38.8029 - loss: 1505.9951 - val_RMSE: 38.7150 - val_loss: 1499.1755 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  67s 26ms/step - RMSE: 38.8021 - loss: 1505.9320 - val_RMSE: 38.7147 - val_loss: 1499.1523 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8067 - loss: 1506.2850 - val_RMSE: 38.7145 - val_loss: 1499.1357 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8023 - loss: 1505.9385 - val_RMSE: 38.7146 - val_loss: 1499.1328 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  67s 26ms/step - RMSE: 38.8052 - loss: 1506.1622 - val_RMSE: 38.7145 - val_loss: 1499.1202 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8058 - loss: 1506.2058 - val_RMSE: 38.7143 - val_loss: 1499.1085 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  68s 26ms/step - RMSE: 38.8083 - loss: 1506.3923 - val_RMSE: 38.7142 - val_loss: 1499.0948 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  68s 26ms/step - RMSE: 38.8070 - loss: 1506.2932 - val_RMSE: 38.7142 - val_loss: 1499.0923 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8021 - loss: 1505.9102 - val_RMSE: 38.7140 - val_loss: 1499.0754 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  117s 27ms/step - RMSE: 43.0855 - loss: 1906.2587 - val_RMSE: 38.7267 - val_loss: 1500.1206 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9105 - loss: 1514.4437 - val_RMSE: 38.7184 - val_loss: 1499.7040 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9080 - loss: 1514.4988 - val_RMSE: 38.7158 - val_loss: 1499.8019 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8925 - loss: 1513.6099 - val_RMSE: 38.7175 - val_loss: 1500.1287 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8570 - loss: 1510.8955 - val_RMSE: 38.7136 - val_loss: 1499.6031 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8476 - loss: 1509.9430 - val_RMSE: 38.7124 - val_loss: 1499.3268 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8423 - loss: 1509.3777 - val_RMSE: 38.7118 - val_loss: 1499.1818 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8473 - loss: 1509.6743 - val_RMSE: 38.7117 - val_loss: 1499.1071 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8451 - loss: 1509.4404 - val_RMSE: 38.7103 - val_loss: 1498.9496 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8402 - loss: 1509.0154 - val_RMSE: 38.7104 - val_loss: 1498.9240 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8427 - loss: 1509.1755 - val_RMSE: 38.7087 - val_loss: 1498.7606 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8390 - loss: 1508.8665 - val_RMSE: 38.7081 - val_loss: 1498.6951 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8345 - loss: 1508.4982 - val_RMSE: 38.7088 - val_loss: 1498.7297 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8402 - loss: 1508.9176 - val_RMSE: 38.7108 - val_loss: 1498.8707 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8328 - loss: 1508.3344 - val_RMSE: 38.7076 - val_loss: 1498.6187 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8302 - loss: 1508.1289 - val_RMSE: 38.7085 - val_loss: 1498.6838 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8355 - loss: 1508.5327 - val_RMSE: 38.7080 - val_loss: 1498.6445 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8296 - loss: 1508.0712 - val_RMSE: 38.7077 - val_loss: 1498.6215 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8297 - loss: 1508.0819 - val_RMSE: 38.7077 - val_loss: 1498.6205 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8306 - loss: 1508.1505 - val_RMSE: 38.7078 - val_loss: 1498.6224 - learning_rate: 1.0000e-06\n",
            "Epoch 21/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8293 - loss: 1508.0455 - val_RMSE: 38.7079 - val_loss: 1498.6296 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8299 - loss: 1508.0964 - val_RMSE: 38.7076 - val_loss: 1498.6091 - learning_rate: 1.0000e-07\n",
            "Epoch 23/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8292 - loss: 1508.0363 - val_RMSE: 38.7081 - val_loss: 1498.6482 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8291 - loss: 1508.0310 - val_RMSE: 38.7076 - val_loss: 1498.6090 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8305 - loss: 1508.1370 - val_RMSE: 38.7077 - val_loss: 1498.6216 - learning_rate: 1.0000e-07\n",
            "1301/1301  11s 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 06:05:34,671] Trial 3 finished with value: 38.701454162597656 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 4, 'transformer_units': 128, 'dropout_rate': 0.32999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  114s 25ms/step - RMSE: 43.1435 - loss: 1912.3213 - val_RMSE: 38.7015 - val_loss: 1498.2279 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.9314 - loss: 1516.1379 - val_RMSE: 38.6943 - val_loss: 1497.9430 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  35s 13ms/step - RMSE: 38.9339 - loss: 1516.6235 - val_RMSE: 38.7009 - val_loss: 1498.7732 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.9153 - loss: 1515.4824 - val_RMSE: 38.6954 - val_loss: 1498.4922 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8841 - loss: 1513.0718 - val_RMSE: 38.6879 - val_loss: 1497.6763 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  35s 14ms/step - RMSE: 38.8692 - loss: 1511.6859 - val_RMSE: 38.6870 - val_loss: 1497.4207 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.8696 - loss: 1511.5499 - val_RMSE: 38.6868 - val_loss: 1497.2927 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.8653 - loss: 1511.1191 - val_RMSE: 38.6867 - val_loss: 1497.2096 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8689 - loss: 1511.3248 - val_RMSE: 38.6857 - val_loss: 1497.0808 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8624 - loss: 1510.7758 - val_RMSE: 38.6853 - val_loss: 1497.0159 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8672 - loss: 1511.1150 - val_RMSE: 38.6857 - val_loss: 1497.0123 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.8592 - loss: 1510.4672 - val_RMSE: 38.6860 - val_loss: 1497.0164 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  35s 14ms/step - RMSE: 38.8670 - loss: 1511.0509 - val_RMSE: 38.6847 - val_loss: 1496.9001 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  35s 14ms/step - RMSE: 38.8603 - loss: 1510.5122 - val_RMSE: 38.6858 - val_loss: 1496.9672 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8612 - loss: 1510.5704 - val_RMSE: 38.6844 - val_loss: 1496.8457 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8588 - loss: 1510.3680 - val_RMSE: 38.6852 - val_loss: 1496.9039 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8553 - loss: 1510.0886 - val_RMSE: 38.6838 - val_loss: 1496.7844 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8563 - loss: 1510.1576 - val_RMSE: 38.6840 - val_loss: 1496.7878 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8532 - loss: 1509.9043 - val_RMSE: 38.6835 - val_loss: 1496.7439 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8521 - loss: 1509.8169 - val_RMSE: 38.6840 - val_loss: 1496.7772 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8500 - loss: 1509.6456 - val_RMSE: 38.6842 - val_loss: 1496.7865 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8509 - loss: 1509.7151 - val_RMSE: 38.6827 - val_loss: 1496.6647 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8442 - loss: 1509.1863 - val_RMSE: 38.6823 - val_loss: 1496.6378 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8501 - loss: 1509.6471 - val_RMSE: 38.6819 - val_loss: 1496.6010 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8485 - loss: 1509.5172 - val_RMSE: 38.6822 - val_loss: 1496.6163 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  123s 26ms/step - RMSE: 43.0874 - loss: 1906.9557 - val_RMSE: 38.7412 - val_loss: 1501.3207 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8807 - loss: 1512.2094 - val_RMSE: 38.7461 - val_loss: 1501.9521 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8773 - loss: 1512.2358 - val_RMSE: 38.7442 - val_loss: 1502.0736 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8416 - loss: 1509.5913 - val_RMSE: 38.7219 - val_loss: 1500.1827 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8313 - loss: 1508.6251 - val_RMSE: 38.7229 - val_loss: 1500.1093 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8353 - loss: 1508.7992 - val_RMSE: 38.7220 - val_loss: 1499.9417 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8277 - loss: 1508.1227 - val_RMSE: 38.7202 - val_loss: 1499.7432 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8231 - loss: 1507.7129 - val_RMSE: 38.7212 - val_loss: 1499.7777 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8271 - loss: 1507.9814 - val_RMSE: 38.7192 - val_loss: 1499.5903 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8260 - loss: 1507.8699 - val_RMSE: 38.7192 - val_loss: 1499.5653 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8207 - loss: 1507.4343 - val_RMSE: 38.7185 - val_loss: 1499.4913 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8225 - loss: 1507.5547 - val_RMSE: 38.7186 - val_loss: 1499.4832 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8168 - loss: 1507.0968 - val_RMSE: 38.7168 - val_loss: 1499.3308 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8162 - loss: 1507.0337 - val_RMSE: 38.7181 - val_loss: 1499.4225 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8180 - loss: 1507.1685 - val_RMSE: 38.7195 - val_loss: 1499.5259 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8098 - loss: 1506.5255 - val_RMSE: 38.7122 - val_loss: 1498.9563 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8071 - loss: 1506.3075 - val_RMSE: 38.7117 - val_loss: 1498.9125 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8066 - loss: 1506.2655 - val_RMSE: 38.7118 - val_loss: 1498.9138 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8111 - loss: 1506.6099 - val_RMSE: 38.7115 - val_loss: 1498.8809 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8070 - loss: 1506.2926 - val_RMSE: 38.7115 - val_loss: 1498.8854 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8087 - loss: 1506.4160 - val_RMSE: 38.7115 - val_loss: 1498.8835 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8091 - loss: 1506.4424 - val_RMSE: 38.7112 - val_loss: 1498.8561 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8121 - loss: 1506.6777 - val_RMSE: 38.7112 - val_loss: 1498.8525 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8110 - loss: 1506.5945 - val_RMSE: 38.7111 - val_loss: 1498.8486 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8058 - loss: 1506.1896 - val_RMSE: 38.7111 - val_loss: 1498.8445 - learning_rate: 1.0000e-05\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  110s 24ms/step - RMSE: 43.0833 - loss: 1906.1544 - val_RMSE: 38.7420 - val_loss: 1501.3306 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.9094 - loss: 1514.3882 - val_RMSE: 38.7177 - val_loss: 1499.7139 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9081 - loss: 1514.5768 - val_RMSE: 38.7518 - val_loss: 1502.6815 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8929 - loss: 1513.7024 - val_RMSE: 38.7400 - val_loss: 1501.9214 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8572 - loss: 1510.9570 - val_RMSE: 38.7119 - val_loss: 1499.5076 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8485 - loss: 1510.0437 - val_RMSE: 38.7106 - val_loss: 1499.2214 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8440 - loss: 1509.5364 - val_RMSE: 38.7105 - val_loss: 1499.1041 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8475 - loss: 1509.7103 - val_RMSE: 38.7099 - val_loss: 1498.9866 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8451 - loss: 1509.4650 - val_RMSE: 38.7095 - val_loss: 1498.9097 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8402 - loss: 1509.0345 - val_RMSE: 38.7087 - val_loss: 1498.8076 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8440 - loss: 1509.2955 - val_RMSE: 38.7089 - val_loss: 1498.8007 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8396 - loss: 1508.9308 - val_RMSE: 38.7083 - val_loss: 1498.7338 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8348 - loss: 1508.5409 - val_RMSE: 38.7094 - val_loss: 1498.8029 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8410 - loss: 1509.0067 - val_RMSE: 38.7087 - val_loss: 1498.7296 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8353 - loss: 1508.5453 - val_RMSE: 38.7075 - val_loss: 1498.6260 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8333 - loss: 1508.3806 - val_RMSE: 38.7075 - val_loss: 1498.6204 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8387 - loss: 1508.7925 - val_RMSE: 38.7073 - val_loss: 1498.5940 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8314 - loss: 1508.2211 - val_RMSE: 38.7061 - val_loss: 1498.4948 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8298 - loss: 1508.0856 - val_RMSE: 38.7073 - val_loss: 1498.5839 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8288 - loss: 1508.0077 - val_RMSE: 38.7078 - val_loss: 1498.6121 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8264 - loss: 1507.8097 - val_RMSE: 38.7056 - val_loss: 1498.4427 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8253 - loss: 1507.7246 - val_RMSE: 38.7052 - val_loss: 1498.4094 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8239 - loss: 1507.6123 - val_RMSE: 38.7052 - val_loss: 1498.4077 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8239 - loss: 1507.6039 - val_RMSE: 38.7051 - val_loss: 1498.3900 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8249 - loss: 1507.6761 - val_RMSE: 38.7051 - val_loss: 1498.3849 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 06:58:27,923] Trial 4 finished with value: 38.699440002441406 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'num_transformer_heads': 2, 'transformer_units': 64, 'dropout_rate': 0.32999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  115s 26ms/step - RMSE: 45.7555 - loss: 2168.1316 - val_RMSE: 38.7151 - val_loss: 1499.0077 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.5285 - loss: 1562.6776 - val_RMSE: 38.6991 - val_loss: 1497.8905 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.3829 - loss: 1551.3192 - val_RMSE: 38.6971 - val_loss: 1497.9207 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.2354 - loss: 1539.9041 - val_RMSE: 38.6969 - val_loss: 1498.0271 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1086 - loss: 1530.0475 - val_RMSE: 38.6931 - val_loss: 1497.6881 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0863 - loss: 1528.2585 - val_RMSE: 38.6916 - val_loss: 1497.5175 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0768 - loss: 1527.4636 - val_RMSE: 38.6921 - val_loss: 1497.5164 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0539 - loss: 1525.6434 - val_RMSE: 38.6907 - val_loss: 1497.3795 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0479 - loss: 1525.1437 - val_RMSE: 38.6900 - val_loss: 1497.3008 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0396 - loss: 1524.4745 - val_RMSE: 38.6901 - val_loss: 1497.2837 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  40s 16ms/step - RMSE: 39.0198 - loss: 1522.9083 - val_RMSE: 38.6902 - val_loss: 1497.2766 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0145 - loss: 1522.4779 - val_RMSE: 38.6895 - val_loss: 1497.2114 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.0014 - loss: 1521.4373 - val_RMSE: 38.6881 - val_loss: 1497.0848 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.9961 - loss: 1521.0112 - val_RMSE: 38.6888 - val_loss: 1497.1262 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.9817 - loss: 1519.8778 - val_RMSE: 38.6881 - val_loss: 1497.0613 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.9768 - loss: 1519.4847 - val_RMSE: 38.6870 - val_loss: 1496.9670 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9651 - loss: 1518.5574 - val_RMSE: 38.6878 - val_loss: 1497.0236 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9532 - loss: 1517.6261 - val_RMSE: 38.6872 - val_loss: 1496.9645 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.9506 - loss: 1517.4102 - val_RMSE: 38.6877 - val_loss: 1496.9958 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9383 - loss: 1516.4506 - val_RMSE: 38.6879 - val_loss: 1497.0070 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9326 - loss: 1515.9972 - val_RMSE: 38.6870 - val_loss: 1496.9302 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9297 - loss: 1515.7722 - val_RMSE: 38.6865 - val_loss: 1496.8931 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9312 - loss: 1515.8837 - val_RMSE: 38.6862 - val_loss: 1496.8665 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9303 - loss: 1515.8151 - val_RMSE: 38.6860 - val_loss: 1496.8502 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9209 - loss: 1515.0839 - val_RMSE: 38.6862 - val_loss: 1496.8690 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  123s 27ms/step - RMSE: 45.7189 - loss: 2164.6008 - val_RMSE: 38.7478 - val_loss: 1501.5664 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.4763 - loss: 1558.5763 - val_RMSE: 38.7292 - val_loss: 1500.2493 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.3425 - loss: 1548.1737 - val_RMSE: 38.7311 - val_loss: 1500.5592 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1711 - loss: 1534.8774 - val_RMSE: 38.7272 - val_loss: 1500.3629 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0583 - loss: 1526.1018 - val_RMSE: 38.7210 - val_loss: 1499.8284 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0409 - loss: 1524.6914 - val_RMSE: 38.7202 - val_loss: 1499.7159 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.0269 - loss: 1523.5452 - val_RMSE: 38.7201 - val_loss: 1499.6591 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0149 - loss: 1522.5728 - val_RMSE: 38.7184 - val_loss: 1499.5040 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9983 - loss: 1521.2456 - val_RMSE: 38.7184 - val_loss: 1499.4797 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.9966 - loss: 1521.0936 - val_RMSE: 38.7172 - val_loss: 1499.3621 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9828 - loss: 1519.9980 - val_RMSE: 38.7179 - val_loss: 1499.4021 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9686 - loss: 1518.8765 - val_RMSE: 38.7168 - val_loss: 1499.3005 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9579 - loss: 1518.0286 - val_RMSE: 38.7175 - val_loss: 1499.3401 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9451 - loss: 1517.0153 - val_RMSE: 38.7166 - val_loss: 1499.2563 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9369 - loss: 1516.3612 - val_RMSE: 38.7166 - val_loss: 1499.2494 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9196 - loss: 1515.0061 - val_RMSE: 38.7169 - val_loss: 1499.2621 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  48s 19ms/step - RMSE: 38.9139 - loss: 1514.5591 - val_RMSE: 38.7162 - val_loss: 1499.2021 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.9066 - loss: 1513.9805 - val_RMSE: 38.7164 - val_loss: 1499.2109 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  49s 19ms/step - RMSE: 38.9014 - loss: 1513.5691 - val_RMSE: 38.7161 - val_loss: 1499.1810 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.8881 - loss: 1512.5328 - val_RMSE: 38.7162 - val_loss: 1499.1799 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  50s 19ms/step - RMSE: 38.8826 - loss: 1512.0980 - val_RMSE: 38.7152 - val_loss: 1499.1049 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601  51s 20ms/step - RMSE: 38.8776 - loss: 1511.7020 - val_RMSE: 38.7153 - val_loss: 1499.1086 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8641 - loss: 1510.6454 - val_RMSE: 38.7155 - val_loss: 1499.1180 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.8575 - loss: 1510.1354 - val_RMSE: 38.7145 - val_loss: 1499.0352 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8588 - loss: 1510.2299 - val_RMSE: 38.7143 - val_loss: 1499.0184 - learning_rate: 1.0000e-04\n",
            "1301/1301  13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  118s 26ms/step - RMSE: 45.7002 - loss: 2162.1248 - val_RMSE: 38.7195 - val_loss: 1499.3488 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  48s 18ms/step - RMSE: 39.5034 - loss: 1560.6990 - val_RMSE: 38.7215 - val_loss: 1499.6198 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.3608 - loss: 1549.5713 - val_RMSE: 38.7163 - val_loss: 1499.3687 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.2126 - loss: 1538.0386 - val_RMSE: 38.7228 - val_loss: 1499.8433 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  46s 17ms/step - RMSE: 39.2021 - loss: 1537.1873 - val_RMSE: 38.7189 - val_loss: 1499.5131 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1778 - loss: 1535.2606 - val_RMSE: 38.7169 - val_loss: 1499.3527 - learning_rate: 1.0000e-04\n",
            "Epoch 7/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1736 - loss: 1534.9222 - val_RMSE: 38.7167 - val_loss: 1499.3334 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1734 - loss: 1534.9026 - val_RMSE: 38.7161 - val_loss: 1499.2797 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.1805 - loss: 1535.4563 - val_RMSE: 38.7167 - val_loss: 1499.3292 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1715 - loss: 1534.7469 - val_RMSE: 38.7160 - val_loss: 1499.2714 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1767 - loss: 1535.1536 - val_RMSE: 38.7156 - val_loss: 1499.2382 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.1627 - loss: 1534.0553 - val_RMSE: 38.7158 - val_loss: 1499.2437 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.1667 - loss: 1534.3608 - val_RMSE: 38.7154 - val_loss: 1499.2161 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.1680 - loss: 1534.4641 - val_RMSE: 38.7161 - val_loss: 1499.2670 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.1670 - loss: 1534.3833 - val_RMSE: 38.7149 - val_loss: 1499.1672 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1491 - loss: 1532.9792 - val_RMSE: 38.7148 - val_loss: 1499.1571 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1539 - loss: 1533.3547 - val_RMSE: 38.7146 - val_loss: 1499.1401 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1601 - loss: 1533.8358 - val_RMSE: 38.7143 - val_loss: 1499.1135 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  47s 18ms/step - RMSE: 39.1611 - loss: 1533.9114 - val_RMSE: 38.7145 - val_loss: 1499.1306 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.1487 - loss: 1532.9391 - val_RMSE: 38.7143 - val_loss: 1499.1112 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.1549 - loss: 1533.4232 - val_RMSE: 38.7144 - val_loss: 1499.1190 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.1485 - loss: 1532.9193 - val_RMSE: 38.7139 - val_loss: 1499.0756 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  43s 17ms/step - RMSE: 39.1406 - loss: 1532.2950 - val_RMSE: 38.7139 - val_loss: 1499.0732 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1514 - loss: 1533.1381 - val_RMSE: 38.7131 - val_loss: 1499.0117 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  43s 17ms/step - RMSE: 39.1498 - loss: 1533.0116 - val_RMSE: 38.7137 - val_loss: 1499.0575 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 07:58:39,266] Trial 5 finished with value: 38.70474751790365 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'gelu', 'num_transformer_heads': 4, 'transformer_units': 128, 'dropout_rate': 0.39, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  111s 25ms/step - RMSE: 42.4475 - loss: 1845.1521 - val_RMSE: 38.7144 - val_loss: 1499.8320 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8458 - loss: 1510.0443 - val_RMSE: 38.6963 - val_loss: 1498.6664 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8482 - loss: 1510.5233 - val_RMSE: 38.6972 - val_loss: 1499.0239 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8444 - loss: 1510.5272 - val_RMSE: 38.7031 - val_loss: 1499.6031 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  46s 17ms/step - RMSE: 38.8118 - loss: 1507.8633 - val_RMSE: 38.6852 - val_loss: 1497.6406 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8080 - loss: 1507.0682 - val_RMSE: 38.6847 - val_loss: 1497.3075 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8027 - loss: 1506.4084 - val_RMSE: 38.6847 - val_loss: 1497.1561 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8008 - loss: 1506.1333 - val_RMSE: 38.6840 - val_loss: 1497.0272 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7933 - loss: 1505.4794 - val_RMSE: 38.6835 - val_loss: 1496.9302 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7955 - loss: 1505.6045 - val_RMSE: 38.6828 - val_loss: 1496.8416 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7970 - loss: 1505.6798 - val_RMSE: 38.6829 - val_loss: 1496.8168 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8002 - loss: 1505.9060 - val_RMSE: 38.6827 - val_loss: 1496.7836 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.7941 - loss: 1505.4130 - val_RMSE: 38.6824 - val_loss: 1496.7395 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7951 - loss: 1505.4664 - val_RMSE: 38.6827 - val_loss: 1496.7531 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7937 - loss: 1505.3501 - val_RMSE: 38.6821 - val_loss: 1496.6962 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7941 - loss: 1505.3696 - val_RMSE: 38.6821 - val_loss: 1496.6874 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7954 - loss: 1505.4630 - val_RMSE: 38.6836 - val_loss: 1496.7948 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7957 - loss: 1505.4836 - val_RMSE: 38.6825 - val_loss: 1496.7125 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7933 - loss: 1505.2955 - val_RMSE: 38.6811 - val_loss: 1496.5918 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7864 - loss: 1504.7499 - val_RMSE: 38.6808 - val_loss: 1496.5557 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7873 - loss: 1504.8098 - val_RMSE: 38.6805 - val_loss: 1496.5270 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.7888 - loss: 1504.9154 - val_RMSE: 38.6804 - val_loss: 1496.5106 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7853 - loss: 1504.6434 - val_RMSE: 38.6803 - val_loss: 1496.5021 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7887 - loss: 1504.8939 - val_RMSE: 38.6801 - val_loss: 1496.4764 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7880 - loss: 1504.8342 - val_RMSE: 38.6801 - val_loss: 1496.4771 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  119s 27ms/step - RMSE: 42.3829 - loss: 1839.0455 - val_RMSE: 38.7509 - val_loss: 1502.5441 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7978 - loss: 1506.2330 - val_RMSE: 38.7444 - val_loss: 1502.3661 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8006 - loss: 1506.8879 - val_RMSE: 38.7451 - val_loss: 1502.9218 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8016 - loss: 1507.3350 - val_RMSE: 38.7529 - val_loss: 1503.6680 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7649 - loss: 1504.4216 - val_RMSE: 38.7237 - val_loss: 1500.7728 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7550 - loss: 1503.1031 - val_RMSE: 38.7243 - val_loss: 1500.4862 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7556 - loss: 1502.8591 - val_RMSE: 38.7232 - val_loss: 1500.2222 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7553 - loss: 1502.6808 - val_RMSE: 38.7209 - val_loss: 1499.9453 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7529 - loss: 1502.4088 - val_RMSE: 38.7244 - val_loss: 1500.1570 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7492 - loss: 1502.0676 - val_RMSE: 38.7239 - val_loss: 1500.0767 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7465 - loss: 1501.8169 - val_RMSE: 38.7148 - val_loss: 1499.3582 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7470 - loss: 1501.8451 - val_RMSE: 38.7148 - val_loss: 1499.3383 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7450 - loss: 1501.6754 - val_RMSE: 38.7148 - val_loss: 1499.3224 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7417 - loss: 1501.4037 - val_RMSE: 38.7147 - val_loss: 1499.3103 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7456 - loss: 1501.7014 - val_RMSE: 38.7146 - val_loss: 1499.2893 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7473 - loss: 1501.8195 - val_RMSE: 38.7148 - val_loss: 1499.3005 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7434 - loss: 1501.5094 - val_RMSE: 38.7147 - val_loss: 1499.2864 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  49s 19ms/step - RMSE: 38.7426 - loss: 1501.4451 - val_RMSE: 38.7144 - val_loss: 1499.2526 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7454 - loss: 1501.6525 - val_RMSE: 38.7143 - val_loss: 1499.2401 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7448 - loss: 1501.5985 - val_RMSE: 38.7143 - val_loss: 1499.2319 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7430 - loss: 1501.4496 - val_RMSE: 38.7143 - val_loss: 1499.2231 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.7407 - loss: 1501.2650 - val_RMSE: 38.7144 - val_loss: 1499.2222 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7424 - loss: 1501.3975 - val_RMSE: 38.7142 - val_loss: 1499.2080 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  46s 17ms/step - RMSE: 38.7424 - loss: 1501.3904 - val_RMSE: 38.7140 - val_loss: 1499.1799 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7403 - loss: 1501.2173 - val_RMSE: 38.7139 - val_loss: 1499.1743 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  115s 26ms/step - RMSE: 42.3021 - loss: 1830.5432 - val_RMSE: 38.7214 - val_loss: 1500.2247 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8304 - loss: 1508.7031 - val_RMSE: 38.7849 - val_loss: 1505.4067 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8303 - loss: 1509.0193 - val_RMSE: 38.7682 - val_loss: 1504.5594 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7944 - loss: 1506.4683 - val_RMSE: 38.7104 - val_loss: 1499.5895 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7859 - loss: 1505.3501 - val_RMSE: 38.7095 - val_loss: 1499.2378 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7803 - loss: 1504.6820 - val_RMSE: 38.7095 - val_loss: 1499.0968 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7818 - loss: 1504.6760 - val_RMSE: 38.7092 - val_loss: 1499.0006 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7776 - loss: 1504.2859 - val_RMSE: 38.7086 - val_loss: 1498.9076 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7744 - loss: 1503.9915 - val_RMSE: 38.7084 - val_loss: 1498.8505 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7756 - loss: 1504.0525 - val_RMSE: 38.7089 - val_loss: 1498.8649 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7775 - loss: 1504.1755 - val_RMSE: 38.7073 - val_loss: 1498.7172 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7803 - loss: 1504.3685 - val_RMSE: 38.7073 - val_loss: 1498.7063 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7707 - loss: 1503.6112 - val_RMSE: 38.7067 - val_loss: 1498.6438 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7757 - loss: 1503.9913 - val_RMSE: 38.7078 - val_loss: 1498.7180 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7779 - loss: 1504.1486 - val_RMSE: 38.7061 - val_loss: 1498.5847 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7761 - loss: 1504.0052 - val_RMSE: 38.7064 - val_loss: 1498.5966 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7717 - loss: 1503.6576 - val_RMSE: 38.7075 - val_loss: 1498.6698 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7686 - loss: 1503.4067 - val_RMSE: 38.7062 - val_loss: 1498.5604 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.7689 - loss: 1503.4172 - val_RMSE: 38.7059 - val_loss: 1498.5315 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7671 - loss: 1503.2646 - val_RMSE: 38.7055 - val_loss: 1498.4899 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7684 - loss: 1503.3551 - val_RMSE: 38.7055 - val_loss: 1498.4773 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7675 - loss: 1503.2802 - val_RMSE: 38.7056 - val_loss: 1498.4763 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7649 - loss: 1503.0704 - val_RMSE: 38.7054 - val_loss: 1498.4600 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7624 - loss: 1502.8688 - val_RMSE: 38.7052 - val_loss: 1498.4392 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7674 - loss: 1503.2544 - val_RMSE: 38.7050 - val_loss: 1498.4156 - learning_rate: 1.0000e-04\n",
            "1301/1301  13s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 08:57:37,306] Trial 6 finished with value: 38.69969813028971 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.3, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  113s 26ms/step - RMSE: 44.4115 - loss: 2034.5447 - val_RMSE: 38.7035 - val_loss: 1498.1129 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.2853 - loss: 1543.5131 - val_RMSE: 38.6987 - val_loss: 1497.8732 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.2164 - loss: 1538.2460 - val_RMSE: 38.6962 - val_loss: 1497.8669 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.1339 - loss: 1531.9917 - val_RMSE: 38.6970 - val_loss: 1498.0996 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0597 - loss: 1526.3479 - val_RMSE: 38.6948 - val_loss: 1498.0144 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.0003 - loss: 1521.7356 - val_RMSE: 38.6919 - val_loss: 1497.7168 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9883 - loss: 1520.7283 - val_RMSE: 38.6920 - val_loss: 1497.6581 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9737 - loss: 1519.5143 - val_RMSE: 38.6924 - val_loss: 1497.6260 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.9651 - loss: 1518.7964 - val_RMSE: 38.6923 - val_loss: 1497.5817 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9688 - loss: 1519.0426 - val_RMSE: 38.6914 - val_loss: 1497.4691 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.9587 - loss: 1518.2207 - val_RMSE: 38.6917 - val_loss: 1497.4679 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.9553 - loss: 1517.9292 - val_RMSE: 38.6923 - val_loss: 1497.4910 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9436 - loss: 1516.9950 - val_RMSE: 38.6907 - val_loss: 1497.3444 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9442 - loss: 1517.0242 - val_RMSE: 38.6909 - val_loss: 1497.3459 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9408 - loss: 1516.7393 - val_RMSE: 38.6908 - val_loss: 1497.3202 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9343 - loss: 1516.2159 - val_RMSE: 38.6914 - val_loss: 1497.3490 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.9229 - loss: 1515.3163 - val_RMSE: 38.6910 - val_loss: 1497.3099 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9186 - loss: 1514.9668 - val_RMSE: 38.6914 - val_loss: 1497.3282 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9116 - loss: 1514.4094 - val_RMSE: 38.6902 - val_loss: 1497.2266 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9112 - loss: 1514.3723 - val_RMSE: 38.6914 - val_loss: 1497.3125 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9129 - loss: 1514.4952 - val_RMSE: 38.6910 - val_loss: 1497.2709 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9024 - loss: 1513.6681 - val_RMSE: 38.6908 - val_loss: 1497.2500 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9059 - loss: 1513.9403 - val_RMSE: 38.6907 - val_loss: 1497.2383 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9035 - loss: 1513.7535 - val_RMSE: 38.6903 - val_loss: 1497.2067 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9047 - loss: 1513.8467 - val_RMSE: 38.6902 - val_loss: 1497.1969 - learning_rate: 1.0000e-05\n",
            "1301/1301  18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  115s 25ms/step - RMSE: 44.3622 - loss: 2029.6350 - val_RMSE: 38.7290 - val_loss: 1500.0935 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.2330 - loss: 1539.4219 - val_RMSE: 38.7302 - val_loss: 1500.3285 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.1566 - loss: 1533.5837 - val_RMSE: 38.7354 - val_loss: 1500.9144 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0768 - loss: 1527.4764 - val_RMSE: 38.7230 - val_loss: 1499.9257 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0550 - loss: 1525.7434 - val_RMSE: 38.7209 - val_loss: 1499.7285 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0521 - loss: 1525.4745 - val_RMSE: 38.7212 - val_loss: 1499.7164 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.0355 - loss: 1524.1504 - val_RMSE: 38.7213 - val_loss: 1499.7041 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  46s 17ms/step - RMSE: 39.0313 - loss: 1523.8004 - val_RMSE: 38.7187 - val_loss: 1499.4832 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  43s 17ms/step - RMSE: 39.0310 - loss: 1523.7611 - val_RMSE: 38.7197 - val_loss: 1499.5424 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.0179 - loss: 1522.7188 - val_RMSE: 38.7191 - val_loss: 1499.4773 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.0137 - loss: 1522.3821 - val_RMSE: 38.7189 - val_loss: 1499.4559 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9969 - loss: 1521.0582 - val_RMSE: 38.7195 - val_loss: 1499.4899 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9922 - loss: 1520.6802 - val_RMSE: 38.7185 - val_loss: 1499.3995 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9817 - loss: 1519.8529 - val_RMSE: 38.7193 - val_loss: 1499.4558 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.9750 - loss: 1519.3152 - val_RMSE: 38.7177 - val_loss: 1499.3195 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.9679 - loss: 1518.7572 - val_RMSE: 38.7185 - val_loss: 1499.3740 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9550 - loss: 1517.7489 - val_RMSE: 38.7192 - val_loss: 1499.4272 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9507 - loss: 1517.4058 - val_RMSE: 38.7186 - val_loss: 1499.3754 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9591 - loss: 1518.0565 - val_RMSE: 38.7185 - val_loss: 1499.3643 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9565 - loss: 1517.8558 - val_RMSE: 38.7182 - val_loss: 1499.3406 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9564 - loss: 1517.8472 - val_RMSE: 38.7181 - val_loss: 1499.3401 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9541 - loss: 1517.6646 - val_RMSE: 38.7180 - val_loss: 1499.3311 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9526 - loss: 1517.5491 - val_RMSE: 38.7181 - val_loss: 1499.3387 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9540 - loss: 1517.6628 - val_RMSE: 38.7181 - val_loss: 1499.3337 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9578 - loss: 1517.9537 - val_RMSE: 38.7182 - val_loss: 1499.3386 - learning_rate: 1.0000e-07\n",
            "1301/1301  13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  112s 25ms/step - RMSE: 44.3326 - loss: 2026.0779 - val_RMSE: 38.7319 - val_loss: 1500.3129 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.2594 - loss: 1541.4846 - val_RMSE: 38.7179 - val_loss: 1499.3530 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.1896 - loss: 1536.1431 - val_RMSE: 38.7392 - val_loss: 1501.1774 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.1068 - loss: 1529.8394 - val_RMSE: 38.7384 - val_loss: 1501.2596 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0326 - loss: 1524.1267 - val_RMSE: 38.7190 - val_loss: 1499.7012 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0253 - loss: 1523.5060 - val_RMSE: 38.7167 - val_loss: 1499.4752 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0072 - loss: 1522.0521 - val_RMSE: 38.7159 - val_loss: 1499.4034 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0092 - loss: 1522.1993 - val_RMSE: 38.7156 - val_loss: 1499.3785 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0034 - loss: 1521.7450 - val_RMSE: 38.7152 - val_loss: 1499.3414 - learning_rate: 1.0000e-05\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0030 - loss: 1521.7135 - val_RMSE: 38.7149 - val_loss: 1499.3220 - learning_rate: 1.0000e-05\n",
            "Epoch 11/25\n",
            "2601/2601  39s 15ms/step - RMSE: 39.0185 - loss: 1522.9198 - val_RMSE: 38.7150 - val_loss: 1499.3256 - learning_rate: 1.0000e-05\n",
            "Epoch 12/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0113 - loss: 1522.3633 - val_RMSE: 38.7148 - val_loss: 1499.3162 - learning_rate: 1.0000e-05\n",
            "Epoch 13/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0095 - loss: 1522.2219 - val_RMSE: 38.7148 - val_loss: 1499.3107 - learning_rate: 1.0000e-05\n",
            "Epoch 14/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0046 - loss: 1521.8325 - val_RMSE: 38.7148 - val_loss: 1499.3109 - learning_rate: 1.0000e-05\n",
            "Epoch 15/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0069 - loss: 1522.0111 - val_RMSE: 38.7147 - val_loss: 1499.3057 - learning_rate: 1.0000e-05\n",
            "Epoch 16/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0069 - loss: 1522.0167 - val_RMSE: 38.7148 - val_loss: 1499.3093 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9997 - loss: 1521.4526 - val_RMSE: 38.7148 - val_loss: 1499.3075 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0118 - loss: 1522.3987 - val_RMSE: 38.7147 - val_loss: 1499.3020 - learning_rate: 1.0000e-06\n",
            "Epoch 19/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0149 - loss: 1522.6351 - val_RMSE: 38.7146 - val_loss: 1499.2985 - learning_rate: 1.0000e-06\n",
            "Epoch 20/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0099 - loss: 1522.2482 - val_RMSE: 38.7147 - val_loss: 1499.3003 - learning_rate: 1.0000e-06\n",
            "Epoch 21/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0012 - loss: 1521.5675 - val_RMSE: 38.7147 - val_loss: 1499.3024 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0048 - loss: 1521.8497 - val_RMSE: 38.7147 - val_loss: 1499.3002 - learning_rate: 1.0000e-07\n",
            "Epoch 23/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0053 - loss: 1521.8860 - val_RMSE: 38.7148 - val_loss: 1499.3057 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0043 - loss: 1521.8080 - val_RMSE: 38.7147 - val_loss: 1499.3025 - learning_rate: 1.0000e-08\n",
            "Epoch 25/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0010 - loss: 1521.5508 - val_RMSE: 38.7147 - val_loss: 1499.3065 - learning_rate: 1.0000e-08\n",
            "1301/1301  12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 09:53:44,757] Trial 7 finished with value: 38.707681020100914 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.44999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  115s 26ms/step - RMSE: 42.6470 - loss: 1863.9602 - val_RMSE: 38.7233 - val_loss: 1500.6809 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9045 - loss: 1514.8726 - val_RMSE: 38.7080 - val_loss: 1500.0286 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9014 - loss: 1515.2236 - val_RMSE: 38.7096 - val_loss: 1500.5956 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8919 - loss: 1514.8608 - val_RMSE: 38.7099 - val_loss: 1500.8645 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8630 - loss: 1512.5320 - val_RMSE: 38.6867 - val_loss: 1498.2809 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8489 - loss: 1510.7285 - val_RMSE: 38.6854 - val_loss: 1497.7341 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8430 - loss: 1509.8857 - val_RMSE: 38.6856 - val_loss: 1497.5156 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8413 - loss: 1509.5498 - val_RMSE: 38.6851 - val_loss: 1497.3406 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8349 - loss: 1508.9290 - val_RMSE: 38.6856 - val_loss: 1497.2944 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8347 - loss: 1508.8298 - val_RMSE: 38.6849 - val_loss: 1497.1731 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8400 - loss: 1509.1870 - val_RMSE: 38.6849 - val_loss: 1497.1284 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8434 - loss: 1509.4121 - val_RMSE: 38.6844 - val_loss: 1497.0596 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.8360 - loss: 1508.8081 - val_RMSE: 38.6852 - val_loss: 1497.0940 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8354 - loss: 1508.7378 - val_RMSE: 38.6847 - val_loss: 1497.0397 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8311 - loss: 1508.3885 - val_RMSE: 38.6839 - val_loss: 1496.9625 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8309 - loss: 1508.3583 - val_RMSE: 38.6847 - val_loss: 1497.0206 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8304 - loss: 1508.3097 - val_RMSE: 38.6842 - val_loss: 1496.9631 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8300 - loss: 1508.2615 - val_RMSE: 38.6834 - val_loss: 1496.8873 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8280 - loss: 1508.0975 - val_RMSE: 38.6833 - val_loss: 1496.8713 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8256 - loss: 1507.8950 - val_RMSE: 38.6831 - val_loss: 1496.8391 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8245 - loss: 1507.7958 - val_RMSE: 38.6832 - val_loss: 1496.8417 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8258 - loss: 1507.8939 - val_RMSE: 38.6831 - val_loss: 1496.8247 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8195 - loss: 1507.3917 - val_RMSE: 38.6830 - val_loss: 1496.8035 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8257 - loss: 1507.8683 - val_RMSE: 38.6830 - val_loss: 1496.7996 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8248 - loss: 1507.7852 - val_RMSE: 38.6831 - val_loss: 1496.7948 - learning_rate: 1.0000e-04\n",
            "1301/1301  18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  122s 27ms/step - RMSE: 42.6789 - loss: 1867.5491 - val_RMSE: 38.7526 - val_loss: 1502.9938 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8560 - loss: 1511.1396 - val_RMSE: 38.7444 - val_loss: 1502.8888 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8534 - loss: 1511.4731 - val_RMSE: 38.7555 - val_loss: 1504.2766 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8508 - loss: 1511.7206 - val_RMSE: 38.7391 - val_loss: 1503.2950 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8094 - loss: 1508.5171 - val_RMSE: 38.7226 - val_loss: 1501.1578 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7924 - loss: 1506.4233 - val_RMSE: 38.7215 - val_loss: 1500.5839 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7914 - loss: 1505.9281 - val_RMSE: 38.7196 - val_loss: 1500.1873 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7950 - loss: 1505.9901 - val_RMSE: 38.7205 - val_loss: 1500.1183 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7862 - loss: 1505.1833 - val_RMSE: 38.7220 - val_loss: 1500.1399 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7864 - loss: 1505.1119 - val_RMSE: 38.7201 - val_loss: 1499.9286 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7867 - loss: 1505.0771 - val_RMSE: 38.7208 - val_loss: 1499.9283 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.7880 - loss: 1505.1301 - val_RMSE: 38.7195 - val_loss: 1499.8014 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7860 - loss: 1504.9496 - val_RMSE: 38.7195 - val_loss: 1499.7706 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7825 - loss: 1504.6515 - val_RMSE: 38.7196 - val_loss: 1499.7528 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7845 - loss: 1504.7850 - val_RMSE: 38.7198 - val_loss: 1499.7534 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7861 - loss: 1504.8862 - val_RMSE: 38.7200 - val_loss: 1499.7610 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7768 - loss: 1504.1593 - val_RMSE: 38.7136 - val_loss: 1499.2472 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7771 - loss: 1504.1647 - val_RMSE: 38.7135 - val_loss: 1499.2288 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  46s 17ms/step - RMSE: 38.7808 - loss: 1504.4373 - val_RMSE: 38.7135 - val_loss: 1499.2152 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7778 - loss: 1504.1967 - val_RMSE: 38.7135 - val_loss: 1499.2048 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7775 - loss: 1504.1656 - val_RMSE: 38.7135 - val_loss: 1499.1959 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7747 - loss: 1503.9414 - val_RMSE: 38.7134 - val_loss: 1499.1766 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7763 - loss: 1504.0522 - val_RMSE: 38.7132 - val_loss: 1499.1594 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7761 - loss: 1504.0264 - val_RMSE: 38.7131 - val_loss: 1499.1449 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  51s 20ms/step - RMSE: 38.7747 - loss: 1503.9128 - val_RMSE: 38.7133 - val_loss: 1499.1497 - learning_rate: 1.0000e-04\n",
            "1301/1301  13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  121s 28ms/step - RMSE: 42.5061 - loss: 1849.7598 - val_RMSE: 38.7247 - val_loss: 1500.7529 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8844 - loss: 1513.2695 - val_RMSE: 38.7218 - val_loss: 1501.0789 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8799 - loss: 1513.4834 - val_RMSE: 38.7221 - val_loss: 1501.6064 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8444 - loss: 1510.9349 - val_RMSE: 38.7112 - val_loss: 1500.1058 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8289 - loss: 1509.1060 - val_RMSE: 38.7101 - val_loss: 1499.5858 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8265 - loss: 1508.5498 - val_RMSE: 38.7098 - val_loss: 1499.3447 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8250 - loss: 1508.2465 - val_RMSE: 38.7091 - val_loss: 1499.1617 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8208 - loss: 1507.7996 - val_RMSE: 38.7097 - val_loss: 1499.1233 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8156 - loss: 1507.3196 - val_RMSE: 38.7094 - val_loss: 1499.0496 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8179 - loss: 1507.4545 - val_RMSE: 38.7087 - val_loss: 1498.9669 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8191 - loss: 1507.5155 - val_RMSE: 38.7076 - val_loss: 1498.8505 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8237 - loss: 1507.8479 - val_RMSE: 38.7067 - val_loss: 1498.7649 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8124 - loss: 1506.9535 - val_RMSE: 38.7073 - val_loss: 1498.7961 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8169 - loss: 1507.2902 - val_RMSE: 38.7079 - val_loss: 1498.8285 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.8139 - loss: 1507.0425 - val_RMSE: 38.7062 - val_loss: 1498.6810 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.8125 - loss: 1506.9236 - val_RMSE: 38.7060 - val_loss: 1498.6515 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8069 - loss: 1506.4723 - val_RMSE: 38.7058 - val_loss: 1498.6256 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.8106 - loss: 1506.7443 - val_RMSE: 38.7055 - val_loss: 1498.5940 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8091 - loss: 1506.6151 - val_RMSE: 38.7053 - val_loss: 1498.5637 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.8070 - loss: 1506.4437 - val_RMSE: 38.7053 - val_loss: 1498.5557 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8082 - loss: 1506.5293 - val_RMSE: 38.7054 - val_loss: 1498.5553 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  49s 19ms/step - RMSE: 38.8086 - loss: 1506.5477 - val_RMSE: 38.7056 - val_loss: 1498.5579 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8045 - loss: 1506.2297 - val_RMSE: 38.7059 - val_loss: 1498.5764 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.8008 - loss: 1505.9331 - val_RMSE: 38.7054 - val_loss: 1498.5397 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8063 - loss: 1506.3608 - val_RMSE: 38.7053 - val_loss: 1498.5286 - learning_rate: 1.0000e-05\n",
            "1301/1301  12s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-17 10:52:39,036] Trial 8 finished with value: 38.700538635253906 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'selu', 'num_transformer_heads': 2, 'transformer_units': 32, 'dropout_rate': 0.42, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            " 709/2601  54s 29ms/step - RMSE: 50.1655 - loss: 2638.2307"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-17 10:54:36,069] Trial 9 failed with parameters: {'units': 512, 'last_layer': 2, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 128, 'dropout_rate': 0.48, 'repeat_att': 2} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-110-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-109-a07edfb686fe>\", line 54, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
            "    return function._call_flat(  # pylint: disable=protected-access\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1322, in _call_flat\n",
            "    return self._inference_function.call_preflattened(args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 216, in call_preflattened\n",
            "    flat_outputs = self.call_flat(*args)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 251, in call_flat\n",
            "    outputs = self._bound_context.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\", line 1683, in call_function\n",
            "    outputs = execute.execute(\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-02-17 10:54:36,072] Trial 9 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-a07edfb686fe>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     55\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 2 finished with value: 38.699286142985024\n",
        "* parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}"
      ],
      "metadata": {
        "id": "FU7j6-OHWPmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9",
        "id": "u4XgZIPLWPmv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jReIEHz6wysG"
      },
      "source": [
        "#### **4.6.4 NeuralNetwork v3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "54a327da-d07c-4706-e6e3-42c6f105ac0a",
        "id": "QfBbW0LUwysH"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "833957       5         4     0             9                   2           2   \n",
              "2068382      5         1     0             3                   2           0   \n",
              "3307684      4         0     0             4                   2           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "833957       0      1              0.653745 -0.371398 -1.749252 -1.621354   \n",
              "2068382      1      0              1.697075  0.669907  1.942067 -1.621354   \n",
              "3307684      3      5              1.401664 -0.006573 -0.385691  0.136860   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "833957            0               0  \n",
              "2068382           0               0  \n",
              "3307684           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-70b4c955-f9b0-4a54-8c04-e0a65005d793\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>833957</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.653745</td>\n",
              "      <td>-0.371398</td>\n",
              "      <td>-1.749252</td>\n",
              "      <td>-1.621354</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2068382</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.697075</td>\n",
              "      <td>0.669907</td>\n",
              "      <td>1.942067</td>\n",
              "      <td>-1.621354</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3307684</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1.401664</td>\n",
              "      <td>-0.006573</td>\n",
              "      <td>-0.385691</td>\n",
              "      <td>0.136860</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70b4c955-f9b0-4a54-8c04-e0a65005d793')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-70b4c955-f9b0-4a54-8c04-e0a65005d793 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-70b4c955-f9b0-4a54-8c04-e0a65005d793');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-35523353-03ba-4c2a-8a0d-ed987608735b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35523353-03ba-4c2a-8a0d-ed987608735b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-35523353-03ba-4c2a-8a0d-ed987608735b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 3,\n        \"max\": 9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.6537449955940247\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.37139827013015747\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.7492516040802002\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.13686025142669678\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedLinearUnit(layers.Layer):\n",
        "    def __init__(self, exite_units,dropout_rate):\n",
        "        super().__init__()\n",
        "        self.exite_units = exite_units\n",
        "        self.reshaped = layers.Reshape((1, -1))\n",
        "        self.exite = layers.Dense(self.exite_units)\n",
        "        self.lnorm_00 = layers.LayerNormalization()\n",
        "        self.lnorm_01 = layers.LayerNormalization()\n",
        "        self.drop = layers.Dropout(rate=dropout_rate)\n",
        "        self.sigmoid = layers.Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.reshaped(inputs)\n",
        "        x = self.exite(x)\n",
        "        x = self.lnorm_00(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if inputs.shape[-1] != self.exite_units:\n",
        "            inputs = self.project(inputs)\n",
        "        x = inputs + self.gated_linear_unit(x)\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n",
        "\n",
        "    # Remove build warnings\n",
        "    def build(self):\n",
        "        self.built = True"
      ],
      "metadata": {
        "id": "UJ5OQf-rDnO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512, last_layer=1, activation=\"relu\",repeat_att=2,dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001): # Reduced transformer_units\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    transformer_outputs = [] # List to store transformer outputs for each categorical feature\n",
        "\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:, j])\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        embs.append(x)\n",
        "\n",
        "        # Reshape for Transformer (batch_size, 1, features) - Crucial!\n",
        "        reshaped_embedding = layers.Reshape((1, -1))(x)\n",
        "\n",
        "        # Transformer Layer for each categorical feature\n",
        "        for q in list(range(repeat_att)):\n",
        "          if q == 0:\n",
        "            attention_output = reshaped_embedding\n",
        "\n",
        "          attention_output_ = layers.MultiHeadAttention(num_heads=num_transformer_heads, key_dim=transformer_units,name=f\"mh_{j}_{q}\")(attention_output, attention_output)\n",
        "          attention_output_ = layers.LayerNormalization(name=f\"mh_ln1_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "          attention_output_ = layers.Dense(reshaped_embedding.shape[-1], activation=activation,name=f\"mh_dense_{j}_{q}\")(attention_output_)\n",
        "          attention_output = layers.LayerNormalization(name=f\"mh_ln2_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "\n",
        "        transformer_outputs.append(layers.Flatten()(attention_output)) # Store flattened transformer output\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    x_orig = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    reshaped_features = layers.Reshape((1, -1))(x_orig)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x_orig, flattened_attention])\n",
        "\n",
        "    # Concatenate Transformer outputs and numerical features\n",
        "    all_features = layers.Concatenate(axis=-1)(transformer_outputs + [x])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(all_features)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    #x = layers.Concatenate(axis=-1)([x_orig, x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "avZAgIvUwysH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in range(1):\n",
        "  print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325859c0-bfb5-43c8-bf8a-d3592d731d07",
        "id": "KTPvy-gAwysI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model()\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "cjfFlBj5wysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(mod_test, show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"TB\")"
      ],
      "metadata": {
        "id": "gxWhVnRkwysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "id": "hTJyGENowysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "697CMLBjwysI"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujnWEIfcwysI"
      },
      "outputs": [],
      "source": [
        "# categorical_feat = t.cat_features.copy()\n",
        "# numerical_feat = t.num_features.copy()\n",
        "\n",
        "# X_train_cat = X_enc[categorical_feat]\n",
        "# X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "# X_test_cat = test_enc[categorical_feat]\n",
        "# X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "# X_train_cat.info()\n",
        "# X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLdEjBBgwysI"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "#(units=512, last_layer=1, activation=\"relu\", dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001)\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': 0.0001, #trial.suggest_float('reg', 1e-4, 0.1, log=True),\n",
        "              \"num_transformer_heads\": trial.suggest_int(\"num_transformer_heads\", 2, 4),\n",
        "              \"transformer_units\": trial.suggest_int(\"transformer_units\", 32, 128,step=32),\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.51,step=0.03),\n",
        "              'repeat_att': trial.suggest_categorical('repeat_att', [1,2]),\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num], batch_size=1024)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyp2r5IkwysI"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57a94819-3390-4ad0-a394-174ce31ca217",
        "id": "ZVi6cBIJwysI"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 00:46:45,620] A new study created in memory with name: no-name-dc44f225-28b0-4ec0-8c91-f67686b2cd34\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  149s 35ms/step - RMSE: 42.5423 - loss: 1853.1946 - val_RMSE: 38.7149 - val_loss: 1499.9546 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9247 - loss: 1516.3540 - val_RMSE: 38.7003 - val_loss: 1499.3855 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9184 - loss: 1516.5168 - val_RMSE: 38.7284 - val_loss: 1502.1870 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9076 - loss: 1516.2740 - val_RMSE: 38.7073 - val_loss: 1500.9421 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8804 - loss: 1514.1544 - val_RMSE: 38.6908 - val_loss: 1498.8295 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8641 - loss: 1512.1082 - val_RMSE: 38.6891 - val_loss: 1498.1650 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8571 - loss: 1511.1117 - val_RMSE: 38.6884 - val_loss: 1497.8333 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8573 - loss: 1510.8917 - val_RMSE: 38.6869 - val_loss: 1497.5610 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8491 - loss: 1510.1136 - val_RMSE: 38.6872 - val_loss: 1497.4873 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8484 - loss: 1509.9669 - val_RMSE: 38.6857 - val_loss: 1497.3024 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8528 - loss: 1510.2468 - val_RMSE: 38.6860 - val_loss: 1497.2677 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8579 - loss: 1510.5892 - val_RMSE: 38.6851 - val_loss: 1497.1603 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8491 - loss: 1509.8719 - val_RMSE: 38.6859 - val_loss: 1497.1989 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8485 - loss: 1509.7949 - val_RMSE: 38.6871 - val_loss: 1497.2744 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8413 - loss: 1509.2239 - val_RMSE: 38.6854 - val_loss: 1497.1283 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8417 - loss: 1509.2438 - val_RMSE: 38.6848 - val_loss: 1497.0713 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8416 - loss: 1509.2209 - val_RMSE: 38.6846 - val_loss: 1497.0367 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8447 - loss: 1509.4489 - val_RMSE: 38.6842 - val_loss: 1496.9976 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8436 - loss: 1509.3550 - val_RMSE: 38.6838 - val_loss: 1496.9580 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8409 - loss: 1509.1313 - val_RMSE: 38.6839 - val_loss: 1496.9517 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8394 - loss: 1509.0048 - val_RMSE: 38.6835 - val_loss: 1496.9149 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8418 - loss: 1509.1865 - val_RMSE: 38.6834 - val_loss: 1496.9009 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8338 - loss: 1508.5527 - val_RMSE: 38.6832 - val_loss: 1496.8790 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8419 - loss: 1509.1719 - val_RMSE: 38.6832 - val_loss: 1496.8655 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8380 - loss: 1508.8633 - val_RMSE: 38.6831 - val_loss: 1496.8566 - learning_rate: 1.0000e-04\n",
            "1301/1301  18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  152s 35ms/step - RMSE: 42.4635 - loss: 1845.5852 - val_RMSE: 38.7378 - val_loss: 1501.7828 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8727 - loss: 1512.3889 - val_RMSE: 38.7483 - val_loss: 1503.2280 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8710 - loss: 1512.9287 - val_RMSE: 38.7473 - val_loss: 1503.9238 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8331 - loss: 1510.4143 - val_RMSE: 38.7256 - val_loss: 1501.5660 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8221 - loss: 1508.8901 - val_RMSE: 38.7289 - val_loss: 1501.3015 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8085 - loss: 1507.3918 - val_RMSE: 38.7245 - val_loss: 1500.6874 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8097 - loss: 1507.2500 - val_RMSE: 38.7236 - val_loss: 1500.4556 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8165 - loss: 1507.6310 - val_RMSE: 38.7238 - val_loss: 1500.3698 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8052 - loss: 1506.6687 - val_RMSE: 38.7224 - val_loss: 1500.1976 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8041 - loss: 1506.5164 - val_RMSE: 38.7234 - val_loss: 1500.2178 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8052 - loss: 1506.5483 - val_RMSE: 38.7254 - val_loss: 1500.3353 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8051 - loss: 1506.5110 - val_RMSE: 38.7175 - val_loss: 1499.7087 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8017 - loss: 1506.2274 - val_RMSE: 38.7176 - val_loss: 1499.6948 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.7980 - loss: 1505.9227 - val_RMSE: 38.7168 - val_loss: 1499.6138 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.7999 - loss: 1506.0510 - val_RMSE: 38.7167 - val_loss: 1499.5980 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8006 - loss: 1506.0966 - val_RMSE: 38.7167 - val_loss: 1499.5845 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.7930 - loss: 1505.4945 - val_RMSE: 38.7163 - val_loss: 1499.5337 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7964 - loss: 1505.7448 - val_RMSE: 38.7163 - val_loss: 1499.5311 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8012 - loss: 1506.1034 - val_RMSE: 38.7164 - val_loss: 1499.5283 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8004 - loss: 1506.0343 - val_RMSE: 38.7161 - val_loss: 1499.4877 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7983 - loss: 1505.8585 - val_RMSE: 38.7167 - val_loss: 1499.5251 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.7916 - loss: 1505.3295 - val_RMSE: 38.7166 - val_loss: 1499.5121 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7955 - loss: 1505.6284 - val_RMSE: 38.7167 - val_loss: 1499.5171 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7963 - loss: 1505.6843 - val_RMSE: 38.7165 - val_loss: 1499.4994 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.7963 - loss: 1505.6914 - val_RMSE: 38.7164 - val_loss: 1499.4944 - learning_rate: 1.0000e-06\n",
            "1301/1301  16s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  149s 35ms/step - RMSE: 42.4441 - loss: 1843.2596 - val_RMSE: 38.7304 - val_loss: 1501.2000 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.9038 - loss: 1514.7972 - val_RMSE: 38.7187 - val_loss: 1500.8505 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8969 - loss: 1514.8219 - val_RMSE: 38.7294 - val_loss: 1502.3921 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8973 - loss: 1515.5651 - val_RMSE: 38.7309 - val_loss: 1503.0609 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8497 - loss: 1512.0414 - val_RMSE: 38.7132 - val_loss: 1500.7770 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8386 - loss: 1510.3381 - val_RMSE: 38.7125 - val_loss: 1500.1444 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8348 - loss: 1509.5366 - val_RMSE: 38.7120 - val_loss: 1499.7932 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8308 - loss: 1508.9564 - val_RMSE: 38.7135 - val_loss: 1499.7236 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8265 - loss: 1508.4512 - val_RMSE: 38.7110 - val_loss: 1499.4092 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  57s 22ms/step - RMSE: 38.8278 - loss: 1508.4470 - val_RMSE: 38.7111 - val_loss: 1499.3374 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8293 - loss: 1508.4926 - val_RMSE: 38.7108 - val_loss: 1499.2607 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8343 - loss: 1508.8291 - val_RMSE: 38.7101 - val_loss: 1499.1632 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8220 - loss: 1507.8356 - val_RMSE: 38.7096 - val_loss: 1499.0898 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8270 - loss: 1508.1875 - val_RMSE: 38.7110 - val_loss: 1499.1808 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8256 - loss: 1508.0579 - val_RMSE: 38.7101 - val_loss: 1499.0874 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8245 - loss: 1507.9469 - val_RMSE: 38.7112 - val_loss: 1499.1582 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  57s 22ms/step - RMSE: 38.8216 - loss: 1507.7120 - val_RMSE: 38.7095 - val_loss: 1499.0039 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8235 - loss: 1507.8420 - val_RMSE: 38.7105 - val_loss: 1499.0677 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8223 - loss: 1507.7360 - val_RMSE: 38.7091 - val_loss: 1498.9581 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8203 - loss: 1507.5741 - val_RMSE: 38.7125 - val_loss: 1499.2124 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8198 - loss: 1507.5265 - val_RMSE: 38.7109 - val_loss: 1499.0803 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8171 - loss: 1507.3075 - val_RMSE: 38.7096 - val_loss: 1498.9631 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8114 - loss: 1506.8531 - val_RMSE: 38.7094 - val_loss: 1498.9340 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  57s 22ms/step - RMSE: 38.8078 - loss: 1506.5636 - val_RMSE: 38.7092 - val_loss: 1498.9062 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8137 - loss: 1507.0103 - val_RMSE: 38.7090 - val_loss: 1498.8834 - learning_rate: 1.0000e-04\n",
            "1301/1301  16s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 02:06:26,496] Trial 0 finished with value: 38.70286560058594 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.44999999999999996, 'repeat_att': 1}. Best is trial 0 with value: 38.70286560058594.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  154s 35ms/step - RMSE: 43.3852 - loss: 1934.7306 - val_RMSE: 38.7027 - val_loss: 1498.9362 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  60s 23ms/step - RMSE: 39.0693 - loss: 1527.6146 - val_RMSE: 38.6950 - val_loss: 1498.9910 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 39.0576 - loss: 1527.3376 - val_RMSE: 38.6957 - val_loss: 1499.5364 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9966 - loss: 1522.7792 - val_RMSE: 38.6897 - val_loss: 1498.5286 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9863 - loss: 1521.4486 - val_RMSE: 38.6876 - val_loss: 1497.9673 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9687 - loss: 1519.7284 - val_RMSE: 38.6857 - val_loss: 1497.5868 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9707 - loss: 1519.6841 - val_RMSE: 38.6867 - val_loss: 1497.5294 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9633 - loss: 1518.9814 - val_RMSE: 38.6871 - val_loss: 1497.4590 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9621 - loss: 1518.7979 - val_RMSE: 38.6859 - val_loss: 1497.2992 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9610 - loss: 1518.6493 - val_RMSE: 38.6857 - val_loss: 1497.2311 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9632 - loss: 1518.7689 - val_RMSE: 38.6860 - val_loss: 1497.2183 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9496 - loss: 1517.6786 - val_RMSE: 38.6855 - val_loss: 1497.1537 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9574 - loss: 1518.2554 - val_RMSE: 38.6853 - val_loss: 1497.1129 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9451 - loss: 1517.2749 - val_RMSE: 38.6860 - val_loss: 1497.1459 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9440 - loss: 1517.1774 - val_RMSE: 38.6850 - val_loss: 1497.0625 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9448 - loss: 1517.2255 - val_RMSE: 38.6860 - val_loss: 1497.1222 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9347 - loss: 1516.4252 - val_RMSE: 38.6840 - val_loss: 1496.9670 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9362 - loss: 1516.5424 - val_RMSE: 38.6845 - val_loss: 1496.9954 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9342 - loss: 1516.3783 - val_RMSE: 38.6845 - val_loss: 1496.9844 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9261 - loss: 1515.7404 - val_RMSE: 38.6839 - val_loss: 1496.9280 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9264 - loss: 1515.7483 - val_RMSE: 38.6836 - val_loss: 1496.8975 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9230 - loss: 1515.4722 - val_RMSE: 38.6835 - val_loss: 1496.8818 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9199 - loss: 1515.2224 - val_RMSE: 38.6836 - val_loss: 1496.8789 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9283 - loss: 1515.8676 - val_RMSE: 38.6840 - val_loss: 1496.9043 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9230 - loss: 1515.4517 - val_RMSE: 38.6840 - val_loss: 1496.8937 - learning_rate: 1.0000e-04\n",
            "1301/1301  16s 9ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  147s 35ms/step - RMSE: 43.3502 - loss: 1931.5552 - val_RMSE: 38.7316 - val_loss: 1501.1777 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  60s 23ms/step - RMSE: 39.0198 - loss: 1523.7283 - val_RMSE: 38.7399 - val_loss: 1502.3658 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9995 - loss: 1522.7172 - val_RMSE: 38.7387 - val_loss: 1502.7998 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9452 - loss: 1518.7039 - val_RMSE: 38.7208 - val_loss: 1500.8723 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9316 - loss: 1517.1335 - val_RMSE: 38.7213 - val_loss: 1500.5249 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9314 - loss: 1516.7751 - val_RMSE: 38.7199 - val_loss: 1500.1968 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9249 - loss: 1516.0735 - val_RMSE: 38.7192 - val_loss: 1500.0155 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9205 - loss: 1515.6187 - val_RMSE: 38.7187 - val_loss: 1499.8903 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9211 - loss: 1515.5809 - val_RMSE: 38.7194 - val_loss: 1499.8732 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9138 - loss: 1514.9574 - val_RMSE: 38.7185 - val_loss: 1499.7616 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9080 - loss: 1514.4657 - val_RMSE: 38.7178 - val_loss: 1499.6732 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9072 - loss: 1514.3712 - val_RMSE: 38.7176 - val_loss: 1499.6364 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.9051 - loss: 1514.1798 - val_RMSE: 38.7188 - val_loss: 1499.7069 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8932 - loss: 1513.2394 - val_RMSE: 38.7182 - val_loss: 1499.6433 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8962 - loss: 1513.4537 - val_RMSE: 38.7157 - val_loss: 1499.4407 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8876 - loss: 1512.7773 - val_RMSE: 38.7159 - val_loss: 1499.4401 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8815 - loss: 1512.2913 - val_RMSE: 38.7154 - val_loss: 1499.3939 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8888 - loss: 1512.8477 - val_RMSE: 38.7155 - val_loss: 1499.3929 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8924 - loss: 1513.1184 - val_RMSE: 38.7154 - val_loss: 1499.3776 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  58s 22ms/step - RMSE: 38.8854 - loss: 1512.5703 - val_RMSE: 38.7153 - val_loss: 1499.3597 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8868 - loss: 1512.6654 - val_RMSE: 38.7155 - val_loss: 1499.3667 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8918 - loss: 1513.0470 - val_RMSE: 38.7155 - val_loss: 1499.3618 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8939 - loss: 1513.2028 - val_RMSE: 38.7155 - val_loss: 1499.3582 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8927 - loss: 1513.1119 - val_RMSE: 38.7155 - val_loss: 1499.3569 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  59s 22ms/step - RMSE: 38.8863 - loss: 1512.6150 - val_RMSE: 38.7154 - val_loss: 1499.3484 - learning_rate: 1.0000e-05\n",
            "1301/1301  17s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  157s 36ms/step - RMSE: 43.2625 - loss: 1922.0507 - val_RMSE: 38.7210 - val_loss: 1500.3311 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  62s 24ms/step - RMSE: 39.0474 - loss: 1525.8640 - val_RMSE: 38.7199 - val_loss: 1500.7574 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  61s 23ms/step - RMSE: 39.0306 - loss: 1525.0643 - val_RMSE: 38.7252 - val_loss: 1501.7371 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9748 - loss: 1520.9982 - val_RMSE: 38.7165 - val_loss: 1500.5239 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9541 - loss: 1518.8676 - val_RMSE: 38.7149 - val_loss: 1500.0076 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9517 - loss: 1518.3409 - val_RMSE: 38.7143 - val_loss: 1499.7439 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9435 - loss: 1517.5121 - val_RMSE: 38.7116 - val_loss: 1499.4058 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9468 - loss: 1517.6493 - val_RMSE: 38.7124 - val_loss: 1499.3759 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9462 - loss: 1517.5144 - val_RMSE: 38.7118 - val_loss: 1499.2648 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9401 - loss: 1516.9845 - val_RMSE: 38.7118 - val_loss: 1499.2213 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9376 - loss: 1516.7467 - val_RMSE: 38.7111 - val_loss: 1499.1396 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  64s 24ms/step - RMSE: 38.9317 - loss: 1516.2589 - val_RMSE: 38.7106 - val_loss: 1499.0748 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9274 - loss: 1515.9056 - val_RMSE: 38.7104 - val_loss: 1499.0415 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9280 - loss: 1515.9312 - val_RMSE: 38.7108 - val_loss: 1499.0625 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9181 - loss: 1515.1509 - val_RMSE: 38.7099 - val_loss: 1498.9727 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9195 - loss: 1515.2476 - val_RMSE: 38.7110 - val_loss: 1499.0500 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  65s 25ms/step - RMSE: 38.9204 - loss: 1515.3063 - val_RMSE: 38.7099 - val_loss: 1498.9590 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9077 - loss: 1514.3134 - val_RMSE: 38.7093 - val_loss: 1498.9106 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9087 - loss: 1514.3798 - val_RMSE: 38.7105 - val_loss: 1498.9954 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9068 - loss: 1514.2285 - val_RMSE: 38.7107 - val_loss: 1499.0084 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  64s 24ms/step - RMSE: 38.8995 - loss: 1513.6593 - val_RMSE: 38.7079 - val_loss: 1498.7753 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8973 - loss: 1513.4734 - val_RMSE: 38.7077 - val_loss: 1498.7532 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  64s 24ms/step - RMSE: 38.8961 - loss: 1513.3704 - val_RMSE: 38.7082 - val_loss: 1498.7817 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  64s 25ms/step - RMSE: 38.9010 - loss: 1513.7474 - val_RMSE: 38.7080 - val_loss: 1498.7625 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  64s 25ms/step - RMSE: 38.9025 - loss: 1513.8590 - val_RMSE: 38.7075 - val_loss: 1498.7212 - learning_rate: 1.0000e-05\n",
            "1301/1301  17s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 03:28:09,029] Trial 1 finished with value: 38.70228703816732 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'selu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.48, 'repeat_att': 1}. Best is trial 1 with value: 38.70228703816732.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  147s 35ms/step - RMSE: 43.2199 - loss: 1919.5537 - val_RMSE: 38.7000 - val_loss: 1498.5961 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9782 - loss: 1520.3369 - val_RMSE: 38.6960 - val_loss: 1498.7506 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9793 - loss: 1520.8539 - val_RMSE: 38.7000 - val_loss: 1499.3563 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9302 - loss: 1517.1078 - val_RMSE: 38.6875 - val_loss: 1497.9254 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9197 - loss: 1515.8606 - val_RMSE: 38.6853 - val_loss: 1497.4617 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9056 - loss: 1514.5089 - val_RMSE: 38.6849 - val_loss: 1497.2765 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9104 - loss: 1514.7513 - val_RMSE: 38.6849 - val_loss: 1497.1888 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9049 - loss: 1514.2415 - val_RMSE: 38.6845 - val_loss: 1497.0953 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9060 - loss: 1514.2686 - val_RMSE: 38.6845 - val_loss: 1497.0533 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.9000 - loss: 1513.7655 - val_RMSE: 38.6841 - val_loss: 1496.9913 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9058 - loss: 1514.1903 - val_RMSE: 38.6838 - val_loss: 1496.9438 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8927 - loss: 1513.1487 - val_RMSE: 38.6834 - val_loss: 1496.9041 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9035 - loss: 1513.9727 - val_RMSE: 38.6831 - val_loss: 1496.8627 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8964 - loss: 1513.4114 - val_RMSE: 38.6833 - val_loss: 1496.8708 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8967 - loss: 1513.4277 - val_RMSE: 38.6825 - val_loss: 1496.8010 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8926 - loss: 1513.1035 - val_RMSE: 38.6832 - val_loss: 1496.8527 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8866 - loss: 1512.6248 - val_RMSE: 38.6826 - val_loss: 1496.8025 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8882 - loss: 1512.7504 - val_RMSE: 38.6817 - val_loss: 1496.7234 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8858 - loss: 1512.5521 - val_RMSE: 38.6815 - val_loss: 1496.6926 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8798 - loss: 1512.0736 - val_RMSE: 38.6814 - val_loss: 1496.6783 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8808 - loss: 1512.1464 - val_RMSE: 38.6811 - val_loss: 1496.6537 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8835 - loss: 1512.3491 - val_RMSE: 38.6811 - val_loss: 1496.6418 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8788 - loss: 1511.9792 - val_RMSE: 38.6811 - val_loss: 1496.6355 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8838 - loss: 1512.3633 - val_RMSE: 38.6809 - val_loss: 1496.6188 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8828 - loss: 1512.2748 - val_RMSE: 38.6809 - val_loss: 1496.6119 - learning_rate: 1.0000e-04\n",
            "1301/1301  18s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  148s 35ms/step - RMSE: 43.1828 - loss: 1916.1990 - val_RMSE: 38.7341 - val_loss: 1501.2308 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9260 - loss: 1516.2416 - val_RMSE: 38.7522 - val_loss: 1503.1017 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9187 - loss: 1516.1676 - val_RMSE: 38.7418 - val_loss: 1502.6028 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8751 - loss: 1512.8329 - val_RMSE: 38.7197 - val_loss: 1500.4335 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8671 - loss: 1511.7841 - val_RMSE: 38.7194 - val_loss: 1500.1112 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8683 - loss: 1511.6200 - val_RMSE: 38.7181 - val_loss: 1499.8542 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8590 - loss: 1510.7631 - val_RMSE: 38.7174 - val_loss: 1499.7125 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8560 - loss: 1510.4453 - val_RMSE: 38.7172 - val_loss: 1499.6349 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8590 - loss: 1510.6244 - val_RMSE: 38.7175 - val_loss: 1499.6127 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8564 - loss: 1510.3749 - val_RMSE: 38.7168 - val_loss: 1499.5217 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8506 - loss: 1509.9030 - val_RMSE: 38.7158 - val_loss: 1499.4307 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8516 - loss: 1509.9583 - val_RMSE: 38.7153 - val_loss: 1499.3761 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8479 - loss: 1509.6581 - val_RMSE: 38.7153 - val_loss: 1499.3641 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8450 - loss: 1509.4153 - val_RMSE: 38.7172 - val_loss: 1499.5005 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8449 - loss: 1509.4033 - val_RMSE: 38.7163 - val_loss: 1499.4207 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8356 - loss: 1508.6681 - val_RMSE: 38.7119 - val_loss: 1499.0619 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8357 - loss: 1508.6619 - val_RMSE: 38.7116 - val_loss: 1499.0386 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8349 - loss: 1508.5945 - val_RMSE: 38.7117 - val_loss: 1499.0326 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8381 - loss: 1508.8324 - val_RMSE: 38.7116 - val_loss: 1499.0187 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8330 - loss: 1508.4298 - val_RMSE: 38.7115 - val_loss: 1499.0021 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8333 - loss: 1508.4491 - val_RMSE: 38.7115 - val_loss: 1498.9976 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8369 - loss: 1508.7228 - val_RMSE: 38.7116 - val_loss: 1498.9965 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8395 - loss: 1508.9120 - val_RMSE: 38.7117 - val_loss: 1499.0038 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  61s 24ms/step - RMSE: 38.8379 - loss: 1508.7859 - val_RMSE: 38.7117 - val_loss: 1498.9905 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8321 - loss: 1508.3302 - val_RMSE: 38.7114 - val_loss: 1498.9688 - learning_rate: 1.0000e-04\n",
            "1301/1301  17s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  155s 36ms/step - RMSE: 43.1270 - loss: 1909.9421 - val_RMSE: 38.7296 - val_loss: 1500.8849 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9558 - loss: 1518.5751 - val_RMSE: 38.7394 - val_loss: 1502.0955 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.9556 - loss: 1518.9839 - val_RMSE: 38.7361 - val_loss: 1502.3036 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9071 - loss: 1515.4553 - val_RMSE: 38.7132 - val_loss: 1500.0302 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8936 - loss: 1513.9342 - val_RMSE: 38.7108 - val_loss: 1499.5002 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8915 - loss: 1513.4712 - val_RMSE: 38.7099 - val_loss: 1499.2487 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8826 - loss: 1512.6215 - val_RMSE: 38.7114 - val_loss: 1499.2659 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8900 - loss: 1513.1058 - val_RMSE: 38.7099 - val_loss: 1499.0817 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8863 - loss: 1512.7578 - val_RMSE: 38.7094 - val_loss: 1498.9949 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8821 - loss: 1512.3795 - val_RMSE: 38.7107 - val_loss: 1499.0641 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8840 - loss: 1512.5049 - val_RMSE: 38.7088 - val_loss: 1498.8883 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8795 - loss: 1512.1324 - val_RMSE: 38.7078 - val_loss: 1498.7916 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8736 - loss: 1511.6577 - val_RMSE: 38.7090 - val_loss: 1498.8766 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8795 - loss: 1512.0961 - val_RMSE: 38.7102 - val_loss: 1498.9519 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8671 - loss: 1511.1216 - val_RMSE: 38.7067 - val_loss: 1498.6674 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8667 - loss: 1511.0752 - val_RMSE: 38.7062 - val_loss: 1498.6179 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8728 - loss: 1511.5441 - val_RMSE: 38.7059 - val_loss: 1498.5858 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8655 - loss: 1510.9685 - val_RMSE: 38.7058 - val_loss: 1498.5698 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8659 - loss: 1510.9923 - val_RMSE: 38.7056 - val_loss: 1498.5522 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8658 - loss: 1510.9742 - val_RMSE: 38.7058 - val_loss: 1498.5548 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  70s 27ms/step - RMSE: 38.8660 - loss: 1510.9816 - val_RMSE: 38.7055 - val_loss: 1498.5292 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  69s 27ms/step - RMSE: 38.8629 - loss: 1510.7365 - val_RMSE: 38.7055 - val_loss: 1498.5220 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  67s 26ms/step - RMSE: 38.8638 - loss: 1510.8021 - val_RMSE: 38.7057 - val_loss: 1498.5306 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8657 - loss: 1510.9388 - val_RMSE: 38.7055 - val_loss: 1498.5111 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  67s 26ms/step - RMSE: 38.8657 - loss: 1510.9323 - val_RMSE: 38.7055 - val_loss: 1498.5066 - learning_rate: 1.0000e-04\n",
            "1301/1301  17s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 04:50:47,418] Trial 2 finished with value: 38.699286142985024 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  153s 37ms/step - RMSE: 43.1467 - loss: 1912.6401 - val_RMSE: 38.6983 - val_loss: 1497.9614 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.9335 - loss: 1516.2772 - val_RMSE: 38.6935 - val_loss: 1497.8076 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.9336 - loss: 1516.4974 - val_RMSE: 38.6955 - val_loss: 1498.1663 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.9154 - loss: 1515.2963 - val_RMSE: 38.6973 - val_loss: 1498.4552 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8831 - loss: 1512.8254 - val_RMSE: 38.6868 - val_loss: 1497.4468 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8683 - loss: 1511.4871 - val_RMSE: 38.6863 - val_loss: 1497.2582 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8694 - loss: 1511.4333 - val_RMSE: 38.6862 - val_loss: 1497.1595 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8638 - loss: 1510.9166 - val_RMSE: 38.6854 - val_loss: 1497.0293 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8688 - loss: 1511.2439 - val_RMSE: 38.6858 - val_loss: 1497.0173 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8612 - loss: 1510.6085 - val_RMSE: 38.6854 - val_loss: 1496.9486 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8672 - loss: 1511.0455 - val_RMSE: 38.6846 - val_loss: 1496.8655 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8593 - loss: 1510.4016 - val_RMSE: 38.6850 - val_loss: 1496.8708 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8674 - loss: 1511.0186 - val_RMSE: 38.6845 - val_loss: 1496.8170 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8612 - loss: 1510.5210 - val_RMSE: 38.6855 - val_loss: 1496.8867 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8607 - loss: 1510.4718 - val_RMSE: 38.6845 - val_loss: 1496.7950 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8577 - loss: 1510.2253 - val_RMSE: 38.6849 - val_loss: 1496.8239 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8546 - loss: 1509.9801 - val_RMSE: 38.6836 - val_loss: 1496.7120 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8552 - loss: 1510.0190 - val_RMSE: 38.6834 - val_loss: 1496.6898 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8520 - loss: 1509.7625 - val_RMSE: 38.6835 - val_loss: 1496.6901 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8515 - loss: 1509.7184 - val_RMSE: 38.6841 - val_loss: 1496.7323 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8479 - loss: 1509.4292 - val_RMSE: 38.6834 - val_loss: 1496.6722 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8515 - loss: 1509.7061 - val_RMSE: 38.6829 - val_loss: 1496.6322 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  64s 24ms/step - RMSE: 38.8445 - loss: 1509.1569 - val_RMSE: 38.6829 - val_loss: 1496.6284 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8512 - loss: 1509.6818 - val_RMSE: 38.6827 - val_loss: 1496.6064 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  59s 23ms/step - RMSE: 38.8499 - loss: 1509.5737 - val_RMSE: 38.6826 - val_loss: 1496.6011 - learning_rate: 1.0000e-04\n",
            "1301/1301  15s 9ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  159s 38ms/step - RMSE: 43.0882 - loss: 1907.0781 - val_RMSE: 38.7436 - val_loss: 1501.4581 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8796 - loss: 1512.0743 - val_RMSE: 38.7295 - val_loss: 1500.6298 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8777 - loss: 1512.2151 - val_RMSE: 38.7367 - val_loss: 1501.4878 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  65s 25ms/step - RMSE: 38.8659 - loss: 1511.5642 - val_RMSE: 38.7309 - val_loss: 1501.2053 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  61s 23ms/step - RMSE: 38.8290 - loss: 1508.7550 - val_RMSE: 38.7219 - val_loss: 1500.2830 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  65s 25ms/step - RMSE: 38.8282 - loss: 1508.4794 - val_RMSE: 38.7212 - val_loss: 1500.0447 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8215 - loss: 1507.7941 - val_RMSE: 38.7204 - val_loss: 1499.8717 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  66s 26ms/step - RMSE: 38.8174 - loss: 1507.3712 - val_RMSE: 38.7206 - val_loss: 1499.8170 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  68s 26ms/step - RMSE: 38.8207 - loss: 1507.5630 - val_RMSE: 38.7213 - val_loss: 1499.8226 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  68s 26ms/step - RMSE: 38.8193 - loss: 1507.4075 - val_RMSE: 38.7201 - val_loss: 1499.6876 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8152 - loss: 1507.0485 - val_RMSE: 38.7198 - val_loss: 1499.6343 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  60s 23ms/step - RMSE: 38.8172 - loss: 1507.1785 - val_RMSE: 38.7192 - val_loss: 1499.5641 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8114 - loss: 1506.7085 - val_RMSE: 38.7183 - val_loss: 1499.4696 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  66s 26ms/step - RMSE: 38.8119 - loss: 1506.7212 - val_RMSE: 38.7187 - val_loss: 1499.4884 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8116 - loss: 1506.6865 - val_RMSE: 38.7199 - val_loss: 1499.5673 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8046 - loss: 1506.1307 - val_RMSE: 38.7150 - val_loss: 1499.1841 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  64s 25ms/step - RMSE: 38.8029 - loss: 1505.9951 - val_RMSE: 38.7150 - val_loss: 1499.1755 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  67s 26ms/step - RMSE: 38.8021 - loss: 1505.9320 - val_RMSE: 38.7147 - val_loss: 1499.1523 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  66s 25ms/step - RMSE: 38.8067 - loss: 1506.2850 - val_RMSE: 38.7145 - val_loss: 1499.1357 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8023 - loss: 1505.9385 - val_RMSE: 38.7146 - val_loss: 1499.1328 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  67s 26ms/step - RMSE: 38.8052 - loss: 1506.1622 - val_RMSE: 38.7145 - val_loss: 1499.1202 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  62s 24ms/step - RMSE: 38.8058 - loss: 1506.2058 - val_RMSE: 38.7143 - val_loss: 1499.1085 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  68s 26ms/step - RMSE: 38.8083 - loss: 1506.3923 - val_RMSE: 38.7142 - val_loss: 1499.0948 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  68s 26ms/step - RMSE: 38.8070 - loss: 1506.2932 - val_RMSE: 38.7142 - val_loss: 1499.0923 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  63s 24ms/step - RMSE: 38.8021 - loss: 1505.9102 - val_RMSE: 38.7140 - val_loss: 1499.0754 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  117s 27ms/step - RMSE: 43.0855 - loss: 1906.2587 - val_RMSE: 38.7267 - val_loss: 1500.1206 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9105 - loss: 1514.4437 - val_RMSE: 38.7184 - val_loss: 1499.7040 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9080 - loss: 1514.4988 - val_RMSE: 38.7158 - val_loss: 1499.8019 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8925 - loss: 1513.6099 - val_RMSE: 38.7175 - val_loss: 1500.1287 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8570 - loss: 1510.8955 - val_RMSE: 38.7136 - val_loss: 1499.6031 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8476 - loss: 1509.9430 - val_RMSE: 38.7124 - val_loss: 1499.3268 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8423 - loss: 1509.3777 - val_RMSE: 38.7118 - val_loss: 1499.1818 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8473 - loss: 1509.6743 - val_RMSE: 38.7117 - val_loss: 1499.1071 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8451 - loss: 1509.4404 - val_RMSE: 38.7103 - val_loss: 1498.9496 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8402 - loss: 1509.0154 - val_RMSE: 38.7104 - val_loss: 1498.9240 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8427 - loss: 1509.1755 - val_RMSE: 38.7087 - val_loss: 1498.7606 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8390 - loss: 1508.8665 - val_RMSE: 38.7081 - val_loss: 1498.6951 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8345 - loss: 1508.4982 - val_RMSE: 38.7088 - val_loss: 1498.7297 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8402 - loss: 1508.9176 - val_RMSE: 38.7108 - val_loss: 1498.8707 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8328 - loss: 1508.3344 - val_RMSE: 38.7076 - val_loss: 1498.6187 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8302 - loss: 1508.1289 - val_RMSE: 38.7085 - val_loss: 1498.6838 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8355 - loss: 1508.5327 - val_RMSE: 38.7080 - val_loss: 1498.6445 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8296 - loss: 1508.0712 - val_RMSE: 38.7077 - val_loss: 1498.6215 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8297 - loss: 1508.0819 - val_RMSE: 38.7077 - val_loss: 1498.6205 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8306 - loss: 1508.1505 - val_RMSE: 38.7078 - val_loss: 1498.6224 - learning_rate: 1.0000e-06\n",
            "Epoch 21/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8293 - loss: 1508.0455 - val_RMSE: 38.7079 - val_loss: 1498.6296 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8299 - loss: 1508.0964 - val_RMSE: 38.7076 - val_loss: 1498.6091 - learning_rate: 1.0000e-07\n",
            "Epoch 23/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8292 - loss: 1508.0363 - val_RMSE: 38.7081 - val_loss: 1498.6482 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8291 - loss: 1508.0310 - val_RMSE: 38.7076 - val_loss: 1498.6090 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8305 - loss: 1508.1370 - val_RMSE: 38.7077 - val_loss: 1498.6216 - learning_rate: 1.0000e-07\n",
            "1301/1301  11s 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 06:05:34,671] Trial 3 finished with value: 38.701454162597656 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 4, 'transformer_units': 128, 'dropout_rate': 0.32999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  114s 25ms/step - RMSE: 43.1435 - loss: 1912.3213 - val_RMSE: 38.7015 - val_loss: 1498.2279 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.9314 - loss: 1516.1379 - val_RMSE: 38.6943 - val_loss: 1497.9430 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  35s 13ms/step - RMSE: 38.9339 - loss: 1516.6235 - val_RMSE: 38.7009 - val_loss: 1498.7732 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.9153 - loss: 1515.4824 - val_RMSE: 38.6954 - val_loss: 1498.4922 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8841 - loss: 1513.0718 - val_RMSE: 38.6879 - val_loss: 1497.6763 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  35s 14ms/step - RMSE: 38.8692 - loss: 1511.6859 - val_RMSE: 38.6870 - val_loss: 1497.4207 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.8696 - loss: 1511.5499 - val_RMSE: 38.6868 - val_loss: 1497.2927 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.8653 - loss: 1511.1191 - val_RMSE: 38.6867 - val_loss: 1497.2096 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8689 - loss: 1511.3248 - val_RMSE: 38.6857 - val_loss: 1497.0808 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8624 - loss: 1510.7758 - val_RMSE: 38.6853 - val_loss: 1497.0159 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8672 - loss: 1511.1150 - val_RMSE: 38.6857 - val_loss: 1497.0123 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  36s 14ms/step - RMSE: 38.8592 - loss: 1510.4672 - val_RMSE: 38.6860 - val_loss: 1497.0164 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  35s 14ms/step - RMSE: 38.8670 - loss: 1511.0509 - val_RMSE: 38.6847 - val_loss: 1496.9001 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  35s 14ms/step - RMSE: 38.8603 - loss: 1510.5122 - val_RMSE: 38.6858 - val_loss: 1496.9672 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8612 - loss: 1510.5704 - val_RMSE: 38.6844 - val_loss: 1496.8457 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8588 - loss: 1510.3680 - val_RMSE: 38.6852 - val_loss: 1496.9039 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8553 - loss: 1510.0886 - val_RMSE: 38.6838 - val_loss: 1496.7844 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8563 - loss: 1510.1576 - val_RMSE: 38.6840 - val_loss: 1496.7878 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8532 - loss: 1509.9043 - val_RMSE: 38.6835 - val_loss: 1496.7439 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8521 - loss: 1509.8169 - val_RMSE: 38.6840 - val_loss: 1496.7772 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8500 - loss: 1509.6456 - val_RMSE: 38.6842 - val_loss: 1496.7865 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8509 - loss: 1509.7151 - val_RMSE: 38.6827 - val_loss: 1496.6647 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8442 - loss: 1509.1863 - val_RMSE: 38.6823 - val_loss: 1496.6378 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8501 - loss: 1509.6471 - val_RMSE: 38.6819 - val_loss: 1496.6010 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8485 - loss: 1509.5172 - val_RMSE: 38.6822 - val_loss: 1496.6163 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  123s 26ms/step - RMSE: 43.0874 - loss: 1906.9557 - val_RMSE: 38.7412 - val_loss: 1501.3207 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8807 - loss: 1512.2094 - val_RMSE: 38.7461 - val_loss: 1501.9521 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8773 - loss: 1512.2358 - val_RMSE: 38.7442 - val_loss: 1502.0736 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8416 - loss: 1509.5913 - val_RMSE: 38.7219 - val_loss: 1500.1827 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8313 - loss: 1508.6251 - val_RMSE: 38.7229 - val_loss: 1500.1093 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8353 - loss: 1508.7992 - val_RMSE: 38.7220 - val_loss: 1499.9417 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8277 - loss: 1508.1227 - val_RMSE: 38.7202 - val_loss: 1499.7432 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8231 - loss: 1507.7129 - val_RMSE: 38.7212 - val_loss: 1499.7777 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8271 - loss: 1507.9814 - val_RMSE: 38.7192 - val_loss: 1499.5903 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8260 - loss: 1507.8699 - val_RMSE: 38.7192 - val_loss: 1499.5653 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8207 - loss: 1507.4343 - val_RMSE: 38.7185 - val_loss: 1499.4913 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8225 - loss: 1507.5547 - val_RMSE: 38.7186 - val_loss: 1499.4832 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8168 - loss: 1507.0968 - val_RMSE: 38.7168 - val_loss: 1499.3308 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8162 - loss: 1507.0337 - val_RMSE: 38.7181 - val_loss: 1499.4225 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8180 - loss: 1507.1685 - val_RMSE: 38.7195 - val_loss: 1499.5259 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8098 - loss: 1506.5255 - val_RMSE: 38.7122 - val_loss: 1498.9563 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8071 - loss: 1506.3075 - val_RMSE: 38.7117 - val_loss: 1498.9125 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8066 - loss: 1506.2655 - val_RMSE: 38.7118 - val_loss: 1498.9138 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8111 - loss: 1506.6099 - val_RMSE: 38.7115 - val_loss: 1498.8809 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8070 - loss: 1506.2926 - val_RMSE: 38.7115 - val_loss: 1498.8854 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8087 - loss: 1506.4160 - val_RMSE: 38.7115 - val_loss: 1498.8835 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8091 - loss: 1506.4424 - val_RMSE: 38.7112 - val_loss: 1498.8561 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8121 - loss: 1506.6777 - val_RMSE: 38.7112 - val_loss: 1498.8525 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8110 - loss: 1506.5945 - val_RMSE: 38.7111 - val_loss: 1498.8486 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8058 - loss: 1506.1896 - val_RMSE: 38.7111 - val_loss: 1498.8445 - learning_rate: 1.0000e-05\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  110s 24ms/step - RMSE: 43.0833 - loss: 1906.1544 - val_RMSE: 38.7420 - val_loss: 1501.3306 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.9094 - loss: 1514.3882 - val_RMSE: 38.7177 - val_loss: 1499.7139 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9081 - loss: 1514.5768 - val_RMSE: 38.7518 - val_loss: 1502.6815 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8929 - loss: 1513.7024 - val_RMSE: 38.7400 - val_loss: 1501.9214 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8572 - loss: 1510.9570 - val_RMSE: 38.7119 - val_loss: 1499.5076 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8485 - loss: 1510.0437 - val_RMSE: 38.7106 - val_loss: 1499.2214 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8440 - loss: 1509.5364 - val_RMSE: 38.7105 - val_loss: 1499.1041 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8475 - loss: 1509.7103 - val_RMSE: 38.7099 - val_loss: 1498.9866 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8451 - loss: 1509.4650 - val_RMSE: 38.7095 - val_loss: 1498.9097 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8402 - loss: 1509.0345 - val_RMSE: 38.7087 - val_loss: 1498.8076 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8440 - loss: 1509.2955 - val_RMSE: 38.7089 - val_loss: 1498.8007 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8396 - loss: 1508.9308 - val_RMSE: 38.7083 - val_loss: 1498.7338 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8348 - loss: 1508.5409 - val_RMSE: 38.7094 - val_loss: 1498.8029 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8410 - loss: 1509.0067 - val_RMSE: 38.7087 - val_loss: 1498.7296 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8353 - loss: 1508.5453 - val_RMSE: 38.7075 - val_loss: 1498.6260 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8333 - loss: 1508.3806 - val_RMSE: 38.7075 - val_loss: 1498.6204 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8387 - loss: 1508.7925 - val_RMSE: 38.7073 - val_loss: 1498.5940 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8314 - loss: 1508.2211 - val_RMSE: 38.7061 - val_loss: 1498.4948 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  38s 14ms/step - RMSE: 38.8298 - loss: 1508.0856 - val_RMSE: 38.7073 - val_loss: 1498.5839 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8288 - loss: 1508.0077 - val_RMSE: 38.7078 - val_loss: 1498.6121 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8264 - loss: 1507.8097 - val_RMSE: 38.7056 - val_loss: 1498.4427 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8253 - loss: 1507.7246 - val_RMSE: 38.7052 - val_loss: 1498.4094 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8239 - loss: 1507.6123 - val_RMSE: 38.7052 - val_loss: 1498.4077 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8239 - loss: 1507.6039 - val_RMSE: 38.7051 - val_loss: 1498.3900 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8249 - loss: 1507.6761 - val_RMSE: 38.7051 - val_loss: 1498.3849 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 06:58:27,923] Trial 4 finished with value: 38.699440002441406 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'num_transformer_heads': 2, 'transformer_units': 64, 'dropout_rate': 0.32999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  115s 26ms/step - RMSE: 45.7555 - loss: 2168.1316 - val_RMSE: 38.7151 - val_loss: 1499.0077 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.5285 - loss: 1562.6776 - val_RMSE: 38.6991 - val_loss: 1497.8905 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.3829 - loss: 1551.3192 - val_RMSE: 38.6971 - val_loss: 1497.9207 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.2354 - loss: 1539.9041 - val_RMSE: 38.6969 - val_loss: 1498.0271 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1086 - loss: 1530.0475 - val_RMSE: 38.6931 - val_loss: 1497.6881 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0863 - loss: 1528.2585 - val_RMSE: 38.6916 - val_loss: 1497.5175 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0768 - loss: 1527.4636 - val_RMSE: 38.6921 - val_loss: 1497.5164 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0539 - loss: 1525.6434 - val_RMSE: 38.6907 - val_loss: 1497.3795 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0479 - loss: 1525.1437 - val_RMSE: 38.6900 - val_loss: 1497.3008 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0396 - loss: 1524.4745 - val_RMSE: 38.6901 - val_loss: 1497.2837 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  40s 16ms/step - RMSE: 39.0198 - loss: 1522.9083 - val_RMSE: 38.6902 - val_loss: 1497.2766 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0145 - loss: 1522.4779 - val_RMSE: 38.6895 - val_loss: 1497.2114 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.0014 - loss: 1521.4373 - val_RMSE: 38.6881 - val_loss: 1497.0848 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.9961 - loss: 1521.0112 - val_RMSE: 38.6888 - val_loss: 1497.1262 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.9817 - loss: 1519.8778 - val_RMSE: 38.6881 - val_loss: 1497.0613 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.9768 - loss: 1519.4847 - val_RMSE: 38.6870 - val_loss: 1496.9670 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9651 - loss: 1518.5574 - val_RMSE: 38.6878 - val_loss: 1497.0236 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9532 - loss: 1517.6261 - val_RMSE: 38.6872 - val_loss: 1496.9645 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.9506 - loss: 1517.4102 - val_RMSE: 38.6877 - val_loss: 1496.9958 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9383 - loss: 1516.4506 - val_RMSE: 38.6879 - val_loss: 1497.0070 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9326 - loss: 1515.9972 - val_RMSE: 38.6870 - val_loss: 1496.9302 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9297 - loss: 1515.7722 - val_RMSE: 38.6865 - val_loss: 1496.8931 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9312 - loss: 1515.8837 - val_RMSE: 38.6862 - val_loss: 1496.8665 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9303 - loss: 1515.8151 - val_RMSE: 38.6860 - val_loss: 1496.8502 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9209 - loss: 1515.0839 - val_RMSE: 38.6862 - val_loss: 1496.8690 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  123s 27ms/step - RMSE: 45.7189 - loss: 2164.6008 - val_RMSE: 38.7478 - val_loss: 1501.5664 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.4763 - loss: 1558.5763 - val_RMSE: 38.7292 - val_loss: 1500.2493 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.3425 - loss: 1548.1737 - val_RMSE: 38.7311 - val_loss: 1500.5592 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1711 - loss: 1534.8774 - val_RMSE: 38.7272 - val_loss: 1500.3629 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0583 - loss: 1526.1018 - val_RMSE: 38.7210 - val_loss: 1499.8284 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0409 - loss: 1524.6914 - val_RMSE: 38.7202 - val_loss: 1499.7159 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.0269 - loss: 1523.5452 - val_RMSE: 38.7201 - val_loss: 1499.6591 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0149 - loss: 1522.5728 - val_RMSE: 38.7184 - val_loss: 1499.5040 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9983 - loss: 1521.2456 - val_RMSE: 38.7184 - val_loss: 1499.4797 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.9966 - loss: 1521.0936 - val_RMSE: 38.7172 - val_loss: 1499.3621 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9828 - loss: 1519.9980 - val_RMSE: 38.7179 - val_loss: 1499.4021 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9686 - loss: 1518.8765 - val_RMSE: 38.7168 - val_loss: 1499.3005 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9579 - loss: 1518.0286 - val_RMSE: 38.7175 - val_loss: 1499.3401 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9451 - loss: 1517.0153 - val_RMSE: 38.7166 - val_loss: 1499.2563 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9369 - loss: 1516.3612 - val_RMSE: 38.7166 - val_loss: 1499.2494 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9196 - loss: 1515.0061 - val_RMSE: 38.7169 - val_loss: 1499.2621 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  48s 19ms/step - RMSE: 38.9139 - loss: 1514.5591 - val_RMSE: 38.7162 - val_loss: 1499.2021 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.9066 - loss: 1513.9805 - val_RMSE: 38.7164 - val_loss: 1499.2109 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  49s 19ms/step - RMSE: 38.9014 - loss: 1513.5691 - val_RMSE: 38.7161 - val_loss: 1499.1810 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.8881 - loss: 1512.5328 - val_RMSE: 38.7162 - val_loss: 1499.1799 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  50s 19ms/step - RMSE: 38.8826 - loss: 1512.0980 - val_RMSE: 38.7152 - val_loss: 1499.1049 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601  51s 20ms/step - RMSE: 38.8776 - loss: 1511.7020 - val_RMSE: 38.7153 - val_loss: 1499.1086 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8641 - loss: 1510.6454 - val_RMSE: 38.7155 - val_loss: 1499.1180 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.8575 - loss: 1510.1354 - val_RMSE: 38.7145 - val_loss: 1499.0352 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8588 - loss: 1510.2299 - val_RMSE: 38.7143 - val_loss: 1499.0184 - learning_rate: 1.0000e-04\n",
            "1301/1301  13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  118s 26ms/step - RMSE: 45.7002 - loss: 2162.1248 - val_RMSE: 38.7195 - val_loss: 1499.3488 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  48s 18ms/step - RMSE: 39.5034 - loss: 1560.6990 - val_RMSE: 38.7215 - val_loss: 1499.6198 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.3608 - loss: 1549.5713 - val_RMSE: 38.7163 - val_loss: 1499.3687 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.2126 - loss: 1538.0386 - val_RMSE: 38.7228 - val_loss: 1499.8433 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  46s 17ms/step - RMSE: 39.2021 - loss: 1537.1873 - val_RMSE: 38.7189 - val_loss: 1499.5131 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1778 - loss: 1535.2606 - val_RMSE: 38.7169 - val_loss: 1499.3527 - learning_rate: 1.0000e-04\n",
            "Epoch 7/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1736 - loss: 1534.9222 - val_RMSE: 38.7167 - val_loss: 1499.3334 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1734 - loss: 1534.9026 - val_RMSE: 38.7161 - val_loss: 1499.2797 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.1805 - loss: 1535.4563 - val_RMSE: 38.7167 - val_loss: 1499.3292 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1715 - loss: 1534.7469 - val_RMSE: 38.7160 - val_loss: 1499.2714 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1767 - loss: 1535.1536 - val_RMSE: 38.7156 - val_loss: 1499.2382 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.1627 - loss: 1534.0553 - val_RMSE: 38.7158 - val_loss: 1499.2437 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.1667 - loss: 1534.3608 - val_RMSE: 38.7154 - val_loss: 1499.2161 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.1680 - loss: 1534.4641 - val_RMSE: 38.7161 - val_loss: 1499.2670 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.1670 - loss: 1534.3833 - val_RMSE: 38.7149 - val_loss: 1499.1672 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1491 - loss: 1532.9792 - val_RMSE: 38.7148 - val_loss: 1499.1571 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1539 - loss: 1533.3547 - val_RMSE: 38.7146 - val_loss: 1499.1401 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.1601 - loss: 1533.8358 - val_RMSE: 38.7143 - val_loss: 1499.1135 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  47s 18ms/step - RMSE: 39.1611 - loss: 1533.9114 - val_RMSE: 38.7145 - val_loss: 1499.1306 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.1487 - loss: 1532.9391 - val_RMSE: 38.7143 - val_loss: 1499.1112 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.1549 - loss: 1533.4232 - val_RMSE: 38.7144 - val_loss: 1499.1190 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.1485 - loss: 1532.9193 - val_RMSE: 38.7139 - val_loss: 1499.0756 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  43s 17ms/step - RMSE: 39.1406 - loss: 1532.2950 - val_RMSE: 38.7139 - val_loss: 1499.0732 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.1514 - loss: 1533.1381 - val_RMSE: 38.7131 - val_loss: 1499.0117 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  43s 17ms/step - RMSE: 39.1498 - loss: 1533.0116 - val_RMSE: 38.7137 - val_loss: 1499.0575 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 07:58:39,266] Trial 5 finished with value: 38.70474751790365 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'gelu', 'num_transformer_heads': 4, 'transformer_units': 128, 'dropout_rate': 0.39, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  111s 25ms/step - RMSE: 42.4475 - loss: 1845.1521 - val_RMSE: 38.7144 - val_loss: 1499.8320 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8458 - loss: 1510.0443 - val_RMSE: 38.6963 - val_loss: 1498.6664 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.8482 - loss: 1510.5233 - val_RMSE: 38.6972 - val_loss: 1499.0239 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8444 - loss: 1510.5272 - val_RMSE: 38.7031 - val_loss: 1499.6031 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  46s 17ms/step - RMSE: 38.8118 - loss: 1507.8633 - val_RMSE: 38.6852 - val_loss: 1497.6406 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8080 - loss: 1507.0682 - val_RMSE: 38.6847 - val_loss: 1497.3075 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8027 - loss: 1506.4084 - val_RMSE: 38.6847 - val_loss: 1497.1561 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8008 - loss: 1506.1333 - val_RMSE: 38.6840 - val_loss: 1497.0272 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7933 - loss: 1505.4794 - val_RMSE: 38.6835 - val_loss: 1496.9302 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7955 - loss: 1505.6045 - val_RMSE: 38.6828 - val_loss: 1496.8416 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7970 - loss: 1505.6798 - val_RMSE: 38.6829 - val_loss: 1496.8168 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8002 - loss: 1505.9060 - val_RMSE: 38.6827 - val_loss: 1496.7836 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.7941 - loss: 1505.4130 - val_RMSE: 38.6824 - val_loss: 1496.7395 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7951 - loss: 1505.4664 - val_RMSE: 38.6827 - val_loss: 1496.7531 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7937 - loss: 1505.3501 - val_RMSE: 38.6821 - val_loss: 1496.6962 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7941 - loss: 1505.3696 - val_RMSE: 38.6821 - val_loss: 1496.6874 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7954 - loss: 1505.4630 - val_RMSE: 38.6836 - val_loss: 1496.7948 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7957 - loss: 1505.4836 - val_RMSE: 38.6825 - val_loss: 1496.7125 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7933 - loss: 1505.2955 - val_RMSE: 38.6811 - val_loss: 1496.5918 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7864 - loss: 1504.7499 - val_RMSE: 38.6808 - val_loss: 1496.5557 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7873 - loss: 1504.8098 - val_RMSE: 38.6805 - val_loss: 1496.5270 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.7888 - loss: 1504.9154 - val_RMSE: 38.6804 - val_loss: 1496.5106 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7853 - loss: 1504.6434 - val_RMSE: 38.6803 - val_loss: 1496.5021 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7887 - loss: 1504.8939 - val_RMSE: 38.6801 - val_loss: 1496.4764 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7880 - loss: 1504.8342 - val_RMSE: 38.6801 - val_loss: 1496.4771 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  119s 27ms/step - RMSE: 42.3829 - loss: 1839.0455 - val_RMSE: 38.7509 - val_loss: 1502.5441 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7978 - loss: 1506.2330 - val_RMSE: 38.7444 - val_loss: 1502.3661 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8006 - loss: 1506.8879 - val_RMSE: 38.7451 - val_loss: 1502.9218 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8016 - loss: 1507.3350 - val_RMSE: 38.7529 - val_loss: 1503.6680 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7649 - loss: 1504.4216 - val_RMSE: 38.7237 - val_loss: 1500.7728 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7550 - loss: 1503.1031 - val_RMSE: 38.7243 - val_loss: 1500.4862 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7556 - loss: 1502.8591 - val_RMSE: 38.7232 - val_loss: 1500.2222 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7553 - loss: 1502.6808 - val_RMSE: 38.7209 - val_loss: 1499.9453 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7529 - loss: 1502.4088 - val_RMSE: 38.7244 - val_loss: 1500.1570 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7492 - loss: 1502.0676 - val_RMSE: 38.7239 - val_loss: 1500.0767 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7465 - loss: 1501.8169 - val_RMSE: 38.7148 - val_loss: 1499.3582 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7470 - loss: 1501.8451 - val_RMSE: 38.7148 - val_loss: 1499.3383 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7450 - loss: 1501.6754 - val_RMSE: 38.7148 - val_loss: 1499.3224 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7417 - loss: 1501.4037 - val_RMSE: 38.7147 - val_loss: 1499.3103 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7456 - loss: 1501.7014 - val_RMSE: 38.7146 - val_loss: 1499.2893 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7473 - loss: 1501.8195 - val_RMSE: 38.7148 - val_loss: 1499.3005 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7434 - loss: 1501.5094 - val_RMSE: 38.7147 - val_loss: 1499.2864 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  49s 19ms/step - RMSE: 38.7426 - loss: 1501.4451 - val_RMSE: 38.7144 - val_loss: 1499.2526 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7454 - loss: 1501.6525 - val_RMSE: 38.7143 - val_loss: 1499.2401 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7448 - loss: 1501.5985 - val_RMSE: 38.7143 - val_loss: 1499.2319 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7430 - loss: 1501.4496 - val_RMSE: 38.7143 - val_loss: 1499.2231 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.7407 - loss: 1501.2650 - val_RMSE: 38.7144 - val_loss: 1499.2222 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7424 - loss: 1501.3975 - val_RMSE: 38.7142 - val_loss: 1499.2080 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  46s 17ms/step - RMSE: 38.7424 - loss: 1501.3904 - val_RMSE: 38.7140 - val_loss: 1499.1799 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7403 - loss: 1501.2173 - val_RMSE: 38.7139 - val_loss: 1499.1743 - learning_rate: 1.0000e-04\n",
            "1301/1301  12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  115s 26ms/step - RMSE: 42.3021 - loss: 1830.5432 - val_RMSE: 38.7214 - val_loss: 1500.2247 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8304 - loss: 1508.7031 - val_RMSE: 38.7849 - val_loss: 1505.4067 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8303 - loss: 1509.0193 - val_RMSE: 38.7682 - val_loss: 1504.5594 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7944 - loss: 1506.4683 - val_RMSE: 38.7104 - val_loss: 1499.5895 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7859 - loss: 1505.3501 - val_RMSE: 38.7095 - val_loss: 1499.2378 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.7803 - loss: 1504.6820 - val_RMSE: 38.7095 - val_loss: 1499.0968 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7818 - loss: 1504.6760 - val_RMSE: 38.7092 - val_loss: 1499.0006 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7776 - loss: 1504.2859 - val_RMSE: 38.7086 - val_loss: 1498.9076 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7744 - loss: 1503.9915 - val_RMSE: 38.7084 - val_loss: 1498.8505 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7756 - loss: 1504.0525 - val_RMSE: 38.7089 - val_loss: 1498.8649 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7775 - loss: 1504.1755 - val_RMSE: 38.7073 - val_loss: 1498.7172 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7803 - loss: 1504.3685 - val_RMSE: 38.7073 - val_loss: 1498.7063 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.7707 - loss: 1503.6112 - val_RMSE: 38.7067 - val_loss: 1498.6438 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7757 - loss: 1503.9913 - val_RMSE: 38.7078 - val_loss: 1498.7180 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.7779 - loss: 1504.1486 - val_RMSE: 38.7061 - val_loss: 1498.5847 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7761 - loss: 1504.0052 - val_RMSE: 38.7064 - val_loss: 1498.5966 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7717 - loss: 1503.6576 - val_RMSE: 38.7075 - val_loss: 1498.6698 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7686 - loss: 1503.4067 - val_RMSE: 38.7062 - val_loss: 1498.5604 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.7689 - loss: 1503.4172 - val_RMSE: 38.7059 - val_loss: 1498.5315 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7671 - loss: 1503.2646 - val_RMSE: 38.7055 - val_loss: 1498.4899 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7684 - loss: 1503.3551 - val_RMSE: 38.7055 - val_loss: 1498.4773 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7675 - loss: 1503.2802 - val_RMSE: 38.7056 - val_loss: 1498.4763 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7649 - loss: 1503.0704 - val_RMSE: 38.7054 - val_loss: 1498.4600 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7624 - loss: 1502.8688 - val_RMSE: 38.7052 - val_loss: 1498.4392 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7674 - loss: 1503.2544 - val_RMSE: 38.7050 - val_loss: 1498.4156 - learning_rate: 1.0000e-04\n",
            "1301/1301  13s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 08:57:37,306] Trial 6 finished with value: 38.69969813028971 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.3, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  113s 26ms/step - RMSE: 44.4115 - loss: 2034.5447 - val_RMSE: 38.7035 - val_loss: 1498.1129 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.2853 - loss: 1543.5131 - val_RMSE: 38.6987 - val_loss: 1497.8732 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.2164 - loss: 1538.2460 - val_RMSE: 38.6962 - val_loss: 1497.8669 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.1339 - loss: 1531.9917 - val_RMSE: 38.6970 - val_loss: 1498.0996 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0597 - loss: 1526.3479 - val_RMSE: 38.6948 - val_loss: 1498.0144 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.0003 - loss: 1521.7356 - val_RMSE: 38.6919 - val_loss: 1497.7168 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9883 - loss: 1520.7283 - val_RMSE: 38.6920 - val_loss: 1497.6581 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9737 - loss: 1519.5143 - val_RMSE: 38.6924 - val_loss: 1497.6260 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.9651 - loss: 1518.7964 - val_RMSE: 38.6923 - val_loss: 1497.5817 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9688 - loss: 1519.0426 - val_RMSE: 38.6914 - val_loss: 1497.4691 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.9587 - loss: 1518.2207 - val_RMSE: 38.6917 - val_loss: 1497.4679 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.9553 - loss: 1517.9292 - val_RMSE: 38.6923 - val_loss: 1497.4910 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9436 - loss: 1516.9950 - val_RMSE: 38.6907 - val_loss: 1497.3444 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9442 - loss: 1517.0242 - val_RMSE: 38.6909 - val_loss: 1497.3459 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9408 - loss: 1516.7393 - val_RMSE: 38.6908 - val_loss: 1497.3202 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9343 - loss: 1516.2159 - val_RMSE: 38.6914 - val_loss: 1497.3490 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.9229 - loss: 1515.3163 - val_RMSE: 38.6910 - val_loss: 1497.3099 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  39s 15ms/step - RMSE: 38.9186 - loss: 1514.9668 - val_RMSE: 38.6914 - val_loss: 1497.3282 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9116 - loss: 1514.4094 - val_RMSE: 38.6902 - val_loss: 1497.2266 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9112 - loss: 1514.3723 - val_RMSE: 38.6914 - val_loss: 1497.3125 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9129 - loss: 1514.4952 - val_RMSE: 38.6910 - val_loss: 1497.2709 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9024 - loss: 1513.6681 - val_RMSE: 38.6908 - val_loss: 1497.2500 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9059 - loss: 1513.9403 - val_RMSE: 38.6907 - val_loss: 1497.2383 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9035 - loss: 1513.7535 - val_RMSE: 38.6903 - val_loss: 1497.2067 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9047 - loss: 1513.8467 - val_RMSE: 38.6902 - val_loss: 1497.1969 - learning_rate: 1.0000e-05\n",
            "1301/1301  18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  115s 25ms/step - RMSE: 44.3622 - loss: 2029.6350 - val_RMSE: 38.7290 - val_loss: 1500.0935 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.2330 - loss: 1539.4219 - val_RMSE: 38.7302 - val_loss: 1500.3285 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.1566 - loss: 1533.5837 - val_RMSE: 38.7354 - val_loss: 1500.9144 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0768 - loss: 1527.4764 - val_RMSE: 38.7230 - val_loss: 1499.9257 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0550 - loss: 1525.7434 - val_RMSE: 38.7209 - val_loss: 1499.7285 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  45s 17ms/step - RMSE: 39.0521 - loss: 1525.4745 - val_RMSE: 38.7212 - val_loss: 1499.7164 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 16ms/step - RMSE: 39.0355 - loss: 1524.1504 - val_RMSE: 38.7213 - val_loss: 1499.7041 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  46s 17ms/step - RMSE: 39.0313 - loss: 1523.8004 - val_RMSE: 38.7187 - val_loss: 1499.4832 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  43s 17ms/step - RMSE: 39.0310 - loss: 1523.7611 - val_RMSE: 38.7197 - val_loss: 1499.5424 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  46s 18ms/step - RMSE: 39.0179 - loss: 1522.7188 - val_RMSE: 38.7191 - val_loss: 1499.4773 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  44s 17ms/step - RMSE: 39.0137 - loss: 1522.3821 - val_RMSE: 38.7189 - val_loss: 1499.4559 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9969 - loss: 1521.0582 - val_RMSE: 38.7195 - val_loss: 1499.4899 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9922 - loss: 1520.6802 - val_RMSE: 38.7185 - val_loss: 1499.3995 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9817 - loss: 1519.8529 - val_RMSE: 38.7193 - val_loss: 1499.4558 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.9750 - loss: 1519.3152 - val_RMSE: 38.7177 - val_loss: 1499.3195 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.9679 - loss: 1518.7572 - val_RMSE: 38.7185 - val_loss: 1499.3740 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9550 - loss: 1517.7489 - val_RMSE: 38.7192 - val_loss: 1499.4272 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9507 - loss: 1517.4058 - val_RMSE: 38.7186 - val_loss: 1499.3754 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9591 - loss: 1518.0565 - val_RMSE: 38.7185 - val_loss: 1499.3643 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.9565 - loss: 1517.8558 - val_RMSE: 38.7182 - val_loss: 1499.3406 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.9564 - loss: 1517.8472 - val_RMSE: 38.7181 - val_loss: 1499.3401 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9541 - loss: 1517.6646 - val_RMSE: 38.7180 - val_loss: 1499.3311 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9526 - loss: 1517.5491 - val_RMSE: 38.7181 - val_loss: 1499.3387 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9540 - loss: 1517.6628 - val_RMSE: 38.7181 - val_loss: 1499.3337 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9578 - loss: 1517.9537 - val_RMSE: 38.7182 - val_loss: 1499.3386 - learning_rate: 1.0000e-07\n",
            "1301/1301  13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  112s 25ms/step - RMSE: 44.3326 - loss: 2026.0779 - val_RMSE: 38.7319 - val_loss: 1500.3129 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.2594 - loss: 1541.4846 - val_RMSE: 38.7179 - val_loss: 1499.3530 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.1896 - loss: 1536.1431 - val_RMSE: 38.7392 - val_loss: 1501.1774 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.1068 - loss: 1529.8394 - val_RMSE: 38.7384 - val_loss: 1501.2596 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0326 - loss: 1524.1267 - val_RMSE: 38.7190 - val_loss: 1499.7012 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0253 - loss: 1523.5060 - val_RMSE: 38.7167 - val_loss: 1499.4752 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0072 - loss: 1522.0521 - val_RMSE: 38.7159 - val_loss: 1499.4034 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0092 - loss: 1522.1993 - val_RMSE: 38.7156 - val_loss: 1499.3785 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0034 - loss: 1521.7450 - val_RMSE: 38.7152 - val_loss: 1499.3414 - learning_rate: 1.0000e-05\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0030 - loss: 1521.7135 - val_RMSE: 38.7149 - val_loss: 1499.3220 - learning_rate: 1.0000e-05\n",
            "Epoch 11/25\n",
            "2601/2601  39s 15ms/step - RMSE: 39.0185 - loss: 1522.9198 - val_RMSE: 38.7150 - val_loss: 1499.3256 - learning_rate: 1.0000e-05\n",
            "Epoch 12/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0113 - loss: 1522.3633 - val_RMSE: 38.7148 - val_loss: 1499.3162 - learning_rate: 1.0000e-05\n",
            "Epoch 13/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0095 - loss: 1522.2219 - val_RMSE: 38.7148 - val_loss: 1499.3107 - learning_rate: 1.0000e-05\n",
            "Epoch 14/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0046 - loss: 1521.8325 - val_RMSE: 38.7148 - val_loss: 1499.3109 - learning_rate: 1.0000e-05\n",
            "Epoch 15/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0069 - loss: 1522.0111 - val_RMSE: 38.7147 - val_loss: 1499.3057 - learning_rate: 1.0000e-05\n",
            "Epoch 16/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0069 - loss: 1522.0167 - val_RMSE: 38.7148 - val_loss: 1499.3093 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.9997 - loss: 1521.4526 - val_RMSE: 38.7148 - val_loss: 1499.3075 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0118 - loss: 1522.3987 - val_RMSE: 38.7147 - val_loss: 1499.3020 - learning_rate: 1.0000e-06\n",
            "Epoch 19/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0149 - loss: 1522.6351 - val_RMSE: 38.7146 - val_loss: 1499.2985 - learning_rate: 1.0000e-06\n",
            "Epoch 20/25\n",
            "2601/2601  41s 16ms/step - RMSE: 39.0099 - loss: 1522.2482 - val_RMSE: 38.7147 - val_loss: 1499.3003 - learning_rate: 1.0000e-06\n",
            "Epoch 21/25\n",
            "2601/2601  42s 16ms/step - RMSE: 39.0012 - loss: 1521.5675 - val_RMSE: 38.7147 - val_loss: 1499.3024 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0048 - loss: 1521.8497 - val_RMSE: 38.7147 - val_loss: 1499.3002 - learning_rate: 1.0000e-07\n",
            "Epoch 23/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0053 - loss: 1521.8860 - val_RMSE: 38.7148 - val_loss: 1499.3057 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0043 - loss: 1521.8080 - val_RMSE: 38.7147 - val_loss: 1499.3025 - learning_rate: 1.0000e-08\n",
            "Epoch 25/25\n",
            "2601/2601  40s 15ms/step - RMSE: 39.0010 - loss: 1521.5508 - val_RMSE: 38.7147 - val_loss: 1499.3065 - learning_rate: 1.0000e-08\n",
            "1301/1301  12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 09:53:44,757] Trial 7 finished with value: 38.707681020100914 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.44999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  115s 26ms/step - RMSE: 42.6470 - loss: 1863.9602 - val_RMSE: 38.7233 - val_loss: 1500.6809 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.9045 - loss: 1514.8726 - val_RMSE: 38.7080 - val_loss: 1500.0286 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.9014 - loss: 1515.2236 - val_RMSE: 38.7096 - val_loss: 1500.5956 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  40s 15ms/step - RMSE: 38.8919 - loss: 1514.8608 - val_RMSE: 38.7099 - val_loss: 1500.8645 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  37s 14ms/step - RMSE: 38.8630 - loss: 1512.5320 - val_RMSE: 38.6867 - val_loss: 1498.2809 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  38s 15ms/step - RMSE: 38.8489 - loss: 1510.7285 - val_RMSE: 38.6854 - val_loss: 1497.7341 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8430 - loss: 1509.8857 - val_RMSE: 38.6856 - val_loss: 1497.5156 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8413 - loss: 1509.5498 - val_RMSE: 38.6851 - val_loss: 1497.3406 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8349 - loss: 1508.9290 - val_RMSE: 38.6856 - val_loss: 1497.2944 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8347 - loss: 1508.8298 - val_RMSE: 38.6849 - val_loss: 1497.1731 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8400 - loss: 1509.1870 - val_RMSE: 38.6849 - val_loss: 1497.1284 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8434 - loss: 1509.4121 - val_RMSE: 38.6844 - val_loss: 1497.0596 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.8360 - loss: 1508.8081 - val_RMSE: 38.6852 - val_loss: 1497.0940 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8354 - loss: 1508.7378 - val_RMSE: 38.6847 - val_loss: 1497.0397 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8311 - loss: 1508.3885 - val_RMSE: 38.6839 - val_loss: 1496.9625 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8309 - loss: 1508.3583 - val_RMSE: 38.6847 - val_loss: 1497.0206 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8304 - loss: 1508.3097 - val_RMSE: 38.6842 - val_loss: 1496.9631 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8300 - loss: 1508.2615 - val_RMSE: 38.6834 - val_loss: 1496.8873 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8280 - loss: 1508.0975 - val_RMSE: 38.6833 - val_loss: 1496.8713 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8256 - loss: 1507.8950 - val_RMSE: 38.6831 - val_loss: 1496.8391 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8245 - loss: 1507.7958 - val_RMSE: 38.6832 - val_loss: 1496.8417 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8258 - loss: 1507.8939 - val_RMSE: 38.6831 - val_loss: 1496.8247 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8195 - loss: 1507.3917 - val_RMSE: 38.6830 - val_loss: 1496.8035 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8257 - loss: 1507.8683 - val_RMSE: 38.6830 - val_loss: 1496.7996 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8248 - loss: 1507.7852 - val_RMSE: 38.6831 - val_loss: 1496.7948 - learning_rate: 1.0000e-04\n",
            "1301/1301  18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  122s 27ms/step - RMSE: 42.6789 - loss: 1867.5491 - val_RMSE: 38.7526 - val_loss: 1502.9938 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8560 - loss: 1511.1396 - val_RMSE: 38.7444 - val_loss: 1502.8888 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8534 - loss: 1511.4731 - val_RMSE: 38.7555 - val_loss: 1504.2766 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8508 - loss: 1511.7206 - val_RMSE: 38.7391 - val_loss: 1503.2950 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8094 - loss: 1508.5171 - val_RMSE: 38.7226 - val_loss: 1501.1578 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7924 - loss: 1506.4233 - val_RMSE: 38.7215 - val_loss: 1500.5839 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7914 - loss: 1505.9281 - val_RMSE: 38.7196 - val_loss: 1500.1873 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7950 - loss: 1505.9901 - val_RMSE: 38.7205 - val_loss: 1500.1183 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.7862 - loss: 1505.1833 - val_RMSE: 38.7220 - val_loss: 1500.1399 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7864 - loss: 1505.1119 - val_RMSE: 38.7201 - val_loss: 1499.9286 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7867 - loss: 1505.0771 - val_RMSE: 38.7208 - val_loss: 1499.9283 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.7880 - loss: 1505.1301 - val_RMSE: 38.7195 - val_loss: 1499.8014 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7860 - loss: 1504.9496 - val_RMSE: 38.7195 - val_loss: 1499.7706 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7825 - loss: 1504.6515 - val_RMSE: 38.7196 - val_loss: 1499.7528 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7845 - loss: 1504.7850 - val_RMSE: 38.7198 - val_loss: 1499.7534 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7861 - loss: 1504.8862 - val_RMSE: 38.7200 - val_loss: 1499.7610 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.7768 - loss: 1504.1593 - val_RMSE: 38.7136 - val_loss: 1499.2472 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7771 - loss: 1504.1647 - val_RMSE: 38.7135 - val_loss: 1499.2288 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  46s 17ms/step - RMSE: 38.7808 - loss: 1504.4373 - val_RMSE: 38.7135 - val_loss: 1499.2152 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7778 - loss: 1504.1967 - val_RMSE: 38.7135 - val_loss: 1499.2048 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.7775 - loss: 1504.1656 - val_RMSE: 38.7135 - val_loss: 1499.1959 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7747 - loss: 1503.9414 - val_RMSE: 38.7134 - val_loss: 1499.1766 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.7763 - loss: 1504.0522 - val_RMSE: 38.7132 - val_loss: 1499.1594 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.7761 - loss: 1504.0264 - val_RMSE: 38.7131 - val_loss: 1499.1449 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  51s 20ms/step - RMSE: 38.7747 - loss: 1503.9128 - val_RMSE: 38.7133 - val_loss: 1499.1497 - learning_rate: 1.0000e-04\n",
            "1301/1301  13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  121s 28ms/step - RMSE: 42.5061 - loss: 1849.7598 - val_RMSE: 38.7247 - val_loss: 1500.7529 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8844 - loss: 1513.2695 - val_RMSE: 38.7218 - val_loss: 1501.0789 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8799 - loss: 1513.4834 - val_RMSE: 38.7221 - val_loss: 1501.6064 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601  43s 17ms/step - RMSE: 38.8444 - loss: 1510.9349 - val_RMSE: 38.7112 - val_loss: 1500.1058 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8289 - loss: 1509.1060 - val_RMSE: 38.7101 - val_loss: 1499.5858 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8265 - loss: 1508.5498 - val_RMSE: 38.7098 - val_loss: 1499.3447 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8250 - loss: 1508.2465 - val_RMSE: 38.7091 - val_loss: 1499.1617 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8208 - loss: 1507.7996 - val_RMSE: 38.7097 - val_loss: 1499.1233 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8156 - loss: 1507.3196 - val_RMSE: 38.7094 - val_loss: 1499.0496 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601  41s 16ms/step - RMSE: 38.8179 - loss: 1507.4545 - val_RMSE: 38.7087 - val_loss: 1498.9669 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8191 - loss: 1507.5155 - val_RMSE: 38.7076 - val_loss: 1498.8505 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8237 - loss: 1507.8479 - val_RMSE: 38.7067 - val_loss: 1498.7649 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601  42s 16ms/step - RMSE: 38.8124 - loss: 1506.9535 - val_RMSE: 38.7073 - val_loss: 1498.7961 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601  43s 16ms/step - RMSE: 38.8169 - loss: 1507.2902 - val_RMSE: 38.7079 - val_loss: 1498.8285 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.8139 - loss: 1507.0425 - val_RMSE: 38.7062 - val_loss: 1498.6810 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  47s 18ms/step - RMSE: 38.8125 - loss: 1506.9236 - val_RMSE: 38.7060 - val_loss: 1498.6515 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8069 - loss: 1506.4723 - val_RMSE: 38.7058 - val_loss: 1498.6256 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  44s 17ms/step - RMSE: 38.8106 - loss: 1506.7443 - val_RMSE: 38.7055 - val_loss: 1498.5940 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8091 - loss: 1506.6151 - val_RMSE: 38.7053 - val_loss: 1498.5637 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.8070 - loss: 1506.4437 - val_RMSE: 38.7053 - val_loss: 1498.5557 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  45s 17ms/step - RMSE: 38.8082 - loss: 1506.5293 - val_RMSE: 38.7054 - val_loss: 1498.5553 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  49s 19ms/step - RMSE: 38.8086 - loss: 1506.5477 - val_RMSE: 38.7056 - val_loss: 1498.5579 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8045 - loss: 1506.2297 - val_RMSE: 38.7059 - val_loss: 1498.5764 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  48s 18ms/step - RMSE: 38.8008 - loss: 1505.9331 - val_RMSE: 38.7054 - val_loss: 1498.5397 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601  46s 18ms/step - RMSE: 38.8063 - loss: 1506.3608 - val_RMSE: 38.7053 - val_loss: 1498.5286 - learning_rate: 1.0000e-05\n",
            "1301/1301  12s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-17 10:52:39,036] Trial 8 finished with value: 38.700538635253906 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'selu', 'num_transformer_heads': 2, 'transformer_units': 32, 'dropout_rate': 0.42, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            " 709/2601  54s 29ms/step - RMSE: 50.1655 - loss: 2638.2307"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-17 10:54:36,069] Trial 9 failed with parameters: {'units': 512, 'last_layer': 2, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 128, 'dropout_rate': 0.48, 'repeat_att': 2} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-110-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-109-a07edfb686fe>\", line 54, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
            "    return function._call_flat(  # pylint: disable=protected-access\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1322, in _call_flat\n",
            "    return self._inference_function.call_preflattened(args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 216, in call_preflattened\n",
            "    flat_outputs = self.call_flat(*args)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 251, in call_flat\n",
            "    outputs = self._bound_context.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\", line 1683, in call_function\n",
            "    outputs = execute.execute(\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-02-17 10:54:36,072] Trial 9 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-a07edfb686fe>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     55\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 2 finished with value: 38.699286142985024\n",
        "* parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}"
      ],
      "metadata": {
        "id": "RWAsq27zwysI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9",
        "id": "PxgRBgt6wysJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RAYXidmYEsqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}