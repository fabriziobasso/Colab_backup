{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Colab_backup/blob/main/File_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGa3SjrXsW_F"
      },
      "source": [
        "# **S5E2 - Backpack Prices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4M8cw5duVkf"
      },
      "outputs": [],
      "source": [
        "# Necessry to run LGBMRegressor\n",
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rYYQgW0Rmxv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qq pytorch_tabnet\n",
        "!pip install optuna\n",
        "!pip install catboost\n",
        "#!pip install optuna-integration-pytorch-tabnet\n",
        "\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade\n",
        "\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "!pip install --upgrade category-encoders\n",
        "!pip install optuna-integration\n",
        "!pip install colorama\n",
        "#!pip install pyfiglet\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install keras-nlp\n",
        "!pip install BorutaShap\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install scikit-lego\n",
        "!pip install skops\n",
        "\n",
        "#from pytorch_tabnet.tab_model import TabNetRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIS1habP8JGi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "08492e0d-0051-4cd4-a430-38d346e51ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Setup notebook\n",
        "from pathlib import Path\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pickle import load, dump\n",
        "import json\n",
        "import joblib\n",
        "#import calplot as cal\n",
        "\n",
        "# Graphic Libraries:\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg\n",
        "# Set Style\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5});\n",
        "sns.despine(left=True, bottom=True, top=False, right=False);\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "mpl.rc('axes', labelsize=12);\n",
        "plt.rc('xtick',labelsize=10);\n",
        "plt.rc('ytick',labelsize=10);\n",
        "\n",
        "mpl.rcParams['axes.spines.top'] = False;\n",
        "mpl.rcParams['axes.spines.right'] = False;\n",
        "mpl.rcParams['axes.spines.left'] = True;\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "# tool for Excel:\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "from openpyxl.styles import Border, Side, PatternFill, Font, GradientFill, Alignment\n",
        "from openpyxl.worksheet.cell_range import CellRange\n",
        "\n",
        "from openpyxl.formatting import Rule\n",
        "from openpyxl.styles import Font, PatternFill, Border\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "\n",
        "# Bloomberg\n",
        "#from xbbg import blp\n",
        "from catboost import CatBoostRegressor, Pool, CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "# Time Management\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BMonthEnd, QuarterEnd\n",
        "import datetime\n",
        "from pandas.tseries.offsets import BDay # BDay is business day, not birthday...\n",
        "import datetime as dt\n",
        "import click\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import string\n",
        "\n",
        "from ipywidgets import AppLayout\n",
        "from ipywidgets import Dropdown, Layout, HTML, AppLayout, VBox, Label, HBox, BoundedFloatText, interact, Output\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Reshape\n",
        "from keras.models import Model\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, cross_val_score, cross_validate, GroupKFold, GridSearchCV, RepeatedStratifiedKFold, cross_val_predict\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   QuantileTransformer,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "# metrics\n",
        "import sklearn\n",
        "#import skops.io as sio\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             root_mean_squared_error,\n",
        "                             root_mean_squared_log_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss,\n",
        "                             make_scorer)\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  HuberRegressor,\n",
        "                                  TweedieRegressor,\n",
        "                                  QuantileRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  TheilSenRegressor,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              AdaBoostClassifier,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              GradientBoostingClassifier,\n",
        "                              StackingRegressor,\n",
        "                              StackingClassifier,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              HistGradientBoostingRegressor,\n",
        "                              ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
        "\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import itertools\n",
        "import warnings\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from pylab import rcParams\n",
        "import scipy.stats as ss\n",
        "\n",
        "from category_encoders.cat_boost import CatBoostEncoder\n",
        "from category_encoders.wrapper import PolynomialWrapper\n",
        "from category_encoders.count import CountEncoder\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "#import pyfiglet\n",
        "#plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkkRPWKZCkYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7421bbe0-e91e-4f8b-f662-84ab25a66bbf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 960x660 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.5,\n",
        "         'grid.alpha'           :0.5,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : 'black',\n",
        "         'axes.titlesize'       : 12,\n",
        "#         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "# Set Style\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "\n",
        "# import font colors\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "init(autoreset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU7oWpLHRmxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "393a3119-5c4f-488c-fa29-5bc894a285b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Connect to Colab:#\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PuCulFRmx1"
      },
      "source": [
        "<div style=\"text-align:center; border-radius:15px; padding:15px; margin:0; font-size:100%; font-family:Arial, sans-serif; background-color:#A8DADC; color:#1D3557; overflow:hidden; box-shadow:0 3px 6px rgba(0, 0, 0, 0.2);\">\n",
        "    <h3>Loading and Preprocessing Data for Compatibility</h3>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3odgloSjRmx4"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_enc_ext.csv\",index_col=0)\n",
        "\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/test_enc_ext.csv\",index_col=0)\n",
        "\n",
        "# df_train_orig = pd.read_csv(\n",
        "#     '/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/credit_risk_dataset.csv'\n",
        "# )\n",
        "\n",
        "df_subm = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\",index_col=0)\n",
        "\n",
        "# df_orig = pd.read_csv(\n",
        "#     \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E12/Insurance Premium Prediction Dataset.csv\",\n",
        "#      parse_dates=['Policy Start Date'],\n",
        "#     #     index_col='id',\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vwd0o1ph1Ai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6cf5c9-7f5e-470c-80b5-81c3dbe8c1e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3994318, 31), (200000, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_train.head()\n",
        "df_train.shape,df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tcWfu7npSHN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "99ad6134-6391-4d1d-fae0-ce6faca4eb2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  Style  \\\n",
              "0      1         1     1             7                   2           1      3   \n",
              "1      1         0     3             1                   2           2      1   \n",
              "2      5         1     3             2                   2           1      1   \n",
              "3      3         3     3             8                   2           1      1   \n",
              "4      0         0     1             0                   2           2      1   \n",
              "\n",
              "   Color  Weight Capacity (kg)  Weight Capacity (kg)_missing  Mat_Siz_Col  \\\n",
              "0      0             -0.917722                             0            0   \n",
              "1      3              1.300573                             0            0   \n",
              "2      6             -0.196013                             0            0   \n",
              "3      3             -0.727615                             0            0   \n",
              "4      3             -0.037447                             0            0   \n",
              "\n",
              "   Siz_Lap_Col  Bra_Siz_Wat  Siz_Lap_Wat  Mat_Lap_Wat  Bra_Siz_Sty  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Bra_Lap_Wat  Siz_Com_Lap  Siz_Lap_Sty  Mat_Com_Lap  Mat_Siz_Com  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Bra_Siz_Com  Com_Lap_Wat  Bra_Siz_Lap  Bra_Mat_Siz  Siz_Com_Wat  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Siz_Com_Sty     TE_wc    skew_0    skew_1      Price  \n",
              "0            0  0.261445 -0.292388 -0.428820  112.15875  \n",
              "1            0  0.621130 -0.302957 -0.460902   68.88056  \n",
              "2            0  0.016408 -0.301780 -1.112454   39.17320  \n",
              "3            0  1.498987 -0.301780 -0.551413   80.60793  \n",
              "4            0  0.016408 -0.375870  0.519525   86.02312  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ea05a6b-2d08-49e3-8922-e29669f6347c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>Weight Capacity (kg)_missing</th>\n",
              "      <th>Mat_Siz_Col</th>\n",
              "      <th>Siz_Lap_Col</th>\n",
              "      <th>Bra_Siz_Wat</th>\n",
              "      <th>Siz_Lap_Wat</th>\n",
              "      <th>Mat_Lap_Wat</th>\n",
              "      <th>Bra_Siz_Sty</th>\n",
              "      <th>Bra_Lap_Wat</th>\n",
              "      <th>Siz_Com_Lap</th>\n",
              "      <th>Siz_Lap_Sty</th>\n",
              "      <th>Mat_Com_Lap</th>\n",
              "      <th>Mat_Siz_Com</th>\n",
              "      <th>Bra_Siz_Com</th>\n",
              "      <th>Com_Lap_Wat</th>\n",
              "      <th>Bra_Siz_Lap</th>\n",
              "      <th>Bra_Mat_Siz</th>\n",
              "      <th>Siz_Com_Wat</th>\n",
              "      <th>Siz_Com_Sty</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.917722</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.261445</td>\n",
              "      <td>-0.292388</td>\n",
              "      <td>-0.428820</td>\n",
              "      <td>112.15875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.300573</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.621130</td>\n",
              "      <td>-0.302957</td>\n",
              "      <td>-0.460902</td>\n",
              "      <td>68.88056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.196013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-0.301780</td>\n",
              "      <td>-1.112454</td>\n",
              "      <td>39.17320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.727615</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.498987</td>\n",
              "      <td>-0.301780</td>\n",
              "      <td>-0.551413</td>\n",
              "      <td>80.60793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.037447</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-0.375870</td>\n",
              "      <td>0.519525</td>\n",
              "      <td>86.02312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ea05a6b-2d08-49e3-8922-e29669f6347c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1ea05a6b-2d08-49e3-8922-e29669f6347c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1ea05a6b-2d08-49e3-8922-e29669f6347c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a51f6529-5e95-4a88-95f6-10fd174893c8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a51f6529-5e95-4a88-95f6-10fd174893c8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a51f6529-5e95-4a88-95f6-10fd174893c8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#df_train_orig.isna().sum()\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njvQSzYr_4BD"
      },
      "outputs": [],
      "source": [
        "def plot_scatter(x=\"Price\",y=\"TE_wc\", df=df_train):\n",
        "\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.scatter(df[x],df[y])\n",
        "  plt.xlabel(x)\n",
        "  plt.ylabel(y)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R5dhMXeAUhG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "8c477f7b-fbed-4b5e-a4e6-6b2b4c0af57b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_scatter(x=\"Price\",y=\"TE_wc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pykP87axQAvp"
      },
      "outputs": [],
      "source": [
        "#df_train[\"person_emp_length\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hREKVL1iPrWR"
      },
      "outputs": [],
      "source": [
        "#df_train[df_train[\"cb_person_cred_hist_length\"]>0.75*df_train[\"person_age\"]]\n",
        "#np.round(df_train[\"cb_person_cred_hist_length\"].mean(),0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vfGRiTeLCd2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am9pd9NwKIBl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho9znWqB1KpL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFmR4Gl6JRAl"
      },
      "source": [
        "**Descriptions of Loan Data**\n",
        "\n",
        "Descriptions for the column names based on the data provided:\n",
        "\n",
        "* **id**: Unique identifier for each record.\n",
        "* **person_age**: Age of the individual, categorized into ranges.\n",
        "* **person_income**: Income of the individual, categorized into income ranges.\n",
        "* **person_home_ownership**: Homeownership status, which includes categories like 'RENT', 'MORTGAGE', etc.\n",
        "* **person_emp_length**: Employment length of the individual, categorized into ranges based on years.\n",
        "* **loan_intent**: The purpose of the loan, with categories such as 'EDUCATION', 'MEDICAL', etc.\n",
        "* **loan_grade**: The credit grade of the loan, such as 'A', 'B', etc.\n",
        "* **loan_amnt**: Loan amount, categorized into ranges.\n",
        "* **loan_int_rate**: Loan interest rate, categorized into percentage ranges.\n",
        "* **loan_percent_income**: Percentage of the individual’s income that the loan represents, categorized into - ranges.\n",
        "* **cb_person_default_on_file**: Whether the person has a history of loan default, with values 'true' or 'false'.\n",
        "* **cb_person_cred_hist_length**: Length of the individual’s credit history, categorized into ranges.\n",
        "* **loan_status**: with values representing whether the loan status approval( binary values)\n",
        "\n",
        "The dataset is a about loan applications, including personal, financial, and loan details. It's likely used for predicting whether a person will default on a loan, making it a binary classification problem. The goal is to figure out which applicants are at higher risk of not paying back their loans based on their age, income, employment, loan purpose, credit history, and other related information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_IkGc2Wya01"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "\n",
        "    state = 42\n",
        "    n_splits = 10\n",
        "    early_stop = 200\n",
        "\n",
        "    target = 'Price'\n",
        "    problem = \"Regression\"\n",
        "    train = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_enc_ext.csv', index_col=0)\n",
        "    test = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/test_enc_ext.csv', index_col=0)\n",
        "    submission = pd.read_csv( \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\", index_col=0)\n",
        "    train_org = None\n",
        "\n",
        "    original_data = 'N'\n",
        "    outliers = 'N'\n",
        "    log_trf = 'N'\n",
        "    scaler_trf = 'Y'\n",
        "    feature_eng = 'Y'\n",
        "    missing = 'Y'\n",
        "    force_normalization=\"N\"\n",
        "    impose_normalization=\"Y\"\n",
        "    trg_enc = \"N\"\n",
        "    metric_goal=\"rmse\"\n",
        "    direction_=\"minimize\"\n",
        "    log_trans_cols = []\n",
        "    force_norm_cols = []\n",
        "    impose_norm_cols = [\"skew_0\",\"skew_1\"]\n",
        "    trg_enc_feat = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjPBIogOJ2mz"
      },
      "outputs": [],
      "source": [
        "class Preprocessing():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train = Config.train\n",
        "        self.test = Config.test\n",
        "        self.targets = Config.target\n",
        "        self.problem = Config.problem\n",
        "        self.submission = Config.submission\n",
        "\n",
        "        self.prp_data()\n",
        "\n",
        "    def prp_data(self):\n",
        "\n",
        "        if Config.original_data == 'Y':\n",
        "            self.train = pd.concat([self.train, Config.train_org], ignore_index=True).drop_duplicates(ignore_index=True)\n",
        "        if 'id' in self.train.columns:\n",
        "            self.train = self.train.drop(['id'], axis=1)\n",
        "            self.test = self.test.drop(['id'], axis=1)\n",
        "\n",
        "        self.cat_features = self.train.drop(self.targets, axis=1).select_dtypes(include=['object', 'bool', 'int', 'category']).columns.tolist()\n",
        "        self.num_features = self.train.drop(self.targets, axis=1).select_dtypes(exclude=['object', 'bool', 'int', 'category']).columns.tolist()\n",
        "\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
        "\n",
        "        self.train = self.reduce_mem(self.train)\n",
        "        self.test = self.reduce_mem(self.test)\n",
        "        return self\n",
        "\n",
        "    def reduce_mem(self, df):\n",
        "\n",
        "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"]\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_type = df[col].dtypes\n",
        "\n",
        "            if col_type in numerics:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "\n",
        "                if \"int\" in str(col_type):\n",
        "                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)\n",
        "                else:\n",
        "                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    if c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICqfSaYMKIF8"
      },
      "outputs": [],
      "source": [
        "class EDA(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_info()\n",
        "        self.heatmap()\n",
        "        self.dist_plots()\n",
        "        self.cat_feature_plots()\n",
        "        self.target_pie()\n",
        "\n",
        "    def data_info(self):\n",
        "\n",
        "        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n",
        "            table_style = [{'selector': 'th:not(.index_name)',\n",
        "                            'props': [('background-color', 'slategrey'),\n",
        "                                      ('color', '#FFFFFF'),\n",
        "                                      ('font-weight', 'bold'),\n",
        "                                      ('border', '1px solid #DCDCDC'),\n",
        "                                      ('text-align', 'center')]\n",
        "                            },\n",
        "                            {'selector': 'tbody td',\n",
        "                             'props': [('border', '1px solid #DCDCDC'),\n",
        "                                       ('font-weight', 'normal')]\n",
        "                            }]\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} head\\n')\n",
        "            display(data.head().style.set_table_styles(table_style))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} info\\n'+Style.RESET_ALL)\n",
        "            display(data.info())\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} describe\\n')\n",
        "            display(data.describe().drop(index='count', columns=self.targets, errors = 'ignore').T\n",
        "                    .style.set_table_styles(table_style).format('{:.3f}'))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} missing values\\n'+Style.RESET_ALL)\n",
        "            display(data.isna().sum())\n",
        "        return self\n",
        "\n",
        "    def heatmap(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nCorrelation Heatmap\\n')\n",
        "        plt.figure(figsize=(7,7))\n",
        "        corr = self.train.select_dtypes(exclude=['object', 'category']).corr(method='pearson')\n",
        "        sns.heatmap(corr, fmt = '0.2f', cmap = 'Blues', annot=True, cbar=False)\n",
        "        plt.show()\n",
        "\n",
        "    def dist_plots(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Numerical\\n\")\n",
        "        df = pd.concat([self.train[self.num_features].assign(Source = 'Train'),\n",
        "                        self.test[self.num_features].assign(Source = 'Test'),],\n",
        "                        axis=0, ignore_index = True)\n",
        "\n",
        "        fig, axes = plt.subplots(len(self.num_features), 2 ,figsize = (18, len(self.num_features) * 6),\n",
        "                                 gridspec_kw = {'hspace': 0.3,\n",
        "                                                'wspace': 0.2,\n",
        "                                                'width_ratios': [0.70, 0.30]\n",
        "                                               }\n",
        "                                )\n",
        "        for i,col in enumerate(self.num_features):\n",
        "            ax = axes[i,0]\n",
        "            sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source',\n",
        "                        palette = ['royalblue', 'tomato'], ax = ax, alpha=0.7, linewidth = 2\n",
        "                       )\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.grid('--',alpha=0.7)\n",
        "\n",
        "            ax = axes[i,1]\n",
        "            sns.boxplot(data = df, y = col, x=df.Source, width = 0.5,\n",
        "                        linewidth = 1, fliersize= 1,\n",
        "                        ax = ax, palette=['royalblue', 'tomato']\n",
        "                       )\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.tick_params(axis='both', which='major')\n",
        "            ax.set_xticklabels(['Train', 'Test'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def cat_feature_plots(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Categorical\\n\")\n",
        "        fig, axes = plt.subplots(len(self.cat_features), 2 ,figsize = (18, len(self.cat_features) * 6),\n",
        "                                 gridspec_kw = {'hspace': 0.5,\n",
        "                                                'wspace': 0.2,\n",
        "                                               }\n",
        "                                )\n",
        "\n",
        "        for i, col in enumerate(self.cat_features):\n",
        "\n",
        "            ax = axes[i,0]\n",
        "            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='royalblue', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Train\")\n",
        "\n",
        "            ax = axes[i,1]\n",
        "            sns.barplot(data=self.test[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='tomato', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Test\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def target_pie(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nTarget feature distribution\\n\")\n",
        "        targets = self.train[self.targets]\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        if self.problem==\"Regression\":\n",
        "          plt.hist(targets, bins=35, color='royalblue',alpha=0.7)\n",
        "        else:\n",
        "          plt.pie(targets.value_counts(), labels=targets.value_counts().index, autopct='%1.2f%%', colors=palette_9)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIy8MLVmvv3N"
      },
      "source": [
        "## 1.0 EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O1IeGiuKwDA"
      },
      "outputs": [],
      "source": [
        "eda = EDA()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jQmFilAvKM4"
      },
      "source": [
        "## 2.0 Data Transformation and Feature Engeneering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxSqeWx-MZHk"
      },
      "outputs": [],
      "source": [
        "class Transform(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        if self.missing == 'Y':\n",
        "            self.missing_values()\n",
        "\n",
        "        self.train_raw = self.train.copy()\n",
        "\n",
        "        if self.feature_eng == 'Y':\n",
        "            self.train = self.new_features(self.train)\n",
        "            self.test = self.new_features(self.test)\n",
        "            self.train_raw = self.new_features(self.train_raw)\n",
        "\n",
        "        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object', 'bool', 'category']).columns.tolist()\n",
        "        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool', 'category']).columns.tolist()\n",
        "\n",
        "        if self.outliers == 'Y':\n",
        "            self.remove_outliers()\n",
        "\n",
        "        if self.log_trf == 'Y':\n",
        "            self.log_transformation()\n",
        "\n",
        "        if self.force_normalization == 'Y':\n",
        "            self.forced_norm_transformation()\n",
        "\n",
        "        if self.impose_normalization == 'Y':\n",
        "            self.impose_normalization_transformation()\n",
        "\n",
        "        if self.trg_enc == 'Y':\n",
        "            self.target_encoding()\n",
        "\n",
        "        if self.scaler_trf == 'Y':\n",
        "            self.scaler()\n",
        "\n",
        "        if self.outliers == 'Y' or self.log_trf == 'Y' or self.scaler_trf =='Y':\n",
        "            self.distribution()\n",
        "\n",
        "    def __call__(self):\n",
        "\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
        "        data = pd.concat([self.test, self.train])\n",
        "        self.train_enc, self.test_enc = self.encode(data)\n",
        "\n",
        "        self.cat_features_card = []\n",
        "        for f in self.cat_features:\n",
        "            self.cat_features_card.append(1 + data[f].max())\n",
        "\n",
        "        self.y = self.train[self.target]\n",
        "        self.train = self.train.drop(self.target, axis=1)\n",
        "        self.train_enc = self.train_enc.drop(self.target, axis=1)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        self.train_enc[self.num_features] = scaler.fit_transform(self.train_enc[self.num_features])\n",
        "        self.test_enc[self.num_features] = scaler.transform(self.test_enc[self.num_features])\n",
        "\n",
        "        return self.train, self.train_enc, self.y, self.test, self.test_enc, self.cat_features\n",
        "\n",
        "    def encode(self, data):\n",
        "\n",
        "        oe = OrdinalEncoder()\n",
        "        data[self.cat_features] = oe.fit_transform(data[self.cat_features]).astype('int')\n",
        "\n",
        "        train_enc = data[~data[self.target].isna()]\n",
        "        test_enc = data[data[self.target].isna()].drop(self.target, axis=1)\n",
        "        return train_enc, test_enc\n",
        "\n",
        "    def new_features(self, df):\n",
        "\n",
        "        price_flags = [\"Mat_Siz_Col\",\t\"Siz_Lap_Col\",\t\"Bra_Siz_Wat\",\t\"Siz_Lap_Wat\",\t\"Mat_Lap_Wat\",\t\"Bra_Siz_Sty\",\t\"Bra_Lap_Wat\",\t\"Siz_Com_Lap\",\t\"Siz_Lap_Sty\",\n",
        "                       \"Mat_Com_Lap\",\t\"Mat_Siz_Com\",\t\"Bra_Siz_Com\",\t\"Com_Lap_Wat\",\t\"Bra_Siz_Lap\",\t\"Bra_Mat_Siz\",\t\"Siz_Com_Wat\",\t\"Siz_Com_Sty\"]\n",
        "\n",
        "        df['cheap_flag'] = df[price_flags].apply(lambda row: 1 in row.values, axis=1).astype(\"category\")\n",
        "        df['expansive_flag'] = df[price_flags].apply(lambda row: 2 in row.values, axis=1).astype(\"category\")\n",
        "\n",
        "        df = df.drop(columns=price_flags)\n",
        "        df = df.drop(columns=[\"Weight Capacity (kg)_missing\"])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def log_transformation(self):\n",
        "\n",
        "        self.train[self.log_trans_cols] = np.log1p(self.train[self.log_trans_cols])\n",
        "        self.test[self.log_trans_cols] = np.log1p(self.test[self.log_trans_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def forced_norm_transformation(self):\n",
        "\n",
        "        self.train[self.force_norm_cols] = np.sqrt(self.train[self.force_norm_cols]+0.1)\n",
        "        self.test[self.force_norm_cols] = np.sqrt(self.test[self.force_norm_cols]+0.1)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impose_normalization_transformation(self):\n",
        "\n",
        "        scaler = QuantileTransformer(output_distribution='normal',subsample=20_000,random_state=42)\n",
        "        self.train[self.impose_norm_cols] = scaler.fit_transform(self.train[self.impose_norm_cols])\n",
        "        self.test[self.impose_norm_cols] = scaler.transform(self.test[self.impose_norm_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def distribution(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nHistograms of distribution\\n')\n",
        "        fig, axes = plt.subplots(nrows=len(self.num_features), ncols=2, figsize=(15, len(self.num_features)*5))\n",
        "\n",
        "        for (ax_r, ax_n), col in zip(axes, self.num_features):\n",
        "\n",
        "            ax_r.set_title(f'{col} ($\\mu=$ {self.train_raw[col].mean():.2f} and $\\sigma=$ {self.train_raw[col].std():.2f} )')\n",
        "            ax_r.hist(self.train_raw[col], bins=30, color='tomato',alpha=0.7)\n",
        "            ax_r.axvline(self.train_raw[col].mean(), color='r', label='Mean')\n",
        "            ax_r.axvline(self.train_raw[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_r.legend()\n",
        "\n",
        "            ax_n.set_title(f'{col} Normalized ($\\mu=$ {self.train[col].mean():.2f} and $\\sigma=$ {self.train[col].std():.2f} )')\n",
        "            ax_n.hist(self.train[col], bins=30, color='royalblue',alpha=0.7)\n",
        "            ax_n.axvline(self.train[col].mean(), color='r', label='Mean')\n",
        "            ax_n.axvline(self.train[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_n.legend()\n",
        "\n",
        "    def remove_outliers(self):\n",
        "        Q1 = self.train[self.targets].quantile(0.25)\n",
        "        Q3 = self.train[self.targets].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_limit = Q1 - 1.5*IQR\n",
        "        upper_limit = Q3 + 1.5*IQR\n",
        "        self.train = self.train[(self.train[self.targets] >= lower_limit) & (self.train[self.targets] <= upper_limit)]\n",
        "        self.train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def scaler(self):\n",
        "        scaler = StandardScaler()\n",
        "        self.train[self.num_features] = scaler.fit_transform(self.train[self.num_features])\n",
        "        self.test[self.num_features] = scaler.transform(self.test[self.num_features])\n",
        "        return self\n",
        "\n",
        "    def missing_values(self):\n",
        "\n",
        "        self.train[self.num_features] = self.train[self.num_features].fillna(self.train[self.num_features].median())\n",
        "        self.test[self.num_features] = self.test[self.num_features].fillna(self.test[self.num_features].median())\n",
        "        for column in self.cat_features:\n",
        "            self.train[column] = self.train[column].fillna(self.train[column].mode()[0])\n",
        "            self.test[column] = self.test[column].fillna(self.test[column].mode()[0])\n",
        "        return self\n",
        "\n",
        "    def target_encoding(self):\n",
        "        te = TargetEncoder()\n",
        "        self.train[self.trg_enc_feat] = te.fit_transform(self.train[self.trg_enc_feat],self.train[self.target])\n",
        "        self.test[self.trg_enc_feat] = te.transform(self.test[self.trg_enc_feat])\n",
        "\n",
        "        for a in self.cat_features:\n",
        "            self.cat_features.remove(a)\n",
        "\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def cat_features(self):\n",
        "        return self._cat_features\n",
        "\n",
        "    @cat_features.setter\n",
        "    def cat_features(self, cat_features):\n",
        "        self._cat_features = cat_features\n",
        "\n",
        "    @property\n",
        "    def num_features(self):\n",
        "        return self._num_features\n",
        "\n",
        "    @num_features.setter\n",
        "    def num_features(self, num_features):\n",
        "        self._num_features = num_features\n",
        "\n",
        "    @property\n",
        "    def cat_features_card(self):\n",
        "        return self._cat_features_card\n",
        "\n",
        "    @cat_features_card.setter\n",
        "    def cat_features_card(self, cat_features_card):\n",
        "        self._cat_features_card = cat_features_card\n",
        "\n",
        "    @property\n",
        "    def train(self):\n",
        "        return self._train\n",
        "\n",
        "    @train.setter\n",
        "    def train(self, train):\n",
        "        self._train = train\n",
        "\n",
        "    @property\n",
        "    def direction(self):\n",
        "        return self._direction\n",
        "\n",
        "    @direction.setter\n",
        "    def direction(self, direction):\n",
        "        self._direction= direction\n",
        "\n",
        "\n",
        "class MixedDataImputer:\n",
        "    \"\"\"\n",
        "    Imputes missing values in mixed-data train and test DataFrames using\n",
        "    separate IterativeImputers for numerical and categorical features.\n",
        "\n",
        "    Args:\n",
        "      train_df: Pandas DataFrame with training data.\n",
        "      test_df: Pandas DataFrame with test data.\n",
        "      target_feature: Name of the target feature column.\n",
        "      random_state: Random state for reproducibility (default=42).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_df, test_df, target_feature=None, random_state=42):\n",
        "        super().__init__()\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.target_feature = target_feature\n",
        "        self.random_state = random_state\n",
        "        self.num_features = None\n",
        "        self.cat_features = None\n",
        "\n",
        "    def _identify_features(self):\n",
        "        \"\"\"Identifies numerical and categorical features.\"\"\"\n",
        "        self.num_features = self.train_df.select_dtypes(include=['number']).columns.tolist()\n",
        "        self.cat_features = self.train_df.select_dtypes(exclude=['number']).columns.tolist()\n",
        "        #self.num_features.remove(self.target_feature)  # Remove target from numerical features\n",
        "\n",
        "    def _impute_data(self, df):\n",
        "        \"\"\"Imputes missing values in a DataFrame.\"\"\"\n",
        "        df_num = df[self.num_features].copy()\n",
        "        df_cat = df[self.cat_features].copy()\n",
        "\n",
        "        # Impute numerical features only if there are missing values\n",
        "        if df_num.isnull().values.any():\n",
        "            num_imputer = IterativeImputer(estimator=BayesianRidge(),\n",
        "                                          random_state=self.random_state)\n",
        "            df_num_imputed = pd.DataFrame(num_imputer.fit_transform(df_num),\n",
        "                                         columns=self.num_features)\n",
        "        else:\n",
        "            df_num_imputed = df_num  # No imputation needed\n",
        "\n",
        "        # Impute categorical features only if there are missing values\n",
        "        if df_cat.isnull().values.any():\n",
        "            cat_imputer = IterativeImputer(estimator=LogisticRegression(),\n",
        "                                          initial_strategy='most_frequent',\n",
        "                                          random_state=self.random_state)\n",
        "            df_cat_imputed = pd.DataFrame(cat_imputer.fit_transform(df_cat),\n",
        "                                         columns=self.cat_features)\n",
        "\n",
        "            # Convert categorical features back to their original datatype\n",
        "            for feature in self.cat_features:\n",
        "                df_cat_imputed[feature] = df_cat_imputed[feature].astype(df[feature].dtype)\n",
        "        else:\n",
        "            df_cat_imputed = df_cat  # No imputation needed\n",
        "\n",
        "        # Concatenate the imputed DataFrames\n",
        "        df_imputed = pd.concat([df_num_imputed, df_cat_imputed], axis=1)\n",
        "\n",
        "        return df_imputed\n",
        "\n",
        "    def transform(self):\n",
        "        \"\"\"\n",
        "        Imputes missing values in both train and test DataFrames.\n",
        "\n",
        "        Returns:\n",
        "          train_df_imputed: Pandas DataFrame with imputed training data.\n",
        "          test_df_imputed: Pandas DataFrame with imputed test data.\n",
        "        \"\"\"\n",
        "        self._identify_features()\n",
        "        train_df_imputed = self._impute_data(self.train_df)\n",
        "        test_df_imputed = self._impute_data(self.test_df)\n",
        "        return train_df_imputed, test_df_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YimbhYB_v_G5"
      },
      "outputs": [],
      "source": [
        "t = Transform()\n",
        "X, X_enc, y, test, test_enc, cat_features = t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQFVvKjm560K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9331b95-991e-4cbb-ac15-0de65535bf23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Brand',\n",
              "  'Material',\n",
              "  'Size',\n",
              "  'Compartments',\n",
              "  'Laptop Compartment',\n",
              "  'Waterproof',\n",
              "  'Style',\n",
              "  'Color',\n",
              "  'cheap_flag',\n",
              "  'expansive_flag'],\n",
              " [6, 5, 4, 10, 3, 3, 4, 7, 2, 2],\n",
              " (3994318, 14),\n",
              " 'minimize')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "t.cat_features, t.cat_features_card, t.train.shape, t.direction_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J1svia6wHk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a46afb3b-0c21-4111-e3bb-2fbb0c5d4388"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3994318, 14), (3994318, 14), (200000, 14), (200000, 14))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "X.shape, X_enc.shape, test.shape, test_enc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BsRXz1PiZfT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "aa4a1aca-94d7-4d42-a72c-f6d98bf70dbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Brand                   0\n",
              "Material                0\n",
              "Size                    0\n",
              "Compartments            0\n",
              "Laptop Compartment      0\n",
              "Waterproof              0\n",
              "Style                   0\n",
              "Color                   0\n",
              "Weight Capacity (kg)    0\n",
              "TE_wc                   0\n",
              "skew_0                  0\n",
              "skew_1                  0\n",
              "cheap_flag              0\n",
              "expansive_flag          0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Brand</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Material</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Compartments</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Waterproof</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Style</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Color</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TE_wc</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheap_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>expansive_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "X_enc.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4StkPOujlvR"
      },
      "outputs": [],
      "source": [
        "imputer = MixedDataImputer(X_enc, test_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6vWq-ulkDQB"
      },
      "outputs": [],
      "source": [
        "train_df_imputed, test_df_imputed = imputer.transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iF1T6CCtD-B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "d98ccc09-ba98-438e-a053-c675fd6958be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Brand                   0\n",
              "Material                0\n",
              "Size                    0\n",
              "Compartments            0\n",
              "Laptop Compartment      0\n",
              "Waterproof              0\n",
              "Style                   0\n",
              "Color                   0\n",
              "Weight Capacity (kg)    0\n",
              "TE_wc                   0\n",
              "skew_0                  0\n",
              "skew_1                  0\n",
              "cheap_flag              0\n",
              "expansive_flag          0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Brand</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Material</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Compartments</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Waterproof</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Style</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Color</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TE_wc</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheap_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>expansive_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_df_imputed.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZsrhGGvPt39"
      },
      "source": [
        "## 3.0 Advanced Feature Engeneering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1kqkztpPy6m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "ca24c119-7677-4cf6-8de0-7752c733902e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "1599364      3         3     0             9                   2           1   \n",
              "195290       0         0     3             2                   2           2   \n",
              "2789824      1         0     2             4                   2           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "1599364      1      2             -1.192626 -0.179441 -0.385691 -0.428730   \n",
              "195290       3      4             -1.428271 -0.571992 -0.846883  0.867903   \n",
              "2789824      0      3              1.702300  1.417259  2.147031  2.077067   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "1599364           0               0  \n",
              "195290            0               0  \n",
              "2789824           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4cabb9d0-cdda-40bd-a501-ff3ec5d194ca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1599364</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.192626</td>\n",
              "      <td>-0.179441</td>\n",
              "      <td>-0.385691</td>\n",
              "      <td>-0.428730</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195290</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>-1.428271</td>\n",
              "      <td>-0.571992</td>\n",
              "      <td>-0.846883</td>\n",
              "      <td>0.867903</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2789824</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.702300</td>\n",
              "      <td>1.417259</td>\n",
              "      <td>2.147031</td>\n",
              "      <td>2.077067</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4cabb9d0-cdda-40bd-a501-ff3ec5d194ca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4cabb9d0-cdda-40bd-a501-ff3ec5d194ca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4cabb9d0-cdda-40bd-a501-ff3ec5d194ca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ba417630-1bf3-4a10-82a2-3ab62a170b7d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ba417630-1bf3-4a10-82a2-3ab62a170b7d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ba417630-1bf3-4a10-82a2-3ab62a170b7d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          9,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.1926263570785522\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.1794411987066269\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.38569140434265137\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.42872950434684753\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLNavxj3aTVe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "4970f937-360e-44d8-f3b2-2aa8a7242b98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "3920860      5         1     0             7                   2           1   \n",
              "2580869      5         1     1             5                   2           1   \n",
              "2596813      0         0     1             9                   1           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "3920860      1      2             -1.155698  0.016408 -0.385691 -1.621354   \n",
              "2580869      3      1              0.108897 -0.252539 -0.168518 -2.060967   \n",
              "2596813      1      3              0.467736  2.692105  0.483511  0.987293   \n",
              "\n",
              "         cheap_flag  expansive_flag       Price  \n",
              "3920860           0               0  131.990707  \n",
              "2580869           0               0   97.162819  \n",
              "2596813           0               0  105.449501  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1d13c3f-4f1c-4e40-b3e6-a400436234ef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3920860</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.155698</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-0.385691</td>\n",
              "      <td>-1.621354</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>131.990707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2580869</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0.108897</td>\n",
              "      <td>-0.252539</td>\n",
              "      <td>-0.168518</td>\n",
              "      <td>-2.060967</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>97.162819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2596813</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.467736</td>\n",
              "      <td>2.692105</td>\n",
              "      <td>0.483511</td>\n",
              "      <td>0.987293</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>105.449501</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1d13c3f-4f1c-4e40-b3e6-a400436234ef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b1d13c3f-4f1c-4e40-b3e6-a400436234ef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b1d13c3f-4f1c-4e40-b3e6-a400436234ef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a327f3f2-516d-42d0-be47-7f7a3993d8cd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a327f3f2-516d-42d0-be47-7f7a3993d8cd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a327f3f2-516d-42d0-be47-7f7a3993d8cd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc_y\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 5,\n        \"max\": 9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          7,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.155698299407959\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.01640772819519043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.38569140434265137\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.621354103088379\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          131.99070739746094\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "X_enc_y = pd.concat([X_enc, y], axis=1)\n",
        "X_enc_y.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1eGrjSSNBTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08398a4-2621-4a2d-ed04-1497942b6440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 15 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float32\n",
            " 9   TE_wc                 float32\n",
            " 10  skew_0                float32\n",
            " 11  skew_1                float32\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            " 14  Price                 float32\n",
            "dtypes: float32(5), int64(10)\n",
            "memory usage: 411.4 MB\n"
          ]
        }
      ],
      "source": [
        "X_enc_y.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzFLG4OrlysF"
      },
      "outputs": [],
      "source": [
        "class plot_class():\n",
        "\n",
        "    def __init__(self,df, target_variable, features_cat,features_num):\n",
        "      self.df = df\n",
        "      self.target_variable = target_variable\n",
        "      self.features_cat = features_cat\n",
        "      self.features_num = features_num\n",
        "\n",
        "    @classmethod\n",
        "    def plot_categorical_features(cls, df, target_variable, features_cat, features_num):\n",
        "        \"\"\"\n",
        "        Plots the frequency of the target variable for each value of multiple categorical features.\n",
        "\n",
        "        Args:\n",
        "          df: Pandas DataFrame containing the data.\n",
        "          target_variable: Name of the target variable column in the DataFrame.\n",
        "          features: List of names of the categorical feature columns to plot.\n",
        "        \"\"\"\n",
        "\n",
        "        num_features = len(features_cat)\n",
        "        num_rows = (num_features + 1) // 2  # Calculate the number of rows needed\n",
        "\n",
        "        fig, axes = plt.subplots(num_rows, 2, figsize=(12, 4 * num_rows))\n",
        "        axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
        "\n",
        "        for i, feature in enumerate(features_cat):\n",
        "            cross_tab = pd.crosstab(df[feature], df[target_variable])\n",
        "            cross_tab.plot(kind='bar', stacked=False, position=0.3, width=0.4, ax=axes[i],colormap=palette_1, alpha=0.6)\n",
        "            axes[i].set_xlabel(feature)\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "            axes[i].set_title(f'Frequency of {target_variable} by {feature}')\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for i in range(num_features, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return cls(df, target_variable, features_cat, features_num)\n",
        "\n",
        "    @classmethod\n",
        "    def plot_numerical_features(cls, df, target_variable, features_cat, features_num):\n",
        "        \"\"\"\n",
        "        Generates violin plots for numerical features, showing the distribution for each target class.\n",
        "\n",
        "        Args:\n",
        "          df: Pandas DataFrame containing the data.\n",
        "          target_variable: Name of the target variable column in the DataFrame.\n",
        "          features: List of names of the numerical feature columns to plot.\n",
        "        \"\"\"\n",
        "\n",
        "        num_features = len(features_num)\n",
        "        num_rows = (num_features + 1) // 2  # Calculate the number of rows needed\n",
        "\n",
        "        fig, axes = plt.subplots(num_rows, 2, figsize=(12, 4 * num_rows))\n",
        "        axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
        "\n",
        "        for i, feature in enumerate(features_num):\n",
        "            sns.violinplot(x=target_variable, y=feature, data=df, ax=axes[i],\n",
        "                           hue=target_variable,  # Use 'hue' to color by target class\n",
        "                           palette=palette_9)\n",
        "            axes[i].set_xlabel(target_variable)\n",
        "            axes[i].set_ylabel(feature)\n",
        "            axes[i].set_title(f'Distribution of {feature} by {target_variable}')\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for i in range(num_features, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return cls(df, target_variable, features_cat, features_num)\n",
        "\n",
        "    def scatter_comp(self, feat_01, feat_02, hue_def):\n",
        "        \"\"\"\n",
        "        Generates a scatter plot between two features, colored by a third\n",
        "        categorical feature using Seaborn.\n",
        "\n",
        "        Args:\n",
        "          df: Pandas DataFrame containing the data.\n",
        "          x_feature: Name of the feature to plot on the x-axis.\n",
        "          y_feature: Name of the feature to plot on the y-axis.\n",
        "          color_feature: Name of the categorical feature to use for coloring.\n",
        "        \"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.scatterplot(\n",
        "            x=feat_01,\n",
        "            y=feat_02,\n",
        "            hue=hue_def,  # Use 'hue' for color encoding\n",
        "            data=self.df,\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        plt.xlabel(feat_01)\n",
        "        plt.ylabel(feat_02)\n",
        "        plt.title(f'ScatterPlot of {feat_01} vs. {feat_02} colored by {hue_def}')\n",
        "        plt.show()\n",
        "\n",
        "    def heatmap_corr(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nCorrelation Heatmap\\n')\n",
        "        plt.figure(figsize=(7,7))\n",
        "        corr = self.df.select_dtypes(exclude='int').corr(method='pearson')\n",
        "        sns.heatmap(corr, fmt = '0.2f', cmap = \"Reds\", annot=True, cbar=False)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YShjpCmaPB7"
      },
      "outputs": [],
      "source": [
        "#plot_instance = plot_class.plot_categorical_features(df=X_enc_y, target_variable=\"loan_status\", features_cat=t.cat_features, features_num=t.num_features);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYJ72HImPy3N"
      },
      "outputs": [],
      "source": [
        "#plot_instance.plot_numerical_features(df=X_enc_y, target_variable=\"loan_status\", features_cat=t.cat_features, features_num=t.num_features);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thXxmDkdPy0L"
      },
      "outputs": [],
      "source": [
        "#plot_instance.scatter_comp(feat_01=\"loan_sustainability\", feat_02=\"loan_grade\", hue_def=\"loan_status\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLWYiuaaPyxD"
      },
      "outputs": [],
      "source": [
        "#plot_instance.heatmap_corr()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc.info()"
      ],
      "metadata": {
        "id": "Y20xEnaSuowD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dab5cc9-6300-4265-993d-3c60469bb0dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float32\n",
            " 9   TE_wc                 float32\n",
            " 10  skew_0                float32\n",
            " 11  skew_1                float32\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            "dtypes: float32(4), int64(10)\n",
            "memory usage: 396.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKAbo5tTtczL"
      },
      "outputs": [],
      "source": [
        "X_enc_y.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_train_enc_expanded.csv\", index=False)\n",
        "test_enc.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_test_enc_expanded.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMb-65cVOuzp"
      },
      "source": [
        "## **4.0 MODELS**\n",
        "\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmZnoc4eO1_E"
      },
      "source": [
        "### **4.1 TREE BASED MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1GyNmMXO7Dw"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "# class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# sample_pos_weight = class_weights[1]/class_weights[0]\n",
        "# sample_pos_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qrTmdIJRsR-"
      },
      "source": [
        "#### 4.1.1 CatBoostClassifier:\n",
        "\n",
        "    class CatBoostClassifier(iterations=None,\n",
        "                            learning_rate=None,\n",
        "                            depth=None,\n",
        "                            l2_leaf_reg=None,\n",
        "                            model_size_reg=None,\n",
        "                            rsm=None,\n",
        "                            loss_function=None,\n",
        "                            border_count=None,\n",
        "                            feature_border_type=None,\n",
        "                            per_float_feature_quantization=None,\n",
        "                            input_borders=None,\n",
        "                            output_borders=None,\n",
        "                            fold_permutation_block=None,\n",
        "                            od_pval=None,\n",
        "                            od_wait=None,\n",
        "                            od_type=None,\n",
        "                            nan_mode=None,\n",
        "                            counter_calc_method=None,\n",
        "                            leaf_estimation_iterations=None,\n",
        "                            leaf_estimation_method=None,\n",
        "                            thread_count=None,\n",
        "                            random_seed=None,\n",
        "                            use_best_model=None,\n",
        "                            verbose=None,\n",
        "                            logging_level=None,\n",
        "                            metric_period=None,\n",
        "                            ctr_leaf_count_limit=None,\n",
        "                            store_all_simple_ctr=None,\n",
        "                            max_ctr_complexity=None,\n",
        "                            has_time=None,\n",
        "                            allow_const_label=None,\n",
        "                            classes_count=None,\n",
        "                            class_weights=None,\n",
        "                            auto_class_weights=None,\n",
        "                            one_hot_max_size=None,\n",
        "                            random_strength=None,\n",
        "                            name=None,\n",
        "                            ignored_features=None,\n",
        "                            train_dir=None,\n",
        "                            custom_loss=None,\n",
        "                            custom_metric=None,\n",
        "                            eval_metric=None,\n",
        "                            bagging_temperature=None,\n",
        "                            save_snapshot=None,\n",
        "                            snapshot_file=None,\n",
        "                            snapshot_interval=None,\n",
        "                            fold_len_multiplier=None,\n",
        "                            used_ram_limit=None,\n",
        "                            gpu_ram_part=None,\n",
        "                            allow_writing_files=None,\n",
        "                            final_ctr_computation_mode=None,\n",
        "                            approx_on_full_history=None,\n",
        "                            boosting_type=None,\n",
        "                            simple_ctr=None,\n",
        "                            combinations_ctr=None,\n",
        "                            per_feature_ctr=None,\n",
        "                            task_type=None,\n",
        "                            device_config=None,\n",
        "                            devices=None,\n",
        "                            bootstrap_type=None,\n",
        "                            subsample=None,\n",
        "                            sampling_unit=None,\n",
        "                            dev_score_calc_obj_block_size=None,\n",
        "                            max_depth=None,\n",
        "                            n_estimators=None,\n",
        "                            num_boost_round=None,\n",
        "                            num_trees=None,\n",
        "                            colsample_bylevel=None,\n",
        "                            random_state=None,\n",
        "                            reg_lambda=None,\n",
        "                            objective=None,\n",
        "                            eta=None,\n",
        "                            max_bin=None,\n",
        "                            scale_pos_weight=None,\n",
        "                            gpu_cat_features_storage=None,\n",
        "                            data_partition=None\n",
        "                            metadata=None,\n",
        "                            early_stopping_rounds=None,\n",
        "                            cat_features=None,\n",
        "                            grow_policy=None,\n",
        "                            min_data_in_leaf=None,\n",
        "                            min_child_samples=None,\n",
        "                            max_leaves=None,\n",
        "                            num_leaves=None,\n",
        "                            score_function=None,\n",
        "                            leaf_estimation_backtracking=None,\n",
        "                            ctr_history_unit=None,\n",
        "                            monotone_constraints=None,\n",
        "                            feature_weights=None,\n",
        "                            penalties_coefficient=None,\n",
        "                            first_feature_use_penalties=None,\n",
        "                            model_shrink_rate=None,\n",
        "                            model_shrink_mode=None,\n",
        "                            langevin=None,\n",
        "                            diffusion_temperature=None,\n",
        "                            posterior_sampling=None,\n",
        "                            boost_from_average=None,\n",
        "                            text_features=None,\n",
        "                            tokenizers=None,\n",
        "                            dictionaries=None,\n",
        "                            feature_calcers=None,\n",
        "                            text_processing=None,\n",
        "                            fixed_binary_splits=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvZIUdxLzo-n"
      },
      "outputs": [],
      "source": [
        "cat_prob = {\"objective\":\"RMSE\",\"eval_metric\":\"RMSE\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJvn8jzFQUNr"
      },
      "source": [
        "##### 4.1.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDGGnrUAQUNr"
      },
      "outputs": [],
      "source": [
        "def objective_catboost(trial, X, y, n_splits, n_repeats, model=CatBoostRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=cat_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {\n",
        "        'iterations': 1000,\n",
        "        'learning_rate': 0.025, #trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'depth': trial.suggest_int('depth', 5, 9),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-4, 0.1, log=True),\n",
        "        \"bootstrap_type\": \"Bayesian\",\n",
        "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.5, 1.5, step=0.1),\n",
        "        'random_strength': trial.suggest_float('random_strength', 0.5, 3.5, step=0.25),\n",
        "        #'border_count': trial.suggest_int('border_count', 32, 255),\n",
        "        'cat_features': categorical_features,\n",
        "        'task_type': 'GPU' if use_gpu else 'CPU',\n",
        "        'random_seed':rs,\n",
        "        'verbose': 250,\n",
        "        'objective': metrics[\"objective\"],\n",
        "        'eval_metric': metrics[\"eval_metric\"],\n",
        "        \"od_type\":'EBS', #Early stopping hyperparmeter\n",
        "        \"od_wait\":101,\n",
        "        #\"sampling_frequency\":\"PerTreeLevel\",\n",
        "        \"use_best_model\":True,\n",
        "    }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "      kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "      kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "      kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "      kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1,1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1,1)\n",
        "\n",
        "        if fit_scaling:\n",
        "          scaler = StandardScaler()\n",
        "          X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "          X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create the Pool objects for CatBoost\n",
        "        train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features)\n",
        "        valid_pool = Pool(data=X_valid, label=y_valid, cat_features=categorical_features)\n",
        "\n",
        "        # Create the pipeline\n",
        "        model = model_class(**params)\n",
        "        # Fit the model:\n",
        "        model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=101,\n",
        "                  #callbacks=[optuna.integration.CatBoostPruningCallback(trial, \"RMSE\")]\n",
        "                  )\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "#        y_pred = np.expm1(y_pred)\n",
        "#        y_valid = np.expm1(y_valid)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4q9nTQpQUNs"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_catboost(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMaZd811QUNs"
      },
      "outputs": [],
      "source": [
        "# usage with XGBRegressor\n",
        "cat_study = tune_hyperparameters(X_enc, y, model_class=CatBoostRegressor, n_trials=31, n_splits_ = 5 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(cat_study, CatBoostRegressor, \"CatBoost_ext\")\n",
        "cat_params = cat_study.best_paramsy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 10 finished with value: 38.67948459216966\n",
        "- Parameters: {'depth': 9, 'l2_leaf_reg': 0.004177701145518355, 'bagging_temperature': 0.5, 'random_strength': 0.5}"
      ],
      "metadata": {
        "id": "c8WTOGC1vY-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-aPe-7lMjdY"
      },
      "outputs": [],
      "source": [
        "X_enc.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvqbaMJMxyqI"
      },
      "source": [
        "#### **4.2.1 LGBMRegressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2VbsJlay765"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm_prob = {\"objective\":\"regression\",\"eval_metric\":\"rmse\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOB-zCzMx9Jr"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cHDHOJMiYKK"
      },
      "outputs": [],
      "source": [
        "X_enc.info()\n",
        "t.cat_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_EHAxqPPyrU"
      },
      "outputs": [],
      "source": [
        "def objective_lgbm(trial, X, y, n_splits, n_repeats, model=LGBMRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=lgbm_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 131),\n",
        "        'learning_rate': 0.02, #trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        #'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 60),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 0.95),\n",
        "        'subsample_freq': trial.suggest_int('subsample_freq', 1, 3),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        \"reg_alpha\" :         trial.suggest_float(\"reg_alpha\", 1e-3, 1.0, log=True),\n",
        "        \"reg_lambda\" :        trial.suggest_float(\"reg_lambda\", 1e-3, 1.0, log=True),\n",
        "        \"boosting_type\":      'gbdt',\n",
        "        'n_estimators': 2501,\n",
        "        'objective': metrics[\"objective\"],\n",
        "        'device': 'gpu' if use_gpu else 'cpu',\n",
        "        'verbose': -1,\n",
        "        #'scale_pos_weight': sample_pos_weight,\n",
        "#       'categorical_feature': [2,4,5,9],\n",
        "        'random_state': rs,\n",
        "    }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1, 1)\n",
        "\n",
        "        if fit_scaling:\n",
        "            scaler = StandardScaler()\n",
        "            X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "            X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create the datasets for LightGBM\n",
        "        # d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "        # d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features)\n",
        "\n",
        "        # Create the model\n",
        "        model = model_class(**params)\n",
        "\n",
        "        # Create the early stopping callback\n",
        "        early_stop = early_stopping(stopping_rounds=101)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[early_stop], eval_metric=metrics[\"eval_metric\"], categorical_feature= categorical_features)\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnC-tzJ-PyoE"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_lgbm(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mXmUXUZPyk8"
      },
      "outputs": [],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=LGBMRegressor, n_trials=31, n_splits_ = 5 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(cat_study, LGBMRegressor, \"LGBMBoost_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 28 finished with value: 38.657451168104984\n",
        "- parameters: {'num_leaves': 103, 'min_child_samples': 36, 'subsample': 0.9131771240297577, 'subsample_freq': 2, 'colsample_bytree': 0.6190291906152294, 'reg_alpha': 0.03976551748855951, 'reg_lambda': 0.2576052197300848}"
      ],
      "metadata": {
        "id": "4Ul6eVTJZNC5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md0hNqSwgfV4"
      },
      "source": [
        "#### **4.3.1 XGBClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yoxlSCCgfV5"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "xgb_prob = {'objective': \"reg:squarederror\",'eval_metric': \"rmse\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc[t.cat_features] = X_enc[t.cat_features].astype(\"category\")\n",
        "test_enc[t.cat_features] = test_enc[t.cat_features].astype(\"category\")\n",
        "test_enc.info()"
      ],
      "metadata": {
        "id": "ELHBSxDA4C95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SglmFB9QgfV5"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ROmQq9OgfV6"
      },
      "outputs": [],
      "source": [
        "def objective_xgb(trial, X, y, n_splits, n_repeats, model=XGBRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=xgb_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {\n",
        "              'n_estimators': 1000,\n",
        "              'learning_rate': 0.025, #trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "              'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "              #'max_bin': trial.suggest_int('max_bin', 255, 511),\n",
        "              'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "              'subsample': trial.suggest_float('subsample', 0.7, 0.95),\n",
        "              'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.95),\n",
        "              'gamma': trial.suggest_float('gamma', 0, 1),\n",
        "              'reg_alpha': trial.suggest_float('reg_alpha', 0.00001, 1.0, log=True),\n",
        "              'reg_lambda': trial.suggest_float('reg_lambda', 0.00001, 10, log=True),\n",
        "              'objective':  metrics[\"objective\"],  # For binary classification\n",
        "              'eval_metric': metrics[\"eval_metric\"],\n",
        "              \"early_stopping_rounds\":51,\n",
        "              'tree_method': 'gpu_hist' if use_gpu else 'hist',  # Use GPU if available\n",
        "              'random_state': rs,\n",
        "              'enable_categorical': True,\n",
        "#              'scale_pos_weight': sample_pos_weight,\n",
        "             }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1, 1)\n",
        "\n",
        "        if fit_scaling:\n",
        "            scaler = StandardScaler()\n",
        "            X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "            X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create DMatrix objects for XGBoost\n",
        "        d_train = xgb.DMatrix(X_train, label=y_train,feature_types=[\"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"q\",\"q\",\"q\",\"q\",\n",
        "                                                                    \"c\",\"c\"],enable_categorical=True)\n",
        "        d_valid = xgb.DMatrix(X_valid, label=y_valid,feature_types=[\"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"q\",\"q\",\"q\",\"q\",\n",
        "                                                                    \"c\",\"c\"],enable_categorical=True)\n",
        "\n",
        "        # Create the model\n",
        "        model = model_class(**params)\n",
        "\n",
        "        # Create the early stopping callback\n",
        "        # early_stop = early_stopping(stopping_rounds=101)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train,\n",
        "                  eval_set=[(X_valid, y_valid)],\n",
        "                  verbose=False)\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1QiMlUGgfV6"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_xgb(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57BpBdEqgfV6"
      },
      "outputs": [],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=XGBRegressor, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(cat_study, XGBRegressor, \"XGB_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 18 finished with value: 38.66071701049805\n",
        "- parameters: {'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.9493097301768285, 'colsample_bytree': 0.750026675584575, 'gamma': 0.5511841320880777, 'reg_alpha': 0.006948944983035811, 'reg_lambda': 0.0015661314558322087}."
      ],
      "metadata": {
        "id": "jxXsfZ_hDsYH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbThfscYTcDW"
      },
      "source": [
        "#### **4.4.1 TabNetClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u__JMQhTcDX"
      },
      "outputs": [],
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "tab_prob = {'eval_metric': \"rmse\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFqDr7bBZKoj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "a3f157d3-60d1-4e6a-c578-752ecc625063"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "3782983      5         3     1             7                   1           1   \n",
              "2003343      3         0     1             1                   2           2   \n",
              "3404963      4         3     3             7                   2           2   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "3782983      3      2              1.451171  0.016408  0.483511 -0.935853   \n",
              "2003343      3      0             -0.433108  0.033023 -1.185578 -0.348455   \n",
              "3404963      3      6              1.522432 -1.099254 -0.846883  0.448205   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "3782983           0               0  \n",
              "2003343           0               0  \n",
              "3404963           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24a6a99d-0672-43cf-92dd-bb5402b777ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3782983</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.451171</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>0.483511</td>\n",
              "      <td>-0.935853</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003343</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.433108</td>\n",
              "      <td>0.033023</td>\n",
              "      <td>-1.185578</td>\n",
              "      <td>-0.348455</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3404963</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1.522432</td>\n",
              "      <td>-1.099254</td>\n",
              "      <td>-0.846883</td>\n",
              "      <td>0.448205</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24a6a99d-0672-43cf-92dd-bb5402b777ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24a6a99d-0672-43cf-92dd-bb5402b777ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24a6a99d-0672-43cf-92dd-bb5402b777ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6f397794-e47b-42fc-85e9-13ceccbf56be\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f397794-e47b-42fc-85e9-13ceccbf56be')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6f397794-e47b-42fc-85e9-13ceccbf56be button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 7,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.4511709213256836\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.01640772819519043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4835107922554016\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.9358525276184082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UikYbKG4ZC24"
      },
      "outputs": [],
      "source": [
        "grouped_features = []\n",
        "feature_cols = X_enc.columns.to_list()\n",
        "\n",
        "group_1 = [\"Brand\",\t\"Material\",\t\"Size\",\t\"Compartments\",\t\"Laptop Compartment\",\t\"Waterproof\",\t\"Style\",\t\"Color\"]\n",
        "group_2 = ['cheap_flag',\t'expansive_flag']\n",
        "#group_3 = ['cb_person_default_on_file']\n",
        "\n",
        "# Iterate through each set of related columns (e.g., blood glucose, insulin, etc.)\n",
        "for colset in [group_1,group_2]:\n",
        "    group_idxs = [idx for idx, col in enumerate(feature_cols) if col in colset]\n",
        "    grouped_features.append(group_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh4feROUZoUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b96d80-f3f7-4c82-e67a-f13ee845aef4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 1, 2, 3, 4, 5, 6, 7], [12, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "grouped_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWEKkZi7TcDX"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyVDQ0FjUBUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daae5a4a-771f-488d-fceb-2741fa8fa4da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 13]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "cate_feat = t.cat_features.copy()\n",
        "\n",
        "for colset in [cate_feat]:\n",
        "    cat_index_cols = [idx for idx, col in enumerate(feature_cols) if col in colset]\n",
        "\n",
        "cat_index_cols[-2:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[len(X_enc[col].unique()) for col in t.cat_features][-2:]"
      ],
      "metadata": {
        "id": "X76B9uW6BSVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ca0d6d-d1d7-4d73-824d-4b7f3ff89c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWyN_Q_rTcDY"
      },
      "outputs": [],
      "source": [
        "def objective_tabnet(trial, X, y, n_splits, n_repeats, model=TabNetRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=tab_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'n_d': trial.suggest_categorical('n_d', [4,8,12]),\n",
        "              'n_a': trial.suggest_categorical('n_d', [4,8,12]),\n",
        "              'n_steps': trial.suggest_int('n_steps', 3, 4),\n",
        "              'gamma': trial.suggest_float('gamma', 1.01, 2),\n",
        "              'lambda_sparse':trial.suggest_float('lambda_sparse', 1e-5, 1e-1),\n",
        "              \"grouped_features\":grouped_features,\n",
        "              #'cat_idxs': cat_index_cols,\n",
        "              #'cat_dims': [len(X_enc[col].unique()) for col in categorical_features],\n",
        "              #'cat_emb_dim': [2,2,1,3,1,1,1,2,1,1],\n",
        "              'n_independent': trial.suggest_int('n_independent', 1, 3),\n",
        "              'n_shared': trial.suggest_int('n_shared', 1, 3),\n",
        "              'mask_type': trial.suggest_categorical('mask_type', ['sparsemax']),\n",
        "              'device_name': 'cuda' if use_gpu else 'cpu',\n",
        "              'seed': rs,\n",
        "              \"optimizer_fn\":torch.optim.Adam,\n",
        "              \"optimizer_params\":dict(lr=0.01),\n",
        "              \"scheduler_params\":{\"patience\":3, # how to use learning rate scheduler\n",
        "                                \"factor\":0.5,\n",
        "                                \"min_lr\":0.0001},\n",
        "              \"scheduler_fn\":torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1, 1)\n",
        "\n",
        "        if fit_scaling:\n",
        "            scaler = StandardScaler()\n",
        "            X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "            X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create the model\n",
        "        model = model_class(**params)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train.values, y_train,\n",
        "                  eval_set=[(X_valid.values, y_valid)],\n",
        "                  batch_size=1024,\n",
        "                  max_epochs=11,\n",
        "                  patience=5,\n",
        "                  eval_metric=[tab_prob[\"eval_metric\"]])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid.values)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfvK9Qw-TcDY"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_tabnet(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJwRqrJRTcDY"
      },
      "outputs": [],
      "source": [
        "tab_study = tune_hyperparameters(X_enc, y, model_class=TabNetRegressor, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(tab_study, TabNetRegressor, \"tabnet_ext\")\n",
        "tab_params = tab_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jch2yojjoiZS"
      },
      "source": [
        "#### 4.5.1 Yggdrasil - RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mgl-1wLt_4I"
      },
      "outputs": [],
      "source": [
        "#!pip install ydf -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_YHewPTzMVM"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vavm8Xws3vO"
      },
      "source": [
        "##### 4.5.2 Optuna Optimization:\n",
        "\n",
        "[HyperParameters Link](https://ydf.readthedocs.io/en/stable/hyperparameters/#lambda_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eJuaeqmPyh0"
      },
      "outputs": [],
      "source": [
        "# import ydf\n",
        "\n",
        "# def objective_ydf_gbt(trial, X, y, n_splits, n_repeats, model_ = ydf.GradientBoostedTreesLearner, rs=42, fit_scaling=False, cv_strategy=\"KFold\", use_gpu=False):\n",
        "\n",
        "#     model_class = model_  # Use ydf's GradientBoostedTreesLearner\n",
        "\n",
        "#     categorical_features = t.cat_features.copy()  # Assuming 't' is defined somewhere with categorical features\n",
        "\n",
        "#     num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "#     auc_scores = []\n",
        "\n",
        "#     params = {\n",
        "#         'loss':\"BINOMIAL_LOG_LIKELIHOOD\",\n",
        "#         'num_trees': trial.suggest_int('num_trees', 100, 1000),  # Number of trees in the forest\n",
        "#         'max_depth': trial.suggest_int('max_depth', 3, 10),  # Maximum depth of the trees\n",
        "#         'min_examples': trial.suggest_int('min_examples', 1, 10),  # Minimum number of samples required to split an internal node\n",
        "#         'l1_regularization': trial.suggest_float('l1_regularization', 1e-5, 10.0, log=True),\n",
        "#         'l2_regularization': trial.suggest_float('l2_regularization', 1e-5, 10.0, log=True),\n",
        "# #         'shrinkage': trial.suggest_float('shrinkage', 0.01, 0.3, log=True),  # Similar to learning rate, but applied after each tree is trained\n",
        "#          'subsample': trial.suggest_float('subsample', 0.7, 1.0),  # Fraction of samples used for training each tree\n",
        "# # #        'max_categorical_cardinality': trial.suggest_int('max_categorical_cardinality', 10, 100),  # Maximum number of unique values for categorical features\n",
        "# #         'categorical_algorithm': trial.suggest_categorical('categorical_algorithm', ['CART', 'RANDOM']),  # Algorithm used for categorical splits\n",
        "# #         'split_axis': trial.suggest_categorical('split_axis', ['AXIS_ALIGNED', 'SPARSE_OBLIQUE']),  # How to split numerical features\n",
        "# #         'sparse_oblique_normalization': trial.suggest_categorical('sparse_oblique_normalization', ['NONE', 'STANDARD_DEVIATION', 'MIN_MAX']),  # Normalization for sparse oblique splits\n",
        "# #         'sparse_oblique_num_projections_exponent': trial.suggest_float('sparse_oblique_num_projections_exponent', 0.5, 1.0),  # Exponent for the number of projections in sparse oblique splits\n",
        "#         'random_seed': rs,  # Random seed for reproducibility\n",
        "#         'early_stopping':\"LOSS_INCREASE\",  # Enable early stopping\n",
        "#         'early_stopping_num_trees_look_ahead': 30,  # Number of trees to look ahead for early stopping\n",
        "# #        'weights': {0:1,1:sample_pos_weight},\n",
        "#         \"label\" : \"loan_status\",\n",
        "#         }\n",
        "\n",
        "\n",
        "#     if cv_strategy == 'RepKFold':\n",
        "#         kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "#     elif cv_strategy == 'KFold':\n",
        "#         kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "#     elif cv_strategy == \"StratKFold\":\n",
        "#         kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "#     elif cv_strategy == \"RepStratKFold\":\n",
        "#         kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "#     for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "#         # Split the data\n",
        "#         X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
        "#         X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
        "\n",
        "#         if fit_scaling:\n",
        "#             scaler = StandardScaler()\n",
        "#             X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "#             X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "#         X_train[\"loan_status\"] = y_train.values\n",
        "#         X_valid[\"loan_status\"] = y_valid.values\n",
        "\n",
        "#         # Train the model with early stopping\n",
        "#         model = model_class(task=ydf.Task.CLASSIFICATION, **params)\n",
        "#         model.train(X_train, valid=(X_valid))  # Use ydf's fit method with early stopping\n",
        "\n",
        "#         # Make predictions\n",
        "#         y_pred = model.get_predictions(X_valid)[\"loan_status_probability_1\"]\n",
        "\n",
        "#         # Calculate AUC\n",
        "#         auc_score = roc_auc_score(y_valid, y_pred)\n",
        "#         auc_scores.append(auc_score)\n",
        "\n",
        "#     # Calculate the mean AUC\n",
        "#     key_metric = np.mean(auc_scores)\n",
        "\n",
        "#     return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbunXYe3vquN"
      },
      "outputs": [],
      "source": [
        "# # Step 2: Tuning Hyperparameters with Optuna\n",
        "# def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "#     study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "#     study.optimize(lambda trial: objective_ydf_gbt(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model_=model_class, use_gpu=use_gpu, cv_strategy=\"StratKFold\"), n_trials=n_trials)\n",
        "#     return study  # Return the study object\n",
        "\n",
        "# # Step 3: Saving Best Results and Models\n",
        "# def save_results(study, model_class, model_name):\n",
        "#     best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "#     joblib.dump(study.best_params, best_params_file)\n",
        "#     print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "#     verbose_file = f\"{model_name}_ydf_verbose.log\"\n",
        "#     with open(verbose_file, \"w\") as f:\n",
        "#         f.write(str(study.trials))\n",
        "#     print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6XXbt231ogK"
      },
      "outputs": [],
      "source": [
        "# cat_study = tune_hyperparameters(X_enc, y, model_class=ydf.GradientBoostedTreesLearner, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=False)\n",
        "# save_results(cat_study, TabNetClassifier, \"ydf_ext\")\n",
        "# cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41K1txKVzFhu"
      },
      "source": [
        "#### **4.6.1 NeuralNetwork**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGyFvBNgzFh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "b73f9188-9dae-44c8-91c6-852eeae6104a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "312954       1         4     0             8                   1           1   \n",
              "2703119      4         4     3             1                   1           1   \n",
              "678964       1         3     1             0                   2           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "312954       1      5             -1.865994 -0.753177  1.391704 -0.179771   \n",
              "2703119      3      3              1.110761  0.290271  1.019273  0.544976   \n",
              "678964       3      5              0.293288  0.645984 -0.168518 -0.517618   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "312954            0               0  \n",
              "2703119           0               0  \n",
              "678964            0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c297503-03e3-442d-865d-2bf33e08dcaf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>312954</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-1.865994</td>\n",
              "      <td>-0.753177</td>\n",
              "      <td>1.391704</td>\n",
              "      <td>-0.179771</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2703119</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1.110761</td>\n",
              "      <td>0.290271</td>\n",
              "      <td>1.019273</td>\n",
              "      <td>0.544976</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>678964</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.293288</td>\n",
              "      <td>0.645984</td>\n",
              "      <td>-0.168518</td>\n",
              "      <td>-0.517618</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c297503-03e3-442d-865d-2bf33e08dcaf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c297503-03e3-442d-865d-2bf33e08dcaf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c297503-03e3-442d-865d-2bf33e08dcaf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ced39ac2-1349-4819-9692-db020c3eb47d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ced39ac2-1349-4819-9692-db020c3eb47d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ced39ac2-1349-4819-9692-db020c3eb47d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 4,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          8,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.865993857383728\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.7531771659851074\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.391703724861145\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.17977148294448853\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512,last_layer = 1, activation=\"relu\"):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "VF6Qd4eziUd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIf9YCb8Bdcu",
        "outputId": "60892f8c-db2e-4b69-9081-f36f67fb21fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([6, 5, 4, 10, 3, 3, 4, 7, 2, 2],\n",
              " array([3., 3., 2., 4., 2., 2., 2., 3., 2., 2.]),\n",
              " 10)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Efx2uIuzFh4"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maUIASqazFh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c620f1d0-d621-4224-dda6-46500e384546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Dtype\n",
            "---  ------              -----\n",
            " 0   Brand               int64\n",
            " 1   Material            int64\n",
            " 2   Size                int64\n",
            " 3   Compartments        int64\n",
            " 4   Laptop Compartment  int64\n",
            " 5   Waterproof          int64\n",
            " 6   Style               int64\n",
            " 7   Color               int64\n",
            " 8   cheap_flag          int64\n",
            " 9   expansive_flag      int64\n",
            "dtypes: int64(10)\n",
            "memory usage: 335.2 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Weight Capacity (kg)  float32\n",
            " 1   TE_wc                 float32\n",
            " 2   skew_0                float32\n",
            " 3   skew_1                float32\n",
            "dtypes: float32(4)\n",
            "memory usage: 91.4 MB\n"
          ]
        }
      ],
      "source": [
        "categorical_feat = t.cat_features.copy()\n",
        "numerical_feat = t.num_features.copy()\n",
        "\n",
        "X_train_cat = X_enc[categorical_feat]\n",
        "X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "X_test_cat = test_enc[categorical_feat]\n",
        "X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "X_train_cat.info()\n",
        "X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GgRZk52zFh5"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"])}\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num])\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYf3TjJVzFh6"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9KKYtiYzFh7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cdfa05b7-110b-498b-f260-2cb6671285c6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 10:16:44,687] A new study created in memory with name: no-name-d7bfcb07-6dd6-4153-9e33-44f99b8d61aa\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 7ms/step - RMSE: 53.3488 - loss: 2996.9104 - val_RMSE: 38.7680 - val_loss: 1502.9572 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7438 - loss: 1501.0842 - val_RMSE: 38.7101 - val_loss: 1498.4749 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7387 - loss: 1500.6854 - val_RMSE: 38.7810 - val_loss: 1503.9675 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7373 - loss: 1500.5754 - val_RMSE: 38.7479 - val_loss: 1501.4033 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7362 - loss: 1500.4922 - val_RMSE: 38.7172 - val_loss: 1499.0236 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7354 - loss: 1500.4320 - val_RMSE: 38.7088 - val_loss: 1498.3741 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7351 - loss: 1500.4102 - val_RMSE: 38.7098 - val_loss: 1498.4497 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.7346 - loss: 1500.3710 - val_RMSE: 38.7177 - val_loss: 1499.0566 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7342 - loss: 1500.3417 - val_RMSE: 38.7288 - val_loss: 1499.9218 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7336 - loss: 1500.2925 - val_RMSE: 38.7177 - val_loss: 1499.0636 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7327 - loss: 1500.2250 - val_RMSE: 38.7158 - val_loss: 1498.9110 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7253 - loss: 1499.6500 - val_RMSE: 38.6850 - val_loss: 1496.5319 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.7234 - loss: 1499.5046 - val_RMSE: 38.6848 - val_loss: 1496.5164 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7230 - loss: 1499.4679 - val_RMSE: 38.6847 - val_loss: 1496.5052 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7226 - loss: 1499.4371 - val_RMSE: 38.6846 - val_loss: 1496.4973 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7223 - loss: 1499.4138 - val_RMSE: 38.6846 - val_loss: 1496.4966 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7220 - loss: 1499.3964 - val_RMSE: 38.6845 - val_loss: 1496.4885 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7218 - loss: 1499.3761 - val_RMSE: 38.6845 - val_loss: 1496.4889 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7215 - loss: 1499.3588 - val_RMSE: 38.6844 - val_loss: 1496.4838 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7213 - loss: 1499.3431 - val_RMSE: 38.6844 - val_loss: 1496.4847 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7212 - loss: 1499.3291 - val_RMSE: 38.6844 - val_loss: 1496.4850 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7210 - loss: 1499.3151 - val_RMSE: 38.6844 - val_loss: 1496.4835 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7208 - loss: 1499.3029 - val_RMSE: 38.6844 - val_loss: 1496.4854 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7206 - loss: 1499.2888 - val_RMSE: 38.6845 - val_loss: 1496.4905 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7205 - loss: 1499.2792 - val_RMSE: 38.6846 - val_loss: 1496.4954 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 26s 8ms/step - RMSE: 53.2917 - loss: 2989.8503 - val_RMSE: 38.7388 - val_loss: 1500.6951 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6913 - loss: 1497.0150 - val_RMSE: 38.7415 - val_loss: 1500.9073 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6887 - loss: 1496.8157 - val_RMSE: 38.7421 - val_loss: 1500.9474 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6866 - loss: 1496.6555 - val_RMSE: 38.7345 - val_loss: 1500.3625 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6854 - loss: 1496.5598 - val_RMSE: 38.7245 - val_loss: 1499.5845 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6848 - loss: 1496.5118 - val_RMSE: 38.7420 - val_loss: 1500.9388 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6840 - loss: 1496.4501 - val_RMSE: 38.7205 - val_loss: 1499.2780 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6833 - loss: 1496.4015 - val_RMSE: 38.7363 - val_loss: 1500.4982 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6826 - loss: 1496.3461 - val_RMSE: 38.7169 - val_loss: 1498.9999 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6821 - loss: 1496.3041 - val_RMSE: 38.7238 - val_loss: 1499.5336 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6815 - loss: 1496.2607 - val_RMSE: 38.7342 - val_loss: 1500.3352 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6812 - loss: 1496.2394 - val_RMSE: 38.7526 - val_loss: 1501.7678 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6810 - loss: 1496.2241 - val_RMSE: 38.7539 - val_loss: 1501.8683 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6807 - loss: 1496.1965 - val_RMSE: 38.7435 - val_loss: 1501.0594 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6729 - loss: 1495.5959 - val_RMSE: 38.7156 - val_loss: 1498.9008 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6717 - loss: 1495.5046 - val_RMSE: 38.7157 - val_loss: 1498.9084 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6714 - loss: 1495.4757 - val_RMSE: 38.7157 - val_loss: 1498.9043 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6711 - loss: 1495.4542 - val_RMSE: 38.7156 - val_loss: 1498.8981 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6709 - loss: 1495.4390 - val_RMSE: 38.7156 - val_loss: 1498.8966 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6707 - loss: 1495.4257 - val_RMSE: 38.7154 - val_loss: 1498.8845 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6705 - loss: 1495.4106 - val_RMSE: 38.7154 - val_loss: 1498.8818 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6703 - loss: 1495.3962 - val_RMSE: 38.7153 - val_loss: 1498.8741 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6702 - loss: 1495.3857 - val_RMSE: 38.7152 - val_loss: 1498.8654 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6701 - loss: 1495.3737 - val_RMSE: 38.7152 - val_loss: 1498.8636 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6699 - loss: 1495.3649 - val_RMSE: 38.7151 - val_loss: 1498.8568 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 27s 8ms/step - RMSE: 53.2472 - loss: 2984.4116 - val_RMSE: 38.7255 - val_loss: 1499.6621 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7226 - loss: 1499.4381 - val_RMSE: 38.7243 - val_loss: 1499.5703 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7181 - loss: 1499.0909 - val_RMSE: 38.7263 - val_loss: 1499.7239 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7163 - loss: 1498.9539 - val_RMSE: 38.9383 - val_loss: 1516.1874 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7141 - loss: 1498.7784 - val_RMSE: 38.8476 - val_loss: 1509.1339 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7127 - loss: 1498.6731 - val_RMSE: 38.9148 - val_loss: 1514.3632 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7118 - loss: 1498.6011 - val_RMSE: 38.8934 - val_loss: 1512.6975 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7026 - loss: 1497.8910 - val_RMSE: 38.7066 - val_loss: 1498.2042 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7010 - loss: 1497.7683 - val_RMSE: 38.7065 - val_loss: 1498.1912 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7004 - loss: 1497.7245 - val_RMSE: 38.7073 - val_loss: 1498.2535 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7001 - loss: 1497.6953 - val_RMSE: 38.7075 - val_loss: 1498.2676 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6998 - loss: 1497.6760 - val_RMSE: 38.7078 - val_loss: 1498.2948 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6996 - loss: 1497.6565 - val_RMSE: 38.7081 - val_loss: 1498.3188 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6993 - loss: 1497.6401 - val_RMSE: 38.7080 - val_loss: 1498.3088 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6982 - loss: 1497.5485 - val_RMSE: 38.7049 - val_loss: 1498.0710 - learning_rate: 1.0000e-05\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6980 - loss: 1497.5325 - val_RMSE: 38.7049 - val_loss: 1498.0703 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6979 - loss: 1497.5300 - val_RMSE: 38.7049 - val_loss: 1498.0695 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6979 - loss: 1497.5261 - val_RMSE: 38.7049 - val_loss: 1498.0696 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6978 - loss: 1497.5223 - val_RMSE: 38.7049 - val_loss: 1498.0685 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6978 - loss: 1497.5204 - val_RMSE: 38.7049 - val_loss: 1498.0692 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6978 - loss: 1497.5181 - val_RMSE: 38.7049 - val_loss: 1498.0693 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6977 - loss: 1497.5155 - val_RMSE: 38.7049 - val_loss: 1498.0699 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6977 - loss: 1497.5131 - val_RMSE: 38.7049 - val_loss: 1498.0698 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6977 - loss: 1497.5107 - val_RMSE: 38.7049 - val_loss: 1498.0698 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6976 - loss: 1497.5073 - val_RMSE: 38.7047 - val_loss: 1498.0518 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 10:44:22,712] Trial 0 finished with value: 38.701437632242836 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'selu'}. Best is trial 0 with value: 38.701437632242836.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 26s 7ms/step - RMSE: 56.3630 - loss: 3340.2073 - val_RMSE: 38.6960 - val_loss: 1497.3799 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7409 - loss: 1500.8616 - val_RMSE: 38.7025 - val_loss: 1497.8840 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7374 - loss: 1500.5897 - val_RMSE: 38.6956 - val_loss: 1497.3497 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7356 - loss: 1500.4476 - val_RMSE: 38.6931 - val_loss: 1497.1523 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7339 - loss: 1500.3163 - val_RMSE: 38.6863 - val_loss: 1496.6328 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7322 - loss: 1500.1838 - val_RMSE: 38.6855 - val_loss: 1496.5647 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7298 - loss: 1500.0016 - val_RMSE: 38.6847 - val_loss: 1496.5066 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7285 - loss: 1499.8959 - val_RMSE: 38.6859 - val_loss: 1496.6007 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7273 - loss: 1499.8015 - val_RMSE: 38.6860 - val_loss: 1496.6062 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7264 - loss: 1499.7316 - val_RMSE: 38.6849 - val_loss: 1496.5193 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7253 - loss: 1499.6483 - val_RMSE: 38.6838 - val_loss: 1496.4329 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7244 - loss: 1499.5797 - val_RMSE: 38.6838 - val_loss: 1496.4369 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7233 - loss: 1499.4962 - val_RMSE: 38.6837 - val_loss: 1496.4279 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7224 - loss: 1499.4224 - val_RMSE: 38.6831 - val_loss: 1496.3823 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7210 - loss: 1499.3186 - val_RMSE: 38.6832 - val_loss: 1496.3865 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7200 - loss: 1499.2412 - val_RMSE: 38.6840 - val_loss: 1496.4532 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7183 - loss: 1499.1088 - val_RMSE: 38.6836 - val_loss: 1496.4241 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7171 - loss: 1499.0129 - val_RMSE: 38.6846 - val_loss: 1496.4962 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7153 - loss: 1498.8740 - val_RMSE: 38.6851 - val_loss: 1496.5378 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7088 - loss: 1498.3752 - val_RMSE: 38.6788 - val_loss: 1496.0526 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7053 - loss: 1498.1029 - val_RMSE: 38.6791 - val_loss: 1496.0709 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7039 - loss: 1497.9907 - val_RMSE: 38.6793 - val_loss: 1496.0872 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7028 - loss: 1497.9105 - val_RMSE: 38.6795 - val_loss: 1496.1014 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7020 - loss: 1497.8457 - val_RMSE: 38.6796 - val_loss: 1496.1132 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7012 - loss: 1497.7875 - val_RMSE: 38.6798 - val_loss: 1496.1257 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 26s 8ms/step - RMSE: 56.3323 - loss: 3335.9031 - val_RMSE: 38.7281 - val_loss: 1499.8630 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6939 - loss: 1497.2158 - val_RMSE: 38.7284 - val_loss: 1499.8890 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6886 - loss: 1496.8049 - val_RMSE: 38.7258 - val_loss: 1499.6859 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6856 - loss: 1496.5751 - val_RMSE: 38.7245 - val_loss: 1499.5905 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6833 - loss: 1496.4011 - val_RMSE: 38.7185 - val_loss: 1499.1229 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6815 - loss: 1496.2599 - val_RMSE: 38.7168 - val_loss: 1498.9868 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6798 - loss: 1496.1265 - val_RMSE: 38.7160 - val_loss: 1498.9304 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6786 - loss: 1496.0315 - val_RMSE: 38.7150 - val_loss: 1498.8538 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6778 - loss: 1495.9731 - val_RMSE: 38.7160 - val_loss: 1498.9301 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6771 - loss: 1495.9177 - val_RMSE: 38.7144 - val_loss: 1498.8081 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6757 - loss: 1495.8132 - val_RMSE: 38.7155 - val_loss: 1498.8861 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6744 - loss: 1495.7072 - val_RMSE: 38.7160 - val_loss: 1498.9326 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6731 - loss: 1495.6069 - val_RMSE: 38.7151 - val_loss: 1498.8607 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6716 - loss: 1495.4904 - val_RMSE: 38.7162 - val_loss: 1498.9424 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6700 - loss: 1495.3658 - val_RMSE: 38.7159 - val_loss: 1498.9176 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6627 - loss: 1494.8049 - val_RMSE: 38.7124 - val_loss: 1498.6525 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6601 - loss: 1494.6075 - val_RMSE: 38.7125 - val_loss: 1498.6609 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6590 - loss: 1494.5162 - val_RMSE: 38.7127 - val_loss: 1498.6738 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6581 - loss: 1494.4482 - val_RMSE: 38.7129 - val_loss: 1498.6860 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6573 - loss: 1494.3903 - val_RMSE: 38.7131 - val_loss: 1498.7030 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6567 - loss: 1494.3374 - val_RMSE: 38.7133 - val_loss: 1498.7192 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6559 - loss: 1494.2758 - val_RMSE: 38.7117 - val_loss: 1498.5962 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6552 - loss: 1494.2219 - val_RMSE: 38.7117 - val_loss: 1498.5952 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6550 - loss: 1494.2095 - val_RMSE: 38.7118 - val_loss: 1498.5997 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6549 - loss: 1494.1990 - val_RMSE: 38.7118 - val_loss: 1498.6012 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 27s 8ms/step - RMSE: 56.2866 - loss: 3330.4028 - val_RMSE: 38.7194 - val_loss: 1499.1917 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7191 - loss: 1499.1687 - val_RMSE: 38.7195 - val_loss: 1499.1991 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7142 - loss: 1498.7919 - val_RMSE: 38.7139 - val_loss: 1498.7687 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7118 - loss: 1498.6016 - val_RMSE: 38.7120 - val_loss: 1498.6177 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7100 - loss: 1498.4615 - val_RMSE: 38.7142 - val_loss: 1498.7891 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7081 - loss: 1498.3149 - val_RMSE: 38.7087 - val_loss: 1498.3608 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7064 - loss: 1498.1826 - val_RMSE: 38.7050 - val_loss: 1498.0765 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7045 - loss: 1498.0358 - val_RMSE: 38.7075 - val_loss: 1498.2732 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7037 - loss: 1497.9795 - val_RMSE: 38.7062 - val_loss: 1498.1704 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7025 - loss: 1497.8868 - val_RMSE: 38.7065 - val_loss: 1498.1903 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7013 - loss: 1497.7908 - val_RMSE: 38.7056 - val_loss: 1498.1235 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7001 - loss: 1497.6946 - val_RMSE: 38.7032 - val_loss: 1497.9351 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6985 - loss: 1497.5751 - val_RMSE: 38.7034 - val_loss: 1497.9548 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6969 - loss: 1497.4501 - val_RMSE: 38.7031 - val_loss: 1497.9271 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6953 - loss: 1497.3273 - val_RMSE: 38.7024 - val_loss: 1497.8740 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6939 - loss: 1497.2183 - val_RMSE: 38.7042 - val_loss: 1498.0181 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6920 - loss: 1497.0734 - val_RMSE: 38.7024 - val_loss: 1497.8735 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6904 - loss: 1496.9506 - val_RMSE: 38.7022 - val_loss: 1497.8588 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6875 - loss: 1496.7223 - val_RMSE: 38.7029 - val_loss: 1497.9164 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6845 - loss: 1496.4904 - val_RMSE: 38.7040 - val_loss: 1498.0021 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6819 - loss: 1496.2886 - val_RMSE: 38.7054 - val_loss: 1498.1082 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6784 - loss: 1496.0183 - val_RMSE: 38.7099 - val_loss: 1498.4568 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6741 - loss: 1495.6871 - val_RMSE: 38.7125 - val_loss: 1498.6545 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6670 - loss: 1495.1350 - val_RMSE: 38.7136 - val_loss: 1498.7467 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6602 - loss: 1494.6125 - val_RMSE: 38.7163 - val_loss: 1498.9550 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 11:11:52,706] Trial 1 finished with value: 38.702624003092446 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu'}. Best is trial 0 with value: 38.701437632242836.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 6ms/step - RMSE: 64.5121 - loss: 4325.4487 - val_RMSE: 38.7013 - val_loss: 1497.7886 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7409 - loss: 1500.8591 - val_RMSE: 38.6934 - val_loss: 1497.1771 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7337 - loss: 1500.2982 - val_RMSE: 38.6902 - val_loss: 1496.9346 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7293 - loss: 1499.9631 - val_RMSE: 38.6879 - val_loss: 1496.7524 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7264 - loss: 1499.7333 - val_RMSE: 38.6879 - val_loss: 1496.7570 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7241 - loss: 1499.5571 - val_RMSE: 38.6863 - val_loss: 1496.6329 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7214 - loss: 1499.3507 - val_RMSE: 38.6874 - val_loss: 1496.7142 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7193 - loss: 1499.1849 - val_RMSE: 38.6880 - val_loss: 1496.7595 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7171 - loss: 1499.0181 - val_RMSE: 38.6892 - val_loss: 1496.8555 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7148 - loss: 1498.8333 - val_RMSE: 38.6907 - val_loss: 1496.9700 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7123 - loss: 1498.6447 - val_RMSE: 38.6905 - val_loss: 1496.9526 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7073 - loss: 1498.2531 - val_RMSE: 38.6841 - val_loss: 1496.4625 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7019 - loss: 1497.8368 - val_RMSE: 38.6845 - val_loss: 1496.4872 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6997 - loss: 1497.6676 - val_RMSE: 38.6850 - val_loss: 1496.5302 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6981 - loss: 1497.5465 - val_RMSE: 38.6855 - val_loss: 1496.5707 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6968 - loss: 1497.4463 - val_RMSE: 38.6861 - val_loss: 1496.6144 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6957 - loss: 1497.3588 - val_RMSE: 38.6866 - val_loss: 1496.6509 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6945 - loss: 1497.2655 - val_RMSE: 38.6865 - val_loss: 1496.6454 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6939 - loss: 1497.2188 - val_RMSE: 38.6865 - val_loss: 1496.6481 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6935 - loss: 1497.1914 - val_RMSE: 38.6866 - val_loss: 1496.6519 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6933 - loss: 1497.1696 - val_RMSE: 38.6866 - val_loss: 1496.6562 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6930 - loss: 1497.1525 - val_RMSE: 38.6867 - val_loss: 1496.6594 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6928 - loss: 1497.1340 - val_RMSE: 38.6867 - val_loss: 1496.6600 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6928 - loss: 1497.1316 - val_RMSE: 38.6867 - val_loss: 1496.6613 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6927 - loss: 1497.1293 - val_RMSE: 38.6867 - val_loss: 1496.6617 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 6ms/step - RMSE: 64.5150 - loss: 4325.3882 - val_RMSE: 38.7267 - val_loss: 1499.7600 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6865 - loss: 1496.6490 - val_RMSE: 38.7232 - val_loss: 1499.4896 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6808 - loss: 1496.2069 - val_RMSE: 38.7217 - val_loss: 1499.3718 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6774 - loss: 1495.9437 - val_RMSE: 38.7216 - val_loss: 1499.3618 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6746 - loss: 1495.7242 - val_RMSE: 38.7198 - val_loss: 1499.2205 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6720 - loss: 1495.5223 - val_RMSE: 38.7208 - val_loss: 1499.3035 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6700 - loss: 1495.3719 - val_RMSE: 38.7202 - val_loss: 1499.2549 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6680 - loss: 1495.2141 - val_RMSE: 38.7199 - val_loss: 1499.2338 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6658 - loss: 1495.0443 - val_RMSE: 38.7208 - val_loss: 1499.2971 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6637 - loss: 1494.8813 - val_RMSE: 38.7213 - val_loss: 1499.3357 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6584 - loss: 1494.4728 - val_RMSE: 38.7114 - val_loss: 1498.5724 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6540 - loss: 1494.1296 - val_RMSE: 38.7118 - val_loss: 1498.6000 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6522 - loss: 1493.9968 - val_RMSE: 38.7122 - val_loss: 1498.6334 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6509 - loss: 1493.8917 - val_RMSE: 38.7127 - val_loss: 1498.6693 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6497 - loss: 1493.8013 - val_RMSE: 38.7131 - val_loss: 1498.7021 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6486 - loss: 1493.7137 - val_RMSE: 38.7135 - val_loss: 1498.7325 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6474 - loss: 1493.6248 - val_RMSE: 38.7130 - val_loss: 1498.6976 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6469 - loss: 1493.5829 - val_RMSE: 38.7129 - val_loss: 1498.6885 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6465 - loss: 1493.5563 - val_RMSE: 38.7129 - val_loss: 1498.6866 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6463 - loss: 1493.5371 - val_RMSE: 38.7129 - val_loss: 1498.6886 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6461 - loss: 1493.5212 - val_RMSE: 38.7129 - val_loss: 1498.6891 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6459 - loss: 1493.5033 - val_RMSE: 38.7129 - val_loss: 1498.6904 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6458 - loss: 1493.5012 - val_RMSE: 38.7129 - val_loss: 1498.6912 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6458 - loss: 1493.4995 - val_RMSE: 38.7129 - val_loss: 1498.6917 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6458 - loss: 1493.4985 - val_RMSE: 38.7129 - val_loss: 1498.6917 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 6ms/step - RMSE: 64.4789 - loss: 4320.5557 - val_RMSE: 38.7139 - val_loss: 1498.7650 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7158 - loss: 1498.9166 - val_RMSE: 38.7123 - val_loss: 1498.6431 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7107 - loss: 1498.5210 - val_RMSE: 38.7098 - val_loss: 1498.4509 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7072 - loss: 1498.2494 - val_RMSE: 38.7093 - val_loss: 1498.4098 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7047 - loss: 1498.0529 - val_RMSE: 38.7125 - val_loss: 1498.6559 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7015 - loss: 1497.8041 - val_RMSE: 38.7116 - val_loss: 1498.5917 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6990 - loss: 1497.6101 - val_RMSE: 38.7113 - val_loss: 1498.5660 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6965 - loss: 1497.4224 - val_RMSE: 38.7091 - val_loss: 1498.3955 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6942 - loss: 1497.2443 - val_RMSE: 38.7086 - val_loss: 1498.3523 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6920 - loss: 1497.0718 - val_RMSE: 38.7089 - val_loss: 1498.3821 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6902 - loss: 1496.9303 - val_RMSE: 38.7095 - val_loss: 1498.4274 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6880 - loss: 1496.7579 - val_RMSE: 38.7107 - val_loss: 1498.5161 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6860 - loss: 1496.6072 - val_RMSE: 38.7122 - val_loss: 1498.6366 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6835 - loss: 1496.4124 - val_RMSE: 38.7126 - val_loss: 1498.6691 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6770 - loss: 1495.9087 - val_RMSE: 38.7081 - val_loss: 1498.3142 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6727 - loss: 1495.5787 - val_RMSE: 38.7089 - val_loss: 1498.3782 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6707 - loss: 1495.4200 - val_RMSE: 38.7097 - val_loss: 1498.4431 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6690 - loss: 1495.2957 - val_RMSE: 38.7105 - val_loss: 1498.5027 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6676 - loss: 1495.1859 - val_RMSE: 38.7113 - val_loss: 1498.5620 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6664 - loss: 1495.0883 - val_RMSE: 38.7120 - val_loss: 1498.6224 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6651 - loss: 1494.9919 - val_RMSE: 38.7119 - val_loss: 1498.6079 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6644 - loss: 1494.9369 - val_RMSE: 38.7119 - val_loss: 1498.6124 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6640 - loss: 1494.9049 - val_RMSE: 38.7120 - val_loss: 1498.6202 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6637 - loss: 1494.8798 - val_RMSE: 38.7121 - val_loss: 1498.6256 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6634 - loss: 1494.8580 - val_RMSE: 38.7122 - val_loss: 1498.6338 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 11:34:24,660] Trial 2 finished with value: 38.703948974609375 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'relu'}. Best is trial 0 with value: 38.701437632242836.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 31s 9ms/step - RMSE: 53.3184 - loss: 2993.5410 - val_RMSE: 38.7082 - val_loss: 1498.3252 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7452 - loss: 1501.1896 - val_RMSE: 38.6995 - val_loss: 1497.6506 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7410 - loss: 1500.8696 - val_RMSE: 38.6889 - val_loss: 1496.8324 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7403 - loss: 1500.8141 - val_RMSE: 38.6914 - val_loss: 1497.0251 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7378 - loss: 1500.6171 - val_RMSE: 38.6944 - val_loss: 1497.2560 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7363 - loss: 1500.4993 - val_RMSE: 38.6877 - val_loss: 1496.7389 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7341 - loss: 1500.3284 - val_RMSE: 38.6887 - val_loss: 1496.8169 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7340 - loss: 1500.3260 - val_RMSE: 38.6899 - val_loss: 1496.9095 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7320 - loss: 1500.1686 - val_RMSE: 38.6838 - val_loss: 1496.4397 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7308 - loss: 1500.0759 - val_RMSE: 38.6849 - val_loss: 1496.5253 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7304 - loss: 1500.0464 - val_RMSE: 38.6823 - val_loss: 1496.3225 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7293 - loss: 1499.9624 - val_RMSE: 38.6849 - val_loss: 1496.5219 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7282 - loss: 1499.8768 - val_RMSE: 38.6849 - val_loss: 1496.5212 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7274 - loss: 1499.8086 - val_RMSE: 38.6847 - val_loss: 1496.5034 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7269 - loss: 1499.7722 - val_RMSE: 38.6852 - val_loss: 1496.5410 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7257 - loss: 1499.6833 - val_RMSE: 38.6826 - val_loss: 1496.3438 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7170 - loss: 1499.0070 - val_RMSE: 38.6794 - val_loss: 1496.0990 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7150 - loss: 1498.8549 - val_RMSE: 38.6794 - val_loss: 1496.0988 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7143 - loss: 1498.7965 - val_RMSE: 38.6794 - val_loss: 1496.0945 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7138 - loss: 1498.7576 - val_RMSE: 38.6794 - val_loss: 1496.0989 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7134 - loss: 1498.7267 - val_RMSE: 38.6794 - val_loss: 1496.0924 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7130 - loss: 1498.7004 - val_RMSE: 38.6794 - val_loss: 1496.0938 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7127 - loss: 1498.6779 - val_RMSE: 38.6795 - val_loss: 1496.1000 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7124 - loss: 1498.6545 - val_RMSE: 38.6795 - val_loss: 1496.1007 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7122 - loss: 1498.6324 - val_RMSE: 38.6796 - val_loss: 1496.1083 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 31s 9ms/step - RMSE: 53.2740 - loss: 2987.8987 - val_RMSE: 38.7414 - val_loss: 1500.8953 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6953 - loss: 1497.3280 - val_RMSE: 38.7230 - val_loss: 1499.4685 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6902 - loss: 1496.9292 - val_RMSE: 38.7269 - val_loss: 1499.7762 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6885 - loss: 1496.8029 - val_RMSE: 38.7665 - val_loss: 1502.8441 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6865 - loss: 1496.6486 - val_RMSE: 38.7212 - val_loss: 1499.3335 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6857 - loss: 1496.5853 - val_RMSE: 38.7222 - val_loss: 1499.4106 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6834 - loss: 1496.4034 - val_RMSE: 38.7256 - val_loss: 1499.6753 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6819 - loss: 1496.2924 - val_RMSE: 38.7209 - val_loss: 1499.3096 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6811 - loss: 1496.2260 - val_RMSE: 38.7284 - val_loss: 1499.8920 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6798 - loss: 1496.1282 - val_RMSE: 38.7241 - val_loss: 1499.5536 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6792 - loss: 1496.0835 - val_RMSE: 38.7164 - val_loss: 1498.9600 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6782 - loss: 1496.0002 - val_RMSE: 38.7220 - val_loss: 1499.3961 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6770 - loss: 1495.9143 - val_RMSE: 38.7200 - val_loss: 1499.2391 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6761 - loss: 1495.8381 - val_RMSE: 38.7197 - val_loss: 1499.2126 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6757 - loss: 1495.8107 - val_RMSE: 38.7148 - val_loss: 1498.8330 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6751 - loss: 1495.7626 - val_RMSE: 38.7225 - val_loss: 1499.4319 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6732 - loss: 1495.6138 - val_RMSE: 38.7425 - val_loss: 1500.9796 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6719 - loss: 1495.5156 - val_RMSE: 38.7356 - val_loss: 1500.4487 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6707 - loss: 1495.4242 - val_RMSE: 38.7387 - val_loss: 1500.6858 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6699 - loss: 1495.3605 - val_RMSE: 38.7460 - val_loss: 1501.2520 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6613 - loss: 1494.6968 - val_RMSE: 38.7109 - val_loss: 1498.5367 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6599 - loss: 1494.5867 - val_RMSE: 38.7112 - val_loss: 1498.5591 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6593 - loss: 1494.5402 - val_RMSE: 38.7115 - val_loss: 1498.5774 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6588 - loss: 1494.5007 - val_RMSE: 38.7117 - val_loss: 1498.5944 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6583 - loss: 1494.4667 - val_RMSE: 38.7119 - val_loss: 1498.6124 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 30s 9ms/step - RMSE: 53.2408 - loss: 2983.8542 - val_RMSE: 38.7222 - val_loss: 1499.4124 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7228 - loss: 1499.4531 - val_RMSE: 38.7305 - val_loss: 1500.0483 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7180 - loss: 1499.0820 - val_RMSE: 38.7285 - val_loss: 1499.8940 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7149 - loss: 1498.8420 - val_RMSE: 38.7249 - val_loss: 1499.6216 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7134 - loss: 1498.7308 - val_RMSE: 38.7163 - val_loss: 1498.9521 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7119 - loss: 1498.6118 - val_RMSE: 38.7098 - val_loss: 1498.4474 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7096 - loss: 1498.4325 - val_RMSE: 38.7109 - val_loss: 1498.5358 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7086 - loss: 1498.3595 - val_RMSE: 38.7109 - val_loss: 1498.5375 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7076 - loss: 1498.2781 - val_RMSE: 38.7153 - val_loss: 1498.8751 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7062 - loss: 1498.1741 - val_RMSE: 38.7062 - val_loss: 1498.1671 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7052 - loss: 1498.0902 - val_RMSE: 38.7064 - val_loss: 1498.1857 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7032 - loss: 1497.9396 - val_RMSE: 38.7063 - val_loss: 1498.1785 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7024 - loss: 1497.8737 - val_RMSE: 38.7093 - val_loss: 1498.4108 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7007 - loss: 1497.7412 - val_RMSE: 38.7069 - val_loss: 1498.2214 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6993 - loss: 1497.6368 - val_RMSE: 38.7066 - val_loss: 1498.1970 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6902 - loss: 1496.9292 - val_RMSE: 38.7027 - val_loss: 1497.9022 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6878 - loss: 1496.7491 - val_RMSE: 38.7028 - val_loss: 1497.9097 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6868 - loss: 1496.6720 - val_RMSE: 38.7030 - val_loss: 1497.9215 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6860 - loss: 1496.6078 - val_RMSE: 38.7031 - val_loss: 1497.9320 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6852 - loss: 1496.5492 - val_RMSE: 38.7033 - val_loss: 1497.9432 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6845 - loss: 1496.4929 - val_RMSE: 38.7034 - val_loss: 1497.9561 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6833 - loss: 1496.4015 - val_RMSE: 38.7025 - val_loss: 1497.8856 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6827 - loss: 1496.3500 - val_RMSE: 38.7026 - val_loss: 1497.8876 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6825 - loss: 1496.3358 - val_RMSE: 38.7026 - val_loss: 1497.8879 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6823 - loss: 1496.3239 - val_RMSE: 38.7026 - val_loss: 1497.8889 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 12:05:41,185] Trial 3 finished with value: 38.69801712036133 and parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 26s 8ms/step - RMSE: 56.4368 - loss: 3348.9377 - val_RMSE: 38.7139 - val_loss: 1498.7676 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7428 - loss: 1501.0071 - val_RMSE: 38.6941 - val_loss: 1497.2349 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7378 - loss: 1500.6176 - val_RMSE: 38.7297 - val_loss: 1499.9932 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7353 - loss: 1500.4229 - val_RMSE: 38.6935 - val_loss: 1497.1893 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7341 - loss: 1500.3314 - val_RMSE: 38.6938 - val_loss: 1497.2128 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7333 - loss: 1500.2690 - val_RMSE: 38.6879 - val_loss: 1496.7505 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7324 - loss: 1500.2023 - val_RMSE: 38.6880 - val_loss: 1496.7637 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7314 - loss: 1500.1254 - val_RMSE: 38.6899 - val_loss: 1496.9089 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7307 - loss: 1500.0691 - val_RMSE: 38.6873 - val_loss: 1496.7039 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7300 - loss: 1500.0103 - val_RMSE: 38.6891 - val_loss: 1496.8473 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7295 - loss: 1499.9738 - val_RMSE: 38.6850 - val_loss: 1496.5292 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7290 - loss: 1499.9391 - val_RMSE: 38.6874 - val_loss: 1496.7168 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7285 - loss: 1499.8978 - val_RMSE: 38.6858 - val_loss: 1496.5950 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7275 - loss: 1499.8175 - val_RMSE: 38.6856 - val_loss: 1496.5793 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7273 - loss: 1499.8031 - val_RMSE: 38.6863 - val_loss: 1496.6307 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7265 - loss: 1499.7390 - val_RMSE: 38.6850 - val_loss: 1496.5288 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7258 - loss: 1499.6855 - val_RMSE: 38.6868 - val_loss: 1496.6686 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7248 - loss: 1499.6100 - val_RMSE: 38.6854 - val_loss: 1496.5591 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7249 - loss: 1499.6204 - val_RMSE: 38.6858 - val_loss: 1496.5930 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7244 - loss: 1499.5779 - val_RMSE: 38.6863 - val_loss: 1496.6265 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7240 - loss: 1499.5492 - val_RMSE: 38.6850 - val_loss: 1496.5275 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7234 - loss: 1499.5000 - val_RMSE: 38.6847 - val_loss: 1496.5068 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7231 - loss: 1499.4777 - val_RMSE: 38.6844 - val_loss: 1496.4818 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7227 - loss: 1499.4506 - val_RMSE: 38.6840 - val_loss: 1496.4491 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7223 - loss: 1499.4203 - val_RMSE: 38.6830 - val_loss: 1496.3767 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 8ms/step - RMSE: 56.3855 - loss: 3342.3340 - val_RMSE: 38.7578 - val_loss: 1502.1672 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6942 - loss: 1497.2444 - val_RMSE: 38.7284 - val_loss: 1499.8899 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6904 - loss: 1496.9506 - val_RMSE: 38.7503 - val_loss: 1501.5886 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6872 - loss: 1496.7040 - val_RMSE: 38.7347 - val_loss: 1500.3752 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6850 - loss: 1496.5262 - val_RMSE: 38.7437 - val_loss: 1501.0710 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6837 - loss: 1496.4263 - val_RMSE: 38.7497 - val_loss: 1501.5380 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6817 - loss: 1496.2728 - val_RMSE: 38.7430 - val_loss: 1501.0221 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6755 - loss: 1495.7985 - val_RMSE: 38.7141 - val_loss: 1498.7809 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6720 - loss: 1495.5222 - val_RMSE: 38.7138 - val_loss: 1498.7609 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6714 - loss: 1495.4755 - val_RMSE: 38.7137 - val_loss: 1498.7501 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6710 - loss: 1495.4434 - val_RMSE: 38.7135 - val_loss: 1498.7352 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6707 - loss: 1495.4203 - val_RMSE: 38.7134 - val_loss: 1498.7253 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6704 - loss: 1495.4017 - val_RMSE: 38.7133 - val_loss: 1498.7189 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6702 - loss: 1495.3862 - val_RMSE: 38.7133 - val_loss: 1498.7178 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6700 - loss: 1495.3719 - val_RMSE: 38.7132 - val_loss: 1498.7155 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6699 - loss: 1495.3593 - val_RMSE: 38.7132 - val_loss: 1498.7114 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6697 - loss: 1495.3469 - val_RMSE: 38.7132 - val_loss: 1498.7098 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6696 - loss: 1495.3354 - val_RMSE: 38.7131 - val_loss: 1498.7039 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6695 - loss: 1495.3293 - val_RMSE: 38.7131 - val_loss: 1498.7006 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6693 - loss: 1495.3195 - val_RMSE: 38.7130 - val_loss: 1498.6985 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6693 - loss: 1495.3119 - val_RMSE: 38.7130 - val_loss: 1498.6959 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6691 - loss: 1495.3025 - val_RMSE: 38.7129 - val_loss: 1498.6918 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6690 - loss: 1495.2944 - val_RMSE: 38.7129 - val_loss: 1498.6870 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.6689 - loss: 1495.2872 - val_RMSE: 38.7128 - val_loss: 1498.6826 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6688 - loss: 1495.2778 - val_RMSE: 38.7128 - val_loss: 1498.6803 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 26s 8ms/step - RMSE: 56.3180 - loss: 3334.0947 - val_RMSE: 38.7402 - val_loss: 1500.8021 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7196 - loss: 1499.2118 - val_RMSE: 38.7488 - val_loss: 1501.4677 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7151 - loss: 1498.8605 - val_RMSE: 38.7237 - val_loss: 1499.5269 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7135 - loss: 1498.7323 - val_RMSE: 38.7372 - val_loss: 1500.5731 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7115 - loss: 1498.5814 - val_RMSE: 38.7149 - val_loss: 1498.8451 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.7101 - loss: 1498.4702 - val_RMSE: 38.7406 - val_loss: 1500.8348 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7093 - loss: 1498.4080 - val_RMSE: 38.7288 - val_loss: 1499.9174 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7076 - loss: 1498.2799 - val_RMSE: 38.7546 - val_loss: 1501.9227 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7064 - loss: 1498.1854 - val_RMSE: 38.7389 - val_loss: 1500.7008 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7062 - loss: 1498.1708 - val_RMSE: 38.7295 - val_loss: 1499.9778 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.7002 - loss: 1497.7090 - val_RMSE: 38.7053 - val_loss: 1498.1018 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6986 - loss: 1497.5848 - val_RMSE: 38.7051 - val_loss: 1498.0845 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6981 - loss: 1497.5450 - val_RMSE: 38.7051 - val_loss: 1498.0831 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6978 - loss: 1497.5199 - val_RMSE: 38.7051 - val_loss: 1498.0813 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6976 - loss: 1497.5016 - val_RMSE: 38.7051 - val_loss: 1498.0815 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6973 - loss: 1497.4838 - val_RMSE: 38.7050 - val_loss: 1498.0758 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6971 - loss: 1497.4681 - val_RMSE: 38.7050 - val_loss: 1498.0740 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6969 - loss: 1497.4546 - val_RMSE: 38.7050 - val_loss: 1498.0737 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6968 - loss: 1497.4404 - val_RMSE: 38.7050 - val_loss: 1498.0739 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 6ms/step - RMSE: 38.6966 - loss: 1497.4277 - val_RMSE: 38.7049 - val_loss: 1498.0731 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6964 - loss: 1497.4154 - val_RMSE: 38.7049 - val_loss: 1498.0717 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6962 - loss: 1497.3998 - val_RMSE: 38.7048 - val_loss: 1498.0649 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6961 - loss: 1497.3860 - val_RMSE: 38.7048 - val_loss: 1498.0631 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6959 - loss: 1497.3723 - val_RMSE: 38.7048 - val_loss: 1498.0631 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 17s 7ms/step - RMSE: 38.6958 - loss: 1497.3623 - val_RMSE: 38.7048 - val_loss: 1498.0623 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 12:33:12,824] Trial 4 finished with value: 38.700225830078125 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'selu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 50.8331 - loss: 2718.5952 - val_RMSE: 38.7244 - val_loss: 1499.5774 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7516 - loss: 1501.6853 - val_RMSE: 38.6992 - val_loss: 1497.6318 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7468 - loss: 1501.3146 - val_RMSE: 38.7076 - val_loss: 1498.2753 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7420 - loss: 1500.9408 - val_RMSE: 38.6994 - val_loss: 1497.6414 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7401 - loss: 1500.7953 - val_RMSE: 38.6930 - val_loss: 1497.1493 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7373 - loss: 1500.5830 - val_RMSE: 38.6882 - val_loss: 1496.7804 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 9ms/step - RMSE: 38.7343 - loss: 1500.3457 - val_RMSE: 38.6894 - val_loss: 1496.8679 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7310 - loss: 1500.0947 - val_RMSE: 38.6885 - val_loss: 1496.8007 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7280 - loss: 1499.8595 - val_RMSE: 38.6958 - val_loss: 1497.3621 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7244 - loss: 1499.5829 - val_RMSE: 38.6919 - val_loss: 1497.0640 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7213 - loss: 1499.3397 - val_RMSE: 38.6948 - val_loss: 1497.2843 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7075 - loss: 1498.2739 - val_RMSE: 38.6879 - val_loss: 1496.7563 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7005 - loss: 1497.7330 - val_RMSE: 38.6898 - val_loss: 1496.8967 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6970 - loss: 1497.4572 - val_RMSE: 38.6916 - val_loss: 1497.0433 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6940 - loss: 1497.2266 - val_RMSE: 38.6935 - val_loss: 1497.1903 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6912 - loss: 1497.0066 - val_RMSE: 38.6961 - val_loss: 1497.3857 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6884 - loss: 1496.7943 - val_RMSE: 38.6981 - val_loss: 1497.5446 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6861 - loss: 1496.6121 - val_RMSE: 38.6980 - val_loss: 1497.5358 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6845 - loss: 1496.4889 - val_RMSE: 38.6981 - val_loss: 1497.5413 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6836 - loss: 1496.4188 - val_RMSE: 38.6982 - val_loss: 1497.5543 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6829 - loss: 1496.3660 - val_RMSE: 38.6985 - val_loss: 1497.5715 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6823 - loss: 1496.3213 - val_RMSE: 38.6987 - val_loss: 1497.5894 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6820 - loss: 1496.2947 - val_RMSE: 38.6987 - val_loss: 1497.5884 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6817 - loss: 1496.2764 - val_RMSE: 38.6987 - val_loss: 1497.5885 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6816 - loss: 1496.2676 - val_RMSE: 38.6987 - val_loss: 1497.5900 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 10ms/step - RMSE: 50.7762 - loss: 2711.9099 - val_RMSE: 38.7446 - val_loss: 1501.1411 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6979 - loss: 1497.5304 - val_RMSE: 38.7504 - val_loss: 1501.5945 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6948 - loss: 1497.2847 - val_RMSE: 38.7520 - val_loss: 1501.7173 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6897 - loss: 1496.8970 - val_RMSE: 38.7326 - val_loss: 1500.2173 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6884 - loss: 1496.7952 - val_RMSE: 38.7685 - val_loss: 1503.0004 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6847 - loss: 1496.5074 - val_RMSE: 38.7424 - val_loss: 1500.9719 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6817 - loss: 1496.2762 - val_RMSE: 38.7456 - val_loss: 1501.2242 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6795 - loss: 1496.1074 - val_RMSE: 38.7454 - val_loss: 1501.2048 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6761 - loss: 1495.8431 - val_RMSE: 38.7467 - val_loss: 1501.3079 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6614 - loss: 1494.7084 - val_RMSE: 38.7118 - val_loss: 1498.6062 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6555 - loss: 1494.2458 - val_RMSE: 38.7124 - val_loss: 1498.6514 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6527 - loss: 1494.0336 - val_RMSE: 38.7133 - val_loss: 1498.7213 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6504 - loss: 1493.8557 - val_RMSE: 38.7144 - val_loss: 1498.8011 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 9ms/step - RMSE: 38.6483 - loss: 1493.6926 - val_RMSE: 38.7154 - val_loss: 1498.8795 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6461 - loss: 1493.5245 - val_RMSE: 38.7164 - val_loss: 1498.9612 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 9ms/step - RMSE: 38.6436 - loss: 1493.3247 - val_RMSE: 38.7140 - val_loss: 1498.7756 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 9ms/step - RMSE: 38.6420 - loss: 1493.2084 - val_RMSE: 38.7140 - val_loss: 1498.7704 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6414 - loss: 1493.1559 - val_RMSE: 38.7140 - val_loss: 1498.7742 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6408 - loss: 1493.1136 - val_RMSE: 38.7141 - val_loss: 1498.7838 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6404 - loss: 1493.0782 - val_RMSE: 38.7143 - val_loss: 1498.7943 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6401 - loss: 1493.0575 - val_RMSE: 38.7136 - val_loss: 1498.7463 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6398 - loss: 1493.0321 - val_RMSE: 38.7136 - val_loss: 1498.7415 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6397 - loss: 1493.0244 - val_RMSE: 38.7136 - val_loss: 1498.7402 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6396 - loss: 1493.0199 - val_RMSE: 38.7136 - val_loss: 1498.7410 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6396 - loss: 1493.0160 - val_RMSE: 38.7136 - val_loss: 1498.7418 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 10ms/step - RMSE: 50.7367 - loss: 2707.1123 - val_RMSE: 38.7286 - val_loss: 1499.9026 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7279 - loss: 1499.8497 - val_RMSE: 38.7156 - val_loss: 1498.8960 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7221 - loss: 1499.4038 - val_RMSE: 38.7098 - val_loss: 1498.4447 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7184 - loss: 1499.1116 - val_RMSE: 38.7103 - val_loss: 1498.4866 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7155 - loss: 1498.8915 - val_RMSE: 38.7079 - val_loss: 1498.2988 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7129 - loss: 1498.6921 - val_RMSE: 38.7208 - val_loss: 1499.3019 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7105 - loss: 1498.5066 - val_RMSE: 38.7156 - val_loss: 1498.9008 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7070 - loss: 1498.2313 - val_RMSE: 38.7098 - val_loss: 1498.4500 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7037 - loss: 1497.9785 - val_RMSE: 38.7078 - val_loss: 1498.2975 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7002 - loss: 1497.7042 - val_RMSE: 38.7165 - val_loss: 1498.9672 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6964 - loss: 1497.4080 - val_RMSE: 38.7104 - val_loss: 1498.4926 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 9ms/step - RMSE: 38.6910 - loss: 1496.9972 - val_RMSE: 38.7123 - val_loss: 1498.6415 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6833 - loss: 1496.3998 - val_RMSE: 38.7199 - val_loss: 1499.2311 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6749 - loss: 1495.7496 - val_RMSE: 38.7286 - val_loss: 1499.9014 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6637 - loss: 1494.8832 - val_RMSE: 38.7278 - val_loss: 1499.8394 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 9ms/step - RMSE: 38.6510 - loss: 1493.9038 - val_RMSE: 38.7368 - val_loss: 1500.5371 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6431 - loss: 1493.2928 - val_RMSE: 38.7439 - val_loss: 1501.0931 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6362 - loss: 1492.7578 - val_RMSE: 38.7501 - val_loss: 1501.5710 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6298 - loss: 1492.2589 - val_RMSE: 38.7569 - val_loss: 1502.1005 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6286 - loss: 1492.1696 - val_RMSE: 38.7519 - val_loss: 1501.7096 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6252 - loss: 1491.9100 - val_RMSE: 38.7521 - val_loss: 1501.7274 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6234 - loss: 1491.7700 - val_RMSE: 38.7526 - val_loss: 1501.7649 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6220 - loss: 1491.6581 - val_RMSE: 38.7532 - val_loss: 1501.8105 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6207 - loss: 1491.5568 - val_RMSE: 38.7539 - val_loss: 1501.8651 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6199 - loss: 1491.5012 - val_RMSE: 38.7536 - val_loss: 1501.8447 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 13:07:52,360] Trial 5 finished with value: 38.72197469075521 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'relu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 50.8548 - loss: 2721.0947 - val_RMSE: 38.7073 - val_loss: 1498.2577 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7491 - loss: 1501.4927 - val_RMSE: 38.7116 - val_loss: 1498.5873 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7482 - loss: 1501.4208 - val_RMSE: 38.6942 - val_loss: 1497.2389 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7431 - loss: 1501.0256 - val_RMSE: 38.7144 - val_loss: 1498.8073 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7419 - loss: 1500.9354 - val_RMSE: 38.7164 - val_loss: 1498.9589 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7403 - loss: 1500.8125 - val_RMSE: 38.7189 - val_loss: 1499.1520 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7393 - loss: 1500.7322 - val_RMSE: 38.6936 - val_loss: 1497.1985 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7375 - loss: 1500.5920 - val_RMSE: 38.7034 - val_loss: 1497.9497 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7368 - loss: 1500.5392 - val_RMSE: 38.6955 - val_loss: 1497.3387 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7355 - loss: 1500.4360 - val_RMSE: 38.6957 - val_loss: 1497.3535 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7337 - loss: 1500.2979 - val_RMSE: 38.6943 - val_loss: 1497.2463 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7329 - loss: 1500.2395 - val_RMSE: 38.6977 - val_loss: 1497.5128 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7223 - loss: 1499.4171 - val_RMSE: 38.6816 - val_loss: 1496.2692 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7195 - loss: 1499.2035 - val_RMSE: 38.6817 - val_loss: 1496.2723 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7187 - loss: 1499.1382 - val_RMSE: 38.6818 - val_loss: 1496.2803 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7182 - loss: 1499.0978 - val_RMSE: 38.6818 - val_loss: 1496.2789 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7177 - loss: 1499.0638 - val_RMSE: 38.6817 - val_loss: 1496.2726 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7173 - loss: 1499.0295 - val_RMSE: 38.6817 - val_loss: 1496.2776 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7153 - loss: 1498.8789 - val_RMSE: 38.6813 - val_loss: 1496.2446 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7151 - loss: 1498.8602 - val_RMSE: 38.6813 - val_loss: 1496.2444 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7150 - loss: 1498.8485 - val_RMSE: 38.6813 - val_loss: 1496.2460 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7148 - loss: 1498.8392 - val_RMSE: 38.6813 - val_loss: 1496.2461 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7147 - loss: 1498.8323 - val_RMSE: 38.6813 - val_loss: 1496.2466 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7146 - loss: 1498.8247 - val_RMSE: 38.6814 - val_loss: 1496.2476 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7146 - loss: 1498.8186 - val_RMSE: 38.6814 - val_loss: 1496.2485 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 50.8159 - loss: 2716.2915 - val_RMSE: 38.7432 - val_loss: 1501.0382 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6999 - loss: 1497.6820 - val_RMSE: 38.7426 - val_loss: 1500.9867 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6957 - loss: 1497.3590 - val_RMSE: 38.7340 - val_loss: 1500.3257 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6916 - loss: 1497.0389 - val_RMSE: 38.7224 - val_loss: 1499.4238 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6886 - loss: 1496.8103 - val_RMSE: 38.7329 - val_loss: 1500.2352 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6869 - loss: 1496.6776 - val_RMSE: 38.7299 - val_loss: 1500.0061 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6856 - loss: 1496.5781 - val_RMSE: 38.7501 - val_loss: 1501.5703 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6842 - loss: 1496.4658 - val_RMSE: 38.7208 - val_loss: 1499.3007 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6819 - loss: 1496.2933 - val_RMSE: 38.7616 - val_loss: 1502.4611 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6806 - loss: 1496.1869 - val_RMSE: 38.7299 - val_loss: 1500.0055 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6796 - loss: 1496.1089 - val_RMSE: 38.7310 - val_loss: 1500.0929 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6782 - loss: 1496.0045 - val_RMSE: 38.7345 - val_loss: 1500.3624 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6759 - loss: 1495.8285 - val_RMSE: 38.7335 - val_loss: 1500.2847 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6639 - loss: 1494.8961 - val_RMSE: 38.7117 - val_loss: 1498.5925 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6612 - loss: 1494.6926 - val_RMSE: 38.7118 - val_loss: 1498.6062 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6601 - loss: 1494.6040 - val_RMSE: 38.7121 - val_loss: 1498.6250 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6592 - loss: 1494.5315 - val_RMSE: 38.7123 - val_loss: 1498.6400 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6583 - loss: 1494.4672 - val_RMSE: 38.7125 - val_loss: 1498.6592 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6576 - loss: 1494.4102 - val_RMSE: 38.7128 - val_loss: 1498.6819 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6561 - loss: 1494.2955 - val_RMSE: 38.7105 - val_loss: 1498.5006 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6554 - loss: 1494.2385 - val_RMSE: 38.7105 - val_loss: 1498.5020 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6551 - loss: 1494.2205 - val_RMSE: 38.7105 - val_loss: 1498.5059 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6549 - loss: 1494.2047 - val_RMSE: 38.7106 - val_loss: 1498.5110 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.6548 - loss: 1494.1921 - val_RMSE: 38.7107 - val_loss: 1498.5156 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6546 - loss: 1494.1805 - val_RMSE: 38.7107 - val_loss: 1498.5209 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 50.7556 - loss: 2709.2288 - val_RMSE: 38.7215 - val_loss: 1499.3531 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7264 - loss: 1499.7343 - val_RMSE: 38.7223 - val_loss: 1499.4198 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7220 - loss: 1499.3903 - val_RMSE: 38.7120 - val_loss: 1498.6226 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7202 - loss: 1499.2543 - val_RMSE: 38.7196 - val_loss: 1499.2111 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7183 - loss: 1499.1062 - val_RMSE: 38.7097 - val_loss: 1498.4419 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7170 - loss: 1499.0045 - val_RMSE: 38.7121 - val_loss: 1498.6268 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7148 - loss: 1498.8376 - val_RMSE: 38.7073 - val_loss: 1498.2518 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7128 - loss: 1498.6788 - val_RMSE: 38.7192 - val_loss: 1499.1736 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7113 - loss: 1498.5619 - val_RMSE: 38.7085 - val_loss: 1498.3450 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7096 - loss: 1498.4318 - val_RMSE: 38.7048 - val_loss: 1498.0632 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7080 - loss: 1498.3113 - val_RMSE: 38.7038 - val_loss: 1497.9880 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7057 - loss: 1498.1285 - val_RMSE: 38.7083 - val_loss: 1498.3315 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7033 - loss: 1497.9475 - val_RMSE: 38.7060 - val_loss: 1498.1506 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.7012 - loss: 1497.7826 - val_RMSE: 38.7064 - val_loss: 1498.1843 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6985 - loss: 1497.5731 - val_RMSE: 38.7075 - val_loss: 1498.2677 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6952 - loss: 1497.3198 - val_RMSE: 38.7084 - val_loss: 1498.3430 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6825 - loss: 1496.3365 - val_RMSE: 38.7047 - val_loss: 1498.0509 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6793 - loss: 1496.0919 - val_RMSE: 38.7053 - val_loss: 1498.0966 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6775 - loss: 1495.9491 - val_RMSE: 38.7058 - val_loss: 1498.1420 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6760 - loss: 1495.8317 - val_RMSE: 38.7065 - val_loss: 1498.1893 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6746 - loss: 1495.7233 - val_RMSE: 38.7071 - val_loss: 1498.2417 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6731 - loss: 1495.6104 - val_RMSE: 38.7062 - val_loss: 1498.1721 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6722 - loss: 1495.5402 - val_RMSE: 38.7065 - val_loss: 1498.1925 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6718 - loss: 1495.5088 - val_RMSE: 38.7067 - val_loss: 1498.2070 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 9ms/step - RMSE: 38.6715 - loss: 1495.4846 - val_RMSE: 38.7069 - val_loss: 1498.2203 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 13:43:14,659] Trial 6 finished with value: 38.69965489705404 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'gelu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 6ms/step - RMSE: 64.5167 - loss: 4326.0571 - val_RMSE: 38.6998 - val_loss: 1497.6766 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7397 - loss: 1500.7635 - val_RMSE: 38.6922 - val_loss: 1497.0889 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7331 - loss: 1500.2554 - val_RMSE: 38.6905 - val_loss: 1496.9545 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7294 - loss: 1499.9686 - val_RMSE: 38.6890 - val_loss: 1496.8379 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7267 - loss: 1499.7560 - val_RMSE: 38.6887 - val_loss: 1496.8121 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7246 - loss: 1499.5919 - val_RMSE: 38.6869 - val_loss: 1496.6796 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7223 - loss: 1499.4199 - val_RMSE: 38.6860 - val_loss: 1496.6075 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7204 - loss: 1499.2717 - val_RMSE: 38.6860 - val_loss: 1496.6072 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7186 - loss: 1499.1293 - val_RMSE: 38.6862 - val_loss: 1496.6184 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7167 - loss: 1498.9807 - val_RMSE: 38.6868 - val_loss: 1496.6718 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7146 - loss: 1498.8195 - val_RMSE: 38.6875 - val_loss: 1496.7207 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7123 - loss: 1498.6395 - val_RMSE: 38.6886 - val_loss: 1496.8044 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7099 - loss: 1498.4536 - val_RMSE: 38.6896 - val_loss: 1496.8857 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7052 - loss: 1498.0973 - val_RMSE: 38.6840 - val_loss: 1496.4507 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6997 - loss: 1497.6710 - val_RMSE: 38.6841 - val_loss: 1496.4604 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6976 - loss: 1497.5018 - val_RMSE: 38.6846 - val_loss: 1496.4976 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6960 - loss: 1497.3812 - val_RMSE: 38.6852 - val_loss: 1496.5416 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6947 - loss: 1497.2786 - val_RMSE: 38.6858 - val_loss: 1496.5889 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6935 - loss: 1497.1869 - val_RMSE: 38.6863 - val_loss: 1496.6333 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6923 - loss: 1497.0963 - val_RMSE: 38.6862 - val_loss: 1496.6251 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6917 - loss: 1497.0496 - val_RMSE: 38.6862 - val_loss: 1496.6248 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6913 - loss: 1497.0206 - val_RMSE: 38.6862 - val_loss: 1496.6259 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6910 - loss: 1496.9980 - val_RMSE: 38.6863 - val_loss: 1496.6279 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6908 - loss: 1496.9799 - val_RMSE: 38.6863 - val_loss: 1496.6318 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6906 - loss: 1496.9611 - val_RMSE: 38.6863 - val_loss: 1496.6328 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 6ms/step - RMSE: 64.5209 - loss: 4326.1514 - val_RMSE: 38.7310 - val_loss: 1500.0906 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6875 - loss: 1496.7255 - val_RMSE: 38.7239 - val_loss: 1499.5428 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6829 - loss: 1496.3674 - val_RMSE: 38.7220 - val_loss: 1499.3896 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6796 - loss: 1496.1140 - val_RMSE: 38.7196 - val_loss: 1499.2107 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6771 - loss: 1495.9210 - val_RMSE: 38.7184 - val_loss: 1499.1115 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6749 - loss: 1495.7512 - val_RMSE: 38.7175 - val_loss: 1499.0411 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6727 - loss: 1495.5812 - val_RMSE: 38.7171 - val_loss: 1499.0106 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6708 - loss: 1495.4329 - val_RMSE: 38.7179 - val_loss: 1499.0736 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6687 - loss: 1495.2682 - val_RMSE: 38.7183 - val_loss: 1499.1074 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6667 - loss: 1495.1161 - val_RMSE: 38.7196 - val_loss: 1499.2053 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6644 - loss: 1494.9368 - val_RMSE: 38.7202 - val_loss: 1499.2515 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6619 - loss: 1494.7406 - val_RMSE: 38.7212 - val_loss: 1499.3282 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6567 - loss: 1494.3383 - val_RMSE: 38.7138 - val_loss: 1498.7599 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6518 - loss: 1493.9613 - val_RMSE: 38.7143 - val_loss: 1498.7976 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6499 - loss: 1493.8162 - val_RMSE: 38.7149 - val_loss: 1498.8464 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6485 - loss: 1493.7086 - val_RMSE: 38.7155 - val_loss: 1498.8939 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6474 - loss: 1493.6196 - val_RMSE: 38.7161 - val_loss: 1498.9375 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6463 - loss: 1493.5380 - val_RMSE: 38.7167 - val_loss: 1498.9822 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6451 - loss: 1493.4484 - val_RMSE: 38.7162 - val_loss: 1498.9456 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6447 - loss: 1493.4100 - val_RMSE: 38.7162 - val_loss: 1498.9406 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6444 - loss: 1493.3871 - val_RMSE: 38.7162 - val_loss: 1498.9438 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6441 - loss: 1493.3690 - val_RMSE: 38.7162 - val_loss: 1498.9474 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6439 - loss: 1493.3535 - val_RMSE: 38.7163 - val_loss: 1498.9518 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6437 - loss: 1493.3356 - val_RMSE: 38.7163 - val_loss: 1498.9535 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6437 - loss: 1493.3337 - val_RMSE: 38.7163 - val_loss: 1498.9543 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 6ms/step - RMSE: 64.4752 - loss: 4320.0786 - val_RMSE: 38.7167 - val_loss: 1498.9790 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7149 - loss: 1498.8474 - val_RMSE: 38.7163 - val_loss: 1498.9487 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7108 - loss: 1498.5229 - val_RMSE: 38.7128 - val_loss: 1498.6794 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7076 - loss: 1498.2799 - val_RMSE: 38.7112 - val_loss: 1498.5579 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7054 - loss: 1498.1057 - val_RMSE: 38.7108 - val_loss: 1498.5284 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7031 - loss: 1497.9335 - val_RMSE: 38.7102 - val_loss: 1498.4799 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7012 - loss: 1497.7812 - val_RMSE: 38.7101 - val_loss: 1498.4750 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6994 - loss: 1497.6471 - val_RMSE: 38.7099 - val_loss: 1498.4536 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6975 - loss: 1497.4999 - val_RMSE: 38.7090 - val_loss: 1498.3870 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6954 - loss: 1497.3309 - val_RMSE: 38.7086 - val_loss: 1498.3552 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6931 - loss: 1497.1576 - val_RMSE: 38.7088 - val_loss: 1498.3699 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6907 - loss: 1496.9701 - val_RMSE: 38.7091 - val_loss: 1498.3942 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6883 - loss: 1496.7844 - val_RMSE: 38.7094 - val_loss: 1498.4147 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6854 - loss: 1496.5642 - val_RMSE: 38.7099 - val_loss: 1498.4557 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6824 - loss: 1496.3311 - val_RMSE: 38.7108 - val_loss: 1498.5288 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6774 - loss: 1495.9399 - val_RMSE: 38.7079 - val_loss: 1498.3007 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6713 - loss: 1495.4668 - val_RMSE: 38.7087 - val_loss: 1498.3658 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6690 - loss: 1495.2920 - val_RMSE: 38.7096 - val_loss: 1498.4351 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6672 - loss: 1495.1554 - val_RMSE: 38.7105 - val_loss: 1498.5001 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6657 - loss: 1495.0396 - val_RMSE: 38.7113 - val_loss: 1498.5619 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6643 - loss: 1494.9321 - val_RMSE: 38.7121 - val_loss: 1498.6233 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6628 - loss: 1494.8140 - val_RMSE: 38.7121 - val_loss: 1498.6232 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6623 - loss: 1494.7714 - val_RMSE: 38.7121 - val_loss: 1498.6290 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6619 - loss: 1494.7441 - val_RMSE: 38.7123 - val_loss: 1498.6387 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6617 - loss: 1494.7245 - val_RMSE: 38.7124 - val_loss: 1498.6477 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 14:06:30,056] Trial 7 finished with value: 38.70501963297526 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'gelu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 7ms/step - RMSE: 56.3068 - loss: 3333.3040 - val_RMSE: 38.7013 - val_loss: 1497.7872 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7423 - loss: 1500.9646 - val_RMSE: 38.6976 - val_loss: 1497.5044 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7369 - loss: 1500.5446 - val_RMSE: 38.6902 - val_loss: 1496.9320 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7329 - loss: 1500.2416 - val_RMSE: 38.6915 - val_loss: 1497.0309 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7307 - loss: 1500.0662 - val_RMSE: 38.6894 - val_loss: 1496.8667 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7285 - loss: 1499.8958 - val_RMSE: 38.6879 - val_loss: 1496.7532 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7261 - loss: 1499.7104 - val_RMSE: 38.6892 - val_loss: 1496.8555 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7239 - loss: 1499.5438 - val_RMSE: 38.6899 - val_loss: 1496.9084 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7215 - loss: 1499.3575 - val_RMSE: 38.6887 - val_loss: 1496.8184 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7189 - loss: 1499.1566 - val_RMSE: 38.6879 - val_loss: 1496.7538 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7162 - loss: 1498.9487 - val_RMSE: 38.6890 - val_loss: 1496.8369 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7077 - loss: 1498.2899 - val_RMSE: 38.6840 - val_loss: 1496.4543 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7022 - loss: 1497.8649 - val_RMSE: 38.6846 - val_loss: 1496.4967 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6997 - loss: 1497.6714 - val_RMSE: 38.6855 - val_loss: 1496.5662 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6978 - loss: 1497.5209 - val_RMSE: 38.6863 - val_loss: 1496.6324 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6961 - loss: 1497.3879 - val_RMSE: 38.6873 - val_loss: 1496.7043 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6945 - loss: 1497.2648 - val_RMSE: 38.6881 - val_loss: 1496.7728 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6929 - loss: 1497.1410 - val_RMSE: 38.6885 - val_loss: 1496.7979 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6920 - loss: 1497.0713 - val_RMSE: 38.6887 - val_loss: 1496.8143 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6915 - loss: 1497.0337 - val_RMSE: 38.6888 - val_loss: 1496.8254 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 38.6911 - loss: 1497.0037 - val_RMSE: 38.6890 - val_loss: 1496.8365 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 38.6908 - loss: 1496.9783 - val_RMSE: 38.6891 - val_loss: 1496.8483 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6905 - loss: 1496.9562 - val_RMSE: 38.6892 - val_loss: 1496.8539 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 38.6904 - loss: 1496.9493 - val_RMSE: 38.6892 - val_loss: 1496.8572 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6904 - loss: 1496.9442 - val_RMSE: 38.6893 - val_loss: 1496.8594 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 7ms/step - RMSE: 56.2791 - loss: 3329.3713 - val_RMSE: 38.7458 - val_loss: 1501.2393 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6939 - loss: 1497.2213 - val_RMSE: 38.7272 - val_loss: 1499.7983 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6870 - loss: 1496.6836 - val_RMSE: 38.7249 - val_loss: 1499.6165 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6825 - loss: 1496.3372 - val_RMSE: 38.7218 - val_loss: 1499.3798 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6793 - loss: 1496.0886 - val_RMSE: 38.7233 - val_loss: 1499.4954 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6763 - loss: 1495.8604 - val_RMSE: 38.7220 - val_loss: 1499.3940 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6738 - loss: 1495.6648 - val_RMSE: 38.7179 - val_loss: 1499.0771 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6710 - loss: 1495.4453 - val_RMSE: 38.7182 - val_loss: 1499.0984 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6686 - loss: 1495.2600 - val_RMSE: 38.7191 - val_loss: 1499.1699 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6651 - loss: 1494.9916 - val_RMSE: 38.7215 - val_loss: 1499.3510 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6614 - loss: 1494.7074 - val_RMSE: 38.7253 - val_loss: 1499.6528 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6571 - loss: 1494.3741 - val_RMSE: 38.7288 - val_loss: 1499.9171 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6491 - loss: 1493.7557 - val_RMSE: 38.7186 - val_loss: 1499.1322 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6412 - loss: 1493.1453 - val_RMSE: 38.7207 - val_loss: 1499.2931 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6371 - loss: 1492.8293 - val_RMSE: 38.7228 - val_loss: 1499.4573 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6339 - loss: 1492.5811 - val_RMSE: 38.7248 - val_loss: 1499.6067 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6310 - loss: 1492.3550 - val_RMSE: 38.7265 - val_loss: 1499.7454 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6291 - loss: 1492.2118 - val_RMSE: 38.7245 - val_loss: 1499.5883 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6275 - loss: 1492.0876 - val_RMSE: 38.7245 - val_loss: 1499.5858 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6268 - loss: 1492.0310 - val_RMSE: 38.7246 - val_loss: 1499.5967 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6263 - loss: 1491.9882 - val_RMSE: 38.7248 - val_loss: 1499.6111 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6258 - loss: 1491.9509 - val_RMSE: 38.7250 - val_loss: 1499.6287 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6253 - loss: 1491.9175 - val_RMSE: 38.7251 - val_loss: 1499.6320 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6253 - loss: 1491.9116 - val_RMSE: 38.7251 - val_loss: 1499.6320 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6252 - loss: 1491.9061 - val_RMSE: 38.7251 - val_loss: 1499.6320 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 7ms/step - RMSE: 56.2757 - loss: 3328.9019 - val_RMSE: 38.7203 - val_loss: 1499.2627 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7203 - loss: 1499.2638 - val_RMSE: 38.7155 - val_loss: 1498.8885 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7144 - loss: 1498.8069 - val_RMSE: 38.7117 - val_loss: 1498.5968 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7107 - loss: 1498.5164 - val_RMSE: 38.7116 - val_loss: 1498.5848 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7073 - loss: 1498.2544 - val_RMSE: 38.7111 - val_loss: 1498.5508 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 38.7042 - loss: 1498.0142 - val_RMSE: 38.7141 - val_loss: 1498.7821 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.7020 - loss: 1497.8428 - val_RMSE: 38.7085 - val_loss: 1498.3462 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6990 - loss: 1497.6136 - val_RMSE: 38.7111 - val_loss: 1498.5498 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6965 - loss: 1497.4165 - val_RMSE: 38.7120 - val_loss: 1498.6190 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6929 - loss: 1497.1406 - val_RMSE: 38.7123 - val_loss: 1498.6420 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6900 - loss: 1496.9147 - val_RMSE: 38.7161 - val_loss: 1498.9366 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6864 - loss: 1496.6355 - val_RMSE: 38.7168 - val_loss: 1498.9889 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6794 - loss: 1496.0945 - val_RMSE: 38.7097 - val_loss: 1498.4427 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6720 - loss: 1495.5265 - val_RMSE: 38.7116 - val_loss: 1498.5875 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6685 - loss: 1495.2556 - val_RMSE: 38.7135 - val_loss: 1498.7386 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6658 - loss: 1495.0413 - val_RMSE: 38.7153 - val_loss: 1498.8745 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 38.6633 - loss: 1494.8488 - val_RMSE: 38.7171 - val_loss: 1499.0142 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6614 - loss: 1494.7063 - val_RMSE: 38.7170 - val_loss: 1499.0067 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6604 - loss: 1494.6240 - val_RMSE: 38.7171 - val_loss: 1499.0115 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 38.6597 - loss: 1494.5747 - val_RMSE: 38.7172 - val_loss: 1499.0221 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6592 - loss: 1494.5363 - val_RMSE: 38.7174 - val_loss: 1499.0366 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6588 - loss: 1494.5020 - val_RMSE: 38.7176 - val_loss: 1499.0520 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6584 - loss: 1494.4727 - val_RMSE: 38.7177 - val_loss: 1499.0580 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6583 - loss: 1494.4662 - val_RMSE: 38.7177 - val_loss: 1499.0599 - learning_rate: 1.0000e-06\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.6582 - loss: 1494.4594 - val_RMSE: 38.7177 - val_loss: 1499.0623 - learning_rate: 1.0000e-06\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 14:31:06,153] Trial 8 finished with value: 38.71068827311198 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'relu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 6ms/step - RMSE: 64.5205 - loss: 4326.5693 - val_RMSE: 38.7010 - val_loss: 1497.7694 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7391 - loss: 1500.7207 - val_RMSE: 38.6970 - val_loss: 1497.4564 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7344 - loss: 1500.3568 - val_RMSE: 38.6930 - val_loss: 1497.1469 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7314 - loss: 1500.1243 - val_RMSE: 38.6910 - val_loss: 1496.9957 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7288 - loss: 1499.9196 - val_RMSE: 38.6897 - val_loss: 1496.8904 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7266 - loss: 1499.7516 - val_RMSE: 38.6882 - val_loss: 1496.7745 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7249 - loss: 1499.6224 - val_RMSE: 38.6872 - val_loss: 1496.6969 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7234 - loss: 1499.5006 - val_RMSE: 38.6861 - val_loss: 1496.6150 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7219 - loss: 1499.3895 - val_RMSE: 38.6858 - val_loss: 1496.5894 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7204 - loss: 1499.2673 - val_RMSE: 38.6853 - val_loss: 1496.5532 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7187 - loss: 1499.1367 - val_RMSE: 38.6848 - val_loss: 1496.5103 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7167 - loss: 1498.9874 - val_RMSE: 38.6846 - val_loss: 1496.4966 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7148 - loss: 1498.8378 - val_RMSE: 38.6846 - val_loss: 1496.4971 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7128 - loss: 1498.6786 - val_RMSE: 38.6851 - val_loss: 1496.5380 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7107 - loss: 1498.5225 - val_RMSE: 38.6856 - val_loss: 1496.5720 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7086 - loss: 1498.3586 - val_RMSE: 38.6860 - val_loss: 1496.6034 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7063 - loss: 1498.1823 - val_RMSE: 38.6865 - val_loss: 1496.6479 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7012 - loss: 1497.7820 - val_RMSE: 38.6829 - val_loss: 1496.3678 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6969 - loss: 1497.4495 - val_RMSE: 38.6835 - val_loss: 1496.4165 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6951 - loss: 1497.3085 - val_RMSE: 38.6842 - val_loss: 1496.4655 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6938 - loss: 1497.2075 - val_RMSE: 38.6847 - val_loss: 1496.5099 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6927 - loss: 1497.1262 - val_RMSE: 38.6853 - val_loss: 1496.5541 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6918 - loss: 1497.0538 - val_RMSE: 38.6859 - val_loss: 1496.5953 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6909 - loss: 1496.9841 - val_RMSE: 38.6858 - val_loss: 1496.5902 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6903 - loss: 1496.9395 - val_RMSE: 38.6858 - val_loss: 1496.5895 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 23s 6ms/step - RMSE: 64.5233 - loss: 4326.4814 - val_RMSE: 38.7250 - val_loss: 1499.6293 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6861 - loss: 1496.6162 - val_RMSE: 38.7194 - val_loss: 1499.1913 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6820 - loss: 1496.2994 - val_RMSE: 38.7185 - val_loss: 1499.1201 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6794 - loss: 1496.0997 - val_RMSE: 38.7188 - val_loss: 1499.1487 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6774 - loss: 1495.9409 - val_RMSE: 38.7171 - val_loss: 1499.0175 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6754 - loss: 1495.7887 - val_RMSE: 38.7164 - val_loss: 1498.9617 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6739 - loss: 1495.6697 - val_RMSE: 38.7163 - val_loss: 1498.9539 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6725 - loss: 1495.5641 - val_RMSE: 38.7163 - val_loss: 1498.9489 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6712 - loss: 1495.4602 - val_RMSE: 38.7161 - val_loss: 1498.9370 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6699 - loss: 1495.3630 - val_RMSE: 38.7156 - val_loss: 1498.9000 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6685 - loss: 1495.2506 - val_RMSE: 38.7153 - val_loss: 1498.8735 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6669 - loss: 1495.1290 - val_RMSE: 38.7154 - val_loss: 1498.8793 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6653 - loss: 1495.0042 - val_RMSE: 38.7155 - val_loss: 1498.8884 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6635 - loss: 1494.8691 - val_RMSE: 38.7158 - val_loss: 1498.9111 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6617 - loss: 1494.7300 - val_RMSE: 38.7163 - val_loss: 1498.9529 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6598 - loss: 1494.5820 - val_RMSE: 38.7171 - val_loss: 1499.0170 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6554 - loss: 1494.2438 - val_RMSE: 38.7120 - val_loss: 1498.6179 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6512 - loss: 1493.9165 - val_RMSE: 38.7126 - val_loss: 1498.6636 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6497 - loss: 1493.7963 - val_RMSE: 38.7131 - val_loss: 1498.7043 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.6486 - loss: 1493.7142 - val_RMSE: 38.7136 - val_loss: 1498.7401 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6477 - loss: 1493.6433 - val_RMSE: 38.7140 - val_loss: 1498.7709 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6468 - loss: 1493.5774 - val_RMSE: 38.7144 - val_loss: 1498.8016 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6458 - loss: 1493.5024 - val_RMSE: 38.7142 - val_loss: 1498.7925 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6455 - loss: 1493.4731 - val_RMSE: 38.7143 - val_loss: 1498.7937 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6452 - loss: 1493.4558 - val_RMSE: 38.7143 - val_loss: 1498.7971 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 6ms/step - RMSE: 64.4759 - loss: 4320.1543 - val_RMSE: 38.7162 - val_loss: 1498.9456 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7165 - loss: 1498.9674 - val_RMSE: 38.7140 - val_loss: 1498.7737 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7127 - loss: 1498.6738 - val_RMSE: 38.7125 - val_loss: 1498.6570 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7098 - loss: 1498.4526 - val_RMSE: 38.7114 - val_loss: 1498.5735 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 13s 5ms/step - RMSE: 38.7078 - loss: 1498.2924 - val_RMSE: 38.7101 - val_loss: 1498.4744 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7060 - loss: 1498.1528 - val_RMSE: 38.7095 - val_loss: 1498.4247 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7043 - loss: 1498.0205 - val_RMSE: 38.7086 - val_loss: 1498.3577 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7023 - loss: 1497.8685 - val_RMSE: 38.7080 - val_loss: 1498.3123 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.7005 - loss: 1497.7267 - val_RMSE: 38.7076 - val_loss: 1498.2804 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6988 - loss: 1497.5990 - val_RMSE: 38.7071 - val_loss: 1498.2391 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6972 - loss: 1497.4728 - val_RMSE: 38.7071 - val_loss: 1498.2383 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6956 - loss: 1497.3521 - val_RMSE: 38.7070 - val_loss: 1498.2296 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6938 - loss: 1497.2139 - val_RMSE: 38.7071 - val_loss: 1498.2358 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6919 - loss: 1497.0665 - val_RMSE: 38.7072 - val_loss: 1498.2505 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6900 - loss: 1496.9147 - val_RMSE: 38.7074 - val_loss: 1498.2626 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6879 - loss: 1496.7509 - val_RMSE: 38.7082 - val_loss: 1498.3237 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6856 - loss: 1496.5726 - val_RMSE: 38.7093 - val_loss: 1498.4061 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6794 - loss: 1496.0999 - val_RMSE: 38.7069 - val_loss: 1498.2238 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6759 - loss: 1495.8245 - val_RMSE: 38.7076 - val_loss: 1498.2793 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6742 - loss: 1495.6978 - val_RMSE: 38.7083 - val_loss: 1498.3309 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6730 - loss: 1495.6025 - val_RMSE: 38.7088 - val_loss: 1498.3740 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6720 - loss: 1495.5222 - val_RMSE: 38.7094 - val_loss: 1498.4156 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6710 - loss: 1495.4490 - val_RMSE: 38.7099 - val_loss: 1498.4529 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6698 - loss: 1495.3577 - val_RMSE: 38.7096 - val_loss: 1498.4365 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.6694 - loss: 1495.3240 - val_RMSE: 38.7097 - val_loss: 1498.4375 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 14:54:24,546] Trial 9 finished with value: 38.7032470703125 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'silu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 31s 9ms/step - RMSE: 53.3186 - loss: 2993.5579 - val_RMSE: 38.7071 - val_loss: 1498.2429 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7440 - loss: 1501.0951 - val_RMSE: 38.6936 - val_loss: 1497.1932 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7411 - loss: 1500.8773 - val_RMSE: 38.7064 - val_loss: 1498.1865 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7388 - loss: 1500.6960 - val_RMSE: 38.7052 - val_loss: 1498.0944 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7383 - loss: 1500.6528 - val_RMSE: 38.6929 - val_loss: 1497.1420 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7352 - loss: 1500.4137 - val_RMSE: 38.6895 - val_loss: 1496.8790 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7336 - loss: 1500.2933 - val_RMSE: 38.6901 - val_loss: 1496.9241 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7323 - loss: 1500.1934 - val_RMSE: 38.6950 - val_loss: 1497.3043 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7310 - loss: 1500.0924 - val_RMSE: 38.7181 - val_loss: 1499.0892 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7309 - loss: 1500.0822 - val_RMSE: 38.6837 - val_loss: 1496.4310 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7291 - loss: 1499.9446 - val_RMSE: 38.6840 - val_loss: 1496.4510 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7285 - loss: 1499.8938 - val_RMSE: 38.6892 - val_loss: 1496.8503 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7278 - loss: 1499.8427 - val_RMSE: 38.6833 - val_loss: 1496.4011 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7268 - loss: 1499.7677 - val_RMSE: 38.6881 - val_loss: 1496.7714 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7270 - loss: 1499.7819 - val_RMSE: 38.6866 - val_loss: 1496.6532 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7260 - loss: 1499.7024 - val_RMSE: 38.6826 - val_loss: 1496.3412 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7251 - loss: 1499.6340 - val_RMSE: 38.6842 - val_loss: 1496.4639 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7239 - loss: 1499.5372 - val_RMSE: 38.6835 - val_loss: 1496.4156 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7226 - loss: 1499.4398 - val_RMSE: 38.6868 - val_loss: 1496.6671 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7213 - loss: 1499.3378 - val_RMSE: 38.6846 - val_loss: 1496.4976 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7206 - loss: 1499.2872 - val_RMSE: 38.6838 - val_loss: 1496.4332 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7126 - loss: 1498.6664 - val_RMSE: 38.6784 - val_loss: 1496.0188 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7093 - loss: 1498.4097 - val_RMSE: 38.6784 - val_loss: 1496.0154 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7083 - loss: 1498.3367 - val_RMSE: 38.6784 - val_loss: 1496.0164 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7077 - loss: 1498.2845 - val_RMSE: 38.6784 - val_loss: 1496.0192 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 30s 9ms/step - RMSE: 53.2733 - loss: 2987.8425 - val_RMSE: 38.7349 - val_loss: 1500.3892 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6927 - loss: 1497.1256 - val_RMSE: 38.7316 - val_loss: 1500.1345 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6904 - loss: 1496.9482 - val_RMSE: 38.7230 - val_loss: 1499.4745 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6877 - loss: 1496.7358 - val_RMSE: 38.7330 - val_loss: 1500.2435 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6855 - loss: 1496.5675 - val_RMSE: 38.7185 - val_loss: 1499.1194 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6842 - loss: 1496.4686 - val_RMSE: 38.7223 - val_loss: 1499.4143 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6830 - loss: 1496.3733 - val_RMSE: 38.7225 - val_loss: 1499.4310 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6820 - loss: 1496.2997 - val_RMSE: 38.7210 - val_loss: 1499.3123 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6813 - loss: 1496.2462 - val_RMSE: 38.7397 - val_loss: 1500.7631 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6796 - loss: 1496.1113 - val_RMSE: 38.7297 - val_loss: 1499.9910 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6713 - loss: 1495.4679 - val_RMSE: 38.7140 - val_loss: 1498.7743 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6696 - loss: 1495.3373 - val_RMSE: 38.7140 - val_loss: 1498.7760 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6690 - loss: 1495.2925 - val_RMSE: 38.7140 - val_loss: 1498.7715 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6686 - loss: 1495.2627 - val_RMSE: 38.7139 - val_loss: 1498.7659 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6682 - loss: 1495.2341 - val_RMSE: 38.7138 - val_loss: 1498.7554 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6680 - loss: 1495.2122 - val_RMSE: 38.7136 - val_loss: 1498.7467 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6677 - loss: 1495.1919 - val_RMSE: 38.7136 - val_loss: 1498.7390 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6674 - loss: 1495.1726 - val_RMSE: 38.7135 - val_loss: 1498.7358 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6672 - loss: 1495.1550 - val_RMSE: 38.7135 - val_loss: 1498.7365 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6670 - loss: 1495.1384 - val_RMSE: 38.7135 - val_loss: 1498.7358 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6668 - loss: 1495.1208 - val_RMSE: 38.7135 - val_loss: 1498.7355 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6665 - loss: 1495.1028 - val_RMSE: 38.7135 - val_loss: 1498.7336 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6663 - loss: 1495.0834 - val_RMSE: 38.7135 - val_loss: 1498.7355 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6661 - loss: 1495.0645 - val_RMSE: 38.7136 - val_loss: 1498.7396 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6658 - loss: 1495.0465 - val_RMSE: 38.7136 - val_loss: 1498.7426 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 30s 9ms/step - RMSE: 53.2412 - loss: 2983.8818 - val_RMSE: 38.7205 - val_loss: 1499.2791 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7226 - loss: 1499.4396 - val_RMSE: 38.7290 - val_loss: 1499.9385 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7183 - loss: 1499.1036 - val_RMSE: 38.7236 - val_loss: 1499.5153 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7176 - loss: 1499.0568 - val_RMSE: 38.7117 - val_loss: 1498.5991 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7150 - loss: 1498.8558 - val_RMSE: 38.7164 - val_loss: 1498.9625 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7139 - loss: 1498.7700 - val_RMSE: 38.7172 - val_loss: 1499.0209 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7123 - loss: 1498.6466 - val_RMSE: 38.7147 - val_loss: 1498.8301 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7106 - loss: 1498.5078 - val_RMSE: 38.7078 - val_loss: 1498.2960 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7096 - loss: 1498.4333 - val_RMSE: 38.7104 - val_loss: 1498.4935 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7077 - loss: 1498.2885 - val_RMSE: 38.7070 - val_loss: 1498.2352 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7066 - loss: 1498.2037 - val_RMSE: 38.7039 - val_loss: 1497.9928 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7049 - loss: 1498.0693 - val_RMSE: 38.7053 - val_loss: 1498.1021 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7041 - loss: 1498.0063 - val_RMSE: 38.7070 - val_loss: 1498.2332 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7027 - loss: 1497.8972 - val_RMSE: 38.7058 - val_loss: 1498.1415 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7011 - loss: 1497.7777 - val_RMSE: 38.7051 - val_loss: 1498.0840 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6996 - loss: 1497.6604 - val_RMSE: 38.7040 - val_loss: 1498.0010 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6902 - loss: 1496.9329 - val_RMSE: 38.7026 - val_loss: 1497.8904 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6883 - loss: 1496.7838 - val_RMSE: 38.7029 - val_loss: 1497.9113 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6871 - loss: 1496.6962 - val_RMSE: 38.7031 - val_loss: 1497.9337 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6862 - loss: 1496.6212 - val_RMSE: 38.7034 - val_loss: 1497.9550 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6853 - loss: 1496.5498 - val_RMSE: 38.7037 - val_loss: 1497.9750 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6844 - loss: 1496.4833 - val_RMSE: 38.7039 - val_loss: 1497.9956 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6830 - loss: 1496.3727 - val_RMSE: 38.7032 - val_loss: 1497.9346 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6824 - loss: 1496.3285 - val_RMSE: 38.7031 - val_loss: 1497.9335 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6822 - loss: 1496.3137 - val_RMSE: 38.7032 - val_loss: 1497.9363 - learning_rate: 1.0000e-05\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 15:26:28,049] Trial 10 finished with value: 38.69839350382487 and parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 30s 9ms/step - RMSE: 53.3182 - loss: 2993.5210 - val_RMSE: 38.7001 - val_loss: 1497.7002 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7444 - loss: 1501.1309 - val_RMSE: 38.6928 - val_loss: 1497.1317 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7420 - loss: 1500.9396 - val_RMSE: 38.6988 - val_loss: 1497.5938 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7382 - loss: 1500.6515 - val_RMSE: 38.6949 - val_loss: 1497.2966 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7358 - loss: 1500.4611 - val_RMSE: 38.6896 - val_loss: 1496.8850 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7345 - loss: 1500.3644 - val_RMSE: 38.6875 - val_loss: 1496.7209 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7341 - loss: 1500.3313 - val_RMSE: 38.6879 - val_loss: 1496.7523 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7324 - loss: 1500.2007 - val_RMSE: 38.6879 - val_loss: 1496.7507 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7328 - loss: 1500.2299 - val_RMSE: 38.6865 - val_loss: 1496.6488 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7315 - loss: 1500.1271 - val_RMSE: 38.6874 - val_loss: 1496.7111 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7302 - loss: 1500.0288 - val_RMSE: 38.6841 - val_loss: 1496.4628 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7289 - loss: 1499.9288 - val_RMSE: 38.6831 - val_loss: 1496.3828 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7297 - loss: 1499.9921 - val_RMSE: 38.6843 - val_loss: 1496.4767 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7278 - loss: 1499.8407 - val_RMSE: 38.6880 - val_loss: 1496.7582 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7272 - loss: 1499.7946 - val_RMSE: 38.6922 - val_loss: 1497.0842 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7268 - loss: 1499.7622 - val_RMSE: 38.6893 - val_loss: 1496.8632 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7256 - loss: 1499.6724 - val_RMSE: 38.6871 - val_loss: 1496.6946 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7173 - loss: 1499.0298 - val_RMSE: 38.6801 - val_loss: 1496.1477 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7151 - loss: 1498.8584 - val_RMSE: 38.6799 - val_loss: 1496.1340 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7143 - loss: 1498.7955 - val_RMSE: 38.6798 - val_loss: 1496.1288 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7137 - loss: 1498.7512 - val_RMSE: 38.6798 - val_loss: 1496.1282 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7132 - loss: 1498.7134 - val_RMSE: 38.6799 - val_loss: 1496.1337 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7128 - loss: 1498.6805 - val_RMSE: 38.6800 - val_loss: 1496.1442 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7124 - loss: 1498.6511 - val_RMSE: 38.6802 - val_loss: 1496.1571 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7120 - loss: 1498.6233 - val_RMSE: 38.6803 - val_loss: 1496.1666 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 30s 9ms/step - RMSE: 53.2736 - loss: 2987.8662 - val_RMSE: 38.7326 - val_loss: 1500.2148 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6918 - loss: 1497.0553 - val_RMSE: 38.7311 - val_loss: 1500.0999 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6885 - loss: 1496.8013 - val_RMSE: 38.7335 - val_loss: 1500.2803 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6870 - loss: 1496.6886 - val_RMSE: 38.7253 - val_loss: 1499.6528 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6855 - loss: 1496.5717 - val_RMSE: 38.7232 - val_loss: 1499.4899 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6842 - loss: 1496.4673 - val_RMSE: 38.7206 - val_loss: 1499.2819 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6829 - loss: 1496.3699 - val_RMSE: 38.7182 - val_loss: 1499.1023 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6829 - loss: 1496.3691 - val_RMSE: 38.7186 - val_loss: 1499.1332 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6808 - loss: 1496.2013 - val_RMSE: 38.7166 - val_loss: 1498.9739 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6795 - loss: 1496.1061 - val_RMSE: 38.7164 - val_loss: 1498.9606 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6790 - loss: 1496.0621 - val_RMSE: 38.7168 - val_loss: 1498.9935 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6780 - loss: 1495.9908 - val_RMSE: 38.7177 - val_loss: 1499.0569 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6777 - loss: 1495.9657 - val_RMSE: 38.7158 - val_loss: 1498.9131 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6766 - loss: 1495.8829 - val_RMSE: 38.7384 - val_loss: 1500.6659 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6758 - loss: 1495.8217 - val_RMSE: 38.7163 - val_loss: 1498.9550 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6754 - loss: 1495.7870 - val_RMSE: 38.7183 - val_loss: 1499.1045 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6746 - loss: 1495.7249 - val_RMSE: 38.7184 - val_loss: 1499.1160 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6741 - loss: 1495.6904 - val_RMSE: 38.7191 - val_loss: 1499.1710 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6654 - loss: 1495.0125 - val_RMSE: 38.7128 - val_loss: 1498.6842 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6639 - loss: 1494.9015 - val_RMSE: 38.7127 - val_loss: 1498.6755 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6632 - loss: 1494.8478 - val_RMSE: 38.7129 - val_loss: 1498.6915 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6628 - loss: 1494.8092 - val_RMSE: 38.7131 - val_loss: 1498.7061 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6623 - loss: 1494.7767 - val_RMSE: 38.7133 - val_loss: 1498.7195 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6620 - loss: 1494.7484 - val_RMSE: 38.7134 - val_loss: 1498.7306 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6617 - loss: 1494.7253 - val_RMSE: 38.7136 - val_loss: 1498.7423 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 30s 9ms/step - RMSE: 53.2410 - loss: 2983.8684 - val_RMSE: 38.7145 - val_loss: 1498.8087 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7225 - loss: 1499.4362 - val_RMSE: 38.7203 - val_loss: 1499.2604 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7191 - loss: 1499.1697 - val_RMSE: 38.7137 - val_loss: 1498.7532 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7175 - loss: 1499.0491 - val_RMSE: 38.7234 - val_loss: 1499.4988 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7151 - loss: 1498.8591 - val_RMSE: 38.7104 - val_loss: 1498.4938 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7136 - loss: 1498.7418 - val_RMSE: 38.7083 - val_loss: 1498.3307 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7118 - loss: 1498.6025 - val_RMSE: 38.7109 - val_loss: 1498.5364 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7109 - loss: 1498.5325 - val_RMSE: 38.7095 - val_loss: 1498.4218 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7094 - loss: 1498.4159 - val_RMSE: 38.7065 - val_loss: 1498.1924 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7080 - loss: 1498.3065 - val_RMSE: 38.7076 - val_loss: 1498.2800 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7067 - loss: 1498.2075 - val_RMSE: 38.7049 - val_loss: 1498.0663 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7058 - loss: 1498.1422 - val_RMSE: 38.7054 - val_loss: 1498.1062 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7040 - loss: 1497.9983 - val_RMSE: 38.7055 - val_loss: 1498.1162 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7027 - loss: 1497.8960 - val_RMSE: 38.7037 - val_loss: 1497.9774 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7012 - loss: 1497.7806 - val_RMSE: 38.7030 - val_loss: 1497.9189 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6996 - loss: 1497.6615 - val_RMSE: 38.7034 - val_loss: 1497.9550 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6982 - loss: 1497.5549 - val_RMSE: 38.7037 - val_loss: 1497.9731 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6963 - loss: 1497.4067 - val_RMSE: 38.7039 - val_loss: 1497.9911 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6945 - loss: 1497.2668 - val_RMSE: 38.7046 - val_loss: 1498.0454 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6927 - loss: 1497.1243 - val_RMSE: 38.7046 - val_loss: 1498.0496 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.6842 - loss: 1496.4664 - val_RMSE: 38.7037 - val_loss: 1497.9731 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6811 - loss: 1496.2301 - val_RMSE: 38.7044 - val_loss: 1498.0341 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6796 - loss: 1496.1124 - val_RMSE: 38.7052 - val_loss: 1498.0941 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6783 - loss: 1496.0143 - val_RMSE: 38.7055 - val_loss: 1498.1146 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.6773 - loss: 1495.9305 - val_RMSE: 38.7058 - val_loss: 1498.1394 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-15 15:58:20,303] Trial 11 finished with value: 38.69990666707357 and parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'}. Best is trial 3 with value: 38.69801712036133.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            " 249/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 7ms/step - RMSE: 84.7905 - loss: 7211.5386"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-15 15:58:29,727] Trial 12 failed with parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-43-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-42-096f0de6de51>\", line 48, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n",
            "    if not opt_outputs.has_value():\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\", line 176, in has_value\n",
            "    return gen_optional_ops.optional_has_value(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\", line 172, in optional_has_value\n",
            "    _result = pywrap_tfe.TFE_Py_FastPathExecute(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-02-15 15:58:29,729] Trial 12 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-096f0de6de51>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train, \n\u001b[0m\u001b[1;32m     49\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtsmIR6j1aLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 3 finished with value: 38.69801712036133\n",
        "* parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'}"
      ],
      "metadata": {
        "id": "egD1bR4BVai7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kz6gSSAQlNe"
      },
      "source": [
        "#### **4.6.1 NeuralNetwork v1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "2b34d4ed-ae4f-436a-d522-17da4a9b5279",
        "id": "Vu4pcEHqQlNf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "56569        3         4     3             8                   2           1   \n",
              "1183394      1         3     3             7                   2           1   \n",
              "855407       4         1     1             9                   2           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "56569        0      0              0.727470 -0.518096 -0.606337 -0.709550   \n",
              "1183394      0      6              1.159509  0.950699 -0.606337 -0.613824   \n",
              "855407       0      6             -0.842369 -0.192151 -0.168518 -0.018634   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "56569             0               0  \n",
              "1183394           0               0  \n",
              "855407            0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-87fca73f-247b-4e84-89db-75fc1afb8d2c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>56569</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.727470</td>\n",
              "      <td>-0.518096</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>-0.709550</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1183394</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1.159509</td>\n",
              "      <td>0.950699</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>-0.613824</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855407</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.842369</td>\n",
              "      <td>-0.192151</td>\n",
              "      <td>-0.168518</td>\n",
              "      <td>-0.018634</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87fca73f-247b-4e84-89db-75fc1afb8d2c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-87fca73f-247b-4e84-89db-75fc1afb8d2c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-87fca73f-247b-4e84-89db-75fc1afb8d2c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5bbe5fca-40db-4be7-888f-af61eba3a9fd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5bbe5fca-40db-4be7-888f-af61eba3a9fd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5bbe5fca-40db-4be7-888f-af61eba3a9fd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 7,\n        \"max\": 9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          8,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7274695038795471\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.5180957317352295\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.16851840913295746\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.7095502018928528\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512,last_layer = 1, activation=\"relu\", reg=0.001, dropout_rate=0.33):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    reshaped_features = layers.Reshape((1, -1))(x)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x, flattened_attention])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "J50ErSTRQlNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model()\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "71HFsxAZVnCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bdaf28-50f6-4c7b-9818-75aa19191dd3",
        "id": "9tzW7MmLQlNg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([6, 5, 4, 10, 3, 3, 4, 7, 2, 2],\n",
              " array([3., 3., 2., 4., 2., 2., 2., 3., 2., 2.]),\n",
              " 10)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ceVtzBwQlNg"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a87fac-94f6-4023-c880-b2082a669485",
        "id": "iQ9CoxtaQlNg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Dtype\n",
            "---  ------              -----\n",
            " 0   Brand               int64\n",
            " 1   Material            int64\n",
            " 2   Size                int64\n",
            " 3   Compartments        int64\n",
            " 4   Laptop Compartment  int64\n",
            " 5   Waterproof          int64\n",
            " 6   Style               int64\n",
            " 7   Color               int64\n",
            " 8   cheap_flag          int64\n",
            " 9   expansive_flag      int64\n",
            "dtypes: int64(10)\n",
            "memory usage: 335.2 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Weight Capacity (kg)  float32\n",
            " 1   TE_wc                 float32\n",
            " 2   skew_0                float32\n",
            " 3   skew_1                float32\n",
            "dtypes: float32(4)\n",
            "memory usage: 91.4 MB\n"
          ]
        }
      ],
      "source": [
        "categorical_feat = t.cat_features.copy()\n",
        "numerical_feat = t.num_features.copy()\n",
        "\n",
        "X_train_cat = X_enc[categorical_feat]\n",
        "X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "X_test_cat = test_enc[categorical_feat]\n",
        "X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "X_train_cat.info()\n",
        "X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tObj5kq1QlNg"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': trial.suggest_float('reg', 1e-4, 0.1, log=True),\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.50)\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num])\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sux1Xc6ZQlNh"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7529e2b3-d69b-4ddb-a44c-2f80dbe52ca4",
        "id": "gW_aoIIBQlNh"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 16:03:45,993] A new study created in memory with name: no-name-36849c32-d360-4d35-8d2a-109bfaa1d4aa\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 12ms/step - RMSE: 51.3745 - loss: 2832.1123 - val_RMSE: 38.7568 - val_loss: 1519.5251 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.9979 - loss: 1536.6611 - val_RMSE: 38.7666 - val_loss: 1515.6813 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8919 - loss: 1525.1195 - val_RMSE: 38.7090 - val_loss: 1509.4550 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8501 - loss: 1519.8094 - val_RMSE: 38.6949 - val_loss: 1506.4019 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8228 - loss: 1516.0771 - val_RMSE: 38.6996 - val_loss: 1505.6942 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8137 - loss: 1514.2294 - val_RMSE: 38.7109 - val_loss: 1505.5392 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8025 - loss: 1512.2858 - val_RMSE: 38.6927 - val_loss: 1503.1927 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7950 - loss: 1510.7366 - val_RMSE: 38.6923 - val_loss: 1502.1176 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7867 - loss: 1509.0934 - val_RMSE: 38.6909 - val_loss: 1500.8621 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7805 - loss: 1507.7684 - val_RMSE: 38.6896 - val_loss: 1500.1909 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7806 - loss: 1507.3898 - val_RMSE: 38.6902 - val_loss: 1500.0416 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7818 - loss: 1507.2737 - val_RMSE: 38.6917 - val_loss: 1500.2687 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7816 - loss: 1507.1475 - val_RMSE: 38.6891 - val_loss: 1499.9069 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7808 - loss: 1506.9562 - val_RMSE: 38.6876 - val_loss: 1499.5712 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7792 - loss: 1506.7211 - val_RMSE: 38.6887 - val_loss: 1499.6896 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7803 - loss: 1506.7301 - val_RMSE: 38.6895 - val_loss: 1499.5400 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7802 - loss: 1506.6393 - val_RMSE: 38.6883 - val_loss: 1499.5293 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7802 - loss: 1506.5903 - val_RMSE: 38.6864 - val_loss: 1499.0499 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7780 - loss: 1506.3221 - val_RMSE: 38.6881 - val_loss: 1499.2017 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7787 - loss: 1506.2954 - val_RMSE: 38.6896 - val_loss: 1499.3910 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7775 - loss: 1506.1781 - val_RMSE: 38.6892 - val_loss: 1499.2687 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7766 - loss: 1506.0671 - val_RMSE: 38.6860 - val_loss: 1499.0186 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7808 - loss: 1506.2799 - val_RMSE: 38.6865 - val_loss: 1498.8927 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7795 - loss: 1506.1647 - val_RMSE: 38.6855 - val_loss: 1498.7249 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7756 - loss: 1505.7499 - val_RMSE: 38.6862 - val_loss: 1498.7686 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 12ms/step - RMSE: 51.3021 - loss: 2823.8208 - val_RMSE: 38.7634 - val_loss: 1520.7347 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.9422 - loss: 1532.8909 - val_RMSE: 38.7831 - val_loss: 1516.9619 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8378 - loss: 1520.9048 - val_RMSE: 38.7349 - val_loss: 1511.1633 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7928 - loss: 1515.2720 - val_RMSE: 38.7321 - val_loss: 1509.1328 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7676 - loss: 1511.5394 - val_RMSE: 38.7262 - val_loss: 1507.4763 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7551 - loss: 1509.4453 - val_RMSE: 38.7316 - val_loss: 1506.8464 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7485 - loss: 1507.9105 - val_RMSE: 38.7240 - val_loss: 1505.2249 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7398 - loss: 1506.3206 - val_RMSE: 38.7215 - val_loss: 1504.0413 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7341 - loss: 1504.8588 - val_RMSE: 38.7214 - val_loss: 1503.0850 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7336 - loss: 1504.0422 - val_RMSE: 38.7209 - val_loss: 1502.7485 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7323 - loss: 1503.5555 - val_RMSE: 38.7200 - val_loss: 1502.3033 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7319 - loss: 1503.3514 - val_RMSE: 38.7201 - val_loss: 1502.2601 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7316 - loss: 1503.2905 - val_RMSE: 38.7225 - val_loss: 1502.4164 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7313 - loss: 1503.1978 - val_RMSE: 38.7182 - val_loss: 1501.9128 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7306 - loss: 1502.9703 - val_RMSE: 38.7212 - val_loss: 1502.1805 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7295 - loss: 1502.8280 - val_RMSE: 38.7195 - val_loss: 1501.8370 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7306 - loss: 1502.7605 - val_RMSE: 38.7213 - val_loss: 1501.8317 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7265 - loss: 1502.3130 - val_RMSE: 38.7208 - val_loss: 1501.7612 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7292 - loss: 1502.4355 - val_RMSE: 38.7230 - val_loss: 1501.8983 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7263 - loss: 1502.2406 - val_RMSE: 38.7213 - val_loss: 1501.7244 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7275 - loss: 1502.2611 - val_RMSE: 38.7168 - val_loss: 1501.2633 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7272 - loss: 1502.1406 - val_RMSE: 38.7202 - val_loss: 1501.5856 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7272 - loss: 1502.1337 - val_RMSE: 38.7190 - val_loss: 1501.3167 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7260 - loss: 1501.9783 - val_RMSE: 38.7203 - val_loss: 1501.3640 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7227 - loss: 1501.6710 - val_RMSE: 38.7188 - val_loss: 1501.1597 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 12ms/step - RMSE: 51.3149 - loss: 2824.5806 - val_RMSE: 38.7822 - val_loss: 1522.3011 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.9822 - loss: 1536.1847 - val_RMSE: 38.7577 - val_loss: 1515.6809 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8762 - loss: 1524.1930 - val_RMSE: 38.7271 - val_loss: 1510.6638 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8197 - loss: 1517.1710 - val_RMSE: 38.7419 - val_loss: 1509.5366 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8001 - loss: 1513.8381 - val_RMSE: 38.7225 - val_loss: 1507.0442 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7900 - loss: 1512.0044 - val_RMSE: 38.7213 - val_loss: 1505.7921 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7782 - loss: 1510.1273 - val_RMSE: 38.7158 - val_loss: 1504.5847 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7653 - loss: 1508.2162 - val_RMSE: 38.7130 - val_loss: 1503.3190 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7611 - loss: 1507.0109 - val_RMSE: 38.7166 - val_loss: 1503.0569 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7581 - loss: 1506.0563 - val_RMSE: 38.7155 - val_loss: 1502.3295 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7562 - loss: 1505.5942 - val_RMSE: 38.7131 - val_loss: 1502.0338 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7568 - loss: 1505.4513 - val_RMSE: 38.7114 - val_loss: 1501.6659 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7535 - loss: 1505.0535 - val_RMSE: 38.7111 - val_loss: 1501.5841 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7596 - loss: 1505.3881 - val_RMSE: 38.7163 - val_loss: 1502.1027 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7549 - loss: 1505.0312 - val_RMSE: 38.7110 - val_loss: 1501.3414 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7559 - loss: 1504.9349 - val_RMSE: 38.7133 - val_loss: 1501.6398 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7529 - loss: 1504.6545 - val_RMSE: 38.7095 - val_loss: 1501.4309 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7559 - loss: 1504.7786 - val_RMSE: 38.7117 - val_loss: 1501.1122 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7561 - loss: 1504.7273 - val_RMSE: 38.7122 - val_loss: 1501.1674 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7546 - loss: 1504.4834 - val_RMSE: 38.7121 - val_loss: 1501.1199 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7566 - loss: 1504.6086 - val_RMSE: 38.7148 - val_loss: 1501.3589 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7537 - loss: 1504.3334 - val_RMSE: 38.7112 - val_loss: 1500.9198 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7567 - loss: 1504.4890 - val_RMSE: 38.7107 - val_loss: 1500.9138 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7536 - loss: 1504.1897 - val_RMSE: 38.7084 - val_loss: 1500.6278 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7551 - loss: 1504.3167 - val_RMSE: 38.7082 - val_loss: 1500.4899 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 16:40:07,636] Trial 0 finished with value: 38.70439020792643 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'relu', 'reg': 0.03532288502832786, 'dropout_rate': 0.31396524201979953}. Best is trial 0 with value: 38.70439020792643.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 54.1264 - loss: 3079.4102 - val_RMSE: 38.7246 - val_loss: 1500.0896 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0518 - loss: 1525.5660 - val_RMSE: 38.7018 - val_loss: 1498.4287 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9228 - loss: 1515.6116 - val_RMSE: 38.6893 - val_loss: 1497.5673 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8847 - loss: 1512.7523 - val_RMSE: 38.6864 - val_loss: 1497.4307 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8597 - loss: 1510.8882 - val_RMSE: 38.6857 - val_loss: 1497.4148 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8535 - loss: 1510.4323 - val_RMSE: 38.6840 - val_loss: 1497.2799 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8469 - loss: 1509.9010 - val_RMSE: 38.6838 - val_loss: 1497.2241 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8435 - loss: 1509.5848 - val_RMSE: 38.6836 - val_loss: 1497.1499 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8411 - loss: 1509.3501 - val_RMSE: 38.6828 - val_loss: 1497.0570 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.8424 - loss: 1509.4250 - val_RMSE: 38.6823 - val_loss: 1497.0051 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8404 - loss: 1509.2593 - val_RMSE: 38.6821 - val_loss: 1496.9814 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8350 - loss: 1508.8384 - val_RMSE: 38.6813 - val_loss: 1496.9292 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8436 - loss: 1509.5121 - val_RMSE: 38.6818 - val_loss: 1496.9677 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8348 - loss: 1508.8245 - val_RMSE: 38.6814 - val_loss: 1496.9409 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8363 - loss: 1508.9521 - val_RMSE: 38.6803 - val_loss: 1496.8596 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8300 - loss: 1508.4655 - val_RMSE: 38.6812 - val_loss: 1496.9338 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8321 - loss: 1508.6306 - val_RMSE: 38.6806 - val_loss: 1496.8925 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8286 - loss: 1508.3646 - val_RMSE: 38.6802 - val_loss: 1496.8665 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.8292 - loss: 1508.4248 - val_RMSE: 38.6799 - val_loss: 1496.8623 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8313 - loss: 1508.6008 - val_RMSE: 38.6797 - val_loss: 1496.8492 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8260 - loss: 1508.1886 - val_RMSE: 38.6804 - val_loss: 1496.9110 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8287 - loss: 1508.4119 - val_RMSE: 38.6807 - val_loss: 1496.9382 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.8261 - loss: 1508.2118 - val_RMSE: 38.6808 - val_loss: 1496.9559 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8216 - loss: 1507.8792 - val_RMSE: 38.6808 - val_loss: 1496.9628 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8245 - loss: 1508.1134 - val_RMSE: 38.6798 - val_loss: 1496.9000 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 31s 9ms/step - RMSE: 54.0879 - loss: 3074.5679 - val_RMSE: 38.7391 - val_loss: 1501.2137 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 39.0029 - loss: 1521.7534 - val_RMSE: 38.7260 - val_loss: 1500.3019 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8782 - loss: 1512.1405 - val_RMSE: 38.7228 - val_loss: 1500.1617 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8270 - loss: 1508.2686 - val_RMSE: 38.7208 - val_loss: 1500.1039 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8100 - loss: 1507.0386 - val_RMSE: 38.7170 - val_loss: 1499.8690 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.8010 - loss: 1506.3768 - val_RMSE: 38.7162 - val_loss: 1499.7908 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7953 - loss: 1505.9154 - val_RMSE: 38.7160 - val_loss: 1499.7277 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7952 - loss: 1505.8512 - val_RMSE: 38.7157 - val_loss: 1499.6515 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7937 - loss: 1505.6903 - val_RMSE: 38.7156 - val_loss: 1499.6105 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7909 - loss: 1505.4437 - val_RMSE: 38.7152 - val_loss: 1499.5591 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7871 - loss: 1505.1288 - val_RMSE: 38.7149 - val_loss: 1499.5374 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7876 - loss: 1505.1685 - val_RMSE: 38.7153 - val_loss: 1499.5674 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7842 - loss: 1504.9037 - val_RMSE: 38.7139 - val_loss: 1499.4578 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7823 - loss: 1504.7612 - val_RMSE: 38.7151 - val_loss: 1499.5588 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7884 - loss: 1505.2439 - val_RMSE: 38.7147 - val_loss: 1499.5333 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7809 - loss: 1504.6642 - val_RMSE: 38.7139 - val_loss: 1499.4816 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7882 - loss: 1505.2380 - val_RMSE: 38.7140 - val_loss: 1499.5017 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7826 - loss: 1504.8125 - val_RMSE: 38.7144 - val_loss: 1499.5380 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7756 - loss: 1504.2653 - val_RMSE: 38.7076 - val_loss: 1498.9706 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7759 - loss: 1504.2529 - val_RMSE: 38.7076 - val_loss: 1498.9368 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7686 - loss: 1503.6591 - val_RMSE: 38.7078 - val_loss: 1498.9263 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7749 - loss: 1504.1180 - val_RMSE: 38.7079 - val_loss: 1498.9117 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7652 - loss: 1503.3452 - val_RMSE: 38.7076 - val_loss: 1498.8704 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7685 - loss: 1503.5879 - val_RMSE: 38.7077 - val_loss: 1498.8667 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7652 - loss: 1503.3159 - val_RMSE: 38.7079 - val_loss: 1498.8663 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.0607 - loss: 3071.0916 - val_RMSE: 38.7247 - val_loss: 1500.0953 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0263 - loss: 1523.5748 - val_RMSE: 38.7151 - val_loss: 1499.4620 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8984 - loss: 1513.7181 - val_RMSE: 38.7100 - val_loss: 1499.1707 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8556 - loss: 1510.4905 - val_RMSE: 38.7084 - val_loss: 1499.1277 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8397 - loss: 1509.3270 - val_RMSE: 38.7072 - val_loss: 1499.0754 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8244 - loss: 1508.1620 - val_RMSE: 38.7072 - val_loss: 1499.0577 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.8248 - loss: 1508.1646 - val_RMSE: 38.7059 - val_loss: 1498.9071 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8213 - loss: 1507.8408 - val_RMSE: 38.7053 - val_loss: 1498.8085 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8185 - loss: 1507.5780 - val_RMSE: 38.7059 - val_loss: 1498.8350 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8207 - loss: 1507.7319 - val_RMSE: 38.7045 - val_loss: 1498.7238 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8156 - loss: 1507.3353 - val_RMSE: 38.7049 - val_loss: 1498.7467 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8149 - loss: 1507.2789 - val_RMSE: 38.7043 - val_loss: 1498.7028 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8107 - loss: 1506.9546 - val_RMSE: 38.7050 - val_loss: 1498.7563 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8134 - loss: 1507.1660 - val_RMSE: 38.7049 - val_loss: 1498.7622 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8092 - loss: 1506.8434 - val_RMSE: 38.7037 - val_loss: 1498.6705 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8119 - loss: 1507.0581 - val_RMSE: 38.7040 - val_loss: 1498.7018 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8108 - loss: 1506.9803 - val_RMSE: 38.7032 - val_loss: 1498.6395 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8082 - loss: 1506.7808 - val_RMSE: 38.7036 - val_loss: 1498.6838 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8077 - loss: 1506.7535 - val_RMSE: 38.7032 - val_loss: 1498.6501 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8067 - loss: 1506.6863 - val_RMSE: 38.7030 - val_loss: 1498.6527 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7974 - loss: 1505.9706 - val_RMSE: 38.7024 - val_loss: 1498.6149 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8040 - loss: 1506.4856 - val_RMSE: 38.7032 - val_loss: 1498.6814 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8049 - loss: 1506.5691 - val_RMSE: 38.7030 - val_loss: 1498.6797 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8021 - loss: 1506.3671 - val_RMSE: 38.7018 - val_loss: 1498.5995 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8007 - loss: 1506.2587 - val_RMSE: 38.7027 - val_loss: 1498.6749 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 17:10:07,902] Trial 1 finished with value: 38.696816762288414 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00027561698595006656, 'dropout_rate': 0.4105017873627729}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 12ms/step - RMSE: 51.4811 - loss: 2785.5352 - val_RMSE: 38.7200 - val_loss: 1501.3983 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 39.0050 - loss: 1523.6791 - val_RMSE: 38.7155 - val_loss: 1501.4891 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.8815 - loss: 1514.4375 - val_RMSE: 38.7056 - val_loss: 1500.8835 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8411 - loss: 1511.3909 - val_RMSE: 38.7018 - val_loss: 1500.5566 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8134 - loss: 1509.1803 - val_RMSE: 38.6988 - val_loss: 1500.1974 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8055 - loss: 1508.4283 - val_RMSE: 38.6927 - val_loss: 1499.5613 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7993 - loss: 1507.7716 - val_RMSE: 38.6914 - val_loss: 1499.2638 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7926 - loss: 1507.0441 - val_RMSE: 38.6929 - val_loss: 1499.1389 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7870 - loss: 1506.3828 - val_RMSE: 38.6878 - val_loss: 1498.5454 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7828 - loss: 1505.8733 - val_RMSE: 38.6885 - val_loss: 1498.4587 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7820 - loss: 1505.6763 - val_RMSE: 38.6873 - val_loss: 1498.2343 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7822 - loss: 1505.5815 - val_RMSE: 38.6873 - val_loss: 1498.1597 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7804 - loss: 1505.3665 - val_RMSE: 38.6853 - val_loss: 1497.9257 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7785 - loss: 1505.1427 - val_RMSE: 38.6838 - val_loss: 1497.7542 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7766 - loss: 1504.9423 - val_RMSE: 38.6827 - val_loss: 1497.6527 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7774 - loss: 1504.9865 - val_RMSE: 38.6820 - val_loss: 1497.5724 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7744 - loss: 1504.7397 - val_RMSE: 38.6823 - val_loss: 1497.5912 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7753 - loss: 1504.8101 - val_RMSE: 38.6818 - val_loss: 1497.5594 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7720 - loss: 1504.5520 - val_RMSE: 38.6817 - val_loss: 1497.5492 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7737 - loss: 1504.6752 - val_RMSE: 38.6824 - val_loss: 1497.6013 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7715 - loss: 1504.5157 - val_RMSE: 38.6819 - val_loss: 1497.5652 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7703 - loss: 1504.4159 - val_RMSE: 38.6815 - val_loss: 1497.5253 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7748 - loss: 1504.7688 - val_RMSE: 38.6811 - val_loss: 1497.5056 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7722 - loss: 1504.5754 - val_RMSE: 38.6815 - val_loss: 1497.5227 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7673 - loss: 1504.1896 - val_RMSE: 38.6817 - val_loss: 1497.5555 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 12ms/step - RMSE: 51.4232 - loss: 2778.6873 - val_RMSE: 38.7358 - val_loss: 1502.5631 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.9488 - loss: 1519.2096 - val_RMSE: 38.7286 - val_loss: 1502.3649 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8307 - loss: 1510.3438 - val_RMSE: 38.7201 - val_loss: 1501.8761 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7850 - loss: 1506.9270 - val_RMSE: 38.7249 - val_loss: 1502.3204 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7640 - loss: 1505.3335 - val_RMSE: 38.7204 - val_loss: 1501.8672 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7538 - loss: 1504.4165 - val_RMSE: 38.7161 - val_loss: 1501.3892 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7451 - loss: 1503.5719 - val_RMSE: 38.7156 - val_loss: 1501.1223 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7413 - loss: 1503.0411 - val_RMSE: 38.7135 - val_loss: 1500.7371 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7359 - loss: 1502.4141 - val_RMSE: 38.7136 - val_loss: 1500.5466 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7355 - loss: 1502.1884 - val_RMSE: 38.7144 - val_loss: 1500.4609 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7333 - loss: 1501.8695 - val_RMSE: 38.7143 - val_loss: 1500.2827 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7309 - loss: 1501.5409 - val_RMSE: 38.7140 - val_loss: 1500.1484 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7289 - loss: 1501.2842 - val_RMSE: 38.7154 - val_loss: 1500.1740 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7294 - loss: 1501.2535 - val_RMSE: 38.7144 - val_loss: 1500.0696 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7274 - loss: 1501.0635 - val_RMSE: 38.7159 - val_loss: 1500.1901 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7267 - loss: 1501.0211 - val_RMSE: 38.7144 - val_loss: 1500.0526 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7277 - loss: 1501.0828 - val_RMSE: 38.7163 - val_loss: 1500.2013 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7225 - loss: 1500.6899 - val_RMSE: 38.7157 - val_loss: 1500.1570 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7254 - loss: 1500.9139 - val_RMSE: 38.7153 - val_loss: 1500.1416 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7226 - loss: 1500.6957 - val_RMSE: 38.7161 - val_loss: 1500.1908 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7224 - loss: 1500.6945 - val_RMSE: 38.7145 - val_loss: 1500.0940 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7145 - loss: 1500.0428 - val_RMSE: 38.7088 - val_loss: 1499.4592 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7091 - loss: 1499.4463 - val_RMSE: 38.7091 - val_loss: 1499.3541 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7079 - loss: 1499.2405 - val_RMSE: 38.7091 - val_loss: 1499.2749 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7049 - loss: 1498.9391 - val_RMSE: 38.7090 - val_loss: 1499.2125 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 12ms/step - RMSE: 51.4282 - loss: 2778.9778 - val_RMSE: 38.7325 - val_loss: 1502.2775 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.9723 - loss: 1521.0208 - val_RMSE: 38.7222 - val_loss: 1501.8488 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8661 - loss: 1513.0850 - val_RMSE: 38.7160 - val_loss: 1501.5682 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8211 - loss: 1509.7300 - val_RMSE: 38.7168 - val_loss: 1501.6185 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7951 - loss: 1507.6799 - val_RMSE: 38.7103 - val_loss: 1501.0483 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7825 - loss: 1506.6033 - val_RMSE: 38.7076 - val_loss: 1500.6799 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7775 - loss: 1506.0386 - val_RMSE: 38.7064 - val_loss: 1500.3843 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7653 - loss: 1504.9027 - val_RMSE: 38.7060 - val_loss: 1500.1735 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7628 - loss: 1504.5100 - val_RMSE: 38.7045 - val_loss: 1499.8124 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7596 - loss: 1504.0413 - val_RMSE: 38.7045 - val_loss: 1499.6505 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7552 - loss: 1503.5354 - val_RMSE: 38.7041 - val_loss: 1499.4872 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7555 - loss: 1503.4480 - val_RMSE: 38.7037 - val_loss: 1499.3448 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7506 - loss: 1502.9628 - val_RMSE: 38.7035 - val_loss: 1499.2722 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7563 - loss: 1503.3506 - val_RMSE: 38.7031 - val_loss: 1499.1765 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7515 - loss: 1502.9250 - val_RMSE: 38.7031 - val_loss: 1499.1622 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7523 - loss: 1502.9747 - val_RMSE: 38.7021 - val_loss: 1499.0925 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7504 - loss: 1502.8300 - val_RMSE: 38.7021 - val_loss: 1499.0677 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7518 - loss: 1502.9227 - val_RMSE: 38.7015 - val_loss: 1499.0411 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7540 - loss: 1503.1014 - val_RMSE: 38.7025 - val_loss: 1499.1080 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7495 - loss: 1502.7600 - val_RMSE: 38.7018 - val_loss: 1499.0720 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7519 - loss: 1502.9512 - val_RMSE: 38.7024 - val_loss: 1499.1102 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7488 - loss: 1502.7002 - val_RMSE: 38.7018 - val_loss: 1499.0699 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7511 - loss: 1502.8854 - val_RMSE: 38.7018 - val_loss: 1499.0673 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7425 - loss: 1502.1704 - val_RMSE: 38.7008 - val_loss: 1498.8035 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7384 - loss: 1501.6848 - val_RMSE: 38.7008 - val_loss: 1498.6813 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 17:46:47,489] Trial 2 finished with value: 38.697156270345054 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0005402789192665301, 'dropout_rate': 0.36286329232617837}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 28s 7ms/step - RMSE: 61.1220 - loss: 3901.1499 - val_RMSE: 38.7016 - val_loss: 1498.0620 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.4299 - loss: 1554.9777 - val_RMSE: 38.6945 - val_loss: 1497.5388 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.3545 - loss: 1549.0581 - val_RMSE: 38.6946 - val_loss: 1497.5609 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3389 - loss: 1547.8391 - val_RMSE: 38.6933 - val_loss: 1497.4711 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3170 - loss: 1546.1323 - val_RMSE: 38.6943 - val_loss: 1497.5634 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3075 - loss: 1545.3971 - val_RMSE: 38.6924 - val_loss: 1497.4204 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2967 - loss: 1544.5590 - val_RMSE: 38.6915 - val_loss: 1497.3578 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2819 - loss: 1543.3956 - val_RMSE: 38.6921 - val_loss: 1497.4036 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2615 - loss: 1541.7911 - val_RMSE: 38.6909 - val_loss: 1497.3091 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2421 - loss: 1540.2733 - val_RMSE: 38.6904 - val_loss: 1497.2765 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2259 - loss: 1538.9979 - val_RMSE: 38.6894 - val_loss: 1497.1974 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2156 - loss: 1538.1879 - val_RMSE: 38.6893 - val_loss: 1497.1865 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2061 - loss: 1537.4473 - val_RMSE: 38.6895 - val_loss: 1497.1979 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1827 - loss: 1535.6063 - val_RMSE: 38.6887 - val_loss: 1497.1351 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1827 - loss: 1535.6050 - val_RMSE: 38.6882 - val_loss: 1497.0979 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1665 - loss: 1534.3417 - val_RMSE: 38.6879 - val_loss: 1497.0800 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1591 - loss: 1533.7563 - val_RMSE: 38.6883 - val_loss: 1497.1042 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1412 - loss: 1532.3553 - val_RMSE: 38.6870 - val_loss: 1497.0095 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1302 - loss: 1531.4967 - val_RMSE: 38.6870 - val_loss: 1497.0079 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1210 - loss: 1530.7767 - val_RMSE: 38.6866 - val_loss: 1496.9836 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1172 - loss: 1530.4889 - val_RMSE: 38.6873 - val_loss: 1497.0319 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0976 - loss: 1528.9534 - val_RMSE: 38.6856 - val_loss: 1496.9064 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0885 - loss: 1528.2468 - val_RMSE: 38.6866 - val_loss: 1496.9849 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0750 - loss: 1527.1884 - val_RMSE: 38.6865 - val_loss: 1496.9753 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.0714 - loss: 1526.9084 - val_RMSE: 38.6863 - val_loss: 1496.9641 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 26s 7ms/step - RMSE: 61.1121 - loss: 3899.5061 - val_RMSE: 38.7294 - val_loss: 1500.2212 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3797 - loss: 1551.0195 - val_RMSE: 38.7242 - val_loss: 1499.8402 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3071 - loss: 1545.3280 - val_RMSE: 38.7235 - val_loss: 1499.7985 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2922 - loss: 1544.1672 - val_RMSE: 38.7232 - val_loss: 1499.7891 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2675 - loss: 1542.2424 - val_RMSE: 38.7241 - val_loss: 1499.8722 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2493 - loss: 1540.8242 - val_RMSE: 38.7234 - val_loss: 1499.8176 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2371 - loss: 1539.8723 - val_RMSE: 38.7225 - val_loss: 1499.7538 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2248 - loss: 1538.9126 - val_RMSE: 38.7214 - val_loss: 1499.6703 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2051 - loss: 1537.3649 - val_RMSE: 38.7217 - val_loss: 1499.6891 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1998 - loss: 1536.9500 - val_RMSE: 38.7198 - val_loss: 1499.5421 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1839 - loss: 1535.6959 - val_RMSE: 38.7200 - val_loss: 1499.5596 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1752 - loss: 1535.0199 - val_RMSE: 38.7201 - val_loss: 1499.5636 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1506 - loss: 1533.0902 - val_RMSE: 38.7186 - val_loss: 1499.4491 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1460 - loss: 1532.7303 - val_RMSE: 38.7186 - val_loss: 1499.4446 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1252 - loss: 1531.0986 - val_RMSE: 38.7181 - val_loss: 1499.4125 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1251 - loss: 1531.0919 - val_RMSE: 38.7181 - val_loss: 1499.4121 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1055 - loss: 1529.5645 - val_RMSE: 38.7177 - val_loss: 1499.3754 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1024 - loss: 1529.3180 - val_RMSE: 38.7171 - val_loss: 1499.3369 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0829 - loss: 1527.7985 - val_RMSE: 38.7170 - val_loss: 1499.3302 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0718 - loss: 1526.9307 - val_RMSE: 38.7177 - val_loss: 1499.3884 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0634 - loss: 1526.2772 - val_RMSE: 38.7171 - val_loss: 1499.3406 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0478 - loss: 1525.0576 - val_RMSE: 38.7162 - val_loss: 1499.2717 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0444 - loss: 1524.7906 - val_RMSE: 38.7165 - val_loss: 1499.2999 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0418 - loss: 1524.5929 - val_RMSE: 38.7172 - val_loss: 1499.3510 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0247 - loss: 1523.2585 - val_RMSE: 38.7163 - val_loss: 1499.2823 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 28s 7ms/step - RMSE: 61.0899 - loss: 3896.7007 - val_RMSE: 38.7189 - val_loss: 1499.4044 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3946 - loss: 1552.1942 - val_RMSE: 38.7210 - val_loss: 1499.5898 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3287 - loss: 1547.0219 - val_RMSE: 38.7218 - val_loss: 1499.6602 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3118 - loss: 1545.7069 - val_RMSE: 38.7199 - val_loss: 1499.5288 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2914 - loss: 1544.1165 - val_RMSE: 38.7237 - val_loss: 1499.8373 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2798 - loss: 1543.2173 - val_RMSE: 38.7212 - val_loss: 1499.6534 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2628 - loss: 1541.8882 - val_RMSE: 38.7151 - val_loss: 1499.1798 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2652 - loss: 1542.0763 - val_RMSE: 38.7151 - val_loss: 1499.1746 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2605 - loss: 1541.6981 - val_RMSE: 38.7142 - val_loss: 1499.0953 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2621 - loss: 1541.8195 - val_RMSE: 38.7142 - val_loss: 1499.0936 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2583 - loss: 1541.5154 - val_RMSE: 38.7140 - val_loss: 1499.0720 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2597 - loss: 1541.6219 - val_RMSE: 38.7138 - val_loss: 1499.0522 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.2608 - loss: 1541.7036 - val_RMSE: 38.7140 - val_loss: 1499.0601 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.2528 - loss: 1541.0702 - val_RMSE: 38.7140 - val_loss: 1499.0582 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2437 - loss: 1540.3527 - val_RMSE: 38.7136 - val_loss: 1499.0215 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2506 - loss: 1540.8850 - val_RMSE: 38.7131 - val_loss: 1498.9785 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2565 - loss: 1541.3456 - val_RMSE: 38.7131 - val_loss: 1498.9720 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2475 - loss: 1540.6411 - val_RMSE: 38.7129 - val_loss: 1498.9581 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2401 - loss: 1540.0510 - val_RMSE: 38.7133 - val_loss: 1498.9821 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2514 - loss: 1540.9315 - val_RMSE: 38.7132 - val_loss: 1498.9702 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2488 - loss: 1540.7273 - val_RMSE: 38.7132 - val_loss: 1498.9698 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.2469 - loss: 1540.5726 - val_RMSE: 38.7129 - val_loss: 1498.9382 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.2395 - loss: 1539.9875 - val_RMSE: 38.7130 - val_loss: 1498.9432 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2509 - loss: 1540.8840 - val_RMSE: 38.7126 - val_loss: 1498.9132 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2343 - loss: 1539.5750 - val_RMSE: 38.7129 - val_loss: 1498.9302 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 18:10:55,657] Trial 3 finished with value: 38.70516586303711 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'selu', 'reg': 0.0006168819899394228, 'dropout_rate': 0.48350841958817187}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 27s 7ms/step - RMSE: 60.7275 - loss: 3860.7412 - val_RMSE: 38.7025 - val_loss: 1503.1106 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1660 - loss: 1538.8193 - val_RMSE: 38.6939 - val_loss: 1500.7462 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0876 - loss: 1530.9313 - val_RMSE: 38.6927 - val_loss: 1499.0325 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0727 - loss: 1528.3319 - val_RMSE: 38.6935 - val_loss: 1498.2562 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0608 - loss: 1526.7576 - val_RMSE: 38.6941 - val_loss: 1498.1229 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0604 - loss: 1526.5984 - val_RMSE: 38.6947 - val_loss: 1498.0813 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0617 - loss: 1526.6384 - val_RMSE: 38.6930 - val_loss: 1497.9167 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0472 - loss: 1525.4810 - val_RMSE: 38.6966 - val_loss: 1498.2043 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0436 - loss: 1525.1986 - val_RMSE: 38.6924 - val_loss: 1497.8715 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0335 - loss: 1524.4050 - val_RMSE: 38.6930 - val_loss: 1497.9349 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.0296 - loss: 1524.1052 - val_RMSE: 38.6930 - val_loss: 1497.9077 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0229 - loss: 1523.5594 - val_RMSE: 38.6927 - val_loss: 1497.9104 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0233 - loss: 1523.6105 - val_RMSE: 38.6926 - val_loss: 1497.8959 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0091 - loss: 1522.5095 - val_RMSE: 38.6918 - val_loss: 1497.8387 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0185 - loss: 1523.2445 - val_RMSE: 38.6916 - val_loss: 1497.8258 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0086 - loss: 1522.4824 - val_RMSE: 38.6933 - val_loss: 1497.9510 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0157 - loss: 1523.0261 - val_RMSE: 38.6940 - val_loss: 1498.0188 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9957 - loss: 1521.4740 - val_RMSE: 38.6899 - val_loss: 1497.7015 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9942 - loss: 1521.3615 - val_RMSE: 38.6917 - val_loss: 1497.8342 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9895 - loss: 1520.9830 - val_RMSE: 38.6923 - val_loss: 1497.9017 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9922 - loss: 1521.2125 - val_RMSE: 38.6909 - val_loss: 1497.7697 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9860 - loss: 1520.7096 - val_RMSE: 38.6904 - val_loss: 1497.7242 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9783 - loss: 1520.1158 - val_RMSE: 38.6915 - val_loss: 1497.8085 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9729 - loss: 1519.6266 - val_RMSE: 38.6862 - val_loss: 1497.2625 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9711 - loss: 1519.3628 - val_RMSE: 38.6859 - val_loss: 1497.1476 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 27s 7ms/step - RMSE: 60.7271 - loss: 3860.0928 - val_RMSE: 38.7317 - val_loss: 1505.3287 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1190 - loss: 1535.1005 - val_RMSE: 38.7229 - val_loss: 1502.9666 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0407 - loss: 1527.2592 - val_RMSE: 38.7253 - val_loss: 1501.5563 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0226 - loss: 1524.4153 - val_RMSE: 38.7230 - val_loss: 1500.5271 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0112 - loss: 1522.8704 - val_RMSE: 38.7265 - val_loss: 1500.5741 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0022 - loss: 1522.0120 - val_RMSE: 38.7238 - val_loss: 1500.3422 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9977 - loss: 1521.6361 - val_RMSE: 38.7243 - val_loss: 1500.3431 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9938 - loss: 1521.3024 - val_RMSE: 38.7228 - val_loss: 1500.2301 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9847 - loss: 1520.5884 - val_RMSE: 38.7241 - val_loss: 1500.3109 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9884 - loss: 1520.8776 - val_RMSE: 38.7239 - val_loss: 1500.3004 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9819 - loss: 1520.3705 - val_RMSE: 38.7220 - val_loss: 1500.1422 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9818 - loss: 1520.3409 - val_RMSE: 38.7224 - val_loss: 1500.1669 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9704 - loss: 1519.4506 - val_RMSE: 38.7217 - val_loss: 1500.1550 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9708 - loss: 1519.5157 - val_RMSE: 38.7195 - val_loss: 1499.9707 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9610 - loss: 1518.7417 - val_RMSE: 38.7219 - val_loss: 1500.1582 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9624 - loss: 1518.8473 - val_RMSE: 38.7227 - val_loss: 1500.2067 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9570 - loss: 1518.4193 - val_RMSE: 38.7220 - val_loss: 1500.1650 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9550 - loss: 1518.2831 - val_RMSE: 38.7208 - val_loss: 1500.0781 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9483 - loss: 1517.7571 - val_RMSE: 38.7223 - val_loss: 1500.2198 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9406 - loss: 1517.1356 - val_RMSE: 38.7152 - val_loss: 1499.5250 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 38.9350 - loss: 1516.5638 - val_RMSE: 38.7150 - val_loss: 1499.4087 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9327 - loss: 1516.2960 - val_RMSE: 38.7147 - val_loss: 1499.3195 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9325 - loss: 1516.2141 - val_RMSE: 38.7146 - val_loss: 1499.2625 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9412 - loss: 1516.8430 - val_RMSE: 38.7141 - val_loss: 1499.1801 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9320 - loss: 1516.0990 - val_RMSE: 38.7147 - val_loss: 1499.2025 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 27s 7ms/step - RMSE: 60.7024 - loss: 3856.9219 - val_RMSE: 38.7191 - val_loss: 1504.3219 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1440 - loss: 1537.0243 - val_RMSE: 38.7173 - val_loss: 1502.4677 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0631 - loss: 1528.9379 - val_RMSE: 38.7180 - val_loss: 1500.9121 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0463 - loss: 1526.1890 - val_RMSE: 38.7190 - val_loss: 1500.1782 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0397 - loss: 1525.0461 - val_RMSE: 38.7199 - val_loss: 1500.0298 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0356 - loss: 1524.5862 - val_RMSE: 38.7168 - val_loss: 1499.7716 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0341 - loss: 1524.4611 - val_RMSE: 38.7176 - val_loss: 1499.8107 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.0218 - loss: 1523.4763 - val_RMSE: 38.7176 - val_loss: 1499.7853 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.0164 - loss: 1523.0387 - val_RMSE: 38.7149 - val_loss: 1499.5848 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0189 - loss: 1523.2378 - val_RMSE: 38.7157 - val_loss: 1499.6538 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0105 - loss: 1522.5756 - val_RMSE: 38.7169 - val_loss: 1499.7473 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0035 - loss: 1522.0476 - val_RMSE: 38.7141 - val_loss: 1499.5364 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0007 - loss: 1521.8361 - val_RMSE: 38.7135 - val_loss: 1499.4797 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9916 - loss: 1521.1102 - val_RMSE: 38.7138 - val_loss: 1499.5151 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9875 - loss: 1520.7906 - val_RMSE: 38.7129 - val_loss: 1499.4524 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 38.9831 - loss: 1520.4603 - val_RMSE: 38.7127 - val_loss: 1499.4526 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 38.9856 - loss: 1520.6663 - val_RMSE: 38.7125 - val_loss: 1499.4307 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9780 - loss: 1520.0653 - val_RMSE: 38.7113 - val_loss: 1499.3521 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9685 - loss: 1519.3345 - val_RMSE: 38.7128 - val_loss: 1499.4357 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9715 - loss: 1519.5533 - val_RMSE: 38.7142 - val_loss: 1499.5631 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9706 - loss: 1519.5002 - val_RMSE: 38.7133 - val_loss: 1499.4885 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9655 - loss: 1519.0929 - val_RMSE: 38.7113 - val_loss: 1499.3341 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9578 - loss: 1518.5057 - val_RMSE: 38.7128 - val_loss: 1499.4435 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9586 - loss: 1518.5446 - val_RMSE: 38.7120 - val_loss: 1499.4027 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 38.9454 - loss: 1517.5496 - val_RMSE: 38.7118 - val_loss: 1499.3850 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 18:35:07,326] Trial 4 finished with value: 38.70415115356445 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'selu', 'reg': 0.019199229166119592, 'dropout_rate': 0.3341626635520698}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 29s 8ms/step - RMSE: 57.1019 - loss: 3422.7141 - val_RMSE: 38.7026 - val_loss: 1498.1483 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.0765 - loss: 1527.2397 - val_RMSE: 38.6906 - val_loss: 1497.2533 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9670 - loss: 1518.7303 - val_RMSE: 38.6889 - val_loss: 1497.1610 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9329 - loss: 1516.1066 - val_RMSE: 38.6860 - val_loss: 1496.9572 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9313 - loss: 1516.0071 - val_RMSE: 38.6864 - val_loss: 1497.0135 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9215 - loss: 1515.2563 - val_RMSE: 38.6849 - val_loss: 1496.9028 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9178 - loss: 1514.9745 - val_RMSE: 38.6852 - val_loss: 1496.9235 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9195 - loss: 1515.1115 - val_RMSE: 38.6845 - val_loss: 1496.8754 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9177 - loss: 1514.9722 - val_RMSE: 38.6844 - val_loss: 1496.8666 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9107 - loss: 1514.4308 - val_RMSE: 38.6840 - val_loss: 1496.8417 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9166 - loss: 1514.8933 - val_RMSE: 38.6827 - val_loss: 1496.7443 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9068 - loss: 1514.1381 - val_RMSE: 38.6840 - val_loss: 1496.8505 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9096 - loss: 1514.3577 - val_RMSE: 38.6829 - val_loss: 1496.7690 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9073 - loss: 1514.1802 - val_RMSE: 38.6824 - val_loss: 1496.7325 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8974 - loss: 1513.4181 - val_RMSE: 38.6822 - val_loss: 1496.7258 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8920 - loss: 1512.9998 - val_RMSE: 38.6818 - val_loss: 1496.6971 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8913 - loss: 1512.9496 - val_RMSE: 38.6827 - val_loss: 1496.7736 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8928 - loss: 1513.0688 - val_RMSE: 38.6816 - val_loss: 1496.6895 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8956 - loss: 1513.2979 - val_RMSE: 38.6812 - val_loss: 1496.6664 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8945 - loss: 1513.2184 - val_RMSE: 38.6820 - val_loss: 1496.7319 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8896 - loss: 1512.8368 - val_RMSE: 38.6813 - val_loss: 1496.6841 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8804 - loss: 1512.1338 - val_RMSE: 38.6815 - val_loss: 1496.7079 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8909 - loss: 1512.9537 - val_RMSE: 38.6813 - val_loss: 1496.7025 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8837 - loss: 1512.4005 - val_RMSE: 38.6808 - val_loss: 1496.6716 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8773 - loss: 1511.9095 - val_RMSE: 38.6795 - val_loss: 1496.5598 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 28s 8ms/step - RMSE: 57.0798 - loss: 3419.6045 - val_RMSE: 38.7378 - val_loss: 1500.8750 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.0223 - loss: 1523.0117 - val_RMSE: 38.7196 - val_loss: 1499.5078 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9216 - loss: 1515.1971 - val_RMSE: 38.7167 - val_loss: 1499.3125 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8893 - loss: 1512.7181 - val_RMSE: 38.7177 - val_loss: 1499.4166 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8841 - loss: 1512.3414 - val_RMSE: 38.7161 - val_loss: 1499.3121 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8756 - loss: 1511.6926 - val_RMSE: 38.7160 - val_loss: 1499.3086 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8671 - loss: 1511.0376 - val_RMSE: 38.7160 - val_loss: 1499.3159 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8607 - loss: 1510.5386 - val_RMSE: 38.7155 - val_loss: 1499.2704 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8628 - loss: 1510.6973 - val_RMSE: 38.7143 - val_loss: 1499.1808 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8610 - loss: 1510.5599 - val_RMSE: 38.7147 - val_loss: 1499.2192 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8595 - loss: 1510.4478 - val_RMSE: 38.7150 - val_loss: 1499.2386 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8596 - loss: 1510.4637 - val_RMSE: 38.7143 - val_loss: 1499.1863 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8578 - loss: 1510.3207 - val_RMSE: 38.7141 - val_loss: 1499.1774 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8555 - loss: 1510.1471 - val_RMSE: 38.7145 - val_loss: 1499.2074 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8551 - loss: 1510.1213 - val_RMSE: 38.7144 - val_loss: 1499.2085 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8446 - loss: 1509.3044 - val_RMSE: 38.7138 - val_loss: 1499.1660 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8508 - loss: 1509.7930 - val_RMSE: 38.7140 - val_loss: 1499.1906 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8441 - loss: 1509.2769 - val_RMSE: 38.7136 - val_loss: 1499.1621 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8407 - loss: 1509.0232 - val_RMSE: 38.7133 - val_loss: 1499.1454 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8394 - loss: 1508.9252 - val_RMSE: 38.7122 - val_loss: 1499.0658 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8354 - loss: 1508.6171 - val_RMSE: 38.7126 - val_loss: 1499.1012 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8336 - loss: 1508.4851 - val_RMSE: 38.7137 - val_loss: 1499.1908 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8339 - loss: 1508.5144 - val_RMSE: 38.7134 - val_loss: 1499.1732 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8360 - loss: 1508.6816 - val_RMSE: 38.7133 - val_loss: 1499.1696 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8341 - loss: 1508.5431 - val_RMSE: 38.7138 - val_loss: 1499.2173 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 30s 8ms/step - RMSE: 57.0575 - loss: 3416.6711 - val_RMSE: 38.7173 - val_loss: 1499.2837 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 39.0492 - loss: 1525.1086 - val_RMSE: 38.7125 - val_loss: 1498.9554 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9459 - loss: 1517.0900 - val_RMSE: 38.7113 - val_loss: 1498.8917 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9137 - loss: 1514.6139 - val_RMSE: 38.7098 - val_loss: 1498.8010 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9009 - loss: 1513.6399 - val_RMSE: 38.7103 - val_loss: 1498.8569 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9018 - loss: 1513.7206 - val_RMSE: 38.7085 - val_loss: 1498.7189 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8966 - loss: 1513.3174 - val_RMSE: 38.7087 - val_loss: 1498.7360 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8899 - loss: 1512.7970 - val_RMSE: 38.7084 - val_loss: 1498.7219 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8896 - loss: 1512.7843 - val_RMSE: 38.7071 - val_loss: 1498.6261 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8919 - loss: 1512.9684 - val_RMSE: 38.7069 - val_loss: 1498.6093 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8831 - loss: 1512.2891 - val_RMSE: 38.7073 - val_loss: 1498.6445 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8795 - loss: 1512.0112 - val_RMSE: 38.7059 - val_loss: 1498.5452 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8859 - loss: 1512.5170 - val_RMSE: 38.7068 - val_loss: 1498.6179 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8774 - loss: 1511.8562 - val_RMSE: 38.7053 - val_loss: 1498.5110 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8771 - loss: 1511.8365 - val_RMSE: 38.7070 - val_loss: 1498.6456 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8774 - loss: 1511.8636 - val_RMSE: 38.7069 - val_loss: 1498.6376 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8718 - loss: 1511.4375 - val_RMSE: 38.7057 - val_loss: 1498.5535 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8679 - loss: 1511.1353 - val_RMSE: 38.7062 - val_loss: 1498.5947 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8715 - loss: 1511.4213 - val_RMSE: 38.7051 - val_loss: 1498.5200 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8597 - loss: 1510.5067 - val_RMSE: 38.7030 - val_loss: 1498.3457 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8619 - loss: 1510.6716 - val_RMSE: 38.7028 - val_loss: 1498.3179 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8558 - loss: 1510.1830 - val_RMSE: 38.7027 - val_loss: 1498.3052 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8602 - loss: 1510.5157 - val_RMSE: 38.7024 - val_loss: 1498.2743 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8590 - loss: 1510.4141 - val_RMSE: 38.7024 - val_loss: 1498.2650 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8598 - loss: 1510.4696 - val_RMSE: 38.7024 - val_loss: 1498.2576 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 19:01:01,290] Trial 5 finished with value: 38.698568979899086 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0003050914095911242, 'dropout_rate': 0.3691098556240708}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 53.8575 - loss: 3059.3425 - val_RMSE: 38.7152 - val_loss: 1507.5746 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9927 - loss: 1528.3422 - val_RMSE: 38.7115 - val_loss: 1504.0999 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8892 - loss: 1517.3347 - val_RMSE: 38.6953 - val_loss: 1500.9766 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8553 - loss: 1513.1832 - val_RMSE: 38.6935 - val_loss: 1500.1469 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8347 - loss: 1510.9808 - val_RMSE: 38.6920 - val_loss: 1499.5760 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8232 - loss: 1509.6844 - val_RMSE: 38.6922 - val_loss: 1499.2373 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8176 - loss: 1508.8828 - val_RMSE: 38.6911 - val_loss: 1498.8298 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8125 - loss: 1508.2145 - val_RMSE: 38.6886 - val_loss: 1498.3792 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8081 - loss: 1507.6248 - val_RMSE: 38.6897 - val_loss: 1498.3115 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8099 - loss: 1507.6558 - val_RMSE: 38.6882 - val_loss: 1498.1571 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8085 - loss: 1507.5332 - val_RMSE: 38.6884 - val_loss: 1498.1942 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8061 - loss: 1507.3561 - val_RMSE: 38.6905 - val_loss: 1498.3774 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8115 - loss: 1507.7490 - val_RMSE: 38.6881 - val_loss: 1498.1760 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8065 - loss: 1507.3190 - val_RMSE: 38.6868 - val_loss: 1497.9678 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8087 - loss: 1507.4465 - val_RMSE: 38.6867 - val_loss: 1497.9766 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8052 - loss: 1507.1985 - val_RMSE: 38.6868 - val_loss: 1497.9678 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8047 - loss: 1507.1356 - val_RMSE: 38.6862 - val_loss: 1497.9454 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8029 - loss: 1507.0101 - val_RMSE: 38.6856 - val_loss: 1497.8439 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8067 - loss: 1507.2838 - val_RMSE: 38.6853 - val_loss: 1497.8370 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8067 - loss: 1507.2637 - val_RMSE: 38.6855 - val_loss: 1497.8129 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8023 - loss: 1506.9001 - val_RMSE: 38.6869 - val_loss: 1497.9243 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8065 - loss: 1507.2102 - val_RMSE: 38.6866 - val_loss: 1497.8113 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8034 - loss: 1506.9292 - val_RMSE: 38.6845 - val_loss: 1497.6653 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8026 - loss: 1506.8529 - val_RMSE: 38.6852 - val_loss: 1497.7114 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8042 - loss: 1506.9939 - val_RMSE: 38.6852 - val_loss: 1497.6940 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 53.8064 - loss: 3052.9604 - val_RMSE: 38.7544 - val_loss: 1510.3811 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9437 - loss: 1524.3280 - val_RMSE: 38.7247 - val_loss: 1504.9838 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8440 - loss: 1513.6884 - val_RMSE: 38.7227 - val_loss: 1503.0750 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8007 - loss: 1508.8926 - val_RMSE: 38.7240 - val_loss: 1502.5308 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7808 - loss: 1506.8301 - val_RMSE: 38.7199 - val_loss: 1501.8140 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7740 - loss: 1505.9149 - val_RMSE: 38.7200 - val_loss: 1501.4786 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7600 - loss: 1504.4626 - val_RMSE: 38.7191 - val_loss: 1500.9484 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7632 - loss: 1504.3308 - val_RMSE: 38.7216 - val_loss: 1500.9006 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7595 - loss: 1503.8173 - val_RMSE: 38.7205 - val_loss: 1500.6738 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7589 - loss: 1503.6686 - val_RMSE: 38.7216 - val_loss: 1500.7496 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7594 - loss: 1503.6960 - val_RMSE: 38.7215 - val_loss: 1500.7327 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7553 - loss: 1503.3489 - val_RMSE: 38.7217 - val_loss: 1500.7219 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7560 - loss: 1503.3950 - val_RMSE: 38.7235 - val_loss: 1500.8422 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7553 - loss: 1503.3322 - val_RMSE: 38.7235 - val_loss: 1500.8344 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7506 - loss: 1502.7841 - val_RMSE: 38.7132 - val_loss: 1499.5673 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7451 - loss: 1501.9807 - val_RMSE: 38.7126 - val_loss: 1499.3402 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7482 - loss: 1502.0668 - val_RMSE: 38.7125 - val_loss: 1499.2322 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7474 - loss: 1501.9209 - val_RMSE: 38.7125 - val_loss: 1499.1783 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7433 - loss: 1501.5502 - val_RMSE: 38.7120 - val_loss: 1499.1029 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7472 - loss: 1501.8118 - val_RMSE: 38.7122 - val_loss: 1499.0840 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7429 - loss: 1501.4557 - val_RMSE: 38.7126 - val_loss: 1499.0936 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7471 - loss: 1501.7670 - val_RMSE: 38.7117 - val_loss: 1499.0170 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7415 - loss: 1501.3187 - val_RMSE: 38.7115 - val_loss: 1498.9858 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7465 - loss: 1501.6902 - val_RMSE: 38.7115 - val_loss: 1498.9705 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7439 - loss: 1501.4828 - val_RMSE: 38.7117 - val_loss: 1498.9807 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 53.7987 - loss: 3051.8386 - val_RMSE: 38.7279 - val_loss: 1508.4514 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9722 - loss: 1526.7030 - val_RMSE: 38.7203 - val_loss: 1504.9979 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8738 - loss: 1516.3914 - val_RMSE: 38.7164 - val_loss: 1502.9056 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8325 - loss: 1511.6307 - val_RMSE: 38.7183 - val_loss: 1502.1237 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8120 - loss: 1509.2515 - val_RMSE: 38.7140 - val_loss: 1501.3054 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7954 - loss: 1507.5232 - val_RMSE: 38.7117 - val_loss: 1500.7601 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7919 - loss: 1506.9127 - val_RMSE: 38.7106 - val_loss: 1500.2843 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7884 - loss: 1506.2535 - val_RMSE: 38.7103 - val_loss: 1500.0157 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7853 - loss: 1505.7999 - val_RMSE: 38.7094 - val_loss: 1499.8024 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7876 - loss: 1505.8838 - val_RMSE: 38.7098 - val_loss: 1499.8402 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7848 - loss: 1505.6862 - val_RMSE: 38.7084 - val_loss: 1499.7268 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7852 - loss: 1505.6782 - val_RMSE: 38.7083 - val_loss: 1499.6956 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7850 - loss: 1505.6604 - val_RMSE: 38.7080 - val_loss: 1499.6895 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7833 - loss: 1505.5394 - val_RMSE: 38.7077 - val_loss: 1499.6508 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7832 - loss: 1505.4836 - val_RMSE: 38.7086 - val_loss: 1499.6971 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7820 - loss: 1505.3712 - val_RMSE: 38.7103 - val_loss: 1499.8469 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7840 - loss: 1505.5438 - val_RMSE: 38.7079 - val_loss: 1499.5906 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7810 - loss: 1505.2521 - val_RMSE: 38.7072 - val_loss: 1499.5189 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7804 - loss: 1505.2148 - val_RMSE: 38.7066 - val_loss: 1499.4785 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7805 - loss: 1505.2145 - val_RMSE: 38.7088 - val_loss: 1499.5863 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7753 - loss: 1504.7609 - val_RMSE: 38.7076 - val_loss: 1499.5492 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7810 - loss: 1505.2401 - val_RMSE: 38.7068 - val_loss: 1499.4810 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7782 - loss: 1505.0005 - val_RMSE: 38.7074 - val_loss: 1499.4962 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7807 - loss: 1505.1736 - val_RMSE: 38.7080 - val_loss: 1499.5042 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7725 - loss: 1504.3693 - val_RMSE: 38.7038 - val_loss: 1498.7878 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 19:31:47,179] Trial 6 finished with value: 38.700243631998696 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'reg': 0.007472176524756258, 'dropout_rate': 0.30743712269143775}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 12ms/step - RMSE: 51.5286 - loss: 2852.2712 - val_RMSE: 38.7185 - val_loss: 1518.3163 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 39.0282 - loss: 1540.8573 - val_RMSE: 38.7080 - val_loss: 1513.8391 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.9052 - loss: 1528.6173 - val_RMSE: 38.7091 - val_loss: 1511.3335 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.8609 - loss: 1523.0334 - val_RMSE: 38.7366 - val_loss: 1511.8179 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.8331 - loss: 1518.8773 - val_RMSE: 38.7156 - val_loss: 1508.7775 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.8215 - loss: 1516.3958 - val_RMSE: 38.6994 - val_loss: 1505.7628 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.8113 - loss: 1514.0636 - val_RMSE: 38.6978 - val_loss: 1503.7344 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.8048 - loss: 1511.7982 - val_RMSE: 38.6993 - val_loss: 1502.6901 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7997 - loss: 1510.1787 - val_RMSE: 38.6985 - val_loss: 1502.1903 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7967 - loss: 1509.6781 - val_RMSE: 38.6974 - val_loss: 1502.1272 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7966 - loss: 1509.6090 - val_RMSE: 38.6967 - val_loss: 1501.5555 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7998 - loss: 1509.6238 - val_RMSE: 38.6971 - val_loss: 1501.6257 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7970 - loss: 1509.3464 - val_RMSE: 38.6937 - val_loss: 1501.2955 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7967 - loss: 1509.2089 - val_RMSE: 38.6942 - val_loss: 1501.3264 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7950 - loss: 1509.0024 - val_RMSE: 38.6969 - val_loss: 1501.2295 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7964 - loss: 1508.9362 - val_RMSE: 38.6936 - val_loss: 1501.1515 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7949 - loss: 1508.8281 - val_RMSE: 38.6933 - val_loss: 1500.7833 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7960 - loss: 1508.8257 - val_RMSE: 38.6913 - val_loss: 1500.4706 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7932 - loss: 1508.4797 - val_RMSE: 38.6929 - val_loss: 1500.7047 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7957 - loss: 1508.7478 - val_RMSE: 38.6936 - val_loss: 1500.6428 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7914 - loss: 1508.3126 - val_RMSE: 38.6910 - val_loss: 1500.4664 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7905 - loss: 1508.2111 - val_RMSE: 38.6927 - val_loss: 1500.4425 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7962 - loss: 1508.5892 - val_RMSE: 38.6912 - val_loss: 1500.2487 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7959 - loss: 1508.5150 - val_RMSE: 38.6902 - val_loss: 1500.0854 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7910 - loss: 1508.0262 - val_RMSE: 38.6916 - val_loss: 1500.1575 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 12ms/step - RMSE: 51.4598 - loss: 2845.1838 - val_RMSE: 38.7767 - val_loss: 1525.3945 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.9769 - loss: 1538.4990 - val_RMSE: 38.7701 - val_loss: 1519.0621 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.8559 - loss: 1524.8116 - val_RMSE: 38.7531 - val_loss: 1514.3655 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.8036 - loss: 1517.7728 - val_RMSE: 38.7422 - val_loss: 1511.1298 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7790 - loss: 1513.6190 - val_RMSE: 38.7304 - val_loss: 1508.8058 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7692 - loss: 1511.3303 - val_RMSE: 38.7288 - val_loss: 1507.1094 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7558 - loss: 1508.8678 - val_RMSE: 38.7251 - val_loss: 1505.3943 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7500 - loss: 1507.0267 - val_RMSE: 38.7246 - val_loss: 1503.9808 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7463 - loss: 1505.6868 - val_RMSE: 38.7226 - val_loss: 1503.6935 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7475 - loss: 1505.5291 - val_RMSE: 38.7233 - val_loss: 1503.5381 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7466 - loss: 1505.4153 - val_RMSE: 38.7257 - val_loss: 1503.6134 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7472 - loss: 1505.2638 - val_RMSE: 38.7216 - val_loss: 1502.9609 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7447 - loss: 1504.9309 - val_RMSE: 38.7237 - val_loss: 1502.9507 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7468 - loss: 1504.9750 - val_RMSE: 38.7254 - val_loss: 1503.3693 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7451 - loss: 1504.7506 - val_RMSE: 38.7264 - val_loss: 1503.3263 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7434 - loss: 1504.7736 - val_RMSE: 38.7219 - val_loss: 1502.7815 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7466 - loss: 1504.7899 - val_RMSE: 38.7261 - val_loss: 1502.9895 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7403 - loss: 1504.2426 - val_RMSE: 38.7247 - val_loss: 1502.9430 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7440 - loss: 1504.5675 - val_RMSE: 38.7308 - val_loss: 1503.5018 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7422 - loss: 1504.3862 - val_RMSE: 38.7229 - val_loss: 1502.6877 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7412 - loss: 1504.2197 - val_RMSE: 38.7232 - val_loss: 1502.7212 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7417 - loss: 1504.2346 - val_RMSE: 38.7234 - val_loss: 1502.6967 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7410 - loss: 1504.0951 - val_RMSE: 38.7241 - val_loss: 1502.6064 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7413 - loss: 1504.0226 - val_RMSE: 38.7300 - val_loss: 1503.1766 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7385 - loss: 1503.8560 - val_RMSE: 38.7266 - val_loss: 1502.8417 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 12ms/step - RMSE: 51.4708 - loss: 2845.7400 - val_RMSE: 38.7702 - val_loss: 1526.5149 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 39.0170 - loss: 1543.0101 - val_RMSE: 38.7751 - val_loss: 1519.9900 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.8921 - loss: 1528.1085 - val_RMSE: 38.7474 - val_loss: 1514.3889 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.8314 - loss: 1520.3462 - val_RMSE: 38.7392 - val_loss: 1512.0295 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.8094 - loss: 1517.1302 - val_RMSE: 38.7303 - val_loss: 1509.6882 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7958 - loss: 1514.3269 - val_RMSE: 38.7269 - val_loss: 1507.7281 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7864 - loss: 1511.8748 - val_RMSE: 38.7228 - val_loss: 1505.3118 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7760 - loss: 1509.1525 - val_RMSE: 38.7221 - val_loss: 1504.1284 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7733 - loss: 1508.0520 - val_RMSE: 38.7229 - val_loss: 1503.9752 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7738 - loss: 1508.0132 - val_RMSE: 38.7251 - val_loss: 1504.2043 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7701 - loss: 1507.5835 - val_RMSE: 38.7185 - val_loss: 1503.3129 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7716 - loss: 1507.5648 - val_RMSE: 38.7164 - val_loss: 1503.1106 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7676 - loss: 1507.1177 - val_RMSE: 38.7166 - val_loss: 1502.8873 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7738 - loss: 1507.5548 - val_RMSE: 38.7151 - val_loss: 1502.6533 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7697 - loss: 1507.0375 - val_RMSE: 38.7156 - val_loss: 1502.8741 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7717 - loss: 1507.1680 - val_RMSE: 38.7158 - val_loss: 1502.8152 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7686 - loss: 1506.8170 - val_RMSE: 38.7149 - val_loss: 1502.3252 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7709 - loss: 1506.8314 - val_RMSE: 38.7137 - val_loss: 1502.1184 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7728 - loss: 1506.8254 - val_RMSE: 38.7131 - val_loss: 1501.9169 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7694 - loss: 1506.5668 - val_RMSE: 38.7127 - val_loss: 1501.9113 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7718 - loss: 1506.6975 - val_RMSE: 38.7137 - val_loss: 1502.3365 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7687 - loss: 1506.4891 - val_RMSE: 38.7136 - val_loss: 1502.0699 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7701 - loss: 1506.5562 - val_RMSE: 38.7130 - val_loss: 1501.8527 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7700 - loss: 1506.3962 - val_RMSE: 38.7123 - val_loss: 1501.7814 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 10ms/step - RMSE: 38.7699 - loss: 1506.3728 - val_RMSE: 38.7127 - val_loss: 1501.7281 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 20:09:20,682] Trial 7 finished with value: 38.710269927978516 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'silu', 'reg': 0.03866576727475806, 'dropout_rate': 0.37177793999597164}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 27s 7ms/step - RMSE: 61.1209 - loss: 3902.0278 - val_RMSE: 38.6977 - val_loss: 1498.7308 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.4198 - loss: 1555.1478 - val_RMSE: 38.6921 - val_loss: 1498.2742 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.3436 - loss: 1549.0868 - val_RMSE: 38.6923 - val_loss: 1498.1431 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3325 - loss: 1548.0499 - val_RMSE: 38.6910 - val_loss: 1497.8624 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3114 - loss: 1546.2186 - val_RMSE: 38.6920 - val_loss: 1497.8009 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3005 - loss: 1545.2455 - val_RMSE: 38.6901 - val_loss: 1497.5774 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2899 - loss: 1544.3424 - val_RMSE: 38.6894 - val_loss: 1497.4891 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2768 - loss: 1543.2798 - val_RMSE: 38.6902 - val_loss: 1497.5203 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2560 - loss: 1541.6190 - val_RMSE: 38.6900 - val_loss: 1497.4908 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2385 - loss: 1540.2305 - val_RMSE: 38.6893 - val_loss: 1497.4287 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2215 - loss: 1538.8915 - val_RMSE: 38.6894 - val_loss: 1497.4237 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2103 - loss: 1538.0042 - val_RMSE: 38.6880 - val_loss: 1497.3157 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2023 - loss: 1537.3796 - val_RMSE: 38.6885 - val_loss: 1497.3528 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1786 - loss: 1535.5190 - val_RMSE: 38.6880 - val_loss: 1497.3240 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1777 - loss: 1535.4551 - val_RMSE: 38.6873 - val_loss: 1497.2700 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1629 - loss: 1534.2987 - val_RMSE: 38.6869 - val_loss: 1497.2450 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1550 - loss: 1533.6946 - val_RMSE: 38.6869 - val_loss: 1497.2510 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1376 - loss: 1532.3247 - val_RMSE: 38.6865 - val_loss: 1497.2130 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1267 - loss: 1531.4701 - val_RMSE: 38.6856 - val_loss: 1497.1497 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1183 - loss: 1530.8130 - val_RMSE: 38.6859 - val_loss: 1497.1729 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1132 - loss: 1530.4182 - val_RMSE: 38.6866 - val_loss: 1497.2325 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0954 - loss: 1529.0322 - val_RMSE: 38.6846 - val_loss: 1497.0836 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0857 - loss: 1528.2788 - val_RMSE: 38.6863 - val_loss: 1497.2079 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0722 - loss: 1527.2186 - val_RMSE: 38.6859 - val_loss: 1497.1895 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.0694 - loss: 1527.0153 - val_RMSE: 38.6851 - val_loss: 1497.1338 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 27s 7ms/step - RMSE: 61.1169 - loss: 3901.1719 - val_RMSE: 38.7299 - val_loss: 1501.2250 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.3685 - loss: 1551.0978 - val_RMSE: 38.7243 - val_loss: 1500.7673 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.3001 - loss: 1545.6677 - val_RMSE: 38.7220 - val_loss: 1500.4536 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.2836 - loss: 1544.2148 - val_RMSE: 38.7220 - val_loss: 1500.2710 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2606 - loss: 1542.2256 - val_RMSE: 38.7228 - val_loss: 1500.1848 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2435 - loss: 1540.7621 - val_RMSE: 38.7218 - val_loss: 1500.0316 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2310 - loss: 1539.7133 - val_RMSE: 38.7199 - val_loss: 1499.8373 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2181 - loss: 1538.6545 - val_RMSE: 38.7215 - val_loss: 1499.9443 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1986 - loss: 1537.1063 - val_RMSE: 38.7201 - val_loss: 1499.8127 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1931 - loss: 1536.6681 - val_RMSE: 38.7186 - val_loss: 1499.6941 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1786 - loss: 1535.5302 - val_RMSE: 38.7184 - val_loss: 1499.6852 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1704 - loss: 1534.8855 - val_RMSE: 38.7184 - val_loss: 1499.6836 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1468 - loss: 1533.0361 - val_RMSE: 38.7167 - val_loss: 1499.5507 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1420 - loss: 1532.6622 - val_RMSE: 38.7175 - val_loss: 1499.6100 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1194 - loss: 1530.8885 - val_RMSE: 38.7176 - val_loss: 1499.6178 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1203 - loss: 1530.9589 - val_RMSE: 38.7181 - val_loss: 1499.6541 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1024 - loss: 1529.5548 - val_RMSE: 38.7178 - val_loss: 1499.6268 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0984 - loss: 1529.2516 - val_RMSE: 38.7168 - val_loss: 1499.5583 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0806 - loss: 1527.8538 - val_RMSE: 38.7145 - val_loss: 1499.3529 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0761 - loss: 1527.4706 - val_RMSE: 38.7143 - val_loss: 1499.3093 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0769 - loss: 1527.5074 - val_RMSE: 38.7143 - val_loss: 1499.2874 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0698 - loss: 1526.9324 - val_RMSE: 38.7139 - val_loss: 1499.2341 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.0740 - loss: 1527.2351 - val_RMSE: 38.7137 - val_loss: 1499.1998 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0821 - loss: 1527.8506 - val_RMSE: 38.7136 - val_loss: 1499.1775 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.0721 - loss: 1527.0569 - val_RMSE: 38.7139 - val_loss: 1499.1780 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 27s 7ms/step - RMSE: 61.0877 - loss: 3897.3528 - val_RMSE: 38.7163 - val_loss: 1500.1722 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3868 - loss: 1552.5505 - val_RMSE: 38.7195 - val_loss: 1500.3879 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3191 - loss: 1547.1432 - val_RMSE: 38.7178 - val_loss: 1500.1010 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.3036 - loss: 1545.7640 - val_RMSE: 38.7175 - val_loss: 1499.9011 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2857 - loss: 1544.1846 - val_RMSE: 38.7180 - val_loss: 1499.7913 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2750 - loss: 1543.2133 - val_RMSE: 38.7164 - val_loss: 1499.5973 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2562 - loss: 1541.6792 - val_RMSE: 38.7169 - val_loss: 1499.5927 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2457 - loss: 1540.8234 - val_RMSE: 38.7190 - val_loss: 1499.7355 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.2291 - loss: 1539.4998 - val_RMSE: 38.7172 - val_loss: 1499.5797 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.2189 - loss: 1538.6761 - val_RMSE: 38.7179 - val_loss: 1499.6318 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.2034 - loss: 1537.4597 - val_RMSE: 38.7166 - val_loss: 1499.5310 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 6ms/step - RMSE: 39.1938 - loss: 1536.7150 - val_RMSE: 38.7150 - val_loss: 1499.4080 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1823 - loss: 1535.8115 - val_RMSE: 38.7155 - val_loss: 1499.4434 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1630 - loss: 1534.2987 - val_RMSE: 38.7165 - val_loss: 1499.5266 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1458 - loss: 1532.9509 - val_RMSE: 38.7150 - val_loss: 1499.4058 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1392 - loss: 1532.4392 - val_RMSE: 38.7137 - val_loss: 1499.3174 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1346 - loss: 1532.0847 - val_RMSE: 38.7145 - val_loss: 1499.3782 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1174 - loss: 1530.7379 - val_RMSE: 38.7114 - val_loss: 1499.1495 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.1018 - loss: 1529.5291 - val_RMSE: 38.7135 - val_loss: 1499.3093 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0977 - loss: 1529.2113 - val_RMSE: 38.7136 - val_loss: 1499.3247 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0894 - loss: 1528.5648 - val_RMSE: 38.7141 - val_loss: 1499.3652 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0776 - loss: 1527.6477 - val_RMSE: 38.7120 - val_loss: 1499.2026 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0636 - loss: 1526.5555 - val_RMSE: 38.7128 - val_loss: 1499.2717 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0640 - loss: 1526.5757 - val_RMSE: 38.7080 - val_loss: 1498.8640 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 14s 5ms/step - RMSE: 39.0491 - loss: 1525.3817 - val_RMSE: 38.7079 - val_loss: 1498.8315 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 20:33:31,551] Trial 8 finished with value: 38.70230611165365 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'silu', 'reg': 0.0032489164477933394, 'dropout_rate': 0.4814024944816824}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 12ms/step - RMSE: 51.4691 - loss: 2813.0303 - val_RMSE: 38.7235 - val_loss: 1517.4996 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 39.0040 - loss: 1536.6215 - val_RMSE: 38.7503 - val_loss: 1512.3701 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8962 - loss: 1523.0516 - val_RMSE: 38.7205 - val_loss: 1507.7740 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8496 - loss: 1517.4974 - val_RMSE: 38.6963 - val_loss: 1504.5991 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8223 - loss: 1514.1731 - val_RMSE: 38.7003 - val_loss: 1503.9467 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8110 - loss: 1512.2694 - val_RMSE: 38.6946 - val_loss: 1502.6302 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8040 - loss: 1510.9678 - val_RMSE: 38.6917 - val_loss: 1501.7863 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7975 - loss: 1509.7816 - val_RMSE: 38.6893 - val_loss: 1500.8617 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7912 - loss: 1508.5957 - val_RMSE: 38.6903 - val_loss: 1500.2661 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7859 - loss: 1507.5677 - val_RMSE: 38.6894 - val_loss: 1499.7230 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7856 - loss: 1507.2086 - val_RMSE: 38.6885 - val_loss: 1499.5718 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7874 - loss: 1507.2902 - val_RMSE: 38.6893 - val_loss: 1499.5602 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7864 - loss: 1507.1146 - val_RMSE: 38.6900 - val_loss: 1499.6432 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7859 - loss: 1507.0447 - val_RMSE: 38.6875 - val_loss: 1499.2913 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7856 - loss: 1506.9264 - val_RMSE: 38.6874 - val_loss: 1499.1938 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7859 - loss: 1506.8579 - val_RMSE: 38.6902 - val_loss: 1499.4567 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7851 - loss: 1506.7717 - val_RMSE: 38.6876 - val_loss: 1499.1912 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7860 - loss: 1506.8245 - val_RMSE: 38.6871 - val_loss: 1498.9998 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7827 - loss: 1506.4755 - val_RMSE: 38.6865 - val_loss: 1499.0103 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7840 - loss: 1506.6088 - val_RMSE: 38.6871 - val_loss: 1498.9474 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7831 - loss: 1506.4626 - val_RMSE: 38.6861 - val_loss: 1498.9922 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7809 - loss: 1506.2688 - val_RMSE: 38.6863 - val_loss: 1498.8827 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7857 - loss: 1506.5446 - val_RMSE: 38.6860 - val_loss: 1498.8080 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7847 - loss: 1506.4434 - val_RMSE: 38.6857 - val_loss: 1498.7134 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7805 - loss: 1506.0798 - val_RMSE: 38.6848 - val_loss: 1498.5958 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 12ms/step - RMSE: 51.4111 - loss: 2806.1509 - val_RMSE: 38.7396 - val_loss: 1519.3008 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.9547 - loss: 1533.0541 - val_RMSE: 38.7394 - val_loss: 1511.6337 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8421 - loss: 1519.1315 - val_RMSE: 38.7240 - val_loss: 1508.8429 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7945 - loss: 1513.7356 - val_RMSE: 38.7302 - val_loss: 1507.1317 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7689 - loss: 1510.0515 - val_RMSE: 38.7255 - val_loss: 1506.1013 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7578 - loss: 1508.3307 - val_RMSE: 38.7252 - val_loss: 1505.1339 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7468 - loss: 1506.6418 - val_RMSE: 38.7224 - val_loss: 1504.1670 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7428 - loss: 1505.5044 - val_RMSE: 38.7185 - val_loss: 1502.8888 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7390 - loss: 1504.3309 - val_RMSE: 38.7176 - val_loss: 1502.1860 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7386 - loss: 1503.7543 - val_RMSE: 38.7167 - val_loss: 1501.8513 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7377 - loss: 1503.4658 - val_RMSE: 38.7186 - val_loss: 1501.9520 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7370 - loss: 1503.3743 - val_RMSE: 38.7165 - val_loss: 1501.5751 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7363 - loss: 1503.2263 - val_RMSE: 38.7191 - val_loss: 1501.6782 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7369 - loss: 1503.2773 - val_RMSE: 38.7176 - val_loss: 1501.6855 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7351 - loss: 1503.0836 - val_RMSE: 38.7167 - val_loss: 1501.6775 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7352 - loss: 1503.0073 - val_RMSE: 38.7156 - val_loss: 1501.3649 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7375 - loss: 1503.0364 - val_RMSE: 38.7189 - val_loss: 1501.4536 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7302 - loss: 1502.4960 - val_RMSE: 38.7179 - val_loss: 1501.4189 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7340 - loss: 1502.7595 - val_RMSE: 38.7202 - val_loss: 1501.5583 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7334 - loss: 1502.6182 - val_RMSE: 38.7164 - val_loss: 1501.3289 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7323 - loss: 1502.5583 - val_RMSE: 38.7170 - val_loss: 1501.1899 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7317 - loss: 1502.4124 - val_RMSE: 38.7171 - val_loss: 1501.1547 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7316 - loss: 1502.3152 - val_RMSE: 38.7164 - val_loss: 1501.0695 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7316 - loss: 1502.2948 - val_RMSE: 38.7190 - val_loss: 1501.3092 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7293 - loss: 1502.1042 - val_RMSE: 38.7187 - val_loss: 1501.1826 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 11ms/step - RMSE: 51.4084 - loss: 2805.2761 - val_RMSE: 38.7452 - val_loss: 1519.3495 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.9788 - loss: 1534.6334 - val_RMSE: 38.7431 - val_loss: 1511.3177 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8782 - loss: 1521.4064 - val_RMSE: 38.7219 - val_loss: 1508.1727 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8237 - loss: 1515.6943 - val_RMSE: 38.7163 - val_loss: 1506.0165 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.8020 - loss: 1512.4397 - val_RMSE: 38.7247 - val_loss: 1505.6578 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7893 - loss: 1510.4745 - val_RMSE: 38.7166 - val_loss: 1504.1571 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7827 - loss: 1509.2094 - val_RMSE: 38.7275 - val_loss: 1504.4454 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7702 - loss: 1507.6050 - val_RMSE: 38.7200 - val_loss: 1503.1456 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7669 - loss: 1506.6617 - val_RMSE: 38.7115 - val_loss: 1501.7999 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7634 - loss: 1505.8108 - val_RMSE: 38.7134 - val_loss: 1501.6399 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7605 - loss: 1505.3065 - val_RMSE: 38.7114 - val_loss: 1501.3373 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7616 - loss: 1505.2423 - val_RMSE: 38.7121 - val_loss: 1501.3223 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7588 - loss: 1504.9919 - val_RMSE: 38.7111 - val_loss: 1501.2677 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7652 - loss: 1505.4694 - val_RMSE: 38.7097 - val_loss: 1501.0770 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7603 - loss: 1505.0127 - val_RMSE: 38.7105 - val_loss: 1501.0117 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 25s 9ms/step - RMSE: 38.7616 - loss: 1505.0355 - val_RMSE: 38.7092 - val_loss: 1500.8599 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7586 - loss: 1504.7434 - val_RMSE: 38.7085 - val_loss: 1500.8516 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7612 - loss: 1504.9330 - val_RMSE: 38.7078 - val_loss: 1500.6390 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7636 - loss: 1505.0391 - val_RMSE: 38.7111 - val_loss: 1500.8312 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7594 - loss: 1504.6670 - val_RMSE: 38.7079 - val_loss: 1500.6687 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7616 - loss: 1504.7908 - val_RMSE: 38.7088 - val_loss: 1500.7115 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7595 - loss: 1504.6287 - val_RMSE: 38.7071 - val_loss: 1500.4790 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7611 - loss: 1504.7207 - val_RMSE: 38.7093 - val_loss: 1500.6246 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7601 - loss: 1504.5687 - val_RMSE: 38.7085 - val_loss: 1500.5195 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 24s 9ms/step - RMSE: 38.7592 - loss: 1504.5013 - val_RMSE: 38.7053 - val_loss: 1500.2549 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 21:10:13,385] Trial 9 finished with value: 38.702955881754555 and parameters: {'units': 1024, 'last_layer': 2, 'activation': 'relu', 'reg': 0.013130181575743027, 'dropout_rate': 0.3554543298268401}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.1922 - loss: 3086.4292 - val_RMSE: 38.7158 - val_loss: 1499.1244 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0593 - loss: 1525.8591 - val_RMSE: 38.6948 - val_loss: 1497.5507 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9279 - loss: 1515.6602 - val_RMSE: 38.6919 - val_loss: 1497.3785 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8905 - loss: 1512.8024 - val_RMSE: 38.6877 - val_loss: 1497.1183 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8687 - loss: 1511.1748 - val_RMSE: 38.6868 - val_loss: 1497.1005 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8609 - loss: 1510.6158 - val_RMSE: 38.6852 - val_loss: 1497.0114 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8567 - loss: 1510.3160 - val_RMSE: 38.6847 - val_loss: 1496.9912 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8530 - loss: 1510.0344 - val_RMSE: 38.6842 - val_loss: 1496.9512 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8508 - loss: 1509.8704 - val_RMSE: 38.6842 - val_loss: 1496.9491 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8524 - loss: 1509.9918 - val_RMSE: 38.6838 - val_loss: 1496.9176 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8510 - loss: 1509.8839 - val_RMSE: 38.6831 - val_loss: 1496.8740 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8456 - loss: 1509.4684 - val_RMSE: 38.6831 - val_loss: 1496.8713 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8522 - loss: 1509.9856 - val_RMSE: 38.6828 - val_loss: 1496.8525 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8437 - loss: 1509.3285 - val_RMSE: 38.6823 - val_loss: 1496.8154 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8455 - loss: 1509.4724 - val_RMSE: 38.6819 - val_loss: 1496.7974 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8390 - loss: 1508.9779 - val_RMSE: 38.6824 - val_loss: 1496.8373 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8423 - loss: 1509.2363 - val_RMSE: 38.6816 - val_loss: 1496.7876 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8362 - loss: 1508.7734 - val_RMSE: 38.6815 - val_loss: 1496.7871 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8393 - loss: 1509.0171 - val_RMSE: 38.6809 - val_loss: 1496.7478 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8401 - loss: 1509.0957 - val_RMSE: 38.6809 - val_loss: 1496.7489 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8337 - loss: 1508.6005 - val_RMSE: 38.6807 - val_loss: 1496.7427 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8368 - loss: 1508.8475 - val_RMSE: 38.6807 - val_loss: 1496.7518 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8340 - loss: 1508.6398 - val_RMSE: 38.6811 - val_loss: 1496.7921 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8302 - loss: 1508.3550 - val_RMSE: 38.6803 - val_loss: 1496.7354 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8317 - loss: 1508.4766 - val_RMSE: 38.6804 - val_loss: 1496.7581 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 10ms/step - RMSE: 54.1436 - loss: 3080.3943 - val_RMSE: 38.7303 - val_loss: 1500.2467 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0116 - loss: 1522.1278 - val_RMSE: 38.7203 - val_loss: 1499.5223 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8858 - loss: 1512.3817 - val_RMSE: 38.7207 - val_loss: 1499.6080 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8341 - loss: 1508.4161 - val_RMSE: 38.7191 - val_loss: 1499.5466 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8198 - loss: 1507.3671 - val_RMSE: 38.7180 - val_loss: 1499.5077 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8091 - loss: 1506.5831 - val_RMSE: 38.7178 - val_loss: 1499.5234 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8052 - loss: 1506.3085 - val_RMSE: 38.7166 - val_loss: 1499.4503 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8063 - loss: 1506.4032 - val_RMSE: 38.7163 - val_loss: 1499.4252 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8051 - loss: 1506.3087 - val_RMSE: 38.7159 - val_loss: 1499.3994 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8004 - loss: 1505.9484 - val_RMSE: 38.7166 - val_loss: 1499.4600 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7960 - loss: 1505.6138 - val_RMSE: 38.7155 - val_loss: 1499.3750 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7963 - loss: 1505.6345 - val_RMSE: 38.7158 - val_loss: 1499.4049 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7946 - loss: 1505.5087 - val_RMSE: 38.7156 - val_loss: 1499.3896 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7929 - loss: 1505.3820 - val_RMSE: 38.7163 - val_loss: 1499.4552 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7978 - loss: 1505.7725 - val_RMSE: 38.7165 - val_loss: 1499.4700 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7894 - loss: 1505.1289 - val_RMSE: 38.7151 - val_loss: 1499.3717 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7985 - loss: 1505.8342 - val_RMSE: 38.7150 - val_loss: 1499.3671 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7911 - loss: 1505.2671 - val_RMSE: 38.7168 - val_loss: 1499.5156 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7890 - loss: 1505.1118 - val_RMSE: 38.7155 - val_loss: 1499.4302 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7938 - loss: 1505.4945 - val_RMSE: 38.7148 - val_loss: 1499.3796 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7852 - loss: 1504.8323 - val_RMSE: 38.7155 - val_loss: 1499.4423 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7886 - loss: 1505.1125 - val_RMSE: 38.7149 - val_loss: 1499.4056 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7758 - loss: 1504.1195 - val_RMSE: 38.7083 - val_loss: 1498.8820 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7787 - loss: 1504.3354 - val_RMSE: 38.7083 - val_loss: 1498.8636 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7756 - loss: 1504.0765 - val_RMSE: 38.7084 - val_loss: 1498.8582 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.1229 - loss: 3077.8313 - val_RMSE: 38.7183 - val_loss: 1499.3210 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0365 - loss: 1524.0742 - val_RMSE: 38.7155 - val_loss: 1499.1550 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9029 - loss: 1513.7166 - val_RMSE: 38.7126 - val_loss: 1498.9885 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8642 - loss: 1510.7618 - val_RMSE: 38.7109 - val_loss: 1498.9165 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8493 - loss: 1509.6608 - val_RMSE: 38.7089 - val_loss: 1498.8103 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8330 - loss: 1508.4442 - val_RMSE: 38.7079 - val_loss: 1498.7695 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8338 - loss: 1508.5392 - val_RMSE: 38.7067 - val_loss: 1498.6917 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8303 - loss: 1508.2820 - val_RMSE: 38.7054 - val_loss: 1498.5994 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8276 - loss: 1508.0770 - val_RMSE: 38.7066 - val_loss: 1498.6935 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8292 - loss: 1508.2025 - val_RMSE: 38.7056 - val_loss: 1498.6188 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8237 - loss: 1507.7791 - val_RMSE: 38.7057 - val_loss: 1498.6217 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8252 - loss: 1507.8901 - val_RMSE: 38.7043 - val_loss: 1498.5242 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8211 - loss: 1507.5760 - val_RMSE: 38.7046 - val_loss: 1498.5520 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8231 - loss: 1507.7375 - val_RMSE: 38.7047 - val_loss: 1498.5641 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8180 - loss: 1507.3539 - val_RMSE: 38.7037 - val_loss: 1498.4885 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8185 - loss: 1507.3956 - val_RMSE: 38.7051 - val_loss: 1498.6013 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8159 - loss: 1507.1997 - val_RMSE: 38.7038 - val_loss: 1498.5093 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8164 - loss: 1507.2449 - val_RMSE: 38.7041 - val_loss: 1498.5470 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8155 - loss: 1507.1851 - val_RMSE: 38.7036 - val_loss: 1498.5156 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8159 - loss: 1507.2233 - val_RMSE: 38.7038 - val_loss: 1498.5430 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8013 - loss: 1506.0947 - val_RMSE: 38.7021 - val_loss: 1498.3947 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8046 - loss: 1506.3385 - val_RMSE: 38.7021 - val_loss: 1498.3796 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8066 - loss: 1506.4810 - val_RMSE: 38.7019 - val_loss: 1498.3558 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8034 - loss: 1506.2188 - val_RMSE: 38.7019 - val_loss: 1498.3466 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8010 - loss: 1506.0227 - val_RMSE: 38.7022 - val_loss: 1498.3558 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 21:41:02,537] Trial 10 finished with value: 38.69698842366537 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.00011431075362835225, 'dropout_rate': 0.42901303537903984}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 9ms/step - RMSE: 54.1763 - loss: 3084.7754 - val_RMSE: 38.7122 - val_loss: 1498.8872 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0579 - loss: 1525.7869 - val_RMSE: 38.6963 - val_loss: 1497.7144 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9301 - loss: 1515.8787 - val_RMSE: 38.6895 - val_loss: 1497.2484 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8883 - loss: 1512.6915 - val_RMSE: 38.6871 - val_loss: 1497.1299 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8667 - loss: 1511.0721 - val_RMSE: 38.6863 - val_loss: 1497.1219 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8596 - loss: 1510.5698 - val_RMSE: 38.6845 - val_loss: 1497.0142 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8551 - loss: 1510.2463 - val_RMSE: 38.6841 - val_loss: 1496.9896 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8522 - loss: 1510.0212 - val_RMSE: 38.6837 - val_loss: 1496.9536 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8491 - loss: 1509.7754 - val_RMSE: 38.6842 - val_loss: 1496.9885 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8509 - loss: 1509.9172 - val_RMSE: 38.6830 - val_loss: 1496.8949 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8494 - loss: 1509.7966 - val_RMSE: 38.6829 - val_loss: 1496.8917 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8449 - loss: 1509.4520 - val_RMSE: 38.6821 - val_loss: 1496.8319 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8514 - loss: 1509.9597 - val_RMSE: 38.6825 - val_loss: 1496.8707 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8418 - loss: 1509.2164 - val_RMSE: 38.6818 - val_loss: 1496.8170 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8447 - loss: 1509.4473 - val_RMSE: 38.6820 - val_loss: 1496.8372 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8360 - loss: 1508.7780 - val_RMSE: 38.6815 - val_loss: 1496.8143 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8408 - loss: 1509.1644 - val_RMSE: 38.6812 - val_loss: 1496.7994 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8347 - loss: 1508.6938 - val_RMSE: 38.6811 - val_loss: 1496.7985 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8380 - loss: 1508.9614 - val_RMSE: 38.6806 - val_loss: 1496.7693 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8388 - loss: 1509.0323 - val_RMSE: 38.6807 - val_loss: 1496.7883 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8322 - loss: 1508.5310 - val_RMSE: 38.6801 - val_loss: 1496.7471 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8353 - loss: 1508.7817 - val_RMSE: 38.6809 - val_loss: 1496.8159 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8327 - loss: 1508.5896 - val_RMSE: 38.6803 - val_loss: 1496.7786 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8292 - loss: 1508.3246 - val_RMSE: 38.6802 - val_loss: 1496.7853 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8297 - loss: 1508.3719 - val_RMSE: 38.6802 - val_loss: 1496.7921 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 54.1315 - loss: 3079.1267 - val_RMSE: 38.7343 - val_loss: 1500.5990 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0113 - loss: 1522.1523 - val_RMSE: 38.7212 - val_loss: 1499.6415 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8836 - loss: 1512.2633 - val_RMSE: 38.7194 - val_loss: 1499.5706 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8322 - loss: 1508.3304 - val_RMSE: 38.7190 - val_loss: 1499.5975 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8196 - loss: 1507.4176 - val_RMSE: 38.7179 - val_loss: 1499.5709 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8081 - loss: 1506.5762 - val_RMSE: 38.7179 - val_loss: 1499.6046 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8043 - loss: 1506.3021 - val_RMSE: 38.7167 - val_loss: 1499.5193 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8049 - loss: 1506.3502 - val_RMSE: 38.7165 - val_loss: 1499.4906 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8040 - loss: 1506.2754 - val_RMSE: 38.7158 - val_loss: 1499.4307 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7997 - loss: 1505.9366 - val_RMSE: 38.7163 - val_loss: 1499.4681 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7959 - loss: 1505.6387 - val_RMSE: 38.7157 - val_loss: 1499.4236 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7956 - loss: 1505.6202 - val_RMSE: 38.7166 - val_loss: 1499.4957 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7939 - loss: 1505.4872 - val_RMSE: 38.7155 - val_loss: 1499.4136 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7920 - loss: 1505.3468 - val_RMSE: 38.7160 - val_loss: 1499.4601 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7968 - loss: 1505.7231 - val_RMSE: 38.7159 - val_loss: 1499.4611 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7884 - loss: 1505.0842 - val_RMSE: 38.7145 - val_loss: 1499.3617 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7973 - loss: 1505.7848 - val_RMSE: 38.7144 - val_loss: 1499.3713 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7897 - loss: 1505.2036 - val_RMSE: 38.7165 - val_loss: 1499.5397 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7874 - loss: 1505.0391 - val_RMSE: 38.7155 - val_loss: 1499.4724 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7922 - loss: 1505.4133 - val_RMSE: 38.7143 - val_loss: 1499.3831 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7838 - loss: 1504.7717 - val_RMSE: 38.7160 - val_loss: 1499.5247 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7827 - loss: 1504.6860 - val_RMSE: 38.7090 - val_loss: 1498.9630 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7739 - loss: 1503.9841 - val_RMSE: 38.7090 - val_loss: 1498.9510 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7759 - loss: 1504.1246 - val_RMSE: 38.7091 - val_loss: 1498.9390 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7736 - loss: 1503.9362 - val_RMSE: 38.7090 - val_loss: 1498.9218 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 9ms/step - RMSE: 54.1161 - loss: 3077.0850 - val_RMSE: 38.7296 - val_loss: 1500.2301 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0351 - loss: 1524.0101 - val_RMSE: 38.7174 - val_loss: 1499.3510 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.9040 - loss: 1513.8521 - val_RMSE: 38.7120 - val_loss: 1499.0028 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8647 - loss: 1510.8666 - val_RMSE: 38.7098 - val_loss: 1498.9033 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8472 - loss: 1509.5715 - val_RMSE: 38.7080 - val_loss: 1498.8170 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8318 - loss: 1508.4305 - val_RMSE: 38.7075 - val_loss: 1498.8112 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8333 - loss: 1508.5674 - val_RMSE: 38.7064 - val_loss: 1498.7351 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8289 - loss: 1508.2319 - val_RMSE: 38.7056 - val_loss: 1498.6703 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8265 - loss: 1508.0457 - val_RMSE: 38.7068 - val_loss: 1498.7614 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8273 - loss: 1508.0999 - val_RMSE: 38.7049 - val_loss: 1498.6068 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8230 - loss: 1507.7583 - val_RMSE: 38.7052 - val_loss: 1498.6263 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8242 - loss: 1507.8521 - val_RMSE: 38.7044 - val_loss: 1498.5568 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8205 - loss: 1507.5604 - val_RMSE: 38.7046 - val_loss: 1498.5757 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8217 - loss: 1507.6604 - val_RMSE: 38.7041 - val_loss: 1498.5426 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8172 - loss: 1507.3151 - val_RMSE: 38.7041 - val_loss: 1498.5443 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8179 - loss: 1507.3704 - val_RMSE: 38.7048 - val_loss: 1498.6072 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8159 - loss: 1507.2201 - val_RMSE: 38.7037 - val_loss: 1498.5337 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8161 - loss: 1507.2452 - val_RMSE: 38.7041 - val_loss: 1498.5703 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8152 - loss: 1507.1847 - val_RMSE: 38.7041 - val_loss: 1498.5776 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8151 - loss: 1507.1897 - val_RMSE: 38.7039 - val_loss: 1498.5745 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8042 - loss: 1506.3496 - val_RMSE: 38.7029 - val_loss: 1498.5033 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8093 - loss: 1506.7581 - val_RMSE: 38.7029 - val_loss: 1498.5149 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8109 - loss: 1506.8901 - val_RMSE: 38.7049 - val_loss: 1498.6849 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8066 - loss: 1506.5692 - val_RMSE: 38.7028 - val_loss: 1498.5244 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8050 - loss: 1506.4512 - val_RMSE: 38.7046 - val_loss: 1498.6814 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 87s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 22:11:45,903] Trial 11 finished with value: 38.69793955485026 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.00013595283372657457, 'dropout_rate': 0.42664745909121726}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 9ms/step - RMSE: 54.1909 - loss: 3086.3071 - val_RMSE: 38.7219 - val_loss: 1499.5839 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0627 - loss: 1526.1074 - val_RMSE: 38.7008 - val_loss: 1497.9996 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9299 - loss: 1515.8036 - val_RMSE: 38.6926 - val_loss: 1497.4127 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8894 - loss: 1512.7048 - val_RMSE: 38.6882 - val_loss: 1497.1348 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8687 - loss: 1511.1492 - val_RMSE: 38.6869 - val_loss: 1497.0865 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8607 - loss: 1510.5780 - val_RMSE: 38.6852 - val_loss: 1496.9894 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8561 - loss: 1510.2482 - val_RMSE: 38.6846 - val_loss: 1496.9614 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8528 - loss: 1510.0039 - val_RMSE: 38.6841 - val_loss: 1496.9238 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8510 - loss: 1509.8691 - val_RMSE: 38.6844 - val_loss: 1496.9553 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8513 - loss: 1509.8944 - val_RMSE: 38.6832 - val_loss: 1496.8602 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8503 - loss: 1509.8154 - val_RMSE: 38.6829 - val_loss: 1496.8400 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8455 - loss: 1509.4471 - val_RMSE: 38.6823 - val_loss: 1496.8037 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8515 - loss: 1509.9181 - val_RMSE: 38.6827 - val_loss: 1496.8345 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8427 - loss: 1509.2408 - val_RMSE: 38.6825 - val_loss: 1496.8235 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8455 - loss: 1509.4712 - val_RMSE: 38.6821 - val_loss: 1496.8068 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8378 - loss: 1508.8770 - val_RMSE: 38.6818 - val_loss: 1496.7957 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8417 - loss: 1509.1940 - val_RMSE: 38.6819 - val_loss: 1496.8142 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8355 - loss: 1508.7183 - val_RMSE: 38.6814 - val_loss: 1496.7753 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8384 - loss: 1508.9492 - val_RMSE: 38.6812 - val_loss: 1496.7728 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8389 - loss: 1508.9973 - val_RMSE: 38.6809 - val_loss: 1496.7520 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8337 - loss: 1508.5984 - val_RMSE: 38.6815 - val_loss: 1496.8038 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8367 - loss: 1508.8386 - val_RMSE: 38.6809 - val_loss: 1496.7734 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8330 - loss: 1508.5625 - val_RMSE: 38.6815 - val_loss: 1496.8279 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8283 - loss: 1508.2123 - val_RMSE: 38.6804 - val_loss: 1496.7528 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8310 - loss: 1508.4293 - val_RMSE: 38.6806 - val_loss: 1496.7800 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 54.1367 - loss: 3079.6467 - val_RMSE: 38.7284 - val_loss: 1500.0900 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0146 - loss: 1522.3511 - val_RMSE: 38.7226 - val_loss: 1499.6888 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8868 - loss: 1512.4427 - val_RMSE: 38.7224 - val_loss: 1499.7280 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8360 - loss: 1508.5499 - val_RMSE: 38.7198 - val_loss: 1499.5858 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8210 - loss: 1507.4440 - val_RMSE: 38.7187 - val_loss: 1499.5490 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8095 - loss: 1506.6018 - val_RMSE: 38.7178 - val_loss: 1499.5184 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8057 - loss: 1506.3345 - val_RMSE: 38.7168 - val_loss: 1499.4583 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8066 - loss: 1506.4176 - val_RMSE: 38.7169 - val_loss: 1499.4635 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8046 - loss: 1506.2675 - val_RMSE: 38.7159 - val_loss: 1499.3970 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8005 - loss: 1505.9498 - val_RMSE: 38.7159 - val_loss: 1499.3976 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7961 - loss: 1505.6115 - val_RMSE: 38.7156 - val_loss: 1499.3722 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7963 - loss: 1505.6316 - val_RMSE: 38.7160 - val_loss: 1499.4141 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7947 - loss: 1505.5127 - val_RMSE: 38.7152 - val_loss: 1499.3586 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7917 - loss: 1505.2908 - val_RMSE: 38.7159 - val_loss: 1499.4211 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7969 - loss: 1505.6969 - val_RMSE: 38.7156 - val_loss: 1499.3998 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7891 - loss: 1505.0994 - val_RMSE: 38.7148 - val_loss: 1499.3491 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7971 - loss: 1505.7301 - val_RMSE: 38.7149 - val_loss: 1499.3639 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7901 - loss: 1505.1948 - val_RMSE: 38.7159 - val_loss: 1499.4548 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7880 - loss: 1505.0436 - val_RMSE: 38.7151 - val_loss: 1499.4009 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7919 - loss: 1505.3596 - val_RMSE: 38.7139 - val_loss: 1499.3234 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7841 - loss: 1504.7611 - val_RMSE: 38.7154 - val_loss: 1499.4457 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7879 - loss: 1505.0723 - val_RMSE: 38.7150 - val_loss: 1499.4261 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7787 - loss: 1504.3616 - val_RMSE: 38.7134 - val_loss: 1499.3108 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7829 - loss: 1504.6980 - val_RMSE: 38.7155 - val_loss: 1499.4840 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7800 - loss: 1504.4830 - val_RMSE: 38.7150 - val_loss: 1499.4509 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 88s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 10ms/step - RMSE: 54.1216 - loss: 3077.6299 - val_RMSE: 38.7213 - val_loss: 1499.5381 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 39.0330 - loss: 1523.7867 - val_RMSE: 38.7167 - val_loss: 1499.2307 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.9011 - loss: 1513.5540 - val_RMSE: 38.7164 - val_loss: 1499.2573 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8663 - loss: 1510.9023 - val_RMSE: 38.7096 - val_loss: 1498.7858 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8485 - loss: 1509.5791 - val_RMSE: 38.7089 - val_loss: 1498.7866 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8327 - loss: 1508.3979 - val_RMSE: 38.7080 - val_loss: 1498.7520 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8338 - loss: 1508.5079 - val_RMSE: 38.7068 - val_loss: 1498.6699 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8295 - loss: 1508.1863 - val_RMSE: 38.7056 - val_loss: 1498.5801 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8265 - loss: 1507.9617 - val_RMSE: 38.7069 - val_loss: 1498.6877 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8288 - loss: 1508.1416 - val_RMSE: 38.7049 - val_loss: 1498.5419 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8235 - loss: 1507.7312 - val_RMSE: 38.7052 - val_loss: 1498.5679 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8241 - loss: 1507.7897 - val_RMSE: 38.7044 - val_loss: 1498.5088 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8202 - loss: 1507.4939 - val_RMSE: 38.7045 - val_loss: 1498.5292 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8221 - loss: 1507.6453 - val_RMSE: 38.7043 - val_loss: 1498.5215 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8177 - loss: 1507.3112 - val_RMSE: 38.7035 - val_loss: 1498.4585 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8180 - loss: 1507.3434 - val_RMSE: 38.7045 - val_loss: 1498.5477 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8152 - loss: 1507.1343 - val_RMSE: 38.7039 - val_loss: 1498.5127 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8156 - loss: 1507.1738 - val_RMSE: 38.7040 - val_loss: 1498.5319 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8153 - loss: 1507.1606 - val_RMSE: 38.7039 - val_loss: 1498.5289 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8149 - loss: 1507.1396 - val_RMSE: 38.7036 - val_loss: 1498.5215 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8005 - loss: 1506.0233 - val_RMSE: 38.7016 - val_loss: 1498.3496 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8047 - loss: 1506.3372 - val_RMSE: 38.7015 - val_loss: 1498.3258 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8054 - loss: 1506.3799 - val_RMSE: 38.7018 - val_loss: 1498.3412 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8020 - loss: 1506.1045 - val_RMSE: 38.7020 - val_loss: 1498.3456 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8001 - loss: 1505.9463 - val_RMSE: 38.7021 - val_loss: 1498.3409 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 22:42:27,728] Trial 12 finished with value: 38.6991933186849 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.00010782612784245901, 'dropout_rate': 0.4285140854773215}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 54.1502 - loss: 3083.9399 - val_RMSE: 38.7238 - val_loss: 1502.3201 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0558 - loss: 1528.2102 - val_RMSE: 38.7006 - val_loss: 1500.6951 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9289 - loss: 1518.4215 - val_RMSE: 38.6945 - val_loss: 1500.1030 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8872 - loss: 1514.9609 - val_RMSE: 38.6906 - val_loss: 1499.3574 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8636 - loss: 1512.6492 - val_RMSE: 38.6900 - val_loss: 1498.7699 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8584 - loss: 1511.7175 - val_RMSE: 38.6868 - val_loss: 1498.0895 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8532 - loss: 1510.9292 - val_RMSE: 38.6867 - val_loss: 1497.8519 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8509 - loss: 1510.5573 - val_RMSE: 38.6867 - val_loss: 1497.7715 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8484 - loss: 1510.3207 - val_RMSE: 38.6872 - val_loss: 1497.8181 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8505 - loss: 1510.4972 - val_RMSE: 38.6856 - val_loss: 1497.6705 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8495 - loss: 1510.3989 - val_RMSE: 38.6861 - val_loss: 1497.7012 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8441 - loss: 1509.9744 - val_RMSE: 38.6853 - val_loss: 1497.6494 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8522 - loss: 1510.5988 - val_RMSE: 38.6851 - val_loss: 1497.6385 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8436 - loss: 1509.9357 - val_RMSE: 38.6844 - val_loss: 1497.5726 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8468 - loss: 1510.1835 - val_RMSE: 38.6846 - val_loss: 1497.6105 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8382 - loss: 1509.5328 - val_RMSE: 38.6849 - val_loss: 1497.6105 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8420 - loss: 1509.8026 - val_RMSE: 38.6848 - val_loss: 1497.5953 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8376 - loss: 1509.4633 - val_RMSE: 38.6846 - val_loss: 1497.5886 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8404 - loss: 1509.6797 - val_RMSE: 38.6843 - val_loss: 1497.5726 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.8381 - loss: 1509.4515 - val_RMSE: 38.6804 - val_loss: 1497.1021 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8307 - loss: 1508.7223 - val_RMSE: 38.6801 - val_loss: 1496.9619 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8349 - loss: 1508.9396 - val_RMSE: 38.6800 - val_loss: 1496.8700 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8315 - loss: 1508.6008 - val_RMSE: 38.6799 - val_loss: 1496.8066 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8273 - loss: 1508.2185 - val_RMSE: 38.6799 - val_loss: 1496.7548 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.8304 - loss: 1508.4109 - val_RMSE: 38.6798 - val_loss: 1496.7188 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.1101 - loss: 3078.8408 - val_RMSE: 38.7350 - val_loss: 1503.1906 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0073 - loss: 1524.4459 - val_RMSE: 38.7260 - val_loss: 1502.7195 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8818 - loss: 1514.7937 - val_RMSE: 38.7244 - val_loss: 1502.4152 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8312 - loss: 1510.6010 - val_RMSE: 38.7204 - val_loss: 1501.6827 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8173 - loss: 1509.0682 - val_RMSE: 38.7194 - val_loss: 1501.1160 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.8059 - loss: 1507.6998 - val_RMSE: 38.7174 - val_loss: 1500.5309 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8009 - loss: 1506.9116 - val_RMSE: 38.7179 - val_loss: 1500.3008 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8024 - loss: 1506.8149 - val_RMSE: 38.7184 - val_loss: 1500.2615 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8020 - loss: 1506.7490 - val_RMSE: 38.7167 - val_loss: 1500.1276 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 18s 7ms/step - RMSE: 38.7985 - loss: 1506.4796 - val_RMSE: 38.7174 - val_loss: 1500.1863 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7952 - loss: 1506.2120 - val_RMSE: 38.7168 - val_loss: 1500.1390 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7960 - loss: 1506.2762 - val_RMSE: 38.7179 - val_loss: 1500.2194 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7935 - loss: 1506.0697 - val_RMSE: 38.7177 - val_loss: 1500.1886 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7912 - loss: 1505.8981 - val_RMSE: 38.7173 - val_loss: 1500.1777 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7929 - loss: 1505.9857 - val_RMSE: 38.7108 - val_loss: 1499.4858 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7831 - loss: 1505.0587 - val_RMSE: 38.7106 - val_loss: 1499.3463 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7908 - loss: 1505.5411 - val_RMSE: 38.7104 - val_loss: 1499.2404 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7851 - loss: 1505.0199 - val_RMSE: 38.7103 - val_loss: 1499.1709 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7834 - loss: 1504.8270 - val_RMSE: 38.7102 - val_loss: 1499.1156 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7876 - loss: 1505.1053 - val_RMSE: 38.7101 - val_loss: 1499.0645 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7805 - loss: 1504.5116 - val_RMSE: 38.7100 - val_loss: 1499.0281 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7879 - loss: 1505.0587 - val_RMSE: 38.7100 - val_loss: 1499.0044 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7777 - loss: 1504.2455 - val_RMSE: 38.7097 - val_loss: 1498.9623 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7810 - loss: 1504.4775 - val_RMSE: 38.7098 - val_loss: 1498.9447 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7811 - loss: 1504.4702 - val_RMSE: 38.7099 - val_loss: 1498.9431 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 9ms/step - RMSE: 54.0903 - loss: 3076.3555 - val_RMSE: 38.7258 - val_loss: 1502.4470 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0317 - loss: 1526.3021 - val_RMSE: 38.7141 - val_loss: 1501.6918 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8995 - loss: 1516.0549 - val_RMSE: 38.7132 - val_loss: 1501.3813 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8609 - loss: 1512.7275 - val_RMSE: 38.7093 - val_loss: 1500.5996 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8458 - loss: 1511.0698 - val_RMSE: 38.7084 - val_loss: 1500.0768 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8304 - loss: 1509.4550 - val_RMSE: 38.7083 - val_loss: 1499.7263 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8311 - loss: 1509.1948 - val_RMSE: 38.7080 - val_loss: 1499.5142 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8273 - loss: 1508.7548 - val_RMSE: 38.7072 - val_loss: 1499.4109 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8249 - loss: 1508.5374 - val_RMSE: 38.7077 - val_loss: 1499.4397 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8273 - loss: 1508.7076 - val_RMSE: 38.7063 - val_loss: 1499.3219 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8222 - loss: 1508.2985 - val_RMSE: 38.7066 - val_loss: 1499.3531 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8228 - loss: 1508.3646 - val_RMSE: 38.7061 - val_loss: 1499.3060 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8187 - loss: 1508.0330 - val_RMSE: 38.7056 - val_loss: 1499.2594 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8215 - loss: 1508.2578 - val_RMSE: 38.7055 - val_loss: 1499.2800 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8172 - loss: 1507.9354 - val_RMSE: 38.7065 - val_loss: 1499.3347 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8195 - loss: 1508.0999 - val_RMSE: 38.7066 - val_loss: 1499.3313 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8181 - loss: 1507.9783 - val_RMSE: 38.7055 - val_loss: 1499.2605 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8172 - loss: 1507.9121 - val_RMSE: 38.7051 - val_loss: 1499.2234 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8162 - loss: 1507.8428 - val_RMSE: 38.7050 - val_loss: 1499.2141 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8162 - loss: 1507.8494 - val_RMSE: 38.7051 - val_loss: 1499.2327 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8076 - loss: 1507.1770 - val_RMSE: 38.7049 - val_loss: 1499.1859 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8135 - loss: 1507.6168 - val_RMSE: 38.7052 - val_loss: 1499.2134 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8145 - loss: 1507.6886 - val_RMSE: 38.7053 - val_loss: 1499.2268 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8116 - loss: 1507.4708 - val_RMSE: 38.7042 - val_loss: 1499.1322 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8108 - loss: 1507.3915 - val_RMSE: 38.7045 - val_loss: 1499.1766 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 88s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 23:12:45,140] Trial 13 finished with value: 38.698099772135414 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.001629847097661793, 'dropout_rate': 0.41742826609863065}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 54.2372 - loss: 3091.4924 - val_RMSE: 38.7170 - val_loss: 1499.5424 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0687 - loss: 1526.9362 - val_RMSE: 38.6961 - val_loss: 1498.0338 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.9385 - loss: 1516.8846 - val_RMSE: 38.6910 - val_loss: 1497.7511 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8992 - loss: 1513.9355 - val_RMSE: 38.6866 - val_loss: 1497.4983 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8761 - loss: 1512.2119 - val_RMSE: 38.6863 - val_loss: 1497.5071 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8708 - loss: 1511.8153 - val_RMSE: 38.6842 - val_loss: 1497.3075 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8647 - loss: 1511.2953 - val_RMSE: 38.6851 - val_loss: 1497.3179 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8628 - loss: 1511.0874 - val_RMSE: 38.6839 - val_loss: 1497.1840 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8612 - loss: 1510.9324 - val_RMSE: 38.6839 - val_loss: 1497.1731 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8610 - loss: 1510.9070 - val_RMSE: 38.6827 - val_loss: 1497.0681 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8606 - loss: 1510.8702 - val_RMSE: 38.6826 - val_loss: 1497.0814 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8537 - loss: 1510.3533 - val_RMSE: 38.6827 - val_loss: 1497.0885 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8609 - loss: 1510.9081 - val_RMSE: 38.6823 - val_loss: 1497.0564 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8497 - loss: 1510.0367 - val_RMSE: 38.6825 - val_loss: 1497.0775 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8548 - loss: 1510.4388 - val_RMSE: 38.6821 - val_loss: 1497.0465 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8466 - loss: 1509.8090 - val_RMSE: 38.6821 - val_loss: 1497.0631 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8509 - loss: 1510.1534 - val_RMSE: 38.6814 - val_loss: 1497.0200 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8459 - loss: 1509.7778 - val_RMSE: 38.6812 - val_loss: 1497.0015 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8472 - loss: 1509.8832 - val_RMSE: 38.6807 - val_loss: 1496.9803 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8497 - loss: 1510.0814 - val_RMSE: 38.6802 - val_loss: 1496.9426 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8437 - loss: 1509.6224 - val_RMSE: 38.6799 - val_loss: 1496.9320 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8465 - loss: 1509.8555 - val_RMSE: 38.6801 - val_loss: 1496.9612 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8435 - loss: 1509.6307 - val_RMSE: 38.6798 - val_loss: 1496.9349 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8385 - loss: 1509.2467 - val_RMSE: 38.6803 - val_loss: 1496.9849 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8394 - loss: 1509.3248 - val_RMSE: 38.6805 - val_loss: 1497.0117 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.1874 - loss: 3085.2871 - val_RMSE: 38.7435 - val_loss: 1501.5917 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0258 - loss: 1523.5741 - val_RMSE: 38.7222 - val_loss: 1500.0571 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8945 - loss: 1513.4518 - val_RMSE: 38.7207 - val_loss: 1500.0442 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8429 - loss: 1509.5470 - val_RMSE: 38.7189 - val_loss: 1499.9951 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8265 - loss: 1508.3622 - val_RMSE: 38.7181 - val_loss: 1499.9818 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8174 - loss: 1507.6851 - val_RMSE: 38.7175 - val_loss: 1499.9148 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8162 - loss: 1507.5602 - val_RMSE: 38.7162 - val_loss: 1499.7758 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8160 - loss: 1507.5010 - val_RMSE: 38.7161 - val_loss: 1499.7245 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8146 - loss: 1507.3451 - val_RMSE: 38.7155 - val_loss: 1499.6410 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8090 - loss: 1506.8844 - val_RMSE: 38.7154 - val_loss: 1499.6195 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8048 - loss: 1506.5414 - val_RMSE: 38.7142 - val_loss: 1499.5243 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8069 - loss: 1506.7064 - val_RMSE: 38.7149 - val_loss: 1499.5750 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8034 - loss: 1506.4352 - val_RMSE: 38.7136 - val_loss: 1499.4709 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8015 - loss: 1506.2897 - val_RMSE: 38.7140 - val_loss: 1499.5062 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8063 - loss: 1506.6606 - val_RMSE: 38.7149 - val_loss: 1499.5800 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7974 - loss: 1505.9839 - val_RMSE: 38.7136 - val_loss: 1499.4976 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8075 - loss: 1506.7770 - val_RMSE: 38.7134 - val_loss: 1499.4849 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8002 - loss: 1506.2155 - val_RMSE: 38.7139 - val_loss: 1499.5319 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7929 - loss: 1505.6433 - val_RMSE: 38.7074 - val_loss: 1498.9861 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7955 - loss: 1505.8116 - val_RMSE: 38.7071 - val_loss: 1498.9315 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7869 - loss: 1505.1101 - val_RMSE: 38.7070 - val_loss: 1498.9012 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7926 - loss: 1505.5201 - val_RMSE: 38.7071 - val_loss: 1498.8779 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7827 - loss: 1504.7317 - val_RMSE: 38.7071 - val_loss: 1498.8577 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7861 - loss: 1504.9731 - val_RMSE: 38.7070 - val_loss: 1498.8345 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7846 - loss: 1504.8422 - val_RMSE: 38.7071 - val_loss: 1498.8226 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 31s 9ms/step - RMSE: 54.1822 - loss: 3084.4521 - val_RMSE: 38.7287 - val_loss: 1500.4498 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0468 - loss: 1525.2246 - val_RMSE: 38.7150 - val_loss: 1499.4998 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9117 - loss: 1514.7979 - val_RMSE: 38.7124 - val_loss: 1499.4175 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8732 - loss: 1511.9169 - val_RMSE: 38.7109 - val_loss: 1499.3944 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8579 - loss: 1510.8069 - val_RMSE: 38.7093 - val_loss: 1499.3021 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8406 - loss: 1509.4832 - val_RMSE: 38.7079 - val_loss: 1499.1726 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8435 - loss: 1509.6738 - val_RMSE: 38.7063 - val_loss: 1498.9808 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8394 - loss: 1509.2943 - val_RMSE: 38.7061 - val_loss: 1498.9274 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8361 - loss: 1509.0034 - val_RMSE: 38.7069 - val_loss: 1498.9799 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8384 - loss: 1509.1742 - val_RMSE: 38.7051 - val_loss: 1498.8281 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8324 - loss: 1508.6997 - val_RMSE: 38.7050 - val_loss: 1498.8247 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8331 - loss: 1508.7588 - val_RMSE: 38.7041 - val_loss: 1498.7686 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8314 - loss: 1508.6329 - val_RMSE: 38.7047 - val_loss: 1498.8119 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8320 - loss: 1508.6835 - val_RMSE: 38.7041 - val_loss: 1498.7722 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8266 - loss: 1508.2732 - val_RMSE: 38.7042 - val_loss: 1498.7761 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8270 - loss: 1508.3079 - val_RMSE: 38.7041 - val_loss: 1498.7855 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8237 - loss: 1508.0518 - val_RMSE: 38.7029 - val_loss: 1498.6909 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8243 - loss: 1508.1083 - val_RMSE: 38.7026 - val_loss: 1498.6742 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8245 - loss: 1508.1329 - val_RMSE: 38.7028 - val_loss: 1498.6964 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8236 - loss: 1508.0685 - val_RMSE: 38.7029 - val_loss: 1498.7174 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8144 - loss: 1507.3624 - val_RMSE: 38.7021 - val_loss: 1498.6572 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8181 - loss: 1507.6537 - val_RMSE: 38.7016 - val_loss: 1498.6260 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8195 - loss: 1507.7714 - val_RMSE: 38.7024 - val_loss: 1498.7084 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8168 - loss: 1507.5803 - val_RMSE: 38.7018 - val_loss: 1498.6704 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8151 - loss: 1507.4535 - val_RMSE: 38.7036 - val_loss: 1498.8120 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 23:43:24,680] Trial 14 finished with value: 38.69705327351888 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002984783533919333, 'dropout_rate': 0.44795270691730493}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 29s 8ms/step - RMSE: 57.1659 - loss: 3430.8306 - val_RMSE: 38.6983 - val_loss: 1498.8441 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 39.0924 - loss: 1529.5322 - val_RMSE: 38.6906 - val_loss: 1498.3064 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9839 - loss: 1521.0792 - val_RMSE: 38.6887 - val_loss: 1498.0994 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9557 - loss: 1518.7845 - val_RMSE: 38.6870 - val_loss: 1497.7704 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9515 - loss: 1518.2507 - val_RMSE: 38.6875 - val_loss: 1497.5935 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9452 - loss: 1517.5596 - val_RMSE: 38.6861 - val_loss: 1497.3618 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9426 - loss: 1517.2594 - val_RMSE: 38.6857 - val_loss: 1497.2886 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9435 - loss: 1517.3000 - val_RMSE: 38.6861 - val_loss: 1497.2948 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9410 - loss: 1517.0885 - val_RMSE: 38.6850 - val_loss: 1497.2120 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9345 - loss: 1516.5826 - val_RMSE: 38.6854 - val_loss: 1497.2504 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9393 - loss: 1516.9591 - val_RMSE: 38.6850 - val_loss: 1497.2163 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9306 - loss: 1516.2841 - val_RMSE: 38.6846 - val_loss: 1497.1846 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9304 - loss: 1516.2693 - val_RMSE: 38.6846 - val_loss: 1497.1831 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9299 - loss: 1516.2289 - val_RMSE: 38.6845 - val_loss: 1497.1815 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9198 - loss: 1515.4485 - val_RMSE: 38.6844 - val_loss: 1497.1842 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9143 - loss: 1515.0242 - val_RMSE: 38.6838 - val_loss: 1497.1359 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9117 - loss: 1514.8230 - val_RMSE: 38.6841 - val_loss: 1497.1598 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9161 - loss: 1515.1650 - val_RMSE: 38.6838 - val_loss: 1497.1333 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9151 - loss: 1515.0920 - val_RMSE: 38.6829 - val_loss: 1497.0654 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9146 - loss: 1515.0533 - val_RMSE: 38.6834 - val_loss: 1497.1141 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9108 - loss: 1514.7668 - val_RMSE: 38.6832 - val_loss: 1497.1071 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9013 - loss: 1514.0417 - val_RMSE: 38.6830 - val_loss: 1497.0922 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9106 - loss: 1514.7650 - val_RMSE: 38.6829 - val_loss: 1497.0931 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9035 - loss: 1514.2114 - val_RMSE: 38.6821 - val_loss: 1497.0345 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9028 - loss: 1514.1614 - val_RMSE: 38.6825 - val_loss: 1497.0570 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 29s 8ms/step - RMSE: 57.1443 - loss: 3427.8079 - val_RMSE: 38.7307 - val_loss: 1501.3450 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 39.0459 - loss: 1525.8882 - val_RMSE: 38.7199 - val_loss: 1500.5787 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9423 - loss: 1517.8477 - val_RMSE: 38.7193 - val_loss: 1500.4846 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9128 - loss: 1515.4645 - val_RMSE: 38.7176 - val_loss: 1500.1639 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9063 - loss: 1514.7551 - val_RMSE: 38.7175 - val_loss: 1499.9242 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8980 - loss: 1513.8986 - val_RMSE: 38.7176 - val_loss: 1499.8086 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8881 - loss: 1513.0250 - val_RMSE: 38.7172 - val_loss: 1499.7290 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8806 - loss: 1512.4054 - val_RMSE: 38.7163 - val_loss: 1499.6388 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8878 - loss: 1512.9453 - val_RMSE: 38.7157 - val_loss: 1499.5846 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8858 - loss: 1512.7778 - val_RMSE: 38.7162 - val_loss: 1499.6216 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8813 - loss: 1512.4236 - val_RMSE: 38.7162 - val_loss: 1499.6226 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8831 - loss: 1512.5712 - val_RMSE: 38.7156 - val_loss: 1499.5880 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8788 - loss: 1512.2426 - val_RMSE: 38.7153 - val_loss: 1499.5529 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8746 - loss: 1511.9155 - val_RMSE: 38.7152 - val_loss: 1499.5577 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8779 - loss: 1512.1763 - val_RMSE: 38.7160 - val_loss: 1499.6110 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8675 - loss: 1511.3635 - val_RMSE: 38.7155 - val_loss: 1499.5750 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8725 - loss: 1511.7579 - val_RMSE: 38.7157 - val_loss: 1499.5924 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8656 - loss: 1511.2235 - val_RMSE: 38.7159 - val_loss: 1499.6100 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8563 - loss: 1510.4905 - val_RMSE: 38.7099 - val_loss: 1499.0966 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8563 - loss: 1510.4412 - val_RMSE: 38.7098 - val_loss: 1499.0450 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8536 - loss: 1510.1877 - val_RMSE: 38.7096 - val_loss: 1498.9941 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8530 - loss: 1510.1133 - val_RMSE: 38.7095 - val_loss: 1498.9586 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8553 - loss: 1510.2628 - val_RMSE: 38.7095 - val_loss: 1498.9364 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8559 - loss: 1510.2888 - val_RMSE: 38.7094 - val_loss: 1498.9080 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8575 - loss: 1510.3870 - val_RMSE: 38.7093 - val_loss: 1498.8811 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 28s 8ms/step - RMSE: 57.1254 - loss: 3425.2498 - val_RMSE: 38.7194 - val_loss: 1500.4795 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.0774 - loss: 1528.3647 - val_RMSE: 38.7150 - val_loss: 1500.2128 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9620 - loss: 1519.3999 - val_RMSE: 38.7112 - val_loss: 1499.8650 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9355 - loss: 1517.2517 - val_RMSE: 38.7109 - val_loss: 1499.6720 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9240 - loss: 1516.1539 - val_RMSE: 38.7116 - val_loss: 1499.4846 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9239 - loss: 1515.9294 - val_RMSE: 38.7100 - val_loss: 1499.2283 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9173 - loss: 1515.3032 - val_RMSE: 38.7097 - val_loss: 1499.1447 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9120 - loss: 1514.8485 - val_RMSE: 38.7105 - val_loss: 1499.2045 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9127 - loss: 1514.8962 - val_RMSE: 38.7092 - val_loss: 1499.0884 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9147 - loss: 1515.0438 - val_RMSE: 38.7078 - val_loss: 1498.9818 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9059 - loss: 1514.3590 - val_RMSE: 38.7077 - val_loss: 1498.9750 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9007 - loss: 1513.9552 - val_RMSE: 38.7064 - val_loss: 1498.8713 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9057 - loss: 1514.3368 - val_RMSE: 38.7081 - val_loss: 1499.0177 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8994 - loss: 1513.8552 - val_RMSE: 38.7072 - val_loss: 1498.9406 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8990 - loss: 1513.8284 - val_RMSE: 38.7070 - val_loss: 1498.9335 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8983 - loss: 1513.7806 - val_RMSE: 38.7086 - val_loss: 1499.0530 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8926 - loss: 1513.3330 - val_RMSE: 38.7070 - val_loss: 1498.9297 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8849 - loss: 1512.7251 - val_RMSE: 38.7036 - val_loss: 1498.6143 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8889 - loss: 1512.9840 - val_RMSE: 38.7033 - val_loss: 1498.5455 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8807 - loss: 1512.3025 - val_RMSE: 38.7033 - val_loss: 1498.5125 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8824 - loss: 1512.4025 - val_RMSE: 38.7030 - val_loss: 1498.4615 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8788 - loss: 1512.0936 - val_RMSE: 38.7029 - val_loss: 1498.4274 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8841 - loss: 1512.4811 - val_RMSE: 38.7028 - val_loss: 1498.3962 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8813 - loss: 1512.2443 - val_RMSE: 38.7027 - val_loss: 1498.3696 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.8824 - loss: 1512.3110 - val_RMSE: 38.7028 - val_loss: 1498.3608 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 00:09:14,427] Trial 15 finished with value: 38.698187510172524 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0016250296084432089, 'dropout_rate': 0.39540101245549514}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 9ms/step - RMSE: 54.2782 - loss: 3095.8782 - val_RMSE: 38.7169 - val_loss: 1499.3932 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0832 - loss: 1527.9094 - val_RMSE: 38.6975 - val_loss: 1497.9794 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.9419 - loss: 1516.9744 - val_RMSE: 38.6904 - val_loss: 1497.5193 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9033 - loss: 1514.0696 - val_RMSE: 38.6883 - val_loss: 1497.4456 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8823 - loss: 1512.5110 - val_RMSE: 38.6860 - val_loss: 1497.3236 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8766 - loss: 1512.1173 - val_RMSE: 38.6845 - val_loss: 1497.2179 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8708 - loss: 1511.6632 - val_RMSE: 38.6846 - val_loss: 1497.2097 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8680 - loss: 1511.4290 - val_RMSE: 38.6840 - val_loss: 1497.1393 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8663 - loss: 1511.2745 - val_RMSE: 38.6831 - val_loss: 1497.0627 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8658 - loss: 1511.2291 - val_RMSE: 38.6831 - val_loss: 1497.0541 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8670 - loss: 1511.3171 - val_RMSE: 38.6825 - val_loss: 1497.0161 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8589 - loss: 1510.6921 - val_RMSE: 38.6816 - val_loss: 1496.9396 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8655 - loss: 1511.1991 - val_RMSE: 38.6816 - val_loss: 1496.9424 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8557 - loss: 1510.4449 - val_RMSE: 38.6815 - val_loss: 1496.9385 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8595 - loss: 1510.7480 - val_RMSE: 38.6817 - val_loss: 1496.9653 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8524 - loss: 1510.2029 - val_RMSE: 38.6811 - val_loss: 1496.9233 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8558 - loss: 1510.4786 - val_RMSE: 38.6810 - val_loss: 1496.9232 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8501 - loss: 1510.0358 - val_RMSE: 38.6805 - val_loss: 1496.8965 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8526 - loss: 1510.2360 - val_RMSE: 38.6808 - val_loss: 1496.9279 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8541 - loss: 1510.3625 - val_RMSE: 38.6808 - val_loss: 1496.9269 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8481 - loss: 1509.9030 - val_RMSE: 38.6810 - val_loss: 1496.9478 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8498 - loss: 1510.0400 - val_RMSE: 38.6798 - val_loss: 1496.8708 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8489 - loss: 1509.9806 - val_RMSE: 38.6802 - val_loss: 1496.9056 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8435 - loss: 1509.5713 - val_RMSE: 38.6800 - val_loss: 1496.8999 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8443 - loss: 1509.6378 - val_RMSE: 38.6797 - val_loss: 1496.8862 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.2253 - loss: 3089.3152 - val_RMSE: 38.7393 - val_loss: 1501.1278 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0370 - loss: 1524.3060 - val_RMSE: 38.7239 - val_loss: 1500.0225 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8952 - loss: 1513.3470 - val_RMSE: 38.7231 - val_loss: 1500.0548 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8459 - loss: 1509.6072 - val_RMSE: 38.7212 - val_loss: 1499.9995 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8323 - loss: 1508.6357 - val_RMSE: 38.7192 - val_loss: 1499.8962 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8250 - loss: 1508.1107 - val_RMSE: 38.7164 - val_loss: 1499.6849 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8238 - loss: 1508.0110 - val_RMSE: 38.7161 - val_loss: 1499.6388 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8235 - loss: 1507.9568 - val_RMSE: 38.7160 - val_loss: 1499.6080 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8220 - loss: 1507.8206 - val_RMSE: 38.7148 - val_loss: 1499.5004 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8152 - loss: 1507.2802 - val_RMSE: 38.7156 - val_loss: 1499.5515 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8102 - loss: 1506.8842 - val_RMSE: 38.7149 - val_loss: 1499.5077 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8129 - loss: 1507.0994 - val_RMSE: 38.7151 - val_loss: 1499.5249 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8108 - loss: 1506.9407 - val_RMSE: 38.7147 - val_loss: 1499.4971 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8068 - loss: 1506.6337 - val_RMSE: 38.7154 - val_loss: 1499.5533 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8133 - loss: 1507.1470 - val_RMSE: 38.7147 - val_loss: 1499.5066 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8031 - loss: 1506.3556 - val_RMSE: 38.7146 - val_loss: 1499.5016 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8121 - loss: 1507.0630 - val_RMSE: 38.7143 - val_loss: 1499.4923 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8042 - loss: 1506.4629 - val_RMSE: 38.7149 - val_loss: 1499.5441 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8039 - loss: 1506.4456 - val_RMSE: 38.7144 - val_loss: 1499.5140 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8069 - loss: 1506.6926 - val_RMSE: 38.7124 - val_loss: 1499.3735 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8012 - loss: 1506.2574 - val_RMSE: 38.7141 - val_loss: 1499.5081 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8050 - loss: 1506.5593 - val_RMSE: 38.7130 - val_loss: 1499.4319 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7932 - loss: 1505.6488 - val_RMSE: 38.7122 - val_loss: 1499.3729 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7973 - loss: 1505.9739 - val_RMSE: 38.7138 - val_loss: 1499.5082 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7952 - loss: 1505.8251 - val_RMSE: 38.7139 - val_loss: 1499.5271 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.2234 - loss: 3088.7781 - val_RMSE: 38.7237 - val_loss: 1499.9156 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0555 - loss: 1525.7467 - val_RMSE: 38.7137 - val_loss: 1499.2261 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9162 - loss: 1514.9709 - val_RMSE: 38.7104 - val_loss: 1499.0614 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8789 - loss: 1512.1599 - val_RMSE: 38.7106 - val_loss: 1499.1636 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8631 - loss: 1511.0133 - val_RMSE: 38.7078 - val_loss: 1499.0028 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8446 - loss: 1509.6190 - val_RMSE: 38.7086 - val_loss: 1499.0797 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8489 - loss: 1509.9587 - val_RMSE: 38.7080 - val_loss: 1499.0126 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8443 - loss: 1509.5802 - val_RMSE: 38.7058 - val_loss: 1498.8291 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8415 - loss: 1509.3489 - val_RMSE: 38.7067 - val_loss: 1498.8939 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8449 - loss: 1509.6073 - val_RMSE: 38.7060 - val_loss: 1498.8324 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8396 - loss: 1509.1887 - val_RMSE: 38.7053 - val_loss: 1498.7786 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8383 - loss: 1509.0919 - val_RMSE: 38.7043 - val_loss: 1498.7078 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8376 - loss: 1509.0490 - val_RMSE: 38.7045 - val_loss: 1498.7247 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8368 - loss: 1508.9845 - val_RMSE: 38.7033 - val_loss: 1498.6464 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8324 - loss: 1508.6527 - val_RMSE: 38.7039 - val_loss: 1498.6877 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8318 - loss: 1508.6100 - val_RMSE: 38.7045 - val_loss: 1498.7450 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8299 - loss: 1508.4669 - val_RMSE: 38.7028 - val_loss: 1498.6188 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8290 - loss: 1508.3986 - val_RMSE: 38.7031 - val_loss: 1498.6448 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8292 - loss: 1508.4265 - val_RMSE: 38.7026 - val_loss: 1498.6185 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8287 - loss: 1508.3964 - val_RMSE: 38.7026 - val_loss: 1498.6252 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8188 - loss: 1507.6318 - val_RMSE: 38.7022 - val_loss: 1498.6003 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8250 - loss: 1508.1219 - val_RMSE: 38.7025 - val_loss: 1498.6249 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8240 - loss: 1508.0503 - val_RMSE: 38.7038 - val_loss: 1498.7491 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8210 - loss: 1507.8329 - val_RMSE: 38.7033 - val_loss: 1498.7197 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8191 - loss: 1507.6973 - val_RMSE: 38.7045 - val_loss: 1498.8224 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 00:39:38,632] Trial 16 finished with value: 38.69933573404948 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00021575974293938053, 'dropout_rate': 0.45990474878515086}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 9ms/step - RMSE: 54.1107 - loss: 3078.6514 - val_RMSE: 38.7144 - val_loss: 1500.0889 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0504 - loss: 1526.2701 - val_RMSE: 38.6957 - val_loss: 1498.8293 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.9274 - loss: 1516.8407 - val_RMSE: 38.6920 - val_loss: 1498.6339 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8814 - loss: 1513.3367 - val_RMSE: 38.6914 - val_loss: 1498.5654 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8604 - loss: 1511.6453 - val_RMSE: 38.6913 - val_loss: 1498.4082 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8519 - loss: 1510.8185 - val_RMSE: 38.6897 - val_loss: 1498.0935 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8459 - loss: 1510.1533 - val_RMSE: 38.6900 - val_loss: 1497.9260 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8447 - loss: 1509.8809 - val_RMSE: 38.6898 - val_loss: 1497.7896 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8418 - loss: 1509.5629 - val_RMSE: 38.6898 - val_loss: 1497.7340 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8426 - loss: 1509.5859 - val_RMSE: 38.6886 - val_loss: 1497.6273 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8438 - loss: 1509.6635 - val_RMSE: 38.6879 - val_loss: 1497.5635 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8385 - loss: 1509.2489 - val_RMSE: 38.6880 - val_loss: 1497.5776 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8459 - loss: 1509.8237 - val_RMSE: 38.6875 - val_loss: 1497.5248 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8377 - loss: 1509.1807 - val_RMSE: 38.6866 - val_loss: 1497.4628 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8398 - loss: 1509.3450 - val_RMSE: 38.6866 - val_loss: 1497.4578 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8321 - loss: 1508.7526 - val_RMSE: 38.6872 - val_loss: 1497.5121 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8345 - loss: 1508.9409 - val_RMSE: 38.6865 - val_loss: 1497.4546 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8314 - loss: 1508.6998 - val_RMSE: 38.6860 - val_loss: 1497.4100 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8344 - loss: 1508.9286 - val_RMSE: 38.6860 - val_loss: 1497.4125 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8368 - loss: 1509.1191 - val_RMSE: 38.6865 - val_loss: 1497.4607 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8315 - loss: 1508.7134 - val_RMSE: 38.6851 - val_loss: 1497.3623 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8340 - loss: 1508.9172 - val_RMSE: 38.6855 - val_loss: 1497.3853 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8323 - loss: 1508.7795 - val_RMSE: 38.6854 - val_loss: 1497.3750 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8294 - loss: 1508.5457 - val_RMSE: 38.6843 - val_loss: 1497.2958 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8321 - loss: 1508.7620 - val_RMSE: 38.6847 - val_loss: 1497.3243 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 9ms/step - RMSE: 54.0633 - loss: 3072.5645 - val_RMSE: 38.7467 - val_loss: 1502.5869 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0063 - loss: 1522.8361 - val_RMSE: 38.7270 - val_loss: 1501.2798 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8792 - loss: 1513.1250 - val_RMSE: 38.7233 - val_loss: 1501.0939 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8261 - loss: 1509.0717 - val_RMSE: 38.7261 - val_loss: 1501.2937 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8118 - loss: 1507.9106 - val_RMSE: 38.7216 - val_loss: 1500.8225 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8011 - loss: 1506.9298 - val_RMSE: 38.7217 - val_loss: 1500.6202 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7958 - loss: 1506.3091 - val_RMSE: 38.7217 - val_loss: 1500.4171 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7957 - loss: 1506.1190 - val_RMSE: 38.7217 - val_loss: 1500.2946 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7959 - loss: 1506.0269 - val_RMSE: 38.7207 - val_loss: 1500.1533 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7920 - loss: 1505.6758 - val_RMSE: 38.7215 - val_loss: 1500.1971 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7896 - loss: 1505.4692 - val_RMSE: 38.7210 - val_loss: 1500.1378 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7907 - loss: 1505.5469 - val_RMSE: 38.7209 - val_loss: 1500.1379 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7869 - loss: 1505.2430 - val_RMSE: 38.7204 - val_loss: 1500.0864 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7865 - loss: 1505.2190 - val_RMSE: 38.7201 - val_loss: 1500.0696 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7896 - loss: 1505.4656 - val_RMSE: 38.7204 - val_loss: 1500.0969 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7845 - loss: 1505.0621 - val_RMSE: 38.7193 - val_loss: 1500.0079 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7900 - loss: 1505.4958 - val_RMSE: 38.7196 - val_loss: 1500.0474 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7857 - loss: 1505.1703 - val_RMSE: 38.7194 - val_loss: 1500.0381 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7837 - loss: 1505.0179 - val_RMSE: 38.7198 - val_loss: 1500.0641 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7879 - loss: 1505.3464 - val_RMSE: 38.7195 - val_loss: 1500.0469 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7807 - loss: 1504.7926 - val_RMSE: 38.7185 - val_loss: 1499.9778 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7869 - loss: 1505.2788 - val_RMSE: 38.7182 - val_loss: 1499.9569 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7769 - loss: 1504.5190 - val_RMSE: 38.7170 - val_loss: 1499.8768 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7826 - loss: 1504.9630 - val_RMSE: 38.7189 - val_loss: 1500.0370 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7790 - loss: 1504.6909 - val_RMSE: 38.7182 - val_loss: 1499.9637 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 31s 9ms/step - RMSE: 54.0401 - loss: 3069.6545 - val_RMSE: 38.7322 - val_loss: 1501.4784 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0275 - loss: 1524.5115 - val_RMSE: 38.7156 - val_loss: 1500.4202 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8948 - loss: 1514.3752 - val_RMSE: 38.7129 - val_loss: 1500.3600 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8564 - loss: 1511.5105 - val_RMSE: 38.7133 - val_loss: 1500.3911 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8421 - loss: 1510.3601 - val_RMSE: 38.7132 - val_loss: 1500.2396 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8246 - loss: 1508.8284 - val_RMSE: 38.7114 - val_loss: 1499.8651 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8245 - loss: 1508.5780 - val_RMSE: 38.7111 - val_loss: 1499.6149 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8208 - loss: 1508.0903 - val_RMSE: 38.7102 - val_loss: 1499.4279 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8183 - loss: 1507.7866 - val_RMSE: 38.7104 - val_loss: 1499.3872 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8217 - loss: 1508.0056 - val_RMSE: 38.7099 - val_loss: 1499.3264 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8168 - loss: 1507.6061 - val_RMSE: 38.7093 - val_loss: 1499.2693 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8163 - loss: 1507.5669 - val_RMSE: 38.7083 - val_loss: 1499.1895 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8126 - loss: 1507.2797 - val_RMSE: 38.7077 - val_loss: 1499.1591 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8144 - loss: 1507.4257 - val_RMSE: 38.7079 - val_loss: 1499.1654 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8109 - loss: 1507.1516 - val_RMSE: 38.7080 - val_loss: 1499.1681 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8133 - loss: 1507.3391 - val_RMSE: 38.7086 - val_loss: 1499.2253 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8124 - loss: 1507.2797 - val_RMSE: 38.7071 - val_loss: 1499.1300 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8103 - loss: 1507.1340 - val_RMSE: 38.7064 - val_loss: 1499.0674 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8095 - loss: 1507.0614 - val_RMSE: 38.7062 - val_loss: 1499.0457 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8099 - loss: 1507.0938 - val_RMSE: 38.7066 - val_loss: 1499.0726 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8032 - loss: 1506.5662 - val_RMSE: 38.7069 - val_loss: 1499.0944 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8087 - loss: 1506.9945 - val_RMSE: 38.7069 - val_loss: 1499.1034 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8084 - loss: 1506.9818 - val_RMSE: 38.7070 - val_loss: 1499.1201 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8069 - loss: 1506.8638 - val_RMSE: 38.7055 - val_loss: 1498.9998 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8038 - loss: 1506.6199 - val_RMSE: 38.7055 - val_loss: 1498.9918 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 01:10:24,832] Trial 17 finished with value: 38.70278676350912 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'selu', 'reg': 0.0007138024821391923, 'dropout_rate': 0.3957879794222315}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 31s 9ms/step - RMSE: 54.2398 - loss: 3091.4988 - val_RMSE: 38.7193 - val_loss: 1499.3783 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0753 - loss: 1527.0886 - val_RMSE: 38.6997 - val_loss: 1497.9049 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9403 - loss: 1516.5986 - val_RMSE: 38.6907 - val_loss: 1497.2632 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8992 - loss: 1513.4532 - val_RMSE: 38.6871 - val_loss: 1497.0455 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8772 - loss: 1511.8029 - val_RMSE: 38.6865 - val_loss: 1497.0535 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8716 - loss: 1511.4158 - val_RMSE: 38.6844 - val_loss: 1496.9219 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8660 - loss: 1511.0156 - val_RMSE: 38.6848 - val_loss: 1496.9785 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8637 - loss: 1510.8527 - val_RMSE: 38.6832 - val_loss: 1496.8630 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8611 - loss: 1510.6633 - val_RMSE: 38.6837 - val_loss: 1496.9169 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8606 - loss: 1510.6301 - val_RMSE: 38.6834 - val_loss: 1496.8917 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8621 - loss: 1510.7542 - val_RMSE: 38.6827 - val_loss: 1496.8439 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8531 - loss: 1510.0630 - val_RMSE: 38.6815 - val_loss: 1496.7584 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8593 - loss: 1510.5491 - val_RMSE: 38.6813 - val_loss: 1496.7513 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8500 - loss: 1509.8315 - val_RMSE: 38.6814 - val_loss: 1496.7617 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8534 - loss: 1510.1022 - val_RMSE: 38.6808 - val_loss: 1496.7251 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8466 - loss: 1509.5798 - val_RMSE: 38.6808 - val_loss: 1496.7343 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8502 - loss: 1509.8713 - val_RMSE: 38.6806 - val_loss: 1496.7273 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8449 - loss: 1509.4708 - val_RMSE: 38.6802 - val_loss: 1496.7064 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8474 - loss: 1509.6674 - val_RMSE: 38.6800 - val_loss: 1496.6989 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8484 - loss: 1509.7550 - val_RMSE: 38.6805 - val_loss: 1496.7426 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8421 - loss: 1509.2701 - val_RMSE: 38.6798 - val_loss: 1496.6989 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8444 - loss: 1509.4558 - val_RMSE: 38.6793 - val_loss: 1496.6677 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8428 - loss: 1509.3433 - val_RMSE: 38.6793 - val_loss: 1496.6768 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8369 - loss: 1508.8928 - val_RMSE: 38.6798 - val_loss: 1496.7220 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8367 - loss: 1508.8910 - val_RMSE: 38.6808 - val_loss: 1496.8113 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 9ms/step - RMSE: 54.1941 - loss: 3085.7605 - val_RMSE: 38.7357 - val_loss: 1500.6461 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0281 - loss: 1523.3992 - val_RMSE: 38.7228 - val_loss: 1499.6948 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8910 - loss: 1512.7607 - val_RMSE: 38.7218 - val_loss: 1499.6647 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8400 - loss: 1508.8491 - val_RMSE: 38.7194 - val_loss: 1499.5392 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8278 - loss: 1507.9591 - val_RMSE: 38.7175 - val_loss: 1499.4386 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8202 - loss: 1507.4189 - val_RMSE: 38.7162 - val_loss: 1499.3790 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8173 - loss: 1507.2258 - val_RMSE: 38.7161 - val_loss: 1499.3939 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8177 - loss: 1507.2728 - val_RMSE: 38.7159 - val_loss: 1499.3864 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8163 - loss: 1507.1754 - val_RMSE: 38.7154 - val_loss: 1499.3591 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8100 - loss: 1506.6975 - val_RMSE: 38.7156 - val_loss: 1499.3827 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8045 - loss: 1506.2773 - val_RMSE: 38.7145 - val_loss: 1499.3005 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8075 - loss: 1506.5176 - val_RMSE: 38.7149 - val_loss: 1499.3445 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8045 - loss: 1506.2889 - val_RMSE: 38.7140 - val_loss: 1499.2806 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8017 - loss: 1506.0815 - val_RMSE: 38.7149 - val_loss: 1499.3608 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8077 - loss: 1506.5504 - val_RMSE: 38.7143 - val_loss: 1499.3213 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7976 - loss: 1505.7821 - val_RMSE: 38.7140 - val_loss: 1499.3085 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8077 - loss: 1506.5778 - val_RMSE: 38.7136 - val_loss: 1499.2871 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7984 - loss: 1505.8611 - val_RMSE: 38.7150 - val_loss: 1499.3993 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7949 - loss: 1505.5833 - val_RMSE: 38.7088 - val_loss: 1498.9016 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7973 - loss: 1505.7582 - val_RMSE: 38.7086 - val_loss: 1498.8751 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7887 - loss: 1505.0756 - val_RMSE: 38.7084 - val_loss: 1498.8481 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7929 - loss: 1505.3882 - val_RMSE: 38.7085 - val_loss: 1498.8446 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7830 - loss: 1504.6169 - val_RMSE: 38.7085 - val_loss: 1498.8344 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7883 - loss: 1505.0145 - val_RMSE: 38.7087 - val_loss: 1498.8383 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7853 - loss: 1504.7732 - val_RMSE: 38.7086 - val_loss: 1498.8279 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 31s 9ms/step - RMSE: 54.1967 - loss: 3085.7881 - val_RMSE: 38.7236 - val_loss: 1499.7102 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0499 - loss: 1525.0978 - val_RMSE: 38.7167 - val_loss: 1499.2147 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9143 - loss: 1514.5691 - val_RMSE: 38.7121 - val_loss: 1498.9109 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8742 - loss: 1511.5024 - val_RMSE: 38.7099 - val_loss: 1498.7930 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8603 - loss: 1510.4794 - val_RMSE: 38.7098 - val_loss: 1498.8401 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8412 - loss: 1509.0469 - val_RMSE: 38.7078 - val_loss: 1498.7269 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8449 - loss: 1509.3674 - val_RMSE: 38.7070 - val_loss: 1498.6874 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8395 - loss: 1508.9677 - val_RMSE: 38.7060 - val_loss: 1498.6287 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8370 - loss: 1508.7920 - val_RMSE: 38.7064 - val_loss: 1498.6707 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8387 - loss: 1508.9312 - val_RMSE: 38.7050 - val_loss: 1498.5713 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8339 - loss: 1508.5682 - val_RMSE: 38.7044 - val_loss: 1498.5265 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8343 - loss: 1508.6080 - val_RMSE: 38.7036 - val_loss: 1498.4749 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8323 - loss: 1508.4562 - val_RMSE: 38.7049 - val_loss: 1498.5798 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8315 - loss: 1508.3954 - val_RMSE: 38.7042 - val_loss: 1498.5349 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8267 - loss: 1508.0354 - val_RMSE: 38.7035 - val_loss: 1498.4894 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8270 - loss: 1508.0687 - val_RMSE: 38.7038 - val_loss: 1498.5228 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8238 - loss: 1507.8303 - val_RMSE: 38.7027 - val_loss: 1498.4489 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8239 - loss: 1507.8403 - val_RMSE: 38.7034 - val_loss: 1498.5072 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8246 - loss: 1507.9081 - val_RMSE: 38.7026 - val_loss: 1498.4523 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8228 - loss: 1507.7750 - val_RMSE: 38.7033 - val_loss: 1498.5188 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8126 - loss: 1506.9957 - val_RMSE: 38.7020 - val_loss: 1498.4283 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8176 - loss: 1507.3906 - val_RMSE: 38.7019 - val_loss: 1498.4310 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8181 - loss: 1507.4351 - val_RMSE: 38.7036 - val_loss: 1498.5718 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8156 - loss: 1507.2567 - val_RMSE: 38.7021 - val_loss: 1498.4636 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8133 - loss: 1507.0874 - val_RMSE: 38.7045 - val_loss: 1498.6572 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 01:40:46,962] Trial 18 finished with value: 38.69798787434896 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00010652411608589064, 'dropout_rate': 0.4509993110355638}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 28s 8ms/step - RMSE: 57.1954 - loss: 3473.9302 - val_RMSE: 38.7020 - val_loss: 1515.2155 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 39.1167 - loss: 1543.0260 - val_RMSE: 38.6971 - val_loss: 1502.5275 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9993 - loss: 1525.3372 - val_RMSE: 38.6965 - val_loss: 1500.6035 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9703 - loss: 1521.5928 - val_RMSE: 38.6932 - val_loss: 1499.4594 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9657 - loss: 1520.6129 - val_RMSE: 38.6957 - val_loss: 1499.6681 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9617 - loss: 1520.3317 - val_RMSE: 38.6932 - val_loss: 1499.3479 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9611 - loss: 1520.2102 - val_RMSE: 38.6935 - val_loss: 1499.3765 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 15s 6ms/step - RMSE: 38.9617 - loss: 1520.2311 - val_RMSE: 38.6924 - val_loss: 1499.2362 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9583 - loss: 1519.9635 - val_RMSE: 38.6926 - val_loss: 1499.3032 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9523 - loss: 1519.4067 - val_RMSE: 38.6930 - val_loss: 1499.1931 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9564 - loss: 1519.7504 - val_RMSE: 38.6916 - val_loss: 1499.0542 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9477 - loss: 1519.0076 - val_RMSE: 38.6935 - val_loss: 1499.2684 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9493 - loss: 1519.1442 - val_RMSE: 38.6910 - val_loss: 1499.0090 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9479 - loss: 1519.0330 - val_RMSE: 38.6913 - val_loss: 1498.9174 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9377 - loss: 1518.1302 - val_RMSE: 38.6906 - val_loss: 1498.8359 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9322 - loss: 1517.6932 - val_RMSE: 38.6895 - val_loss: 1498.8109 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9295 - loss: 1517.5000 - val_RMSE: 38.6908 - val_loss: 1498.8977 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9335 - loss: 1517.8086 - val_RMSE: 38.6901 - val_loss: 1498.8118 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9331 - loss: 1517.7023 - val_RMSE: 38.6900 - val_loss: 1498.9238 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9332 - loss: 1517.8079 - val_RMSE: 38.6907 - val_loss: 1498.9600 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9290 - loss: 1517.4254 - val_RMSE: 38.6906 - val_loss: 1498.8538 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9144 - loss: 1515.8588 - val_RMSE: 38.6861 - val_loss: 1497.5668 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9223 - loss: 1515.8263 - val_RMSE: 38.6860 - val_loss: 1497.3218 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9161 - loss: 1515.1467 - val_RMSE: 38.6856 - val_loss: 1497.1844 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9166 - loss: 1515.0980 - val_RMSE: 38.6852 - val_loss: 1497.1039 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 30s 8ms/step - RMSE: 57.1779 - loss: 3471.2241 - val_RMSE: 38.7458 - val_loss: 1518.5850 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 39.0603 - loss: 1538.6108 - val_RMSE: 38.7352 - val_loss: 1505.8210 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9562 - loss: 1522.1332 - val_RMSE: 38.7293 - val_loss: 1503.0566 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9251 - loss: 1518.0636 - val_RMSE: 38.7242 - val_loss: 1501.8026 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9206 - loss: 1517.1541 - val_RMSE: 38.7234 - val_loss: 1501.9583 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9142 - loss: 1516.7419 - val_RMSE: 38.7213 - val_loss: 1501.7061 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9061 - loss: 1516.0527 - val_RMSE: 38.7223 - val_loss: 1501.6149 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8986 - loss: 1515.3788 - val_RMSE: 38.7211 - val_loss: 1501.5238 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9041 - loss: 1515.7919 - val_RMSE: 38.7224 - val_loss: 1501.5063 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9031 - loss: 1515.6868 - val_RMSE: 38.7253 - val_loss: 1501.7135 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8989 - loss: 1515.2574 - val_RMSE: 38.7205 - val_loss: 1501.5234 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9019 - loss: 1515.5226 - val_RMSE: 38.7202 - val_loss: 1501.2913 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8955 - loss: 1514.9628 - val_RMSE: 38.7188 - val_loss: 1501.3521 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8919 - loss: 1514.7085 - val_RMSE: 38.7213 - val_loss: 1501.3600 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8957 - loss: 1514.9402 - val_RMSE: 38.7224 - val_loss: 1501.4587 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8848 - loss: 1514.0964 - val_RMSE: 38.7210 - val_loss: 1501.2278 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8892 - loss: 1514.3898 - val_RMSE: 38.7200 - val_loss: 1501.2106 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8819 - loss: 1513.8618 - val_RMSE: 38.7201 - val_loss: 1501.2452 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8764 - loss: 1513.3997 - val_RMSE: 38.7203 - val_loss: 1501.2402 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8771 - loss: 1513.4261 - val_RMSE: 38.7196 - val_loss: 1501.1266 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8731 - loss: 1513.1027 - val_RMSE: 38.7211 - val_loss: 1501.2502 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8713 - loss: 1512.9431 - val_RMSE: 38.7221 - val_loss: 1501.3231 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8722 - loss: 1512.9980 - val_RMSE: 38.7187 - val_loss: 1501.0776 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8708 - loss: 1512.8766 - val_RMSE: 38.7192 - val_loss: 1501.1128 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8710 - loss: 1512.8568 - val_RMSE: 38.7192 - val_loss: 1501.0691 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 28s 8ms/step - RMSE: 57.1586 - loss: 3468.8442 - val_RMSE: 38.7308 - val_loss: 1518.0854 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 39.0940 - loss: 1542.0193 - val_RMSE: 38.7218 - val_loss: 1504.8708 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9738 - loss: 1523.6556 - val_RMSE: 38.7192 - val_loss: 1502.2158 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9475 - loss: 1519.7352 - val_RMSE: 38.7162 - val_loss: 1501.1461 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9395 - loss: 1518.5706 - val_RMSE: 38.7153 - val_loss: 1501.1970 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9397 - loss: 1518.6503 - val_RMSE: 38.7170 - val_loss: 1501.2604 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9350 - loss: 1518.2765 - val_RMSE: 38.7143 - val_loss: 1500.9779 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9308 - loss: 1517.8375 - val_RMSE: 38.7154 - val_loss: 1501.0334 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9315 - loss: 1517.8890 - val_RMSE: 38.7131 - val_loss: 1500.8451 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9334 - loss: 1517.9983 - val_RMSE: 38.7131 - val_loss: 1500.8263 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9250 - loss: 1517.3690 - val_RMSE: 38.7133 - val_loss: 1500.6941 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9186 - loss: 1516.7828 - val_RMSE: 38.7143 - val_loss: 1500.9939 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9247 - loss: 1517.2903 - val_RMSE: 38.7143 - val_loss: 1500.7407 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9185 - loss: 1516.7043 - val_RMSE: 38.7149 - val_loss: 1501.0273 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9173 - loss: 1516.6989 - val_RMSE: 38.7129 - val_loss: 1500.6477 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9170 - loss: 1516.5720 - val_RMSE: 38.7133 - val_loss: 1500.6489 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9109 - loss: 1516.0662 - val_RMSE: 38.7128 - val_loss: 1500.7288 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9071 - loss: 1515.8297 - val_RMSE: 38.7115 - val_loss: 1500.4503 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9111 - loss: 1516.0710 - val_RMSE: 38.7126 - val_loss: 1500.6029 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9004 - loss: 1515.2291 - val_RMSE: 38.7126 - val_loss: 1500.6469 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9030 - loss: 1515.4309 - val_RMSE: 38.7118 - val_loss: 1500.5255 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8961 - loss: 1514.8849 - val_RMSE: 38.7118 - val_loss: 1500.3914 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.9016 - loss: 1515.2128 - val_RMSE: 38.7141 - val_loss: 1500.5745 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8976 - loss: 1514.9122 - val_RMSE: 38.7143 - val_loss: 1500.6284 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 16s 6ms/step - RMSE: 38.8964 - loss: 1514.8342 - val_RMSE: 38.7157 - val_loss: 1500.7217 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 02:07:06,390] Trial 19 finished with value: 38.706722259521484 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.08811299737907308, 'dropout_rate': 0.4052062072468139}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.0384 - loss: 3071.1411 - val_RMSE: 38.7142 - val_loss: 1500.9333 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0361 - loss: 1526.0258 - val_RMSE: 38.6967 - val_loss: 1499.7330 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9130 - loss: 1516.5133 - val_RMSE: 38.6891 - val_loss: 1499.0563 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8749 - loss: 1513.4211 - val_RMSE: 38.6889 - val_loss: 1498.7920 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8519 - loss: 1511.3660 - val_RMSE: 38.6881 - val_loss: 1498.4183 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8443 - loss: 1510.4503 - val_RMSE: 38.6856 - val_loss: 1497.9375 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8365 - loss: 1509.5820 - val_RMSE: 38.6860 - val_loss: 1497.7517 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8354 - loss: 1509.2983 - val_RMSE: 38.6856 - val_loss: 1497.6060 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8336 - loss: 1509.0789 - val_RMSE: 38.6861 - val_loss: 1497.6204 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8335 - loss: 1509.0549 - val_RMSE: 38.6848 - val_loss: 1497.5051 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8340 - loss: 1509.0795 - val_RMSE: 38.6846 - val_loss: 1497.4790 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8298 - loss: 1508.7493 - val_RMSE: 38.6844 - val_loss: 1497.4545 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8362 - loss: 1509.2292 - val_RMSE: 38.6840 - val_loss: 1497.4305 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8289 - loss: 1508.6766 - val_RMSE: 38.6843 - val_loss: 1497.4526 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8312 - loss: 1508.8406 - val_RMSE: 38.6832 - val_loss: 1497.3586 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8247 - loss: 1508.3398 - val_RMSE: 38.6833 - val_loss: 1497.3785 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8261 - loss: 1508.4633 - val_RMSE: 38.6837 - val_loss: 1497.4025 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8226 - loss: 1508.1925 - val_RMSE: 38.6833 - val_loss: 1497.3867 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8254 - loss: 1508.4210 - val_RMSE: 38.6833 - val_loss: 1497.3918 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8276 - loss: 1508.5923 - val_RMSE: 38.6823 - val_loss: 1497.3270 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8223 - loss: 1508.1713 - val_RMSE: 38.6819 - val_loss: 1497.2715 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8249 - loss: 1508.3746 - val_RMSE: 38.6822 - val_loss: 1497.2911 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8235 - loss: 1508.2632 - val_RMSE: 38.6819 - val_loss: 1497.2640 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8207 - loss: 1508.0426 - val_RMSE: 38.6826 - val_loss: 1497.3433 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8220 - loss: 1508.1719 - val_RMSE: 38.6823 - val_loss: 1497.3188 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.0019 - loss: 3066.4866 - val_RMSE: 38.7354 - val_loss: 1502.5933 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9869 - loss: 1522.1981 - val_RMSE: 38.7256 - val_loss: 1502.0167 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8696 - loss: 1513.2081 - val_RMSE: 38.7239 - val_loss: 1501.8569 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8212 - loss: 1509.3451 - val_RMSE: 38.7208 - val_loss: 1501.3527 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8023 - loss: 1507.5884 - val_RMSE: 38.7185 - val_loss: 1500.8442 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7922 - loss: 1506.4725 - val_RMSE: 38.7183 - val_loss: 1500.5333 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7862 - loss: 1505.7325 - val_RMSE: 38.7167 - val_loss: 1500.1764 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7869 - loss: 1505.5648 - val_RMSE: 38.7172 - val_loss: 1500.0515 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7854 - loss: 1505.3170 - val_RMSE: 38.7173 - val_loss: 1500.0198 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7815 - loss: 1504.9998 - val_RMSE: 38.7173 - val_loss: 1500.0129 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7798 - loss: 1504.8539 - val_RMSE: 38.7163 - val_loss: 1499.9261 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7812 - loss: 1504.9542 - val_RMSE: 38.7168 - val_loss: 1499.9580 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7781 - loss: 1504.7061 - val_RMSE: 38.7181 - val_loss: 1500.0417 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7781 - loss: 1504.7037 - val_RMSE: 38.7178 - val_loss: 1500.0302 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7808 - loss: 1504.9240 - val_RMSE: 38.7188 - val_loss: 1500.1210 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7756 - loss: 1504.5253 - val_RMSE: 38.7173 - val_loss: 1500.0052 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7756 - loss: 1504.4865 - val_RMSE: 38.7097 - val_loss: 1499.2786 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7702 - loss: 1503.9390 - val_RMSE: 38.7095 - val_loss: 1499.1625 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7666 - loss: 1503.5729 - val_RMSE: 38.7092 - val_loss: 1499.0795 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7697 - loss: 1503.7505 - val_RMSE: 38.7092 - val_loss: 1499.0214 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7644 - loss: 1503.2910 - val_RMSE: 38.7092 - val_loss: 1498.9847 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7707 - loss: 1503.7433 - val_RMSE: 38.7091 - val_loss: 1498.9496 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7616 - loss: 1503.0100 - val_RMSE: 38.7090 - val_loss: 1498.9152 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7664 - loss: 1503.3531 - val_RMSE: 38.7090 - val_loss: 1498.8895 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7642 - loss: 1503.1687 - val_RMSE: 38.7090 - val_loss: 1498.8744 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 53.9871 - loss: 3064.5459 - val_RMSE: 38.7325 - val_loss: 1502.3867 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0128 - loss: 1524.2484 - val_RMSE: 38.7143 - val_loss: 1501.1544 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8919 - loss: 1514.9514 - val_RMSE: 38.7147 - val_loss: 1501.1276 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8487 - loss: 1511.4740 - val_RMSE: 38.7109 - val_loss: 1500.5569 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8346 - loss: 1510.0759 - val_RMSE: 38.7098 - val_loss: 1500.1423 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8176 - loss: 1508.4363 - val_RMSE: 38.7084 - val_loss: 1499.7786 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8156 - loss: 1508.0280 - val_RMSE: 38.7078 - val_loss: 1499.5171 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8111 - loss: 1507.4880 - val_RMSE: 38.7059 - val_loss: 1499.2349 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8094 - loss: 1507.2377 - val_RMSE: 38.7074 - val_loss: 1499.2845 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8119 - loss: 1507.3927 - val_RMSE: 38.7058 - val_loss: 1499.1450 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8076 - loss: 1507.0417 - val_RMSE: 38.7066 - val_loss: 1499.1943 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8069 - loss: 1506.9802 - val_RMSE: 38.7049 - val_loss: 1499.0840 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8039 - loss: 1506.7445 - val_RMSE: 38.7058 - val_loss: 1499.1261 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8066 - loss: 1506.9498 - val_RMSE: 38.7050 - val_loss: 1499.0845 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8027 - loss: 1506.6494 - val_RMSE: 38.7054 - val_loss: 1499.1024 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8048 - loss: 1506.8075 - val_RMSE: 38.7062 - val_loss: 1499.1653 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8049 - loss: 1506.8127 - val_RMSE: 38.7051 - val_loss: 1499.0726 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8022 - loss: 1506.6107 - val_RMSE: 38.7046 - val_loss: 1499.0479 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8015 - loss: 1506.5698 - val_RMSE: 38.7045 - val_loss: 1499.0398 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8015 - loss: 1506.5397 - val_RMSE: 38.7049 - val_loss: 1499.0450 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7939 - loss: 1505.9456 - val_RMSE: 38.7041 - val_loss: 1498.9883 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8000 - loss: 1506.4276 - val_RMSE: 38.7045 - val_loss: 1499.0416 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8001 - loss: 1506.4540 - val_RMSE: 38.7044 - val_loss: 1499.0538 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7984 - loss: 1506.3348 - val_RMSE: 38.7046 - val_loss: 1499.0408 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7958 - loss: 1506.1191 - val_RMSE: 38.7032 - val_loss: 1498.9443 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 87s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 02:38:07,287] Trial 20 finished with value: 38.698177337646484 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'gelu', 'reg': 0.001253520376146537, 'dropout_rate': 0.383273235469179}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 32s 9ms/step - RMSE: 54.2278 - loss: 3090.4089 - val_RMSE: 38.7251 - val_loss: 1500.1178 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0667 - loss: 1526.7211 - val_RMSE: 38.6989 - val_loss: 1498.1919 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.9381 - loss: 1516.7971 - val_RMSE: 38.6926 - val_loss: 1497.8243 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8963 - loss: 1513.6533 - val_RMSE: 38.6882 - val_loss: 1497.5773 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8748 - loss: 1512.0690 - val_RMSE: 38.6857 - val_loss: 1497.4275 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8683 - loss: 1511.5898 - val_RMSE: 38.6845 - val_loss: 1497.3188 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8640 - loss: 1511.2229 - val_RMSE: 38.6848 - val_loss: 1497.2889 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8605 - loss: 1510.9011 - val_RMSE: 38.6833 - val_loss: 1497.1279 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8586 - loss: 1510.7120 - val_RMSE: 38.6836 - val_loss: 1497.1306 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8591 - loss: 1510.7355 - val_RMSE: 38.6826 - val_loss: 1497.0344 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8588 - loss: 1510.6996 - val_RMSE: 38.6817 - val_loss: 1496.9689 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8521 - loss: 1510.1796 - val_RMSE: 38.6818 - val_loss: 1496.9781 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8587 - loss: 1510.7000 - val_RMSE: 38.6816 - val_loss: 1496.9597 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8497 - loss: 1509.9961 - val_RMSE: 38.6817 - val_loss: 1496.9636 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8527 - loss: 1510.2360 - val_RMSE: 38.6812 - val_loss: 1496.9463 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8459 - loss: 1509.7155 - val_RMSE: 38.6816 - val_loss: 1496.9846 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8490 - loss: 1509.9684 - val_RMSE: 38.6807 - val_loss: 1496.9202 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8438 - loss: 1509.5717 - val_RMSE: 38.6810 - val_loss: 1496.9476 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8468 - loss: 1509.8046 - val_RMSE: 38.6806 - val_loss: 1496.9320 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8482 - loss: 1509.9277 - val_RMSE: 38.6807 - val_loss: 1496.9377 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8413 - loss: 1509.3867 - val_RMSE: 38.6804 - val_loss: 1496.9171 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8460 - loss: 1509.7573 - val_RMSE: 38.6805 - val_loss: 1496.9319 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 8ms/step - RMSE: 38.8430 - loss: 1509.5291 - val_RMSE: 38.6805 - val_loss: 1496.9321 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8369 - loss: 1509.0646 - val_RMSE: 38.6803 - val_loss: 1496.9240 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8379 - loss: 1509.1442 - val_RMSE: 38.6798 - val_loss: 1496.9020 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 31s 9ms/step - RMSE: 54.1753 - loss: 3083.8904 - val_RMSE: 38.7344 - val_loss: 1500.8442 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 39.0260 - loss: 1523.5448 - val_RMSE: 38.7225 - val_loss: 1500.0184 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8898 - loss: 1513.0251 - val_RMSE: 38.7197 - val_loss: 1499.8984 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8396 - loss: 1509.2179 - val_RMSE: 38.7181 - val_loss: 1499.8555 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8256 - loss: 1508.2124 - val_RMSE: 38.7181 - val_loss: 1499.9015 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8166 - loss: 1507.5430 - val_RMSE: 38.7169 - val_loss: 1499.8066 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8138 - loss: 1507.3083 - val_RMSE: 38.7160 - val_loss: 1499.6951 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8150 - loss: 1507.3573 - val_RMSE: 38.7165 - val_loss: 1499.6984 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8134 - loss: 1507.2026 - val_RMSE: 38.7148 - val_loss: 1499.5537 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8068 - loss: 1506.6768 - val_RMSE: 38.7147 - val_loss: 1499.5270 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8034 - loss: 1506.4026 - val_RMSE: 38.7146 - val_loss: 1499.5198 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8047 - loss: 1506.5060 - val_RMSE: 38.7145 - val_loss: 1499.5148 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8013 - loss: 1506.2509 - val_RMSE: 38.7147 - val_loss: 1499.5470 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.8000 - loss: 1506.1589 - val_RMSE: 38.7144 - val_loss: 1499.5309 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8043 - loss: 1506.4999 - val_RMSE: 38.7144 - val_loss: 1499.5460 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7966 - loss: 1505.9175 - val_RMSE: 38.7138 - val_loss: 1499.5031 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8050 - loss: 1506.5771 - val_RMSE: 38.7140 - val_loss: 1499.5243 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.7977 - loss: 1506.0137 - val_RMSE: 38.7148 - val_loss: 1499.5895 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 19s 7ms/step - RMSE: 38.7950 - loss: 1505.8059 - val_RMSE: 38.7138 - val_loss: 1499.5175 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8011 - loss: 1506.2845 - val_RMSE: 38.7130 - val_loss: 1499.4683 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.7932 - loss: 1505.6808 - val_RMSE: 38.7146 - val_loss: 1499.5977 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7970 - loss: 1505.9814 - val_RMSE: 38.7146 - val_loss: 1499.5991 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7873 - loss: 1505.2322 - val_RMSE: 38.7132 - val_loss: 1499.4913 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7893 - loss: 1505.3969 - val_RMSE: 38.7138 - val_loss: 1499.5492 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7899 - loss: 1505.4431 - val_RMSE: 38.7137 - val_loss: 1499.5426 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 54.1695 - loss: 3083.0068 - val_RMSE: 38.7236 - val_loss: 1500.0024 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 39.0481 - loss: 1525.2650 - val_RMSE: 38.7175 - val_loss: 1499.6311 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.9107 - loss: 1514.6559 - val_RMSE: 38.7133 - val_loss: 1499.4006 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8713 - loss: 1511.6849 - val_RMSE: 38.7095 - val_loss: 1499.1990 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8561 - loss: 1510.5833 - val_RMSE: 38.7087 - val_loss: 1499.1779 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8389 - loss: 1509.2760 - val_RMSE: 38.7076 - val_loss: 1499.0699 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8417 - loss: 1509.4633 - val_RMSE: 38.7064 - val_loss: 1498.9381 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8383 - loss: 1509.1564 - val_RMSE: 38.7057 - val_loss: 1498.8528 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8345 - loss: 1508.8416 - val_RMSE: 38.7069 - val_loss: 1498.9324 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8368 - loss: 1509.0072 - val_RMSE: 38.7053 - val_loss: 1498.8063 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8312 - loss: 1508.5721 - val_RMSE: 38.7051 - val_loss: 1498.7968 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8313 - loss: 1508.5808 - val_RMSE: 38.7041 - val_loss: 1498.7208 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8297 - loss: 1508.4636 - val_RMSE: 38.7046 - val_loss: 1498.7661 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8301 - loss: 1508.4952 - val_RMSE: 38.7042 - val_loss: 1498.7338 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8247 - loss: 1508.0795 - val_RMSE: 38.7038 - val_loss: 1498.7101 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8252 - loss: 1508.1283 - val_RMSE: 38.7034 - val_loss: 1498.6868 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8228 - loss: 1507.9518 - val_RMSE: 38.7039 - val_loss: 1498.7324 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8224 - loss: 1507.9238 - val_RMSE: 38.7029 - val_loss: 1498.6663 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8237 - loss: 1508.0353 - val_RMSE: 38.7023 - val_loss: 1498.6287 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8227 - loss: 1507.9691 - val_RMSE: 38.7027 - val_loss: 1498.6659 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8133 - loss: 1507.2393 - val_RMSE: 38.7020 - val_loss: 1498.6255 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8179 - loss: 1507.6034 - val_RMSE: 38.7018 - val_loss: 1498.6021 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8183 - loss: 1507.6455 - val_RMSE: 38.7034 - val_loss: 1498.7487 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8161 - loss: 1507.4789 - val_RMSE: 38.7021 - val_loss: 1498.6378 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8130 - loss: 1507.2428 - val_RMSE: 38.7036 - val_loss: 1498.7642 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 90s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 03:10:03,649] Trial 21 finished with value: 38.69902038574219 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00026956737552824854, 'dropout_rate': 0.4450751108802483}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 54.3208 - loss: 3100.5127 - val_RMSE: 38.7158 - val_loss: 1499.3030 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0905 - loss: 1528.4830 - val_RMSE: 38.6945 - val_loss: 1497.7473 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9464 - loss: 1517.3270 - val_RMSE: 38.6909 - val_loss: 1497.5587 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9080 - loss: 1514.4309 - val_RMSE: 38.6876 - val_loss: 1497.3884 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8864 - loss: 1512.8289 - val_RMSE: 38.6866 - val_loss: 1497.3728 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8823 - loss: 1512.5542 - val_RMSE: 38.6843 - val_loss: 1497.2012 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8777 - loss: 1512.1973 - val_RMSE: 38.6838 - val_loss: 1497.1503 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8734 - loss: 1511.8530 - val_RMSE: 38.6831 - val_loss: 1497.0818 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8727 - loss: 1511.7786 - val_RMSE: 38.6836 - val_loss: 1497.1160 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8721 - loss: 1511.7355 - val_RMSE: 38.6828 - val_loss: 1497.0492 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8731 - loss: 1511.8104 - val_RMSE: 38.6819 - val_loss: 1496.9794 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8648 - loss: 1511.1669 - val_RMSE: 38.6822 - val_loss: 1497.0123 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8720 - loss: 1511.7264 - val_RMSE: 38.6823 - val_loss: 1497.0066 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8605 - loss: 1510.8334 - val_RMSE: 38.6821 - val_loss: 1497.0029 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8672 - loss: 1511.3625 - val_RMSE: 38.6815 - val_loss: 1496.9563 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8572 - loss: 1510.5892 - val_RMSE: 38.6814 - val_loss: 1496.9607 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8613 - loss: 1510.9116 - val_RMSE: 38.6809 - val_loss: 1496.9263 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8557 - loss: 1510.4866 - val_RMSE: 38.6811 - val_loss: 1496.9486 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8564 - loss: 1510.5496 - val_RMSE: 38.6810 - val_loss: 1496.9546 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 8ms/step - RMSE: 38.8597 - loss: 1510.8116 - val_RMSE: 38.6814 - val_loss: 1496.9923 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8535 - loss: 1510.3367 - val_RMSE: 38.6811 - val_loss: 1496.9855 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8554 - loss: 1510.5006 - val_RMSE: 38.6810 - val_loss: 1496.9847 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8487 - loss: 1509.9783 - val_RMSE: 38.6790 - val_loss: 1496.7957 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8426 - loss: 1509.4738 - val_RMSE: 38.6788 - val_loss: 1496.7542 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8437 - loss: 1509.5366 - val_RMSE: 38.6787 - val_loss: 1496.7229 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 91s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 54.2604 - loss: 3093.1023 - val_RMSE: 38.7338 - val_loss: 1500.6934 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 39.0424 - loss: 1524.7195 - val_RMSE: 38.7224 - val_loss: 1499.8884 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8997 - loss: 1513.6796 - val_RMSE: 38.7198 - val_loss: 1499.7743 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8526 - loss: 1510.0967 - val_RMSE: 38.7192 - val_loss: 1499.8121 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8367 - loss: 1508.9478 - val_RMSE: 38.7175 - val_loss: 1499.7349 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8296 - loss: 1508.4362 - val_RMSE: 38.7173 - val_loss: 1499.7438 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8281 - loss: 1508.3344 - val_RMSE: 38.7166 - val_loss: 1499.6716 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8283 - loss: 1508.3250 - val_RMSE: 38.7163 - val_loss: 1499.6329 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8280 - loss: 1508.2902 - val_RMSE: 38.7150 - val_loss: 1499.5267 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8199 - loss: 1507.6559 - val_RMSE: 38.7147 - val_loss: 1499.4972 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8151 - loss: 1507.2803 - val_RMSE: 38.7138 - val_loss: 1499.4309 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8192 - loss: 1507.6010 - val_RMSE: 38.7140 - val_loss: 1499.4523 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8156 - loss: 1507.3221 - val_RMSE: 38.7140 - val_loss: 1499.4518 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8118 - loss: 1507.0330 - val_RMSE: 38.7149 - val_loss: 1499.5189 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8178 - loss: 1507.5011 - val_RMSE: 38.7144 - val_loss: 1499.4840 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8098 - loss: 1506.8849 - val_RMSE: 38.7141 - val_loss: 1499.4712 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8145 - loss: 1507.2468 - val_RMSE: 38.7084 - val_loss: 1498.9991 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 8ms/step - RMSE: 38.8040 - loss: 1506.4032 - val_RMSE: 38.7081 - val_loss: 1498.9456 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 8ms/step - RMSE: 38.8022 - loss: 1506.2330 - val_RMSE: 38.7079 - val_loss: 1498.9111 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8066 - loss: 1506.5564 - val_RMSE: 38.7078 - val_loss: 1498.8834 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8003 - loss: 1506.0463 - val_RMSE: 38.7078 - val_loss: 1498.8658 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8064 - loss: 1506.5037 - val_RMSE: 38.7078 - val_loss: 1498.8503 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7963 - loss: 1505.7012 - val_RMSE: 38.7078 - val_loss: 1498.8289 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7990 - loss: 1505.8971 - val_RMSE: 38.7077 - val_loss: 1498.8143 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.7989 - loss: 1505.8805 - val_RMSE: 38.7078 - val_loss: 1498.8075 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 90s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 10ms/step - RMSE: 54.2571 - loss: 3092.4207 - val_RMSE: 38.7283 - val_loss: 1500.2661 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 39.0618 - loss: 1526.2323 - val_RMSE: 38.7137 - val_loss: 1499.2183 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9238 - loss: 1515.5464 - val_RMSE: 38.7104 - val_loss: 1499.0369 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8829 - loss: 1512.4448 - val_RMSE: 38.7091 - val_loss: 1499.0220 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8683 - loss: 1511.3890 - val_RMSE: 38.7083 - val_loss: 1499.0106 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8521 - loss: 1510.1748 - val_RMSE: 38.7077 - val_loss: 1498.9742 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8557 - loss: 1510.4541 - val_RMSE: 38.7066 - val_loss: 1498.8805 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8502 - loss: 1510.0190 - val_RMSE: 38.7059 - val_loss: 1498.8292 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8468 - loss: 1509.7466 - val_RMSE: 38.7065 - val_loss: 1498.8591 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8500 - loss: 1509.9894 - val_RMSE: 38.7048 - val_loss: 1498.7224 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8453 - loss: 1509.6240 - val_RMSE: 38.7048 - val_loss: 1498.7218 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8445 - loss: 1509.5668 - val_RMSE: 38.7035 - val_loss: 1498.6393 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8431 - loss: 1509.4688 - val_RMSE: 38.7042 - val_loss: 1498.6980 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8429 - loss: 1509.4601 - val_RMSE: 38.7042 - val_loss: 1498.7042 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8383 - loss: 1509.1041 - val_RMSE: 38.7028 - val_loss: 1498.6055 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8362 - loss: 1508.9556 - val_RMSE: 38.7037 - val_loss: 1498.6831 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8330 - loss: 1508.7144 - val_RMSE: 38.7025 - val_loss: 1498.5972 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8339 - loss: 1508.7906 - val_RMSE: 38.7024 - val_loss: 1498.5980 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8357 - loss: 1508.9348 - val_RMSE: 38.7022 - val_loss: 1498.5911 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8339 - loss: 1508.8019 - val_RMSE: 38.7027 - val_loss: 1498.6316 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8238 - loss: 1508.0271 - val_RMSE: 38.7012 - val_loss: 1498.5250 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8275 - loss: 1508.3156 - val_RMSE: 38.7011 - val_loss: 1498.5302 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8277 - loss: 1508.3470 - val_RMSE: 38.7032 - val_loss: 1498.7075 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8252 - loss: 1508.1738 - val_RMSE: 38.7020 - val_loss: 1498.6212 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8236 - loss: 1508.0454 - val_RMSE: 38.7028 - val_loss: 1498.6865 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 88s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 03:42:28,918] Trial 22 finished with value: 38.69640350341797 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002133354415296107, 'dropout_rate': 0.47090232042440616}. Best is trial 22 with value: 38.69640350341797.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 54.3926 - loss: 3108.2227 - val_RMSE: 38.7105 - val_loss: 1498.8347 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.1030 - loss: 1529.3966 - val_RMSE: 38.6939 - val_loss: 1497.6128 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9579 - loss: 1518.1373 - val_RMSE: 38.6908 - val_loss: 1497.4556 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9221 - loss: 1515.4268 - val_RMSE: 38.6872 - val_loss: 1497.2570 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9022 - loss: 1513.9501 - val_RMSE: 38.6860 - val_loss: 1497.2203 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8956 - loss: 1513.4907 - val_RMSE: 38.6842 - val_loss: 1497.1089 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8931 - loss: 1513.3197 - val_RMSE: 38.6840 - val_loss: 1497.1019 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8883 - loss: 1512.9509 - val_RMSE: 38.6836 - val_loss: 1497.0714 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8847 - loss: 1512.6777 - val_RMSE: 38.6841 - val_loss: 1497.1112 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8871 - loss: 1512.8662 - val_RMSE: 38.6830 - val_loss: 1497.0245 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8866 - loss: 1512.8291 - val_RMSE: 38.6824 - val_loss: 1496.9918 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8792 - loss: 1512.2565 - val_RMSE: 38.6822 - val_loss: 1496.9862 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8865 - loss: 1512.8339 - val_RMSE: 38.6813 - val_loss: 1496.9243 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8736 - loss: 1511.8363 - val_RMSE: 38.6814 - val_loss: 1496.9330 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8801 - loss: 1512.3489 - val_RMSE: 38.6810 - val_loss: 1496.9075 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8719 - loss: 1511.7194 - val_RMSE: 38.6807 - val_loss: 1496.8986 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8725 - loss: 1511.7711 - val_RMSE: 38.6801 - val_loss: 1496.8505 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8668 - loss: 1511.3331 - val_RMSE: 38.6802 - val_loss: 1496.8661 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8687 - loss: 1511.4895 - val_RMSE: 38.6802 - val_loss: 1496.8788 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8725 - loss: 1511.7947 - val_RMSE: 38.6797 - val_loss: 1496.8425 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8640 - loss: 1511.1376 - val_RMSE: 38.6804 - val_loss: 1496.9043 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8661 - loss: 1511.3091 - val_RMSE: 38.6798 - val_loss: 1496.8618 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8657 - loss: 1511.2834 - val_RMSE: 38.6800 - val_loss: 1496.8942 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8597 - loss: 1510.8302 - val_RMSE: 38.6799 - val_loss: 1496.8971 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8606 - loss: 1510.9110 - val_RMSE: 38.6797 - val_loss: 1496.8875 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 54.3526 - loss: 3103.1470 - val_RMSE: 38.7315 - val_loss: 1500.4688 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0578 - loss: 1525.8635 - val_RMSE: 38.7208 - val_loss: 1499.7095 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9140 - loss: 1514.7240 - val_RMSE: 38.7194 - val_loss: 1499.6794 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8650 - loss: 1510.9910 - val_RMSE: 38.7180 - val_loss: 1499.6410 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8501 - loss: 1509.9091 - val_RMSE: 38.7170 - val_loss: 1499.6246 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8459 - loss: 1509.6292 - val_RMSE: 38.7166 - val_loss: 1499.6102 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8440 - loss: 1509.4971 - val_RMSE: 38.7168 - val_loss: 1499.6326 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8422 - loss: 1509.3621 - val_RMSE: 38.7165 - val_loss: 1499.6129 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8419 - loss: 1509.3339 - val_RMSE: 38.7151 - val_loss: 1499.5034 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8336 - loss: 1508.6904 - val_RMSE: 38.7153 - val_loss: 1499.5222 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8306 - loss: 1508.4633 - val_RMSE: 38.7148 - val_loss: 1499.4825 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8329 - loss: 1508.6409 - val_RMSE: 38.7149 - val_loss: 1499.4979 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8289 - loss: 1508.3391 - val_RMSE: 38.7149 - val_loss: 1499.5079 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8249 - loss: 1508.0361 - val_RMSE: 38.7153 - val_loss: 1499.5428 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8320 - loss: 1508.5900 - val_RMSE: 38.7147 - val_loss: 1499.5044 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8220 - loss: 1507.8259 - val_RMSE: 38.7139 - val_loss: 1499.4518 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8304 - loss: 1508.4857 - val_RMSE: 38.7136 - val_loss: 1499.4414 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8219 - loss: 1507.8411 - val_RMSE: 38.7153 - val_loss: 1499.5741 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8208 - loss: 1507.7557 - val_RMSE: 38.7149 - val_loss: 1499.5541 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8251 - loss: 1508.1027 - val_RMSE: 38.7133 - val_loss: 1499.4435 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8171 - loss: 1507.4922 - val_RMSE: 38.7144 - val_loss: 1499.5375 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8227 - loss: 1507.9366 - val_RMSE: 38.7145 - val_loss: 1499.5503 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8077 - loss: 1506.7759 - val_RMSE: 38.7085 - val_loss: 1499.0688 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8092 - loss: 1506.8654 - val_RMSE: 38.7085 - val_loss: 1499.0408 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8088 - loss: 1506.8107 - val_RMSE: 38.7085 - val_loss: 1499.0178 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 34s 10ms/step - RMSE: 54.3420 - loss: 3101.5876 - val_RMSE: 38.7273 - val_loss: 1500.1343 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0831 - loss: 1527.8464 - val_RMSE: 38.7149 - val_loss: 1499.2526 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9375 - loss: 1516.5575 - val_RMSE: 38.7110 - val_loss: 1499.0278 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8972 - loss: 1513.5001 - val_RMSE: 38.7095 - val_loss: 1498.9880 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8800 - loss: 1512.2302 - val_RMSE: 38.7090 - val_loss: 1499.0065 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8669 - loss: 1511.2573 - val_RMSE: 38.7081 - val_loss: 1498.9573 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8720 - loss: 1511.6740 - val_RMSE: 38.7076 - val_loss: 1498.9209 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8660 - loss: 1511.2126 - val_RMSE: 38.7062 - val_loss: 1498.8158 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8617 - loss: 1510.8754 - val_RMSE: 38.7073 - val_loss: 1498.8992 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8653 - loss: 1511.1545 - val_RMSE: 38.7055 - val_loss: 1498.7643 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8594 - loss: 1510.7045 - val_RMSE: 38.7061 - val_loss: 1498.8151 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8592 - loss: 1510.6851 - val_RMSE: 38.7047 - val_loss: 1498.7052 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8546 - loss: 1510.3394 - val_RMSE: 38.7050 - val_loss: 1498.7295 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8552 - loss: 1510.3861 - val_RMSE: 38.7035 - val_loss: 1498.6238 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8519 - loss: 1510.1377 - val_RMSE: 38.7040 - val_loss: 1498.6747 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8495 - loss: 1509.9532 - val_RMSE: 38.7043 - val_loss: 1498.6893 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8494 - loss: 1509.9414 - val_RMSE: 38.7031 - val_loss: 1498.6071 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8474 - loss: 1509.7963 - val_RMSE: 38.7037 - val_loss: 1498.6588 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8482 - loss: 1509.8666 - val_RMSE: 38.7033 - val_loss: 1498.6365 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8471 - loss: 1509.7942 - val_RMSE: 38.7037 - val_loss: 1498.6721 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 8ms/step - RMSE: 38.8384 - loss: 1509.1168 - val_RMSE: 38.7041 - val_loss: 1498.7097 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8426 - loss: 1509.4498 - val_RMSE: 38.7027 - val_loss: 1498.6167 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8352 - loss: 1508.8816 - val_RMSE: 38.7016 - val_loss: 1498.5007 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8333 - loss: 1508.7089 - val_RMSE: 38.7016 - val_loss: 1498.4830 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8312 - loss: 1508.5260 - val_RMSE: 38.7016 - val_loss: 1498.4552 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 92s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 04:14:41,087] Trial 23 finished with value: 38.69656880696615 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00018285344196860296, 'dropout_rate': 0.4958852821505541}. Best is trial 22 with value: 38.69640350341797.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 10ms/step - RMSE: 54.3742 - loss: 3106.7458 - val_RMSE: 38.7131 - val_loss: 1499.6084 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.1008 - loss: 1529.8162 - val_RMSE: 38.6954 - val_loss: 1498.3958 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9560 - loss: 1518.6693 - val_RMSE: 38.6901 - val_loss: 1498.1415 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9177 - loss: 1515.8304 - val_RMSE: 38.6869 - val_loss: 1497.9764 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8996 - loss: 1514.4752 - val_RMSE: 38.6867 - val_loss: 1497.9011 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8926 - loss: 1513.8458 - val_RMSE: 38.6844 - val_loss: 1497.5876 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8902 - loss: 1513.5194 - val_RMSE: 38.6845 - val_loss: 1497.4938 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8858 - loss: 1513.0938 - val_RMSE: 38.6845 - val_loss: 1497.4396 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8831 - loss: 1512.8446 - val_RMSE: 38.6848 - val_loss: 1497.4486 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8842 - loss: 1512.9211 - val_RMSE: 38.6838 - val_loss: 1497.3635 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8854 - loss: 1513.0110 - val_RMSE: 38.6829 - val_loss: 1497.3047 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8781 - loss: 1512.4506 - val_RMSE: 38.6824 - val_loss: 1497.2651 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8855 - loss: 1513.0215 - val_RMSE: 38.6829 - val_loss: 1497.3037 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8733 - loss: 1512.0736 - val_RMSE: 38.6833 - val_loss: 1497.3385 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8791 - loss: 1512.5287 - val_RMSE: 38.6826 - val_loss: 1497.2965 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8707 - loss: 1511.8906 - val_RMSE: 38.6824 - val_loss: 1497.2834 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8723 - loss: 1512.0159 - val_RMSE: 38.6818 - val_loss: 1497.2421 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8676 - loss: 1511.6558 - val_RMSE: 38.6817 - val_loss: 1497.2469 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8686 - loss: 1511.7430 - val_RMSE: 38.6813 - val_loss: 1497.2260 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8722 - loss: 1512.0331 - val_RMSE: 38.6811 - val_loss: 1497.2048 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8653 - loss: 1511.4901 - val_RMSE: 38.6810 - val_loss: 1497.2039 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8659 - loss: 1511.5535 - val_RMSE: 38.6806 - val_loss: 1497.1833 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8669 - loss: 1511.6404 - val_RMSE: 38.6802 - val_loss: 1497.1604 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8607 - loss: 1511.1592 - val_RMSE: 38.6815 - val_loss: 1497.2582 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8617 - loss: 1511.2428 - val_RMSE: 38.6810 - val_loss: 1497.2234 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 54.3294 - loss: 3101.0984 - val_RMSE: 38.7362 - val_loss: 1501.3876 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0573 - loss: 1526.4099 - val_RMSE: 38.7201 - val_loss: 1500.3046 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9122 - loss: 1515.2542 - val_RMSE: 38.7197 - val_loss: 1500.4049 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8611 - loss: 1511.3992 - val_RMSE: 38.7180 - val_loss: 1500.3362 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8472 - loss: 1510.3560 - val_RMSE: 38.7171 - val_loss: 1500.2311 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8411 - loss: 1509.8203 - val_RMSE: 38.7167 - val_loss: 1500.0773 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8396 - loss: 1509.5865 - val_RMSE: 38.7171 - val_loss: 1500.0164 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8389 - loss: 1509.4502 - val_RMSE: 38.7165 - val_loss: 1499.9355 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8384 - loss: 1509.3826 - val_RMSE: 38.7153 - val_loss: 1499.8303 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8317 - loss: 1508.8485 - val_RMSE: 38.7155 - val_loss: 1499.8304 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 8ms/step - RMSE: 38.8283 - loss: 1508.5773 - val_RMSE: 38.7146 - val_loss: 1499.7714 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 22s 8ms/step - RMSE: 38.8303 - loss: 1508.7450 - val_RMSE: 38.7148 - val_loss: 1499.7863 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8264 - loss: 1508.4324 - val_RMSE: 38.7155 - val_loss: 1499.8446 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8240 - loss: 1508.2527 - val_RMSE: 38.7147 - val_loss: 1499.7885 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8291 - loss: 1508.6553 - val_RMSE: 38.7144 - val_loss: 1499.7731 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8208 - loss: 1508.0199 - val_RMSE: 38.7144 - val_loss: 1499.7777 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8248 - loss: 1508.3191 - val_RMSE: 38.7086 - val_loss: 1499.2599 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8143 - loss: 1507.4417 - val_RMSE: 38.7083 - val_loss: 1499.1759 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8137 - loss: 1507.3374 - val_RMSE: 38.7080 - val_loss: 1499.1064 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8183 - loss: 1507.6465 - val_RMSE: 38.7079 - val_loss: 1499.0591 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8108 - loss: 1507.0297 - val_RMSE: 38.7079 - val_loss: 1499.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8181 - loss: 1507.5647 - val_RMSE: 38.7078 - val_loss: 1498.9885 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8066 - loss: 1506.6410 - val_RMSE: 38.7078 - val_loss: 1498.9668 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8099 - loss: 1506.8774 - val_RMSE: 38.7076 - val_loss: 1498.9280 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8096 - loss: 1506.8282 - val_RMSE: 38.7078 - val_loss: 1498.9246 - learning_rate: 1.0000e-04\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 95s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 54.3186 - loss: 3099.4873 - val_RMSE: 38.7199 - val_loss: 1500.1370 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.0769 - loss: 1527.9541 - val_RMSE: 38.7163 - val_loss: 1500.0200 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9302 - loss: 1516.6616 - val_RMSE: 38.7110 - val_loss: 1499.7379 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8936 - loss: 1513.9352 - val_RMSE: 38.7093 - val_loss: 1499.6638 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8770 - loss: 1512.6691 - val_RMSE: 38.7089 - val_loss: 1499.5791 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8645 - loss: 1511.6184 - val_RMSE: 38.7080 - val_loss: 1499.3773 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8692 - loss: 1511.8556 - val_RMSE: 38.7076 - val_loss: 1499.2562 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8627 - loss: 1511.2736 - val_RMSE: 38.7066 - val_loss: 1499.1486 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8591 - loss: 1510.9698 - val_RMSE: 38.7068 - val_loss: 1499.1526 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8616 - loss: 1511.1550 - val_RMSE: 38.7061 - val_loss: 1499.0885 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8573 - loss: 1510.8187 - val_RMSE: 38.7054 - val_loss: 1499.0260 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8564 - loss: 1510.7313 - val_RMSE: 38.7052 - val_loss: 1499.0048 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8546 - loss: 1510.5918 - val_RMSE: 38.7054 - val_loss: 1499.0148 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8536 - loss: 1510.5154 - val_RMSE: 38.7049 - val_loss: 1498.9822 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8512 - loss: 1510.3356 - val_RMSE: 38.7045 - val_loss: 1498.9658 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8495 - loss: 1510.2122 - val_RMSE: 38.7049 - val_loss: 1498.9963 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8469 - loss: 1510.0051 - val_RMSE: 38.7044 - val_loss: 1498.9785 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8467 - loss: 1510.0103 - val_RMSE: 38.7047 - val_loss: 1498.9928 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8482 - loss: 1510.1261 - val_RMSE: 38.7041 - val_loss: 1498.9618 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8469 - loss: 1510.0453 - val_RMSE: 38.7041 - val_loss: 1498.9664 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8389 - loss: 1509.4177 - val_RMSE: 38.7027 - val_loss: 1498.8628 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8409 - loss: 1509.5818 - val_RMSE: 38.7025 - val_loss: 1498.8490 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8395 - loss: 1509.4774 - val_RMSE: 38.7042 - val_loss: 1498.9983 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8377 - loss: 1509.3455 - val_RMSE: 38.7028 - val_loss: 1498.8794 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8365 - loss: 1509.2500 - val_RMSE: 38.7032 - val_loss: 1498.9156 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 88s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 04:47:00,751] Trial 24 finished with value: 38.69732538859049 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0005025556166064429, 'dropout_rate': 0.48984751751659333}. Best is trial 22 with value: 38.69640350341797.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 10ms/step - RMSE: 54.4057 - loss: 3109.7021 - val_RMSE: 38.7136 - val_loss: 1499.1035 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 39.1025 - loss: 1529.3859 - val_RMSE: 38.6958 - val_loss: 1497.7939 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.9640 - loss: 1518.6431 - val_RMSE: 38.6897 - val_loss: 1497.4073 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.9243 - loss: 1515.6382 - val_RMSE: 38.6868 - val_loss: 1497.2644 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.9051 - loss: 1514.2286 - val_RMSE: 38.6861 - val_loss: 1497.2644 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8989 - loss: 1513.7876 - val_RMSE: 38.6842 - val_loss: 1497.1451 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8961 - loss: 1513.5814 - val_RMSE: 38.6843 - val_loss: 1497.1528 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8911 - loss: 1513.1978 - val_RMSE: 38.6838 - val_loss: 1497.1125 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8882 - loss: 1512.9745 - val_RMSE: 38.6836 - val_loss: 1497.1016 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8899 - loss: 1513.1068 - val_RMSE: 38.6831 - val_loss: 1497.0591 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8898 - loss: 1513.0931 - val_RMSE: 38.6827 - val_loss: 1497.0402 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8824 - loss: 1512.5287 - val_RMSE: 38.6820 - val_loss: 1496.9803 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8896 - loss: 1513.0913 - val_RMSE: 38.6819 - val_loss: 1496.9784 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8778 - loss: 1512.1780 - val_RMSE: 38.6817 - val_loss: 1496.9678 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8827 - loss: 1512.5575 - val_RMSE: 38.6816 - val_loss: 1496.9596 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8761 - loss: 1512.0461 - val_RMSE: 38.6818 - val_loss: 1496.9812 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.8759 - loss: 1512.0347 - val_RMSE: 38.6814 - val_loss: 1496.9561 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8700 - loss: 1511.5802 - val_RMSE: 38.6813 - val_loss: 1496.9491 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8735 - loss: 1511.8597 - val_RMSE: 38.6813 - val_loss: 1496.9619 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8761 - loss: 1512.0709 - val_RMSE: 38.6808 - val_loss: 1496.9270 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8669 - loss: 1511.3622 - val_RMSE: 38.6802 - val_loss: 1496.8854 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 7ms/step - RMSE: 38.8698 - loss: 1511.5920 - val_RMSE: 38.6809 - val_loss: 1496.9432 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8692 - loss: 1511.5511 - val_RMSE: 38.6814 - val_loss: 1496.9891 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8635 - loss: 1511.1118 - val_RMSE: 38.6807 - val_loss: 1496.9469 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 20s 8ms/step - RMSE: 38.8641 - loss: 1511.1655 - val_RMSE: 38.6801 - val_loss: 1496.9037 - learning_rate: 0.0010\n",
            "41608/41608 ━━━━━━━━━━━━━━━━━━━━ 87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 33s 10ms/step - RMSE: 54.3674 - loss: 3104.8064 - val_RMSE: 38.7339 - val_loss: 1500.6754 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 39.0589 - loss: 1525.9745 - val_RMSE: 38.7232 - val_loss: 1499.9159 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 21s 8ms/step - RMSE: 38.9153 - loss: 1514.8567 - val_RMSE: 38.7201 - val_loss: 1499.7589 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "1923/2601 ━━━━━━━━━━━━━━━━━━━━ 4s 7ms/step - RMSE: 38.8621 - loss: 1510.7944"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-16 04:59:10,475] Trial 25 failed with parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00019662830318865977, 'dropout_rate': 0.49978399797646733} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-54-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-53-bcff8c4fc905>\", line 51, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 132, in call_function\n",
            "    function = trace_function(\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n",
            "    concrete_function = _maybe_define_function(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 239, in _maybe_define_function\n",
            "    concrete_function = tracing_options.function_cache.lookup(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\", line 50, in lookup\n",
            "    return self._primary[(context, dispatch_type)]\n",
            "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 456, in __hash__\n",
            "    return hash((tuple(self.parameters.items()), tuple(self.captures.items())))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 893, in __hash__\n",
            "    def __hash__(self):\n",
            "    \n",
            "KeyboardInterrupt\n",
            "[W 2025-02-16 04:59:10,478] Trial 25 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-bcff8c4fc905>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     52\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   function = trace_function(\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     concrete_function = tracing_options.function_cache.lookup(\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_func_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, function_type, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mdispatch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdispatch_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_primary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m         type(self).__name__, self.shape, repr(self.dtype), repr(self.name))\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 22 finished with value: 38.69640350341797\n",
        "* parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002133354415296107, 'dropout_rate': 0.47090232042440616}. Best is trial 22 with value: 38.69640350341797."
      ],
      "metadata": {
        "id": "dwbBHeboznPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9",
        "id": "ayypCUhQQlNh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QaE2SucWPmo"
      },
      "source": [
        "#### **4.6.3 NeuralNetwork v2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "188ce703-686f-421a-9f60-16c93ddd74f5",
        "id": "A9WMnacSWPms"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "3817382      5         4     3             8                   1           1   \n",
              "3390957      3         1     0             4                   1           2   \n",
              "2083310      0         0     1             2                   1           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "3817382      3      3              0.844471  0.725961  1.019273 -1.055655   \n",
              "3390957      3      2              0.618669 -0.149067  0.044250  0.216236   \n",
              "2083310      0      1             -0.588908 -0.824154  0.483511  0.987293   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "3817382           0               0  \n",
              "3390957           0               0  \n",
              "2083310           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2eda4f68-2010-473b-88f1-48a6eae0b4b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3817382</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.844471</td>\n",
              "      <td>0.725961</td>\n",
              "      <td>1.019273</td>\n",
              "      <td>-1.055655</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3390957</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.618669</td>\n",
              "      <td>-0.149067</td>\n",
              "      <td>0.044250</td>\n",
              "      <td>0.216236</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2083310</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.588908</td>\n",
              "      <td>-0.824154</td>\n",
              "      <td>0.483511</td>\n",
              "      <td>0.987293</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2eda4f68-2010-473b-88f1-48a6eae0b4b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2eda4f68-2010-473b-88f1-48a6eae0b4b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2eda4f68-2010-473b-88f1-48a6eae0b4b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7029e872-3655-4aa8-99b1-95e158a855a2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7029e872-3655-4aa8-99b1-95e158a855a2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7029e872-3655-4aa8-99b1-95e158a855a2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2,\n        \"max\": 8,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          8,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.8444705605506897\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.725960910320282\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0192734003067017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.0556546449661255\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512, last_layer=1, activation=\"relu\",repeat_att=2,dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001): # Reduced transformer_units\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    transformer_outputs = [] # List to store transformer outputs for each categorical feature\n",
        "\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:, j])\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        embs.append(x)\n",
        "\n",
        "        # Reshape for Transformer (batch_size, 1, features) - Crucial!\n",
        "        reshaped_embedding = layers.Reshape((1, -1))(x)\n",
        "\n",
        "        # Transformer Layer for each categorical feature\n",
        "        for q in list(range(repeat_att)):\n",
        "          if q == 0:\n",
        "            attention_output = reshaped_embedding\n",
        "\n",
        "          attention_output_ = layers.MultiHeadAttention(num_heads=num_transformer_heads, key_dim=transformer_units,name=f\"mh_{j}_{q}\")(attention_output, attention_output)\n",
        "          attention_output_ = layers.LayerNormalization(name=f\"mh_ln1_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "          attention_output_ = layers.Dense(reshaped_embedding.shape[-1], activation=activation,name=f\"mh_dense_{j}_{q}\")(attention_output_)\n",
        "          attention_output = layers.LayerNormalization(name=f\"mh_ln2_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "\n",
        "        transformer_outputs.append(layers.Flatten()(attention_output)) # Store flattened transformer output\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    x_orig = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    reshaped_features = layers.Reshape((1, -1))(x_orig)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x_orig, flattened_attention])\n",
        "\n",
        "    # Concatenate Transformer outputs and numerical features\n",
        "    all_features = layers.Concatenate(axis=-1)(transformer_outputs + [x])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(all_features)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    #x = layers.Concatenate(axis=-1)([x_orig, x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "T7x-clb6WPms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in range(1):\n",
        "  print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX6xpQsF-vsl",
        "outputId": "325859c0-bfb5-43c8-bf8a-d3592d731d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model()\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "g0C5K7thWPmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(mod_test, show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"TB\")"
      ],
      "metadata": {
        "id": "xsUPc7xAb4zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "id": "8cmiDeT2WPmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D61nTPd2WPmu"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69Nrn1WWPmu"
      },
      "outputs": [],
      "source": [
        "# categorical_feat = t.cat_features.copy()\n",
        "# numerical_feat = t.num_features.copy()\n",
        "\n",
        "# X_train_cat = X_enc[categorical_feat]\n",
        "# X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "# X_test_cat = test_enc[categorical_feat]\n",
        "# X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "# X_train_cat.info()\n",
        "# X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsrSslecWPmu"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "#(units=512, last_layer=1, activation=\"relu\", dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001)\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': 0.0001, #trial.suggest_float('reg', 1e-4, 0.1, log=True),\n",
        "              \"num_transformer_heads\": trial.suggest_int(\"num_transformer_heads\", 2, 4),\n",
        "              \"transformer_units\": trial.suggest_int(\"transformer_units\", 32, 128,step=32),\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.51,step=0.03),\n",
        "              'repeat_att': trial.suggest_categorical('repeat_att', [1,2]),\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num], batch_size=1024)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHngrYKwWPmu"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57a94819-3390-4ad0-a394-174ce31ca217",
        "id": "ghy_gPSMWPmv"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 00:46:45,620] A new study created in memory with name: no-name-dc44f225-28b0-4ec0-8c91-f67686b2cd34\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 149s 35ms/step - RMSE: 42.5423 - loss: 1853.1946 - val_RMSE: 38.7149 - val_loss: 1499.9546 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9247 - loss: 1516.3540 - val_RMSE: 38.7003 - val_loss: 1499.3855 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9184 - loss: 1516.5168 - val_RMSE: 38.7284 - val_loss: 1502.1870 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9076 - loss: 1516.2740 - val_RMSE: 38.7073 - val_loss: 1500.9421 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8804 - loss: 1514.1544 - val_RMSE: 38.6908 - val_loss: 1498.8295 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8641 - loss: 1512.1082 - val_RMSE: 38.6891 - val_loss: 1498.1650 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8571 - loss: 1511.1117 - val_RMSE: 38.6884 - val_loss: 1497.8333 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8573 - loss: 1510.8917 - val_RMSE: 38.6869 - val_loss: 1497.5610 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8491 - loss: 1510.1136 - val_RMSE: 38.6872 - val_loss: 1497.4873 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8484 - loss: 1509.9669 - val_RMSE: 38.6857 - val_loss: 1497.3024 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8528 - loss: 1510.2468 - val_RMSE: 38.6860 - val_loss: 1497.2677 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8579 - loss: 1510.5892 - val_RMSE: 38.6851 - val_loss: 1497.1603 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8491 - loss: 1509.8719 - val_RMSE: 38.6859 - val_loss: 1497.1989 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8485 - loss: 1509.7949 - val_RMSE: 38.6871 - val_loss: 1497.2744 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8413 - loss: 1509.2239 - val_RMSE: 38.6854 - val_loss: 1497.1283 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8417 - loss: 1509.2438 - val_RMSE: 38.6848 - val_loss: 1497.0713 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8416 - loss: 1509.2209 - val_RMSE: 38.6846 - val_loss: 1497.0367 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8447 - loss: 1509.4489 - val_RMSE: 38.6842 - val_loss: 1496.9976 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8436 - loss: 1509.3550 - val_RMSE: 38.6838 - val_loss: 1496.9580 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8409 - loss: 1509.1313 - val_RMSE: 38.6839 - val_loss: 1496.9517 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8394 - loss: 1509.0048 - val_RMSE: 38.6835 - val_loss: 1496.9149 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8418 - loss: 1509.1865 - val_RMSE: 38.6834 - val_loss: 1496.9009 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8338 - loss: 1508.5527 - val_RMSE: 38.6832 - val_loss: 1496.8790 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8419 - loss: 1509.1719 - val_RMSE: 38.6832 - val_loss: 1496.8655 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8380 - loss: 1508.8633 - val_RMSE: 38.6831 - val_loss: 1496.8566 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 152s 35ms/step - RMSE: 42.4635 - loss: 1845.5852 - val_RMSE: 38.7378 - val_loss: 1501.7828 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8727 - loss: 1512.3889 - val_RMSE: 38.7483 - val_loss: 1503.2280 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8710 - loss: 1512.9287 - val_RMSE: 38.7473 - val_loss: 1503.9238 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8331 - loss: 1510.4143 - val_RMSE: 38.7256 - val_loss: 1501.5660 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8221 - loss: 1508.8901 - val_RMSE: 38.7289 - val_loss: 1501.3015 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8085 - loss: 1507.3918 - val_RMSE: 38.7245 - val_loss: 1500.6874 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8097 - loss: 1507.2500 - val_RMSE: 38.7236 - val_loss: 1500.4556 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8165 - loss: 1507.6310 - val_RMSE: 38.7238 - val_loss: 1500.3698 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8052 - loss: 1506.6687 - val_RMSE: 38.7224 - val_loss: 1500.1976 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8041 - loss: 1506.5164 - val_RMSE: 38.7234 - val_loss: 1500.2178 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8052 - loss: 1506.5483 - val_RMSE: 38.7254 - val_loss: 1500.3353 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8051 - loss: 1506.5110 - val_RMSE: 38.7175 - val_loss: 1499.7087 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8017 - loss: 1506.2274 - val_RMSE: 38.7176 - val_loss: 1499.6948 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.7980 - loss: 1505.9227 - val_RMSE: 38.7168 - val_loss: 1499.6138 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.7999 - loss: 1506.0510 - val_RMSE: 38.7167 - val_loss: 1499.5980 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8006 - loss: 1506.0966 - val_RMSE: 38.7167 - val_loss: 1499.5845 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.7930 - loss: 1505.4945 - val_RMSE: 38.7163 - val_loss: 1499.5337 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7964 - loss: 1505.7448 - val_RMSE: 38.7163 - val_loss: 1499.5311 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8012 - loss: 1506.1034 - val_RMSE: 38.7164 - val_loss: 1499.5283 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8004 - loss: 1506.0343 - val_RMSE: 38.7161 - val_loss: 1499.4877 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7983 - loss: 1505.8585 - val_RMSE: 38.7167 - val_loss: 1499.5251 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.7916 - loss: 1505.3295 - val_RMSE: 38.7166 - val_loss: 1499.5121 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7955 - loss: 1505.6284 - val_RMSE: 38.7167 - val_loss: 1499.5171 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7963 - loss: 1505.6843 - val_RMSE: 38.7165 - val_loss: 1499.4994 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7963 - loss: 1505.6914 - val_RMSE: 38.7164 - val_loss: 1499.4944 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 16s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 149s 35ms/step - RMSE: 42.4441 - loss: 1843.2596 - val_RMSE: 38.7304 - val_loss: 1501.2000 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.9038 - loss: 1514.7972 - val_RMSE: 38.7187 - val_loss: 1500.8505 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8969 - loss: 1514.8219 - val_RMSE: 38.7294 - val_loss: 1502.3921 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8973 - loss: 1515.5651 - val_RMSE: 38.7309 - val_loss: 1503.0609 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8497 - loss: 1512.0414 - val_RMSE: 38.7132 - val_loss: 1500.7770 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8386 - loss: 1510.3381 - val_RMSE: 38.7125 - val_loss: 1500.1444 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8348 - loss: 1509.5366 - val_RMSE: 38.7120 - val_loss: 1499.7932 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8308 - loss: 1508.9564 - val_RMSE: 38.7135 - val_loss: 1499.7236 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8265 - loss: 1508.4512 - val_RMSE: 38.7110 - val_loss: 1499.4092 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 57s 22ms/step - RMSE: 38.8278 - loss: 1508.4470 - val_RMSE: 38.7111 - val_loss: 1499.3374 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8293 - loss: 1508.4926 - val_RMSE: 38.7108 - val_loss: 1499.2607 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8343 - loss: 1508.8291 - val_RMSE: 38.7101 - val_loss: 1499.1632 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8220 - loss: 1507.8356 - val_RMSE: 38.7096 - val_loss: 1499.0898 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8270 - loss: 1508.1875 - val_RMSE: 38.7110 - val_loss: 1499.1808 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8256 - loss: 1508.0579 - val_RMSE: 38.7101 - val_loss: 1499.0874 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8245 - loss: 1507.9469 - val_RMSE: 38.7112 - val_loss: 1499.1582 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 57s 22ms/step - RMSE: 38.8216 - loss: 1507.7120 - val_RMSE: 38.7095 - val_loss: 1499.0039 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8235 - loss: 1507.8420 - val_RMSE: 38.7105 - val_loss: 1499.0677 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8223 - loss: 1507.7360 - val_RMSE: 38.7091 - val_loss: 1498.9581 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8203 - loss: 1507.5741 - val_RMSE: 38.7125 - val_loss: 1499.2124 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8198 - loss: 1507.5265 - val_RMSE: 38.7109 - val_loss: 1499.0803 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8171 - loss: 1507.3075 - val_RMSE: 38.7096 - val_loss: 1498.9631 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8114 - loss: 1506.8531 - val_RMSE: 38.7094 - val_loss: 1498.9340 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 57s 22ms/step - RMSE: 38.8078 - loss: 1506.5636 - val_RMSE: 38.7092 - val_loss: 1498.9062 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8137 - loss: 1507.0103 - val_RMSE: 38.7090 - val_loss: 1498.8834 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 16s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 02:06:26,496] Trial 0 finished with value: 38.70286560058594 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.44999999999999996, 'repeat_att': 1}. Best is trial 0 with value: 38.70286560058594.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 154s 35ms/step - RMSE: 43.3852 - loss: 1934.7306 - val_RMSE: 38.7027 - val_loss: 1498.9362 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 39.0693 - loss: 1527.6146 - val_RMSE: 38.6950 - val_loss: 1498.9910 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 39.0576 - loss: 1527.3376 - val_RMSE: 38.6957 - val_loss: 1499.5364 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9966 - loss: 1522.7792 - val_RMSE: 38.6897 - val_loss: 1498.5286 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9863 - loss: 1521.4486 - val_RMSE: 38.6876 - val_loss: 1497.9673 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9687 - loss: 1519.7284 - val_RMSE: 38.6857 - val_loss: 1497.5868 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9707 - loss: 1519.6841 - val_RMSE: 38.6867 - val_loss: 1497.5294 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9633 - loss: 1518.9814 - val_RMSE: 38.6871 - val_loss: 1497.4590 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9621 - loss: 1518.7979 - val_RMSE: 38.6859 - val_loss: 1497.2992 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9610 - loss: 1518.6493 - val_RMSE: 38.6857 - val_loss: 1497.2311 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9632 - loss: 1518.7689 - val_RMSE: 38.6860 - val_loss: 1497.2183 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9496 - loss: 1517.6786 - val_RMSE: 38.6855 - val_loss: 1497.1537 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9574 - loss: 1518.2554 - val_RMSE: 38.6853 - val_loss: 1497.1129 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9451 - loss: 1517.2749 - val_RMSE: 38.6860 - val_loss: 1497.1459 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9440 - loss: 1517.1774 - val_RMSE: 38.6850 - val_loss: 1497.0625 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9448 - loss: 1517.2255 - val_RMSE: 38.6860 - val_loss: 1497.1222 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9347 - loss: 1516.4252 - val_RMSE: 38.6840 - val_loss: 1496.9670 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9362 - loss: 1516.5424 - val_RMSE: 38.6845 - val_loss: 1496.9954 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9342 - loss: 1516.3783 - val_RMSE: 38.6845 - val_loss: 1496.9844 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9261 - loss: 1515.7404 - val_RMSE: 38.6839 - val_loss: 1496.9280 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9264 - loss: 1515.7483 - val_RMSE: 38.6836 - val_loss: 1496.8975 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9230 - loss: 1515.4722 - val_RMSE: 38.6835 - val_loss: 1496.8818 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9199 - loss: 1515.2224 - val_RMSE: 38.6836 - val_loss: 1496.8789 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9283 - loss: 1515.8676 - val_RMSE: 38.6840 - val_loss: 1496.9043 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9230 - loss: 1515.4517 - val_RMSE: 38.6840 - val_loss: 1496.8937 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 16s 9ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 147s 35ms/step - RMSE: 43.3502 - loss: 1931.5552 - val_RMSE: 38.7316 - val_loss: 1501.1777 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 39.0198 - loss: 1523.7283 - val_RMSE: 38.7399 - val_loss: 1502.3658 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9995 - loss: 1522.7172 - val_RMSE: 38.7387 - val_loss: 1502.7998 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9452 - loss: 1518.7039 - val_RMSE: 38.7208 - val_loss: 1500.8723 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9316 - loss: 1517.1335 - val_RMSE: 38.7213 - val_loss: 1500.5249 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9314 - loss: 1516.7751 - val_RMSE: 38.7199 - val_loss: 1500.1968 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9249 - loss: 1516.0735 - val_RMSE: 38.7192 - val_loss: 1500.0155 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9205 - loss: 1515.6187 - val_RMSE: 38.7187 - val_loss: 1499.8903 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9211 - loss: 1515.5809 - val_RMSE: 38.7194 - val_loss: 1499.8732 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9138 - loss: 1514.9574 - val_RMSE: 38.7185 - val_loss: 1499.7616 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9080 - loss: 1514.4657 - val_RMSE: 38.7178 - val_loss: 1499.6732 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9072 - loss: 1514.3712 - val_RMSE: 38.7176 - val_loss: 1499.6364 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.9051 - loss: 1514.1798 - val_RMSE: 38.7188 - val_loss: 1499.7069 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8932 - loss: 1513.2394 - val_RMSE: 38.7182 - val_loss: 1499.6433 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8962 - loss: 1513.4537 - val_RMSE: 38.7157 - val_loss: 1499.4407 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8876 - loss: 1512.7773 - val_RMSE: 38.7159 - val_loss: 1499.4401 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8815 - loss: 1512.2913 - val_RMSE: 38.7154 - val_loss: 1499.3939 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8888 - loss: 1512.8477 - val_RMSE: 38.7155 - val_loss: 1499.3929 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8924 - loss: 1513.1184 - val_RMSE: 38.7154 - val_loss: 1499.3776 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8854 - loss: 1512.5703 - val_RMSE: 38.7153 - val_loss: 1499.3597 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8868 - loss: 1512.6654 - val_RMSE: 38.7155 - val_loss: 1499.3667 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8918 - loss: 1513.0470 - val_RMSE: 38.7155 - val_loss: 1499.3618 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8939 - loss: 1513.2028 - val_RMSE: 38.7155 - val_loss: 1499.3582 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8927 - loss: 1513.1119 - val_RMSE: 38.7155 - val_loss: 1499.3569 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8863 - loss: 1512.6150 - val_RMSE: 38.7154 - val_loss: 1499.3484 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 17s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 157s 36ms/step - RMSE: 43.2625 - loss: 1922.0507 - val_RMSE: 38.7210 - val_loss: 1500.3311 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 39.0474 - loss: 1525.8640 - val_RMSE: 38.7199 - val_loss: 1500.7574 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 39.0306 - loss: 1525.0643 - val_RMSE: 38.7252 - val_loss: 1501.7371 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9748 - loss: 1520.9982 - val_RMSE: 38.7165 - val_loss: 1500.5239 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9541 - loss: 1518.8676 - val_RMSE: 38.7149 - val_loss: 1500.0076 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9517 - loss: 1518.3409 - val_RMSE: 38.7143 - val_loss: 1499.7439 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9435 - loss: 1517.5121 - val_RMSE: 38.7116 - val_loss: 1499.4058 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9468 - loss: 1517.6493 - val_RMSE: 38.7124 - val_loss: 1499.3759 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9462 - loss: 1517.5144 - val_RMSE: 38.7118 - val_loss: 1499.2648 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9401 - loss: 1516.9845 - val_RMSE: 38.7118 - val_loss: 1499.2213 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9376 - loss: 1516.7467 - val_RMSE: 38.7111 - val_loss: 1499.1396 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 24ms/step - RMSE: 38.9317 - loss: 1516.2589 - val_RMSE: 38.7106 - val_loss: 1499.0748 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9274 - loss: 1515.9056 - val_RMSE: 38.7104 - val_loss: 1499.0415 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9280 - loss: 1515.9312 - val_RMSE: 38.7108 - val_loss: 1499.0625 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9181 - loss: 1515.1509 - val_RMSE: 38.7099 - val_loss: 1498.9727 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9195 - loss: 1515.2476 - val_RMSE: 38.7110 - val_loss: 1499.0500 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 65s 25ms/step - RMSE: 38.9204 - loss: 1515.3063 - val_RMSE: 38.7099 - val_loss: 1498.9590 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9077 - loss: 1514.3134 - val_RMSE: 38.7093 - val_loss: 1498.9106 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9087 - loss: 1514.3798 - val_RMSE: 38.7105 - val_loss: 1498.9954 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9068 - loss: 1514.2285 - val_RMSE: 38.7107 - val_loss: 1499.0084 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 24ms/step - RMSE: 38.8995 - loss: 1513.6593 - val_RMSE: 38.7079 - val_loss: 1498.7753 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8973 - loss: 1513.4734 - val_RMSE: 38.7077 - val_loss: 1498.7532 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 24ms/step - RMSE: 38.8961 - loss: 1513.3704 - val_RMSE: 38.7082 - val_loss: 1498.7817 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 25ms/step - RMSE: 38.9010 - loss: 1513.7474 - val_RMSE: 38.7080 - val_loss: 1498.7625 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 25ms/step - RMSE: 38.9025 - loss: 1513.8590 - val_RMSE: 38.7075 - val_loss: 1498.7212 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 17s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 03:28:09,029] Trial 1 finished with value: 38.70228703816732 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'selu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.48, 'repeat_att': 1}. Best is trial 1 with value: 38.70228703816732.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 147s 35ms/step - RMSE: 43.2199 - loss: 1919.5537 - val_RMSE: 38.7000 - val_loss: 1498.5961 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9782 - loss: 1520.3369 - val_RMSE: 38.6960 - val_loss: 1498.7506 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9793 - loss: 1520.8539 - val_RMSE: 38.7000 - val_loss: 1499.3563 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9302 - loss: 1517.1078 - val_RMSE: 38.6875 - val_loss: 1497.9254 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9197 - loss: 1515.8606 - val_RMSE: 38.6853 - val_loss: 1497.4617 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9056 - loss: 1514.5089 - val_RMSE: 38.6849 - val_loss: 1497.2765 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9104 - loss: 1514.7513 - val_RMSE: 38.6849 - val_loss: 1497.1888 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9049 - loss: 1514.2415 - val_RMSE: 38.6845 - val_loss: 1497.0953 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9060 - loss: 1514.2686 - val_RMSE: 38.6845 - val_loss: 1497.0533 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9000 - loss: 1513.7655 - val_RMSE: 38.6841 - val_loss: 1496.9913 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9058 - loss: 1514.1903 - val_RMSE: 38.6838 - val_loss: 1496.9438 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8927 - loss: 1513.1487 - val_RMSE: 38.6834 - val_loss: 1496.9041 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9035 - loss: 1513.9727 - val_RMSE: 38.6831 - val_loss: 1496.8627 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8964 - loss: 1513.4114 - val_RMSE: 38.6833 - val_loss: 1496.8708 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8967 - loss: 1513.4277 - val_RMSE: 38.6825 - val_loss: 1496.8010 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8926 - loss: 1513.1035 - val_RMSE: 38.6832 - val_loss: 1496.8527 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8866 - loss: 1512.6248 - val_RMSE: 38.6826 - val_loss: 1496.8025 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8882 - loss: 1512.7504 - val_RMSE: 38.6817 - val_loss: 1496.7234 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8858 - loss: 1512.5521 - val_RMSE: 38.6815 - val_loss: 1496.6926 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8798 - loss: 1512.0736 - val_RMSE: 38.6814 - val_loss: 1496.6783 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8808 - loss: 1512.1464 - val_RMSE: 38.6811 - val_loss: 1496.6537 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8835 - loss: 1512.3491 - val_RMSE: 38.6811 - val_loss: 1496.6418 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8788 - loss: 1511.9792 - val_RMSE: 38.6811 - val_loss: 1496.6355 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8838 - loss: 1512.3633 - val_RMSE: 38.6809 - val_loss: 1496.6188 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8828 - loss: 1512.2748 - val_RMSE: 38.6809 - val_loss: 1496.6119 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 18s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 148s 35ms/step - RMSE: 43.1828 - loss: 1916.1990 - val_RMSE: 38.7341 - val_loss: 1501.2308 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9260 - loss: 1516.2416 - val_RMSE: 38.7522 - val_loss: 1503.1017 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9187 - loss: 1516.1676 - val_RMSE: 38.7418 - val_loss: 1502.6028 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8751 - loss: 1512.8329 - val_RMSE: 38.7197 - val_loss: 1500.4335 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8671 - loss: 1511.7841 - val_RMSE: 38.7194 - val_loss: 1500.1112 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8683 - loss: 1511.6200 - val_RMSE: 38.7181 - val_loss: 1499.8542 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8590 - loss: 1510.7631 - val_RMSE: 38.7174 - val_loss: 1499.7125 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8560 - loss: 1510.4453 - val_RMSE: 38.7172 - val_loss: 1499.6349 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8590 - loss: 1510.6244 - val_RMSE: 38.7175 - val_loss: 1499.6127 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8564 - loss: 1510.3749 - val_RMSE: 38.7168 - val_loss: 1499.5217 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8506 - loss: 1509.9030 - val_RMSE: 38.7158 - val_loss: 1499.4307 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8516 - loss: 1509.9583 - val_RMSE: 38.7153 - val_loss: 1499.3761 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8479 - loss: 1509.6581 - val_RMSE: 38.7153 - val_loss: 1499.3641 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8450 - loss: 1509.4153 - val_RMSE: 38.7172 - val_loss: 1499.5005 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8449 - loss: 1509.4033 - val_RMSE: 38.7163 - val_loss: 1499.4207 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8356 - loss: 1508.6681 - val_RMSE: 38.7119 - val_loss: 1499.0619 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8357 - loss: 1508.6619 - val_RMSE: 38.7116 - val_loss: 1499.0386 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8349 - loss: 1508.5945 - val_RMSE: 38.7117 - val_loss: 1499.0326 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8381 - loss: 1508.8324 - val_RMSE: 38.7116 - val_loss: 1499.0187 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8330 - loss: 1508.4298 - val_RMSE: 38.7115 - val_loss: 1499.0021 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8333 - loss: 1508.4491 - val_RMSE: 38.7115 - val_loss: 1498.9976 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8369 - loss: 1508.7228 - val_RMSE: 38.7116 - val_loss: 1498.9965 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8395 - loss: 1508.9120 - val_RMSE: 38.7117 - val_loss: 1499.0038 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8379 - loss: 1508.7859 - val_RMSE: 38.7117 - val_loss: 1498.9905 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8321 - loss: 1508.3302 - val_RMSE: 38.7114 - val_loss: 1498.9688 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 17s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 155s 36ms/step - RMSE: 43.1270 - loss: 1909.9421 - val_RMSE: 38.7296 - val_loss: 1500.8849 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9558 - loss: 1518.5751 - val_RMSE: 38.7394 - val_loss: 1502.0955 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9556 - loss: 1518.9839 - val_RMSE: 38.7361 - val_loss: 1502.3036 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9071 - loss: 1515.4553 - val_RMSE: 38.7132 - val_loss: 1500.0302 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8936 - loss: 1513.9342 - val_RMSE: 38.7108 - val_loss: 1499.5002 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8915 - loss: 1513.4712 - val_RMSE: 38.7099 - val_loss: 1499.2487 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8826 - loss: 1512.6215 - val_RMSE: 38.7114 - val_loss: 1499.2659 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8900 - loss: 1513.1058 - val_RMSE: 38.7099 - val_loss: 1499.0817 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8863 - loss: 1512.7578 - val_RMSE: 38.7094 - val_loss: 1498.9949 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8821 - loss: 1512.3795 - val_RMSE: 38.7107 - val_loss: 1499.0641 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8840 - loss: 1512.5049 - val_RMSE: 38.7088 - val_loss: 1498.8883 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8795 - loss: 1512.1324 - val_RMSE: 38.7078 - val_loss: 1498.7916 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8736 - loss: 1511.6577 - val_RMSE: 38.7090 - val_loss: 1498.8766 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8795 - loss: 1512.0961 - val_RMSE: 38.7102 - val_loss: 1498.9519 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8671 - loss: 1511.1216 - val_RMSE: 38.7067 - val_loss: 1498.6674 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8667 - loss: 1511.0752 - val_RMSE: 38.7062 - val_loss: 1498.6179 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8728 - loss: 1511.5441 - val_RMSE: 38.7059 - val_loss: 1498.5858 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8655 - loss: 1510.9685 - val_RMSE: 38.7058 - val_loss: 1498.5698 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8659 - loss: 1510.9923 - val_RMSE: 38.7056 - val_loss: 1498.5522 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8658 - loss: 1510.9742 - val_RMSE: 38.7058 - val_loss: 1498.5548 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 70s 27ms/step - RMSE: 38.8660 - loss: 1510.9816 - val_RMSE: 38.7055 - val_loss: 1498.5292 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 69s 27ms/step - RMSE: 38.8629 - loss: 1510.7365 - val_RMSE: 38.7055 - val_loss: 1498.5220 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 67s 26ms/step - RMSE: 38.8638 - loss: 1510.8021 - val_RMSE: 38.7057 - val_loss: 1498.5306 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8657 - loss: 1510.9388 - val_RMSE: 38.7055 - val_loss: 1498.5111 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 67s 26ms/step - RMSE: 38.8657 - loss: 1510.9323 - val_RMSE: 38.7055 - val_loss: 1498.5066 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 17s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 04:50:47,418] Trial 2 finished with value: 38.699286142985024 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 153s 37ms/step - RMSE: 43.1467 - loss: 1912.6401 - val_RMSE: 38.6983 - val_loss: 1497.9614 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9335 - loss: 1516.2772 - val_RMSE: 38.6935 - val_loss: 1497.8076 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9336 - loss: 1516.4974 - val_RMSE: 38.6955 - val_loss: 1498.1663 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9154 - loss: 1515.2963 - val_RMSE: 38.6973 - val_loss: 1498.4552 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8831 - loss: 1512.8254 - val_RMSE: 38.6868 - val_loss: 1497.4468 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8683 - loss: 1511.4871 - val_RMSE: 38.6863 - val_loss: 1497.2582 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8694 - loss: 1511.4333 - val_RMSE: 38.6862 - val_loss: 1497.1595 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8638 - loss: 1510.9166 - val_RMSE: 38.6854 - val_loss: 1497.0293 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8688 - loss: 1511.2439 - val_RMSE: 38.6858 - val_loss: 1497.0173 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8612 - loss: 1510.6085 - val_RMSE: 38.6854 - val_loss: 1496.9486 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8672 - loss: 1511.0455 - val_RMSE: 38.6846 - val_loss: 1496.8655 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8593 - loss: 1510.4016 - val_RMSE: 38.6850 - val_loss: 1496.8708 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8674 - loss: 1511.0186 - val_RMSE: 38.6845 - val_loss: 1496.8170 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8612 - loss: 1510.5210 - val_RMSE: 38.6855 - val_loss: 1496.8867 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8607 - loss: 1510.4718 - val_RMSE: 38.6845 - val_loss: 1496.7950 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8577 - loss: 1510.2253 - val_RMSE: 38.6849 - val_loss: 1496.8239 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8546 - loss: 1509.9801 - val_RMSE: 38.6836 - val_loss: 1496.7120 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8552 - loss: 1510.0190 - val_RMSE: 38.6834 - val_loss: 1496.6898 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8520 - loss: 1509.7625 - val_RMSE: 38.6835 - val_loss: 1496.6901 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8515 - loss: 1509.7184 - val_RMSE: 38.6841 - val_loss: 1496.7323 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8479 - loss: 1509.4292 - val_RMSE: 38.6834 - val_loss: 1496.6722 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8515 - loss: 1509.7061 - val_RMSE: 38.6829 - val_loss: 1496.6322 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 24ms/step - RMSE: 38.8445 - loss: 1509.1569 - val_RMSE: 38.6829 - val_loss: 1496.6284 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8512 - loss: 1509.6818 - val_RMSE: 38.6827 - val_loss: 1496.6064 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8499 - loss: 1509.5737 - val_RMSE: 38.6826 - val_loss: 1496.6011 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 15s 9ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 159s 38ms/step - RMSE: 43.0882 - loss: 1907.0781 - val_RMSE: 38.7436 - val_loss: 1501.4581 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8796 - loss: 1512.0743 - val_RMSE: 38.7295 - val_loss: 1500.6298 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8777 - loss: 1512.2151 - val_RMSE: 38.7367 - val_loss: 1501.4878 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 65s 25ms/step - RMSE: 38.8659 - loss: 1511.5642 - val_RMSE: 38.7309 - val_loss: 1501.2053 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8290 - loss: 1508.7550 - val_RMSE: 38.7219 - val_loss: 1500.2830 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 65s 25ms/step - RMSE: 38.8282 - loss: 1508.4794 - val_RMSE: 38.7212 - val_loss: 1500.0447 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8215 - loss: 1507.7941 - val_RMSE: 38.7204 - val_loss: 1499.8717 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 26ms/step - RMSE: 38.8174 - loss: 1507.3712 - val_RMSE: 38.7206 - val_loss: 1499.8170 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 68s 26ms/step - RMSE: 38.8207 - loss: 1507.5630 - val_RMSE: 38.7213 - val_loss: 1499.8226 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 68s 26ms/step - RMSE: 38.8193 - loss: 1507.4075 - val_RMSE: 38.7201 - val_loss: 1499.6876 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8152 - loss: 1507.0485 - val_RMSE: 38.7198 - val_loss: 1499.6343 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8172 - loss: 1507.1785 - val_RMSE: 38.7192 - val_loss: 1499.5641 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8114 - loss: 1506.7085 - val_RMSE: 38.7183 - val_loss: 1499.4696 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 26ms/step - RMSE: 38.8119 - loss: 1506.7212 - val_RMSE: 38.7187 - val_loss: 1499.4884 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8116 - loss: 1506.6865 - val_RMSE: 38.7199 - val_loss: 1499.5673 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8046 - loss: 1506.1307 - val_RMSE: 38.7150 - val_loss: 1499.1841 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 25ms/step - RMSE: 38.8029 - loss: 1505.9951 - val_RMSE: 38.7150 - val_loss: 1499.1755 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 67s 26ms/step - RMSE: 38.8021 - loss: 1505.9320 - val_RMSE: 38.7147 - val_loss: 1499.1523 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8067 - loss: 1506.2850 - val_RMSE: 38.7145 - val_loss: 1499.1357 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8023 - loss: 1505.9385 - val_RMSE: 38.7146 - val_loss: 1499.1328 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 67s 26ms/step - RMSE: 38.8052 - loss: 1506.1622 - val_RMSE: 38.7145 - val_loss: 1499.1202 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8058 - loss: 1506.2058 - val_RMSE: 38.7143 - val_loss: 1499.1085 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 68s 26ms/step - RMSE: 38.8083 - loss: 1506.3923 - val_RMSE: 38.7142 - val_loss: 1499.0948 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 68s 26ms/step - RMSE: 38.8070 - loss: 1506.2932 - val_RMSE: 38.7142 - val_loss: 1499.0923 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8021 - loss: 1505.9102 - val_RMSE: 38.7140 - val_loss: 1499.0754 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 117s 27ms/step - RMSE: 43.0855 - loss: 1906.2587 - val_RMSE: 38.7267 - val_loss: 1500.1206 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9105 - loss: 1514.4437 - val_RMSE: 38.7184 - val_loss: 1499.7040 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9080 - loss: 1514.4988 - val_RMSE: 38.7158 - val_loss: 1499.8019 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8925 - loss: 1513.6099 - val_RMSE: 38.7175 - val_loss: 1500.1287 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8570 - loss: 1510.8955 - val_RMSE: 38.7136 - val_loss: 1499.6031 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8476 - loss: 1509.9430 - val_RMSE: 38.7124 - val_loss: 1499.3268 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8423 - loss: 1509.3777 - val_RMSE: 38.7118 - val_loss: 1499.1818 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8473 - loss: 1509.6743 - val_RMSE: 38.7117 - val_loss: 1499.1071 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8451 - loss: 1509.4404 - val_RMSE: 38.7103 - val_loss: 1498.9496 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8402 - loss: 1509.0154 - val_RMSE: 38.7104 - val_loss: 1498.9240 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8427 - loss: 1509.1755 - val_RMSE: 38.7087 - val_loss: 1498.7606 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8390 - loss: 1508.8665 - val_RMSE: 38.7081 - val_loss: 1498.6951 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8345 - loss: 1508.4982 - val_RMSE: 38.7088 - val_loss: 1498.7297 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8402 - loss: 1508.9176 - val_RMSE: 38.7108 - val_loss: 1498.8707 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8328 - loss: 1508.3344 - val_RMSE: 38.7076 - val_loss: 1498.6187 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8302 - loss: 1508.1289 - val_RMSE: 38.7085 - val_loss: 1498.6838 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8355 - loss: 1508.5327 - val_RMSE: 38.7080 - val_loss: 1498.6445 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8296 - loss: 1508.0712 - val_RMSE: 38.7077 - val_loss: 1498.6215 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8297 - loss: 1508.0819 - val_RMSE: 38.7077 - val_loss: 1498.6205 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8306 - loss: 1508.1505 - val_RMSE: 38.7078 - val_loss: 1498.6224 - learning_rate: 1.0000e-06\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8293 - loss: 1508.0455 - val_RMSE: 38.7079 - val_loss: 1498.6296 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8299 - loss: 1508.0964 - val_RMSE: 38.7076 - val_loss: 1498.6091 - learning_rate: 1.0000e-07\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8292 - loss: 1508.0363 - val_RMSE: 38.7081 - val_loss: 1498.6482 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8291 - loss: 1508.0310 - val_RMSE: 38.7076 - val_loss: 1498.6090 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8305 - loss: 1508.1370 - val_RMSE: 38.7077 - val_loss: 1498.6216 - learning_rate: 1.0000e-07\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 06:05:34,671] Trial 3 finished with value: 38.701454162597656 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 4, 'transformer_units': 128, 'dropout_rate': 0.32999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 114s 25ms/step - RMSE: 43.1435 - loss: 1912.3213 - val_RMSE: 38.7015 - val_loss: 1498.2279 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.9314 - loss: 1516.1379 - val_RMSE: 38.6943 - val_loss: 1497.9430 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 13ms/step - RMSE: 38.9339 - loss: 1516.6235 - val_RMSE: 38.7009 - val_loss: 1498.7732 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.9153 - loss: 1515.4824 - val_RMSE: 38.6954 - val_loss: 1498.4922 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8841 - loss: 1513.0718 - val_RMSE: 38.6879 - val_loss: 1497.6763 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.8692 - loss: 1511.6859 - val_RMSE: 38.6870 - val_loss: 1497.4207 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.8696 - loss: 1511.5499 - val_RMSE: 38.6868 - val_loss: 1497.2927 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.8653 - loss: 1511.1191 - val_RMSE: 38.6867 - val_loss: 1497.2096 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8689 - loss: 1511.3248 - val_RMSE: 38.6857 - val_loss: 1497.0808 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8624 - loss: 1510.7758 - val_RMSE: 38.6853 - val_loss: 1497.0159 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8672 - loss: 1511.1150 - val_RMSE: 38.6857 - val_loss: 1497.0123 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.8592 - loss: 1510.4672 - val_RMSE: 38.6860 - val_loss: 1497.0164 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.8670 - loss: 1511.0509 - val_RMSE: 38.6847 - val_loss: 1496.9001 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.8603 - loss: 1510.5122 - val_RMSE: 38.6858 - val_loss: 1496.9672 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8612 - loss: 1510.5704 - val_RMSE: 38.6844 - val_loss: 1496.8457 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8588 - loss: 1510.3680 - val_RMSE: 38.6852 - val_loss: 1496.9039 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8553 - loss: 1510.0886 - val_RMSE: 38.6838 - val_loss: 1496.7844 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8563 - loss: 1510.1576 - val_RMSE: 38.6840 - val_loss: 1496.7878 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8532 - loss: 1509.9043 - val_RMSE: 38.6835 - val_loss: 1496.7439 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8521 - loss: 1509.8169 - val_RMSE: 38.6840 - val_loss: 1496.7772 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8500 - loss: 1509.6456 - val_RMSE: 38.6842 - val_loss: 1496.7865 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8509 - loss: 1509.7151 - val_RMSE: 38.6827 - val_loss: 1496.6647 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8442 - loss: 1509.1863 - val_RMSE: 38.6823 - val_loss: 1496.6378 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8501 - loss: 1509.6471 - val_RMSE: 38.6819 - val_loss: 1496.6010 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8485 - loss: 1509.5172 - val_RMSE: 38.6822 - val_loss: 1496.6163 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 123s 26ms/step - RMSE: 43.0874 - loss: 1906.9557 - val_RMSE: 38.7412 - val_loss: 1501.3207 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8807 - loss: 1512.2094 - val_RMSE: 38.7461 - val_loss: 1501.9521 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8773 - loss: 1512.2358 - val_RMSE: 38.7442 - val_loss: 1502.0736 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8416 - loss: 1509.5913 - val_RMSE: 38.7219 - val_loss: 1500.1827 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8313 - loss: 1508.6251 - val_RMSE: 38.7229 - val_loss: 1500.1093 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8353 - loss: 1508.7992 - val_RMSE: 38.7220 - val_loss: 1499.9417 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8277 - loss: 1508.1227 - val_RMSE: 38.7202 - val_loss: 1499.7432 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8231 - loss: 1507.7129 - val_RMSE: 38.7212 - val_loss: 1499.7777 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8271 - loss: 1507.9814 - val_RMSE: 38.7192 - val_loss: 1499.5903 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8260 - loss: 1507.8699 - val_RMSE: 38.7192 - val_loss: 1499.5653 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8207 - loss: 1507.4343 - val_RMSE: 38.7185 - val_loss: 1499.4913 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8225 - loss: 1507.5547 - val_RMSE: 38.7186 - val_loss: 1499.4832 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8168 - loss: 1507.0968 - val_RMSE: 38.7168 - val_loss: 1499.3308 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8162 - loss: 1507.0337 - val_RMSE: 38.7181 - val_loss: 1499.4225 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8180 - loss: 1507.1685 - val_RMSE: 38.7195 - val_loss: 1499.5259 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8098 - loss: 1506.5255 - val_RMSE: 38.7122 - val_loss: 1498.9563 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8071 - loss: 1506.3075 - val_RMSE: 38.7117 - val_loss: 1498.9125 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8066 - loss: 1506.2655 - val_RMSE: 38.7118 - val_loss: 1498.9138 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8111 - loss: 1506.6099 - val_RMSE: 38.7115 - val_loss: 1498.8809 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8070 - loss: 1506.2926 - val_RMSE: 38.7115 - val_loss: 1498.8854 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8087 - loss: 1506.4160 - val_RMSE: 38.7115 - val_loss: 1498.8835 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8091 - loss: 1506.4424 - val_RMSE: 38.7112 - val_loss: 1498.8561 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8121 - loss: 1506.6777 - val_RMSE: 38.7112 - val_loss: 1498.8525 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8110 - loss: 1506.5945 - val_RMSE: 38.7111 - val_loss: 1498.8486 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8058 - loss: 1506.1896 - val_RMSE: 38.7111 - val_loss: 1498.8445 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 110s 24ms/step - RMSE: 43.0833 - loss: 1906.1544 - val_RMSE: 38.7420 - val_loss: 1501.3306 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.9094 - loss: 1514.3882 - val_RMSE: 38.7177 - val_loss: 1499.7139 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9081 - loss: 1514.5768 - val_RMSE: 38.7518 - val_loss: 1502.6815 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8929 - loss: 1513.7024 - val_RMSE: 38.7400 - val_loss: 1501.9214 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8572 - loss: 1510.9570 - val_RMSE: 38.7119 - val_loss: 1499.5076 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8485 - loss: 1510.0437 - val_RMSE: 38.7106 - val_loss: 1499.2214 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8440 - loss: 1509.5364 - val_RMSE: 38.7105 - val_loss: 1499.1041 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8475 - loss: 1509.7103 - val_RMSE: 38.7099 - val_loss: 1498.9866 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8451 - loss: 1509.4650 - val_RMSE: 38.7095 - val_loss: 1498.9097 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8402 - loss: 1509.0345 - val_RMSE: 38.7087 - val_loss: 1498.8076 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8440 - loss: 1509.2955 - val_RMSE: 38.7089 - val_loss: 1498.8007 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8396 - loss: 1508.9308 - val_RMSE: 38.7083 - val_loss: 1498.7338 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8348 - loss: 1508.5409 - val_RMSE: 38.7094 - val_loss: 1498.8029 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8410 - loss: 1509.0067 - val_RMSE: 38.7087 - val_loss: 1498.7296 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8353 - loss: 1508.5453 - val_RMSE: 38.7075 - val_loss: 1498.6260 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8333 - loss: 1508.3806 - val_RMSE: 38.7075 - val_loss: 1498.6204 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8387 - loss: 1508.7925 - val_RMSE: 38.7073 - val_loss: 1498.5940 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8314 - loss: 1508.2211 - val_RMSE: 38.7061 - val_loss: 1498.4948 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8298 - loss: 1508.0856 - val_RMSE: 38.7073 - val_loss: 1498.5839 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8288 - loss: 1508.0077 - val_RMSE: 38.7078 - val_loss: 1498.6121 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8264 - loss: 1507.8097 - val_RMSE: 38.7056 - val_loss: 1498.4427 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8253 - loss: 1507.7246 - val_RMSE: 38.7052 - val_loss: 1498.4094 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8239 - loss: 1507.6123 - val_RMSE: 38.7052 - val_loss: 1498.4077 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8239 - loss: 1507.6039 - val_RMSE: 38.7051 - val_loss: 1498.3900 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8249 - loss: 1507.6761 - val_RMSE: 38.7051 - val_loss: 1498.3849 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 06:58:27,923] Trial 4 finished with value: 38.699440002441406 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'num_transformer_heads': 2, 'transformer_units': 64, 'dropout_rate': 0.32999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 115s 26ms/step - RMSE: 45.7555 - loss: 2168.1316 - val_RMSE: 38.7151 - val_loss: 1499.0077 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.5285 - loss: 1562.6776 - val_RMSE: 38.6991 - val_loss: 1497.8905 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.3829 - loss: 1551.3192 - val_RMSE: 38.6971 - val_loss: 1497.9207 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.2354 - loss: 1539.9041 - val_RMSE: 38.6969 - val_loss: 1498.0271 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1086 - loss: 1530.0475 - val_RMSE: 38.6931 - val_loss: 1497.6881 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0863 - loss: 1528.2585 - val_RMSE: 38.6916 - val_loss: 1497.5175 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0768 - loss: 1527.4636 - val_RMSE: 38.6921 - val_loss: 1497.5164 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0539 - loss: 1525.6434 - val_RMSE: 38.6907 - val_loss: 1497.3795 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0479 - loss: 1525.1437 - val_RMSE: 38.6900 - val_loss: 1497.3008 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0396 - loss: 1524.4745 - val_RMSE: 38.6901 - val_loss: 1497.2837 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 16ms/step - RMSE: 39.0198 - loss: 1522.9083 - val_RMSE: 38.6902 - val_loss: 1497.2766 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0145 - loss: 1522.4779 - val_RMSE: 38.6895 - val_loss: 1497.2114 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.0014 - loss: 1521.4373 - val_RMSE: 38.6881 - val_loss: 1497.0848 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.9961 - loss: 1521.0112 - val_RMSE: 38.6888 - val_loss: 1497.1262 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.9817 - loss: 1519.8778 - val_RMSE: 38.6881 - val_loss: 1497.0613 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.9768 - loss: 1519.4847 - val_RMSE: 38.6870 - val_loss: 1496.9670 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9651 - loss: 1518.5574 - val_RMSE: 38.6878 - val_loss: 1497.0236 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9532 - loss: 1517.6261 - val_RMSE: 38.6872 - val_loss: 1496.9645 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.9506 - loss: 1517.4102 - val_RMSE: 38.6877 - val_loss: 1496.9958 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9383 - loss: 1516.4506 - val_RMSE: 38.6879 - val_loss: 1497.0070 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9326 - loss: 1515.9972 - val_RMSE: 38.6870 - val_loss: 1496.9302 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9297 - loss: 1515.7722 - val_RMSE: 38.6865 - val_loss: 1496.8931 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9312 - loss: 1515.8837 - val_RMSE: 38.6862 - val_loss: 1496.8665 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9303 - loss: 1515.8151 - val_RMSE: 38.6860 - val_loss: 1496.8502 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9209 - loss: 1515.0839 - val_RMSE: 38.6862 - val_loss: 1496.8690 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 123s 27ms/step - RMSE: 45.7189 - loss: 2164.6008 - val_RMSE: 38.7478 - val_loss: 1501.5664 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.4763 - loss: 1558.5763 - val_RMSE: 38.7292 - val_loss: 1500.2493 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.3425 - loss: 1548.1737 - val_RMSE: 38.7311 - val_loss: 1500.5592 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1711 - loss: 1534.8774 - val_RMSE: 38.7272 - val_loss: 1500.3629 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0583 - loss: 1526.1018 - val_RMSE: 38.7210 - val_loss: 1499.8284 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0409 - loss: 1524.6914 - val_RMSE: 38.7202 - val_loss: 1499.7159 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.0269 - loss: 1523.5452 - val_RMSE: 38.7201 - val_loss: 1499.6591 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0149 - loss: 1522.5728 - val_RMSE: 38.7184 - val_loss: 1499.5040 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9983 - loss: 1521.2456 - val_RMSE: 38.7184 - val_loss: 1499.4797 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.9966 - loss: 1521.0936 - val_RMSE: 38.7172 - val_loss: 1499.3621 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9828 - loss: 1519.9980 - val_RMSE: 38.7179 - val_loss: 1499.4021 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9686 - loss: 1518.8765 - val_RMSE: 38.7168 - val_loss: 1499.3005 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9579 - loss: 1518.0286 - val_RMSE: 38.7175 - val_loss: 1499.3401 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9451 - loss: 1517.0153 - val_RMSE: 38.7166 - val_loss: 1499.2563 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9369 - loss: 1516.3612 - val_RMSE: 38.7166 - val_loss: 1499.2494 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9196 - loss: 1515.0061 - val_RMSE: 38.7169 - val_loss: 1499.2621 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.9139 - loss: 1514.5591 - val_RMSE: 38.7162 - val_loss: 1499.2021 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.9066 - loss: 1513.9805 - val_RMSE: 38.7164 - val_loss: 1499.2109 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.9014 - loss: 1513.5691 - val_RMSE: 38.7161 - val_loss: 1499.1810 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.8881 - loss: 1512.5328 - val_RMSE: 38.7162 - val_loss: 1499.1799 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.8826 - loss: 1512.0980 - val_RMSE: 38.7152 - val_loss: 1499.1049 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.8776 - loss: 1511.7020 - val_RMSE: 38.7153 - val_loss: 1499.1086 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8641 - loss: 1510.6454 - val_RMSE: 38.7155 - val_loss: 1499.1180 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.8575 - loss: 1510.1354 - val_RMSE: 38.7145 - val_loss: 1499.0352 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8588 - loss: 1510.2299 - val_RMSE: 38.7143 - val_loss: 1499.0184 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 118s 26ms/step - RMSE: 45.7002 - loss: 2162.1248 - val_RMSE: 38.7195 - val_loss: 1499.3488 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 39.5034 - loss: 1560.6990 - val_RMSE: 38.7215 - val_loss: 1499.6198 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.3608 - loss: 1549.5713 - val_RMSE: 38.7163 - val_loss: 1499.3687 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.2126 - loss: 1538.0386 - val_RMSE: 38.7228 - val_loss: 1499.8433 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 39.2021 - loss: 1537.1873 - val_RMSE: 38.7189 - val_loss: 1499.5131 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1778 - loss: 1535.2606 - val_RMSE: 38.7169 - val_loss: 1499.3527 - learning_rate: 1.0000e-04\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1736 - loss: 1534.9222 - val_RMSE: 38.7167 - val_loss: 1499.3334 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1734 - loss: 1534.9026 - val_RMSE: 38.7161 - val_loss: 1499.2797 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.1805 - loss: 1535.4563 - val_RMSE: 38.7167 - val_loss: 1499.3292 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1715 - loss: 1534.7469 - val_RMSE: 38.7160 - val_loss: 1499.2714 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1767 - loss: 1535.1536 - val_RMSE: 38.7156 - val_loss: 1499.2382 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.1627 - loss: 1534.0553 - val_RMSE: 38.7158 - val_loss: 1499.2437 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.1667 - loss: 1534.3608 - val_RMSE: 38.7154 - val_loss: 1499.2161 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.1680 - loss: 1534.4641 - val_RMSE: 38.7161 - val_loss: 1499.2670 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.1670 - loss: 1534.3833 - val_RMSE: 38.7149 - val_loss: 1499.1672 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1491 - loss: 1532.9792 - val_RMSE: 38.7148 - val_loss: 1499.1571 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1539 - loss: 1533.3547 - val_RMSE: 38.7146 - val_loss: 1499.1401 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1601 - loss: 1533.8358 - val_RMSE: 38.7143 - val_loss: 1499.1135 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 39.1611 - loss: 1533.9114 - val_RMSE: 38.7145 - val_loss: 1499.1306 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.1487 - loss: 1532.9391 - val_RMSE: 38.7143 - val_loss: 1499.1112 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.1549 - loss: 1533.4232 - val_RMSE: 38.7144 - val_loss: 1499.1190 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.1485 - loss: 1532.9193 - val_RMSE: 38.7139 - val_loss: 1499.0756 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 39.1406 - loss: 1532.2950 - val_RMSE: 38.7139 - val_loss: 1499.0732 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1514 - loss: 1533.1381 - val_RMSE: 38.7131 - val_loss: 1499.0117 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 39.1498 - loss: 1533.0116 - val_RMSE: 38.7137 - val_loss: 1499.0575 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 07:58:39,266] Trial 5 finished with value: 38.70474751790365 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'gelu', 'num_transformer_heads': 4, 'transformer_units': 128, 'dropout_rate': 0.39, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 111s 25ms/step - RMSE: 42.4475 - loss: 1845.1521 - val_RMSE: 38.7144 - val_loss: 1499.8320 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8458 - loss: 1510.0443 - val_RMSE: 38.6963 - val_loss: 1498.6664 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8482 - loss: 1510.5233 - val_RMSE: 38.6972 - val_loss: 1499.0239 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8444 - loss: 1510.5272 - val_RMSE: 38.7031 - val_loss: 1499.6031 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.8118 - loss: 1507.8633 - val_RMSE: 38.6852 - val_loss: 1497.6406 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8080 - loss: 1507.0682 - val_RMSE: 38.6847 - val_loss: 1497.3075 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8027 - loss: 1506.4084 - val_RMSE: 38.6847 - val_loss: 1497.1561 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8008 - loss: 1506.1333 - val_RMSE: 38.6840 - val_loss: 1497.0272 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7933 - loss: 1505.4794 - val_RMSE: 38.6835 - val_loss: 1496.9302 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7955 - loss: 1505.6045 - val_RMSE: 38.6828 - val_loss: 1496.8416 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7970 - loss: 1505.6798 - val_RMSE: 38.6829 - val_loss: 1496.8168 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8002 - loss: 1505.9060 - val_RMSE: 38.6827 - val_loss: 1496.7836 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.7941 - loss: 1505.4130 - val_RMSE: 38.6824 - val_loss: 1496.7395 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7951 - loss: 1505.4664 - val_RMSE: 38.6827 - val_loss: 1496.7531 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7937 - loss: 1505.3501 - val_RMSE: 38.6821 - val_loss: 1496.6962 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7941 - loss: 1505.3696 - val_RMSE: 38.6821 - val_loss: 1496.6874 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7954 - loss: 1505.4630 - val_RMSE: 38.6836 - val_loss: 1496.7948 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7957 - loss: 1505.4836 - val_RMSE: 38.6825 - val_loss: 1496.7125 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7933 - loss: 1505.2955 - val_RMSE: 38.6811 - val_loss: 1496.5918 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7864 - loss: 1504.7499 - val_RMSE: 38.6808 - val_loss: 1496.5557 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7873 - loss: 1504.8098 - val_RMSE: 38.6805 - val_loss: 1496.5270 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7888 - loss: 1504.9154 - val_RMSE: 38.6804 - val_loss: 1496.5106 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7853 - loss: 1504.6434 - val_RMSE: 38.6803 - val_loss: 1496.5021 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7887 - loss: 1504.8939 - val_RMSE: 38.6801 - val_loss: 1496.4764 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7880 - loss: 1504.8342 - val_RMSE: 38.6801 - val_loss: 1496.4771 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 119s 27ms/step - RMSE: 42.3829 - loss: 1839.0455 - val_RMSE: 38.7509 - val_loss: 1502.5441 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7978 - loss: 1506.2330 - val_RMSE: 38.7444 - val_loss: 1502.3661 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8006 - loss: 1506.8879 - val_RMSE: 38.7451 - val_loss: 1502.9218 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8016 - loss: 1507.3350 - val_RMSE: 38.7529 - val_loss: 1503.6680 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7649 - loss: 1504.4216 - val_RMSE: 38.7237 - val_loss: 1500.7728 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7550 - loss: 1503.1031 - val_RMSE: 38.7243 - val_loss: 1500.4862 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7556 - loss: 1502.8591 - val_RMSE: 38.7232 - val_loss: 1500.2222 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7553 - loss: 1502.6808 - val_RMSE: 38.7209 - val_loss: 1499.9453 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7529 - loss: 1502.4088 - val_RMSE: 38.7244 - val_loss: 1500.1570 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7492 - loss: 1502.0676 - val_RMSE: 38.7239 - val_loss: 1500.0767 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7465 - loss: 1501.8169 - val_RMSE: 38.7148 - val_loss: 1499.3582 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7470 - loss: 1501.8451 - val_RMSE: 38.7148 - val_loss: 1499.3383 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7450 - loss: 1501.6754 - val_RMSE: 38.7148 - val_loss: 1499.3224 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7417 - loss: 1501.4037 - val_RMSE: 38.7147 - val_loss: 1499.3103 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7456 - loss: 1501.7014 - val_RMSE: 38.7146 - val_loss: 1499.2893 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7473 - loss: 1501.8195 - val_RMSE: 38.7148 - val_loss: 1499.3005 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7434 - loss: 1501.5094 - val_RMSE: 38.7147 - val_loss: 1499.2864 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7426 - loss: 1501.4451 - val_RMSE: 38.7144 - val_loss: 1499.2526 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7454 - loss: 1501.6525 - val_RMSE: 38.7143 - val_loss: 1499.2401 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7448 - loss: 1501.5985 - val_RMSE: 38.7143 - val_loss: 1499.2319 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7430 - loss: 1501.4496 - val_RMSE: 38.7143 - val_loss: 1499.2231 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7407 - loss: 1501.2650 - val_RMSE: 38.7144 - val_loss: 1499.2222 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7424 - loss: 1501.3975 - val_RMSE: 38.7142 - val_loss: 1499.2080 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7424 - loss: 1501.3904 - val_RMSE: 38.7140 - val_loss: 1499.1799 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7403 - loss: 1501.2173 - val_RMSE: 38.7139 - val_loss: 1499.1743 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 115s 26ms/step - RMSE: 42.3021 - loss: 1830.5432 - val_RMSE: 38.7214 - val_loss: 1500.2247 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8304 - loss: 1508.7031 - val_RMSE: 38.7849 - val_loss: 1505.4067 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8303 - loss: 1509.0193 - val_RMSE: 38.7682 - val_loss: 1504.5594 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7944 - loss: 1506.4683 - val_RMSE: 38.7104 - val_loss: 1499.5895 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7859 - loss: 1505.3501 - val_RMSE: 38.7095 - val_loss: 1499.2378 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7803 - loss: 1504.6820 - val_RMSE: 38.7095 - val_loss: 1499.0968 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7818 - loss: 1504.6760 - val_RMSE: 38.7092 - val_loss: 1499.0006 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7776 - loss: 1504.2859 - val_RMSE: 38.7086 - val_loss: 1498.9076 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7744 - loss: 1503.9915 - val_RMSE: 38.7084 - val_loss: 1498.8505 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7756 - loss: 1504.0525 - val_RMSE: 38.7089 - val_loss: 1498.8649 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7775 - loss: 1504.1755 - val_RMSE: 38.7073 - val_loss: 1498.7172 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7803 - loss: 1504.3685 - val_RMSE: 38.7073 - val_loss: 1498.7063 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7707 - loss: 1503.6112 - val_RMSE: 38.7067 - val_loss: 1498.6438 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7757 - loss: 1503.9913 - val_RMSE: 38.7078 - val_loss: 1498.7180 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7779 - loss: 1504.1486 - val_RMSE: 38.7061 - val_loss: 1498.5847 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7761 - loss: 1504.0052 - val_RMSE: 38.7064 - val_loss: 1498.5966 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7717 - loss: 1503.6576 - val_RMSE: 38.7075 - val_loss: 1498.6698 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7686 - loss: 1503.4067 - val_RMSE: 38.7062 - val_loss: 1498.5604 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7689 - loss: 1503.4172 - val_RMSE: 38.7059 - val_loss: 1498.5315 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7671 - loss: 1503.2646 - val_RMSE: 38.7055 - val_loss: 1498.4899 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7684 - loss: 1503.3551 - val_RMSE: 38.7055 - val_loss: 1498.4773 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7675 - loss: 1503.2802 - val_RMSE: 38.7056 - val_loss: 1498.4763 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7649 - loss: 1503.0704 - val_RMSE: 38.7054 - val_loss: 1498.4600 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7624 - loss: 1502.8688 - val_RMSE: 38.7052 - val_loss: 1498.4392 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7674 - loss: 1503.2544 - val_RMSE: 38.7050 - val_loss: 1498.4156 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 13s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 08:57:37,306] Trial 6 finished with value: 38.69969813028971 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.3, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 113s 26ms/step - RMSE: 44.4115 - loss: 2034.5447 - val_RMSE: 38.7035 - val_loss: 1498.1129 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.2853 - loss: 1543.5131 - val_RMSE: 38.6987 - val_loss: 1497.8732 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.2164 - loss: 1538.2460 - val_RMSE: 38.6962 - val_loss: 1497.8669 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.1339 - loss: 1531.9917 - val_RMSE: 38.6970 - val_loss: 1498.0996 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0597 - loss: 1526.3479 - val_RMSE: 38.6948 - val_loss: 1498.0144 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.0003 - loss: 1521.7356 - val_RMSE: 38.6919 - val_loss: 1497.7168 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9883 - loss: 1520.7283 - val_RMSE: 38.6920 - val_loss: 1497.6581 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9737 - loss: 1519.5143 - val_RMSE: 38.6924 - val_loss: 1497.6260 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.9651 - loss: 1518.7964 - val_RMSE: 38.6923 - val_loss: 1497.5817 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9688 - loss: 1519.0426 - val_RMSE: 38.6914 - val_loss: 1497.4691 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.9587 - loss: 1518.2207 - val_RMSE: 38.6917 - val_loss: 1497.4679 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.9553 - loss: 1517.9292 - val_RMSE: 38.6923 - val_loss: 1497.4910 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9436 - loss: 1516.9950 - val_RMSE: 38.6907 - val_loss: 1497.3444 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9442 - loss: 1517.0242 - val_RMSE: 38.6909 - val_loss: 1497.3459 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9408 - loss: 1516.7393 - val_RMSE: 38.6908 - val_loss: 1497.3202 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9343 - loss: 1516.2159 - val_RMSE: 38.6914 - val_loss: 1497.3490 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.9229 - loss: 1515.3163 - val_RMSE: 38.6910 - val_loss: 1497.3099 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9186 - loss: 1514.9668 - val_RMSE: 38.6914 - val_loss: 1497.3282 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9116 - loss: 1514.4094 - val_RMSE: 38.6902 - val_loss: 1497.2266 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9112 - loss: 1514.3723 - val_RMSE: 38.6914 - val_loss: 1497.3125 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9129 - loss: 1514.4952 - val_RMSE: 38.6910 - val_loss: 1497.2709 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9024 - loss: 1513.6681 - val_RMSE: 38.6908 - val_loss: 1497.2500 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9059 - loss: 1513.9403 - val_RMSE: 38.6907 - val_loss: 1497.2383 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9035 - loss: 1513.7535 - val_RMSE: 38.6903 - val_loss: 1497.2067 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9047 - loss: 1513.8467 - val_RMSE: 38.6902 - val_loss: 1497.1969 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 115s 25ms/step - RMSE: 44.3622 - loss: 2029.6350 - val_RMSE: 38.7290 - val_loss: 1500.0935 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.2330 - loss: 1539.4219 - val_RMSE: 38.7302 - val_loss: 1500.3285 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.1566 - loss: 1533.5837 - val_RMSE: 38.7354 - val_loss: 1500.9144 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0768 - loss: 1527.4764 - val_RMSE: 38.7230 - val_loss: 1499.9257 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0550 - loss: 1525.7434 - val_RMSE: 38.7209 - val_loss: 1499.7285 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0521 - loss: 1525.4745 - val_RMSE: 38.7212 - val_loss: 1499.7164 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.0355 - loss: 1524.1504 - val_RMSE: 38.7213 - val_loss: 1499.7041 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 39.0313 - loss: 1523.8004 - val_RMSE: 38.7187 - val_loss: 1499.4832 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 39.0310 - loss: 1523.7611 - val_RMSE: 38.7197 - val_loss: 1499.5424 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.0179 - loss: 1522.7188 - val_RMSE: 38.7191 - val_loss: 1499.4773 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.0137 - loss: 1522.3821 - val_RMSE: 38.7189 - val_loss: 1499.4559 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9969 - loss: 1521.0582 - val_RMSE: 38.7195 - val_loss: 1499.4899 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9922 - loss: 1520.6802 - val_RMSE: 38.7185 - val_loss: 1499.3995 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9817 - loss: 1519.8529 - val_RMSE: 38.7193 - val_loss: 1499.4558 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.9750 - loss: 1519.3152 - val_RMSE: 38.7177 - val_loss: 1499.3195 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.9679 - loss: 1518.7572 - val_RMSE: 38.7185 - val_loss: 1499.3740 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9550 - loss: 1517.7489 - val_RMSE: 38.7192 - val_loss: 1499.4272 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9507 - loss: 1517.4058 - val_RMSE: 38.7186 - val_loss: 1499.3754 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9591 - loss: 1518.0565 - val_RMSE: 38.7185 - val_loss: 1499.3643 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9565 - loss: 1517.8558 - val_RMSE: 38.7182 - val_loss: 1499.3406 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9564 - loss: 1517.8472 - val_RMSE: 38.7181 - val_loss: 1499.3401 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9541 - loss: 1517.6646 - val_RMSE: 38.7180 - val_loss: 1499.3311 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9526 - loss: 1517.5491 - val_RMSE: 38.7181 - val_loss: 1499.3387 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9540 - loss: 1517.6628 - val_RMSE: 38.7181 - val_loss: 1499.3337 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9578 - loss: 1517.9537 - val_RMSE: 38.7182 - val_loss: 1499.3386 - learning_rate: 1.0000e-07\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 112s 25ms/step - RMSE: 44.3326 - loss: 2026.0779 - val_RMSE: 38.7319 - val_loss: 1500.3129 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.2594 - loss: 1541.4846 - val_RMSE: 38.7179 - val_loss: 1499.3530 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.1896 - loss: 1536.1431 - val_RMSE: 38.7392 - val_loss: 1501.1774 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.1068 - loss: 1529.8394 - val_RMSE: 38.7384 - val_loss: 1501.2596 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0326 - loss: 1524.1267 - val_RMSE: 38.7190 - val_loss: 1499.7012 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0253 - loss: 1523.5060 - val_RMSE: 38.7167 - val_loss: 1499.4752 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0072 - loss: 1522.0521 - val_RMSE: 38.7159 - val_loss: 1499.4034 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0092 - loss: 1522.1993 - val_RMSE: 38.7156 - val_loss: 1499.3785 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0034 - loss: 1521.7450 - val_RMSE: 38.7152 - val_loss: 1499.3414 - learning_rate: 1.0000e-05\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0030 - loss: 1521.7135 - val_RMSE: 38.7149 - val_loss: 1499.3220 - learning_rate: 1.0000e-05\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 39.0185 - loss: 1522.9198 - val_RMSE: 38.7150 - val_loss: 1499.3256 - learning_rate: 1.0000e-05\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0113 - loss: 1522.3633 - val_RMSE: 38.7148 - val_loss: 1499.3162 - learning_rate: 1.0000e-05\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0095 - loss: 1522.2219 - val_RMSE: 38.7148 - val_loss: 1499.3107 - learning_rate: 1.0000e-05\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0046 - loss: 1521.8325 - val_RMSE: 38.7148 - val_loss: 1499.3109 - learning_rate: 1.0000e-05\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0069 - loss: 1522.0111 - val_RMSE: 38.7147 - val_loss: 1499.3057 - learning_rate: 1.0000e-05\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0069 - loss: 1522.0167 - val_RMSE: 38.7148 - val_loss: 1499.3093 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9997 - loss: 1521.4526 - val_RMSE: 38.7148 - val_loss: 1499.3075 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0118 - loss: 1522.3987 - val_RMSE: 38.7147 - val_loss: 1499.3020 - learning_rate: 1.0000e-06\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0149 - loss: 1522.6351 - val_RMSE: 38.7146 - val_loss: 1499.2985 - learning_rate: 1.0000e-06\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0099 - loss: 1522.2482 - val_RMSE: 38.7147 - val_loss: 1499.3003 - learning_rate: 1.0000e-06\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0012 - loss: 1521.5675 - val_RMSE: 38.7147 - val_loss: 1499.3024 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0048 - loss: 1521.8497 - val_RMSE: 38.7147 - val_loss: 1499.3002 - learning_rate: 1.0000e-07\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0053 - loss: 1521.8860 - val_RMSE: 38.7148 - val_loss: 1499.3057 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0043 - loss: 1521.8080 - val_RMSE: 38.7147 - val_loss: 1499.3025 - learning_rate: 1.0000e-08\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0010 - loss: 1521.5508 - val_RMSE: 38.7147 - val_loss: 1499.3065 - learning_rate: 1.0000e-08\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 09:53:44,757] Trial 7 finished with value: 38.707681020100914 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.44999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 115s 26ms/step - RMSE: 42.6470 - loss: 1863.9602 - val_RMSE: 38.7233 - val_loss: 1500.6809 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9045 - loss: 1514.8726 - val_RMSE: 38.7080 - val_loss: 1500.0286 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9014 - loss: 1515.2236 - val_RMSE: 38.7096 - val_loss: 1500.5956 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8919 - loss: 1514.8608 - val_RMSE: 38.7099 - val_loss: 1500.8645 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8630 - loss: 1512.5320 - val_RMSE: 38.6867 - val_loss: 1498.2809 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8489 - loss: 1510.7285 - val_RMSE: 38.6854 - val_loss: 1497.7341 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8430 - loss: 1509.8857 - val_RMSE: 38.6856 - val_loss: 1497.5156 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8413 - loss: 1509.5498 - val_RMSE: 38.6851 - val_loss: 1497.3406 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8349 - loss: 1508.9290 - val_RMSE: 38.6856 - val_loss: 1497.2944 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8347 - loss: 1508.8298 - val_RMSE: 38.6849 - val_loss: 1497.1731 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8400 - loss: 1509.1870 - val_RMSE: 38.6849 - val_loss: 1497.1284 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8434 - loss: 1509.4121 - val_RMSE: 38.6844 - val_loss: 1497.0596 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.8360 - loss: 1508.8081 - val_RMSE: 38.6852 - val_loss: 1497.0940 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8354 - loss: 1508.7378 - val_RMSE: 38.6847 - val_loss: 1497.0397 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8311 - loss: 1508.3885 - val_RMSE: 38.6839 - val_loss: 1496.9625 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8309 - loss: 1508.3583 - val_RMSE: 38.6847 - val_loss: 1497.0206 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8304 - loss: 1508.3097 - val_RMSE: 38.6842 - val_loss: 1496.9631 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8300 - loss: 1508.2615 - val_RMSE: 38.6834 - val_loss: 1496.8873 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8280 - loss: 1508.0975 - val_RMSE: 38.6833 - val_loss: 1496.8713 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8256 - loss: 1507.8950 - val_RMSE: 38.6831 - val_loss: 1496.8391 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8245 - loss: 1507.7958 - val_RMSE: 38.6832 - val_loss: 1496.8417 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8258 - loss: 1507.8939 - val_RMSE: 38.6831 - val_loss: 1496.8247 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8195 - loss: 1507.3917 - val_RMSE: 38.6830 - val_loss: 1496.8035 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8257 - loss: 1507.8683 - val_RMSE: 38.6830 - val_loss: 1496.7996 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8248 - loss: 1507.7852 - val_RMSE: 38.6831 - val_loss: 1496.7948 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 122s 27ms/step - RMSE: 42.6789 - loss: 1867.5491 - val_RMSE: 38.7526 - val_loss: 1502.9938 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8560 - loss: 1511.1396 - val_RMSE: 38.7444 - val_loss: 1502.8888 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8534 - loss: 1511.4731 - val_RMSE: 38.7555 - val_loss: 1504.2766 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8508 - loss: 1511.7206 - val_RMSE: 38.7391 - val_loss: 1503.2950 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8094 - loss: 1508.5171 - val_RMSE: 38.7226 - val_loss: 1501.1578 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7924 - loss: 1506.4233 - val_RMSE: 38.7215 - val_loss: 1500.5839 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7914 - loss: 1505.9281 - val_RMSE: 38.7196 - val_loss: 1500.1873 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7950 - loss: 1505.9901 - val_RMSE: 38.7205 - val_loss: 1500.1183 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7862 - loss: 1505.1833 - val_RMSE: 38.7220 - val_loss: 1500.1399 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7864 - loss: 1505.1119 - val_RMSE: 38.7201 - val_loss: 1499.9286 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7867 - loss: 1505.0771 - val_RMSE: 38.7208 - val_loss: 1499.9283 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7880 - loss: 1505.1301 - val_RMSE: 38.7195 - val_loss: 1499.8014 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7860 - loss: 1504.9496 - val_RMSE: 38.7195 - val_loss: 1499.7706 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7825 - loss: 1504.6515 - val_RMSE: 38.7196 - val_loss: 1499.7528 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7845 - loss: 1504.7850 - val_RMSE: 38.7198 - val_loss: 1499.7534 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7861 - loss: 1504.8862 - val_RMSE: 38.7200 - val_loss: 1499.7610 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7768 - loss: 1504.1593 - val_RMSE: 38.7136 - val_loss: 1499.2472 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7771 - loss: 1504.1647 - val_RMSE: 38.7135 - val_loss: 1499.2288 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7808 - loss: 1504.4373 - val_RMSE: 38.7135 - val_loss: 1499.2152 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7778 - loss: 1504.1967 - val_RMSE: 38.7135 - val_loss: 1499.2048 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7775 - loss: 1504.1656 - val_RMSE: 38.7135 - val_loss: 1499.1959 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7747 - loss: 1503.9414 - val_RMSE: 38.7134 - val_loss: 1499.1766 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7763 - loss: 1504.0522 - val_RMSE: 38.7132 - val_loss: 1499.1594 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7761 - loss: 1504.0264 - val_RMSE: 38.7131 - val_loss: 1499.1449 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7747 - loss: 1503.9128 - val_RMSE: 38.7133 - val_loss: 1499.1497 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 121s 28ms/step - RMSE: 42.5061 - loss: 1849.7598 - val_RMSE: 38.7247 - val_loss: 1500.7529 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8844 - loss: 1513.2695 - val_RMSE: 38.7218 - val_loss: 1501.0789 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8799 - loss: 1513.4834 - val_RMSE: 38.7221 - val_loss: 1501.6064 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8444 - loss: 1510.9349 - val_RMSE: 38.7112 - val_loss: 1500.1058 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8289 - loss: 1509.1060 - val_RMSE: 38.7101 - val_loss: 1499.5858 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8265 - loss: 1508.5498 - val_RMSE: 38.7098 - val_loss: 1499.3447 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8250 - loss: 1508.2465 - val_RMSE: 38.7091 - val_loss: 1499.1617 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8208 - loss: 1507.7996 - val_RMSE: 38.7097 - val_loss: 1499.1233 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8156 - loss: 1507.3196 - val_RMSE: 38.7094 - val_loss: 1499.0496 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8179 - loss: 1507.4545 - val_RMSE: 38.7087 - val_loss: 1498.9669 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8191 - loss: 1507.5155 - val_RMSE: 38.7076 - val_loss: 1498.8505 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8237 - loss: 1507.8479 - val_RMSE: 38.7067 - val_loss: 1498.7649 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8124 - loss: 1506.9535 - val_RMSE: 38.7073 - val_loss: 1498.7961 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8169 - loss: 1507.2902 - val_RMSE: 38.7079 - val_loss: 1498.8285 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.8139 - loss: 1507.0425 - val_RMSE: 38.7062 - val_loss: 1498.6810 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.8125 - loss: 1506.9236 - val_RMSE: 38.7060 - val_loss: 1498.6515 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8069 - loss: 1506.4723 - val_RMSE: 38.7058 - val_loss: 1498.6256 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.8106 - loss: 1506.7443 - val_RMSE: 38.7055 - val_loss: 1498.5940 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8091 - loss: 1506.6151 - val_RMSE: 38.7053 - val_loss: 1498.5637 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.8070 - loss: 1506.4437 - val_RMSE: 38.7053 - val_loss: 1498.5557 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8082 - loss: 1506.5293 - val_RMSE: 38.7054 - val_loss: 1498.5553 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.8086 - loss: 1506.5477 - val_RMSE: 38.7056 - val_loss: 1498.5579 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8045 - loss: 1506.2297 - val_RMSE: 38.7059 - val_loss: 1498.5764 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.8008 - loss: 1505.9331 - val_RMSE: 38.7054 - val_loss: 1498.5397 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8063 - loss: 1506.3608 - val_RMSE: 38.7053 - val_loss: 1498.5286 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-17 10:52:39,036] Trial 8 finished with value: 38.700538635253906 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'selu', 'num_transformer_heads': 2, 'transformer_units': 32, 'dropout_rate': 0.42, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            " 709/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 29ms/step - RMSE: 50.1655 - loss: 2638.2307"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-17 10:54:36,069] Trial 9 failed with parameters: {'units': 512, 'last_layer': 2, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 128, 'dropout_rate': 0.48, 'repeat_att': 2} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-110-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-109-a07edfb686fe>\", line 54, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
            "    return function._call_flat(  # pylint: disable=protected-access\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1322, in _call_flat\n",
            "    return self._inference_function.call_preflattened(args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 216, in call_preflattened\n",
            "    flat_outputs = self.call_flat(*args)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 251, in call_flat\n",
            "    outputs = self._bound_context.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\", line 1683, in call_function\n",
            "    outputs = execute.execute(\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-02-17 10:54:36,072] Trial 9 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-a07edfb686fe>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     55\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 2 finished with value: 38.699286142985024\n",
        "* parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}"
      ],
      "metadata": {
        "id": "FU7j6-OHWPmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9",
        "id": "u4XgZIPLWPmv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jReIEHz6wysG"
      },
      "source": [
        "#### **4.6.4 NeuralNetwork v3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "54a327da-d07c-4706-e6e3-42c6f105ac0a",
        "id": "QfBbW0LUwysH"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "833957       5         4     0             9                   2           2   \n",
              "2068382      5         1     0             3                   2           0   \n",
              "3307684      4         0     0             4                   2           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "833957       0      1              0.653745 -0.371398 -1.749252 -1.621354   \n",
              "2068382      1      0              1.697075  0.669907  1.942067 -1.621354   \n",
              "3307684      3      5              1.401664 -0.006573 -0.385691  0.136860   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "833957            0               0  \n",
              "2068382           0               0  \n",
              "3307684           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-70b4c955-f9b0-4a54-8c04-e0a65005d793\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>833957</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.653745</td>\n",
              "      <td>-0.371398</td>\n",
              "      <td>-1.749252</td>\n",
              "      <td>-1.621354</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2068382</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.697075</td>\n",
              "      <td>0.669907</td>\n",
              "      <td>1.942067</td>\n",
              "      <td>-1.621354</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3307684</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1.401664</td>\n",
              "      <td>-0.006573</td>\n",
              "      <td>-0.385691</td>\n",
              "      <td>0.136860</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-70b4c955-f9b0-4a54-8c04-e0a65005d793')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-70b4c955-f9b0-4a54-8c04-e0a65005d793 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-70b4c955-f9b0-4a54-8c04-e0a65005d793');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-35523353-03ba-4c2a-8a0d-ed987608735b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35523353-03ba-4c2a-8a0d-ed987608735b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-35523353-03ba-4c2a-8a0d-ed987608735b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 3,\n        \"max\": 9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.6537449955940247\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.37139827013015747\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.7492516040802002\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.13686025142669678\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GatedLinearUnit(layers.Layer):\n",
        "    def __init__(self, exite_units,dropout_rate):\n",
        "        super().__init__()\n",
        "        self.exite_units = exite_units\n",
        "        self.reshaped = layers.Reshape((1, -1))\n",
        "        self.exite = layers.Dense(self.exite_units)\n",
        "        self.lnorm_00 = layers.LayerNormalization()\n",
        "        self.lnorm_01 = layers.LayerNormalization()\n",
        "        self.drop = layers.Dropout(rate=dropout_rate)\n",
        "        self.sigmoid = layers.Dense(1, activation=\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.reshaped(inputs)\n",
        "        x = self.exite(x)\n",
        "        x = self.lnorm_00(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if inputs.shape[-1] != self.exite_units:\n",
        "            inputs = self.project(inputs)\n",
        "        x = inputs + self.gated_linear_unit(x)\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n",
        "\n",
        "    # Remove build warnings\n",
        "    def build(self):\n",
        "        self.built = True"
      ],
      "metadata": {
        "id": "UJ5OQf-rDnO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512, last_layer=1, activation=\"relu\",repeat_att=2,dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001): # Reduced transformer_units\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    transformer_outputs = [] # List to store transformer outputs for each categorical feature\n",
        "\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:, j])\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        embs.append(x)\n",
        "\n",
        "        # Reshape for Transformer (batch_size, 1, features) - Crucial!\n",
        "        reshaped_embedding = layers.Reshape((1, -1))(x)\n",
        "\n",
        "        # Transformer Layer for each categorical feature\n",
        "        for q in list(range(repeat_att)):\n",
        "          if q == 0:\n",
        "            attention_output = reshaped_embedding\n",
        "\n",
        "          attention_output_ = layers.MultiHeadAttention(num_heads=num_transformer_heads, key_dim=transformer_units,name=f\"mh_{j}_{q}\")(attention_output, attention_output)\n",
        "          attention_output_ = layers.LayerNormalization(name=f\"mh_ln1_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "          attention_output_ = layers.Dense(reshaped_embedding.shape[-1], activation=activation,name=f\"mh_dense_{j}_{q}\")(attention_output_)\n",
        "          attention_output = layers.LayerNormalization(name=f\"mh_ln2_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "\n",
        "        transformer_outputs.append(layers.Flatten()(attention_output)) # Store flattened transformer output\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    x_orig = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    reshaped_features = layers.Reshape((1, -1))(x_orig)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x_orig, flattened_attention])\n",
        "\n",
        "    # Concatenate Transformer outputs and numerical features\n",
        "    all_features = layers.Concatenate(axis=-1)(transformer_outputs + [x])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(all_features)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    #x = layers.Concatenate(axis=-1)([x_orig, x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "avZAgIvUwysH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for q in range(1):\n",
        "  print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325859c0-bfb5-43c8-bf8a-d3592d731d07",
        "id": "KTPvy-gAwysI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model()\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "cjfFlBj5wysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(mod_test, show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"TB\")"
      ],
      "metadata": {
        "id": "gxWhVnRkwysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "id": "hTJyGENowysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "697CMLBjwysI"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujnWEIfcwysI"
      },
      "outputs": [],
      "source": [
        "# categorical_feat = t.cat_features.copy()\n",
        "# numerical_feat = t.num_features.copy()\n",
        "\n",
        "# X_train_cat = X_enc[categorical_feat]\n",
        "# X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "# X_test_cat = test_enc[categorical_feat]\n",
        "# X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "# X_train_cat.info()\n",
        "# X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLdEjBBgwysI"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "#(units=512, last_layer=1, activation=\"relu\", dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001)\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': 0.0001, #trial.suggest_float('reg', 1e-4, 0.1, log=True),\n",
        "              \"num_transformer_heads\": trial.suggest_int(\"num_transformer_heads\", 2, 4),\n",
        "              \"transformer_units\": trial.suggest_int(\"transformer_units\", 32, 128,step=32),\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.51,step=0.03),\n",
        "              'repeat_att': trial.suggest_categorical('repeat_att', [1,2]),\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num], batch_size=1024)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyp2r5IkwysI"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57a94819-3390-4ad0-a394-174ce31ca217",
        "id": "ZVi6cBIJwysI"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 00:46:45,620] A new study created in memory with name: no-name-dc44f225-28b0-4ec0-8c91-f67686b2cd34\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 149s 35ms/step - RMSE: 42.5423 - loss: 1853.1946 - val_RMSE: 38.7149 - val_loss: 1499.9546 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9247 - loss: 1516.3540 - val_RMSE: 38.7003 - val_loss: 1499.3855 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9184 - loss: 1516.5168 - val_RMSE: 38.7284 - val_loss: 1502.1870 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9076 - loss: 1516.2740 - val_RMSE: 38.7073 - val_loss: 1500.9421 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8804 - loss: 1514.1544 - val_RMSE: 38.6908 - val_loss: 1498.8295 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8641 - loss: 1512.1082 - val_RMSE: 38.6891 - val_loss: 1498.1650 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8571 - loss: 1511.1117 - val_RMSE: 38.6884 - val_loss: 1497.8333 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8573 - loss: 1510.8917 - val_RMSE: 38.6869 - val_loss: 1497.5610 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8491 - loss: 1510.1136 - val_RMSE: 38.6872 - val_loss: 1497.4873 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8484 - loss: 1509.9669 - val_RMSE: 38.6857 - val_loss: 1497.3024 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8528 - loss: 1510.2468 - val_RMSE: 38.6860 - val_loss: 1497.2677 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8579 - loss: 1510.5892 - val_RMSE: 38.6851 - val_loss: 1497.1603 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8491 - loss: 1509.8719 - val_RMSE: 38.6859 - val_loss: 1497.1989 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8485 - loss: 1509.7949 - val_RMSE: 38.6871 - val_loss: 1497.2744 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8413 - loss: 1509.2239 - val_RMSE: 38.6854 - val_loss: 1497.1283 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8417 - loss: 1509.2438 - val_RMSE: 38.6848 - val_loss: 1497.0713 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8416 - loss: 1509.2209 - val_RMSE: 38.6846 - val_loss: 1497.0367 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8447 - loss: 1509.4489 - val_RMSE: 38.6842 - val_loss: 1496.9976 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8436 - loss: 1509.3550 - val_RMSE: 38.6838 - val_loss: 1496.9580 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8409 - loss: 1509.1313 - val_RMSE: 38.6839 - val_loss: 1496.9517 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8394 - loss: 1509.0048 - val_RMSE: 38.6835 - val_loss: 1496.9149 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8418 - loss: 1509.1865 - val_RMSE: 38.6834 - val_loss: 1496.9009 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8338 - loss: 1508.5527 - val_RMSE: 38.6832 - val_loss: 1496.8790 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8419 - loss: 1509.1719 - val_RMSE: 38.6832 - val_loss: 1496.8655 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8380 - loss: 1508.8633 - val_RMSE: 38.6831 - val_loss: 1496.8566 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 152s 35ms/step - RMSE: 42.4635 - loss: 1845.5852 - val_RMSE: 38.7378 - val_loss: 1501.7828 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8727 - loss: 1512.3889 - val_RMSE: 38.7483 - val_loss: 1503.2280 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8710 - loss: 1512.9287 - val_RMSE: 38.7473 - val_loss: 1503.9238 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8331 - loss: 1510.4143 - val_RMSE: 38.7256 - val_loss: 1501.5660 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8221 - loss: 1508.8901 - val_RMSE: 38.7289 - val_loss: 1501.3015 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8085 - loss: 1507.3918 - val_RMSE: 38.7245 - val_loss: 1500.6874 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8097 - loss: 1507.2500 - val_RMSE: 38.7236 - val_loss: 1500.4556 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8165 - loss: 1507.6310 - val_RMSE: 38.7238 - val_loss: 1500.3698 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8052 - loss: 1506.6687 - val_RMSE: 38.7224 - val_loss: 1500.1976 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8041 - loss: 1506.5164 - val_RMSE: 38.7234 - val_loss: 1500.2178 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8052 - loss: 1506.5483 - val_RMSE: 38.7254 - val_loss: 1500.3353 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8051 - loss: 1506.5110 - val_RMSE: 38.7175 - val_loss: 1499.7087 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8017 - loss: 1506.2274 - val_RMSE: 38.7176 - val_loss: 1499.6948 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.7980 - loss: 1505.9227 - val_RMSE: 38.7168 - val_loss: 1499.6138 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.7999 - loss: 1506.0510 - val_RMSE: 38.7167 - val_loss: 1499.5980 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8006 - loss: 1506.0966 - val_RMSE: 38.7167 - val_loss: 1499.5845 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.7930 - loss: 1505.4945 - val_RMSE: 38.7163 - val_loss: 1499.5337 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7964 - loss: 1505.7448 - val_RMSE: 38.7163 - val_loss: 1499.5311 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8012 - loss: 1506.1034 - val_RMSE: 38.7164 - val_loss: 1499.5283 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8004 - loss: 1506.0343 - val_RMSE: 38.7161 - val_loss: 1499.4877 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7983 - loss: 1505.8585 - val_RMSE: 38.7167 - val_loss: 1499.5251 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.7916 - loss: 1505.3295 - val_RMSE: 38.7166 - val_loss: 1499.5121 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7955 - loss: 1505.6284 - val_RMSE: 38.7167 - val_loss: 1499.5171 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7963 - loss: 1505.6843 - val_RMSE: 38.7165 - val_loss: 1499.4994 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.7963 - loss: 1505.6914 - val_RMSE: 38.7164 - val_loss: 1499.4944 - learning_rate: 1.0000e-06\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 16s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 149s 35ms/step - RMSE: 42.4441 - loss: 1843.2596 - val_RMSE: 38.7304 - val_loss: 1501.2000 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.9038 - loss: 1514.7972 - val_RMSE: 38.7187 - val_loss: 1500.8505 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8969 - loss: 1514.8219 - val_RMSE: 38.7294 - val_loss: 1502.3921 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8973 - loss: 1515.5651 - val_RMSE: 38.7309 - val_loss: 1503.0609 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8497 - loss: 1512.0414 - val_RMSE: 38.7132 - val_loss: 1500.7770 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8386 - loss: 1510.3381 - val_RMSE: 38.7125 - val_loss: 1500.1444 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8348 - loss: 1509.5366 - val_RMSE: 38.7120 - val_loss: 1499.7932 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8308 - loss: 1508.9564 - val_RMSE: 38.7135 - val_loss: 1499.7236 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8265 - loss: 1508.4512 - val_RMSE: 38.7110 - val_loss: 1499.4092 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 57s 22ms/step - RMSE: 38.8278 - loss: 1508.4470 - val_RMSE: 38.7111 - val_loss: 1499.3374 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8293 - loss: 1508.4926 - val_RMSE: 38.7108 - val_loss: 1499.2607 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8343 - loss: 1508.8291 - val_RMSE: 38.7101 - val_loss: 1499.1632 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8220 - loss: 1507.8356 - val_RMSE: 38.7096 - val_loss: 1499.0898 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8270 - loss: 1508.1875 - val_RMSE: 38.7110 - val_loss: 1499.1808 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8256 - loss: 1508.0579 - val_RMSE: 38.7101 - val_loss: 1499.0874 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8245 - loss: 1507.9469 - val_RMSE: 38.7112 - val_loss: 1499.1582 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 57s 22ms/step - RMSE: 38.8216 - loss: 1507.7120 - val_RMSE: 38.7095 - val_loss: 1499.0039 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8235 - loss: 1507.8420 - val_RMSE: 38.7105 - val_loss: 1499.0677 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8223 - loss: 1507.7360 - val_RMSE: 38.7091 - val_loss: 1498.9581 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8203 - loss: 1507.5741 - val_RMSE: 38.7125 - val_loss: 1499.2124 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8198 - loss: 1507.5265 - val_RMSE: 38.7109 - val_loss: 1499.0803 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8171 - loss: 1507.3075 - val_RMSE: 38.7096 - val_loss: 1498.9631 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8114 - loss: 1506.8531 - val_RMSE: 38.7094 - val_loss: 1498.9340 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 57s 22ms/step - RMSE: 38.8078 - loss: 1506.5636 - val_RMSE: 38.7092 - val_loss: 1498.9062 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8137 - loss: 1507.0103 - val_RMSE: 38.7090 - val_loss: 1498.8834 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 16s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 02:06:26,496] Trial 0 finished with value: 38.70286560058594 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.44999999999999996, 'repeat_att': 1}. Best is trial 0 with value: 38.70286560058594.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 154s 35ms/step - RMSE: 43.3852 - loss: 1934.7306 - val_RMSE: 38.7027 - val_loss: 1498.9362 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 39.0693 - loss: 1527.6146 - val_RMSE: 38.6950 - val_loss: 1498.9910 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 39.0576 - loss: 1527.3376 - val_RMSE: 38.6957 - val_loss: 1499.5364 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9966 - loss: 1522.7792 - val_RMSE: 38.6897 - val_loss: 1498.5286 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9863 - loss: 1521.4486 - val_RMSE: 38.6876 - val_loss: 1497.9673 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9687 - loss: 1519.7284 - val_RMSE: 38.6857 - val_loss: 1497.5868 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9707 - loss: 1519.6841 - val_RMSE: 38.6867 - val_loss: 1497.5294 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9633 - loss: 1518.9814 - val_RMSE: 38.6871 - val_loss: 1497.4590 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9621 - loss: 1518.7979 - val_RMSE: 38.6859 - val_loss: 1497.2992 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9610 - loss: 1518.6493 - val_RMSE: 38.6857 - val_loss: 1497.2311 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9632 - loss: 1518.7689 - val_RMSE: 38.6860 - val_loss: 1497.2183 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9496 - loss: 1517.6786 - val_RMSE: 38.6855 - val_loss: 1497.1537 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9574 - loss: 1518.2554 - val_RMSE: 38.6853 - val_loss: 1497.1129 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9451 - loss: 1517.2749 - val_RMSE: 38.6860 - val_loss: 1497.1459 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9440 - loss: 1517.1774 - val_RMSE: 38.6850 - val_loss: 1497.0625 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9448 - loss: 1517.2255 - val_RMSE: 38.6860 - val_loss: 1497.1222 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9347 - loss: 1516.4252 - val_RMSE: 38.6840 - val_loss: 1496.9670 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9362 - loss: 1516.5424 - val_RMSE: 38.6845 - val_loss: 1496.9954 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9342 - loss: 1516.3783 - val_RMSE: 38.6845 - val_loss: 1496.9844 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9261 - loss: 1515.7404 - val_RMSE: 38.6839 - val_loss: 1496.9280 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9264 - loss: 1515.7483 - val_RMSE: 38.6836 - val_loss: 1496.8975 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9230 - loss: 1515.4722 - val_RMSE: 38.6835 - val_loss: 1496.8818 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9199 - loss: 1515.2224 - val_RMSE: 38.6836 - val_loss: 1496.8789 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9283 - loss: 1515.8676 - val_RMSE: 38.6840 - val_loss: 1496.9043 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9230 - loss: 1515.4517 - val_RMSE: 38.6840 - val_loss: 1496.8937 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 16s 9ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 147s 35ms/step - RMSE: 43.3502 - loss: 1931.5552 - val_RMSE: 38.7316 - val_loss: 1501.1777 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 39.0198 - loss: 1523.7283 - val_RMSE: 38.7399 - val_loss: 1502.3658 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9995 - loss: 1522.7172 - val_RMSE: 38.7387 - val_loss: 1502.7998 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9452 - loss: 1518.7039 - val_RMSE: 38.7208 - val_loss: 1500.8723 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9316 - loss: 1517.1335 - val_RMSE: 38.7213 - val_loss: 1500.5249 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9314 - loss: 1516.7751 - val_RMSE: 38.7199 - val_loss: 1500.1968 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9249 - loss: 1516.0735 - val_RMSE: 38.7192 - val_loss: 1500.0155 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9205 - loss: 1515.6187 - val_RMSE: 38.7187 - val_loss: 1499.8903 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9211 - loss: 1515.5809 - val_RMSE: 38.7194 - val_loss: 1499.8732 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9138 - loss: 1514.9574 - val_RMSE: 38.7185 - val_loss: 1499.7616 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9080 - loss: 1514.4657 - val_RMSE: 38.7178 - val_loss: 1499.6732 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9072 - loss: 1514.3712 - val_RMSE: 38.7176 - val_loss: 1499.6364 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.9051 - loss: 1514.1798 - val_RMSE: 38.7188 - val_loss: 1499.7069 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8932 - loss: 1513.2394 - val_RMSE: 38.7182 - val_loss: 1499.6433 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8962 - loss: 1513.4537 - val_RMSE: 38.7157 - val_loss: 1499.4407 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8876 - loss: 1512.7773 - val_RMSE: 38.7159 - val_loss: 1499.4401 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8815 - loss: 1512.2913 - val_RMSE: 38.7154 - val_loss: 1499.3939 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8888 - loss: 1512.8477 - val_RMSE: 38.7155 - val_loss: 1499.3929 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8924 - loss: 1513.1184 - val_RMSE: 38.7154 - val_loss: 1499.3776 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 58s 22ms/step - RMSE: 38.8854 - loss: 1512.5703 - val_RMSE: 38.7153 - val_loss: 1499.3597 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8868 - loss: 1512.6654 - val_RMSE: 38.7155 - val_loss: 1499.3667 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8918 - loss: 1513.0470 - val_RMSE: 38.7155 - val_loss: 1499.3618 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8939 - loss: 1513.2028 - val_RMSE: 38.7155 - val_loss: 1499.3582 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8927 - loss: 1513.1119 - val_RMSE: 38.7155 - val_loss: 1499.3569 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 22ms/step - RMSE: 38.8863 - loss: 1512.6150 - val_RMSE: 38.7154 - val_loss: 1499.3484 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 17s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 157s 36ms/step - RMSE: 43.2625 - loss: 1922.0507 - val_RMSE: 38.7210 - val_loss: 1500.3311 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 39.0474 - loss: 1525.8640 - val_RMSE: 38.7199 - val_loss: 1500.7574 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 39.0306 - loss: 1525.0643 - val_RMSE: 38.7252 - val_loss: 1501.7371 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9748 - loss: 1520.9982 - val_RMSE: 38.7165 - val_loss: 1500.5239 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9541 - loss: 1518.8676 - val_RMSE: 38.7149 - val_loss: 1500.0076 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9517 - loss: 1518.3409 - val_RMSE: 38.7143 - val_loss: 1499.7439 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9435 - loss: 1517.5121 - val_RMSE: 38.7116 - val_loss: 1499.4058 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9468 - loss: 1517.6493 - val_RMSE: 38.7124 - val_loss: 1499.3759 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9462 - loss: 1517.5144 - val_RMSE: 38.7118 - val_loss: 1499.2648 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9401 - loss: 1516.9845 - val_RMSE: 38.7118 - val_loss: 1499.2213 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9376 - loss: 1516.7467 - val_RMSE: 38.7111 - val_loss: 1499.1396 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 24ms/step - RMSE: 38.9317 - loss: 1516.2589 - val_RMSE: 38.7106 - val_loss: 1499.0748 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9274 - loss: 1515.9056 - val_RMSE: 38.7104 - val_loss: 1499.0415 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9280 - loss: 1515.9312 - val_RMSE: 38.7108 - val_loss: 1499.0625 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9181 - loss: 1515.1509 - val_RMSE: 38.7099 - val_loss: 1498.9727 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9195 - loss: 1515.2476 - val_RMSE: 38.7110 - val_loss: 1499.0500 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 65s 25ms/step - RMSE: 38.9204 - loss: 1515.3063 - val_RMSE: 38.7099 - val_loss: 1498.9590 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9077 - loss: 1514.3134 - val_RMSE: 38.7093 - val_loss: 1498.9106 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9087 - loss: 1514.3798 - val_RMSE: 38.7105 - val_loss: 1498.9954 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9068 - loss: 1514.2285 - val_RMSE: 38.7107 - val_loss: 1499.0084 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 24ms/step - RMSE: 38.8995 - loss: 1513.6593 - val_RMSE: 38.7079 - val_loss: 1498.7753 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8973 - loss: 1513.4734 - val_RMSE: 38.7077 - val_loss: 1498.7532 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 24ms/step - RMSE: 38.8961 - loss: 1513.3704 - val_RMSE: 38.7082 - val_loss: 1498.7817 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 25ms/step - RMSE: 38.9010 - loss: 1513.7474 - val_RMSE: 38.7080 - val_loss: 1498.7625 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 25ms/step - RMSE: 38.9025 - loss: 1513.8590 - val_RMSE: 38.7075 - val_loss: 1498.7212 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 17s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 03:28:09,029] Trial 1 finished with value: 38.70228703816732 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'selu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.48, 'repeat_att': 1}. Best is trial 1 with value: 38.70228703816732.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 147s 35ms/step - RMSE: 43.2199 - loss: 1919.5537 - val_RMSE: 38.7000 - val_loss: 1498.5961 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9782 - loss: 1520.3369 - val_RMSE: 38.6960 - val_loss: 1498.7506 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9793 - loss: 1520.8539 - val_RMSE: 38.7000 - val_loss: 1499.3563 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9302 - loss: 1517.1078 - val_RMSE: 38.6875 - val_loss: 1497.9254 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9197 - loss: 1515.8606 - val_RMSE: 38.6853 - val_loss: 1497.4617 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9056 - loss: 1514.5089 - val_RMSE: 38.6849 - val_loss: 1497.2765 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9104 - loss: 1514.7513 - val_RMSE: 38.6849 - val_loss: 1497.1888 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9049 - loss: 1514.2415 - val_RMSE: 38.6845 - val_loss: 1497.0953 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9060 - loss: 1514.2686 - val_RMSE: 38.6845 - val_loss: 1497.0533 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.9000 - loss: 1513.7655 - val_RMSE: 38.6841 - val_loss: 1496.9913 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9058 - loss: 1514.1903 - val_RMSE: 38.6838 - val_loss: 1496.9438 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8927 - loss: 1513.1487 - val_RMSE: 38.6834 - val_loss: 1496.9041 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9035 - loss: 1513.9727 - val_RMSE: 38.6831 - val_loss: 1496.8627 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8964 - loss: 1513.4114 - val_RMSE: 38.6833 - val_loss: 1496.8708 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8967 - loss: 1513.4277 - val_RMSE: 38.6825 - val_loss: 1496.8010 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8926 - loss: 1513.1035 - val_RMSE: 38.6832 - val_loss: 1496.8527 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8866 - loss: 1512.6248 - val_RMSE: 38.6826 - val_loss: 1496.8025 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8882 - loss: 1512.7504 - val_RMSE: 38.6817 - val_loss: 1496.7234 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8858 - loss: 1512.5521 - val_RMSE: 38.6815 - val_loss: 1496.6926 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8798 - loss: 1512.0736 - val_RMSE: 38.6814 - val_loss: 1496.6783 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8808 - loss: 1512.1464 - val_RMSE: 38.6811 - val_loss: 1496.6537 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8835 - loss: 1512.3491 - val_RMSE: 38.6811 - val_loss: 1496.6418 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8788 - loss: 1511.9792 - val_RMSE: 38.6811 - val_loss: 1496.6355 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8838 - loss: 1512.3633 - val_RMSE: 38.6809 - val_loss: 1496.6188 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8828 - loss: 1512.2748 - val_RMSE: 38.6809 - val_loss: 1496.6119 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 18s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 148s 35ms/step - RMSE: 43.1828 - loss: 1916.1990 - val_RMSE: 38.7341 - val_loss: 1501.2308 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9260 - loss: 1516.2416 - val_RMSE: 38.7522 - val_loss: 1503.1017 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9187 - loss: 1516.1676 - val_RMSE: 38.7418 - val_loss: 1502.6028 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8751 - loss: 1512.8329 - val_RMSE: 38.7197 - val_loss: 1500.4335 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8671 - loss: 1511.7841 - val_RMSE: 38.7194 - val_loss: 1500.1112 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8683 - loss: 1511.6200 - val_RMSE: 38.7181 - val_loss: 1499.8542 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8590 - loss: 1510.7631 - val_RMSE: 38.7174 - val_loss: 1499.7125 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8560 - loss: 1510.4453 - val_RMSE: 38.7172 - val_loss: 1499.6349 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8590 - loss: 1510.6244 - val_RMSE: 38.7175 - val_loss: 1499.6127 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8564 - loss: 1510.3749 - val_RMSE: 38.7168 - val_loss: 1499.5217 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8506 - loss: 1509.9030 - val_RMSE: 38.7158 - val_loss: 1499.4307 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8516 - loss: 1509.9583 - val_RMSE: 38.7153 - val_loss: 1499.3761 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8479 - loss: 1509.6581 - val_RMSE: 38.7153 - val_loss: 1499.3641 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8450 - loss: 1509.4153 - val_RMSE: 38.7172 - val_loss: 1499.5005 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8449 - loss: 1509.4033 - val_RMSE: 38.7163 - val_loss: 1499.4207 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8356 - loss: 1508.6681 - val_RMSE: 38.7119 - val_loss: 1499.0619 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8357 - loss: 1508.6619 - val_RMSE: 38.7116 - val_loss: 1499.0386 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8349 - loss: 1508.5945 - val_RMSE: 38.7117 - val_loss: 1499.0326 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8381 - loss: 1508.8324 - val_RMSE: 38.7116 - val_loss: 1499.0187 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8330 - loss: 1508.4298 - val_RMSE: 38.7115 - val_loss: 1499.0021 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8333 - loss: 1508.4491 - val_RMSE: 38.7115 - val_loss: 1498.9976 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8369 - loss: 1508.7228 - val_RMSE: 38.7116 - val_loss: 1498.9965 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8395 - loss: 1508.9120 - val_RMSE: 38.7117 - val_loss: 1499.0038 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 24ms/step - RMSE: 38.8379 - loss: 1508.7859 - val_RMSE: 38.7117 - val_loss: 1498.9905 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8321 - loss: 1508.3302 - val_RMSE: 38.7114 - val_loss: 1498.9688 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 17s 10ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 155s 36ms/step - RMSE: 43.1270 - loss: 1909.9421 - val_RMSE: 38.7296 - val_loss: 1500.8849 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9558 - loss: 1518.5751 - val_RMSE: 38.7394 - val_loss: 1502.0955 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.9556 - loss: 1518.9839 - val_RMSE: 38.7361 - val_loss: 1502.3036 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9071 - loss: 1515.4553 - val_RMSE: 38.7132 - val_loss: 1500.0302 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8936 - loss: 1513.9342 - val_RMSE: 38.7108 - val_loss: 1499.5002 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8915 - loss: 1513.4712 - val_RMSE: 38.7099 - val_loss: 1499.2487 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8826 - loss: 1512.6215 - val_RMSE: 38.7114 - val_loss: 1499.2659 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8900 - loss: 1513.1058 - val_RMSE: 38.7099 - val_loss: 1499.0817 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8863 - loss: 1512.7578 - val_RMSE: 38.7094 - val_loss: 1498.9949 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8821 - loss: 1512.3795 - val_RMSE: 38.7107 - val_loss: 1499.0641 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8840 - loss: 1512.5049 - val_RMSE: 38.7088 - val_loss: 1498.8883 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8795 - loss: 1512.1324 - val_RMSE: 38.7078 - val_loss: 1498.7916 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8736 - loss: 1511.6577 - val_RMSE: 38.7090 - val_loss: 1498.8766 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8795 - loss: 1512.0961 - val_RMSE: 38.7102 - val_loss: 1498.9519 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8671 - loss: 1511.1216 - val_RMSE: 38.7067 - val_loss: 1498.6674 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8667 - loss: 1511.0752 - val_RMSE: 38.7062 - val_loss: 1498.6179 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8728 - loss: 1511.5441 - val_RMSE: 38.7059 - val_loss: 1498.5858 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8655 - loss: 1510.9685 - val_RMSE: 38.7058 - val_loss: 1498.5698 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8659 - loss: 1510.9923 - val_RMSE: 38.7056 - val_loss: 1498.5522 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8658 - loss: 1510.9742 - val_RMSE: 38.7058 - val_loss: 1498.5548 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 70s 27ms/step - RMSE: 38.8660 - loss: 1510.9816 - val_RMSE: 38.7055 - val_loss: 1498.5292 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 69s 27ms/step - RMSE: 38.8629 - loss: 1510.7365 - val_RMSE: 38.7055 - val_loss: 1498.5220 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 67s 26ms/step - RMSE: 38.8638 - loss: 1510.8021 - val_RMSE: 38.7057 - val_loss: 1498.5306 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8657 - loss: 1510.9388 - val_RMSE: 38.7055 - val_loss: 1498.5111 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 67s 26ms/step - RMSE: 38.8657 - loss: 1510.9323 - val_RMSE: 38.7055 - val_loss: 1498.5066 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 17s 10ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 04:50:47,418] Trial 2 finished with value: 38.699286142985024 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 153s 37ms/step - RMSE: 43.1467 - loss: 1912.6401 - val_RMSE: 38.6983 - val_loss: 1497.9614 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.9335 - loss: 1516.2772 - val_RMSE: 38.6935 - val_loss: 1497.8076 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.9336 - loss: 1516.4974 - val_RMSE: 38.6955 - val_loss: 1498.1663 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.9154 - loss: 1515.2963 - val_RMSE: 38.6973 - val_loss: 1498.4552 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8831 - loss: 1512.8254 - val_RMSE: 38.6868 - val_loss: 1497.4468 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8683 - loss: 1511.4871 - val_RMSE: 38.6863 - val_loss: 1497.2582 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8694 - loss: 1511.4333 - val_RMSE: 38.6862 - val_loss: 1497.1595 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8638 - loss: 1510.9166 - val_RMSE: 38.6854 - val_loss: 1497.0293 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8688 - loss: 1511.2439 - val_RMSE: 38.6858 - val_loss: 1497.0173 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8612 - loss: 1510.6085 - val_RMSE: 38.6854 - val_loss: 1496.9486 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8672 - loss: 1511.0455 - val_RMSE: 38.6846 - val_loss: 1496.8655 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8593 - loss: 1510.4016 - val_RMSE: 38.6850 - val_loss: 1496.8708 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8674 - loss: 1511.0186 - val_RMSE: 38.6845 - val_loss: 1496.8170 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8612 - loss: 1510.5210 - val_RMSE: 38.6855 - val_loss: 1496.8867 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8607 - loss: 1510.4718 - val_RMSE: 38.6845 - val_loss: 1496.7950 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8577 - loss: 1510.2253 - val_RMSE: 38.6849 - val_loss: 1496.8239 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8546 - loss: 1509.9801 - val_RMSE: 38.6836 - val_loss: 1496.7120 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8552 - loss: 1510.0190 - val_RMSE: 38.6834 - val_loss: 1496.6898 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8520 - loss: 1509.7625 - val_RMSE: 38.6835 - val_loss: 1496.6901 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8515 - loss: 1509.7184 - val_RMSE: 38.6841 - val_loss: 1496.7323 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8479 - loss: 1509.4292 - val_RMSE: 38.6834 - val_loss: 1496.6722 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8515 - loss: 1509.7061 - val_RMSE: 38.6829 - val_loss: 1496.6322 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 24ms/step - RMSE: 38.8445 - loss: 1509.1569 - val_RMSE: 38.6829 - val_loss: 1496.6284 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8512 - loss: 1509.6818 - val_RMSE: 38.6827 - val_loss: 1496.6064 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 59s 23ms/step - RMSE: 38.8499 - loss: 1509.5737 - val_RMSE: 38.6826 - val_loss: 1496.6011 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 15s 9ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 159s 38ms/step - RMSE: 43.0882 - loss: 1907.0781 - val_RMSE: 38.7436 - val_loss: 1501.4581 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8796 - loss: 1512.0743 - val_RMSE: 38.7295 - val_loss: 1500.6298 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8777 - loss: 1512.2151 - val_RMSE: 38.7367 - val_loss: 1501.4878 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 65s 25ms/step - RMSE: 38.8659 - loss: 1511.5642 - val_RMSE: 38.7309 - val_loss: 1501.2053 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 61s 23ms/step - RMSE: 38.8290 - loss: 1508.7550 - val_RMSE: 38.7219 - val_loss: 1500.2830 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 65s 25ms/step - RMSE: 38.8282 - loss: 1508.4794 - val_RMSE: 38.7212 - val_loss: 1500.0447 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8215 - loss: 1507.7941 - val_RMSE: 38.7204 - val_loss: 1499.8717 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 26ms/step - RMSE: 38.8174 - loss: 1507.3712 - val_RMSE: 38.7206 - val_loss: 1499.8170 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 68s 26ms/step - RMSE: 38.8207 - loss: 1507.5630 - val_RMSE: 38.7213 - val_loss: 1499.8226 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 68s 26ms/step - RMSE: 38.8193 - loss: 1507.4075 - val_RMSE: 38.7201 - val_loss: 1499.6876 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8152 - loss: 1507.0485 - val_RMSE: 38.7198 - val_loss: 1499.6343 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 60s 23ms/step - RMSE: 38.8172 - loss: 1507.1785 - val_RMSE: 38.7192 - val_loss: 1499.5641 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8114 - loss: 1506.7085 - val_RMSE: 38.7183 - val_loss: 1499.4696 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 26ms/step - RMSE: 38.8119 - loss: 1506.7212 - val_RMSE: 38.7187 - val_loss: 1499.4884 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8116 - loss: 1506.6865 - val_RMSE: 38.7199 - val_loss: 1499.5673 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8046 - loss: 1506.1307 - val_RMSE: 38.7150 - val_loss: 1499.1841 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 64s 25ms/step - RMSE: 38.8029 - loss: 1505.9951 - val_RMSE: 38.7150 - val_loss: 1499.1755 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 67s 26ms/step - RMSE: 38.8021 - loss: 1505.9320 - val_RMSE: 38.7147 - val_loss: 1499.1523 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 66s 25ms/step - RMSE: 38.8067 - loss: 1506.2850 - val_RMSE: 38.7145 - val_loss: 1499.1357 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8023 - loss: 1505.9385 - val_RMSE: 38.7146 - val_loss: 1499.1328 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 67s 26ms/step - RMSE: 38.8052 - loss: 1506.1622 - val_RMSE: 38.7145 - val_loss: 1499.1202 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 62s 24ms/step - RMSE: 38.8058 - loss: 1506.2058 - val_RMSE: 38.7143 - val_loss: 1499.1085 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 68s 26ms/step - RMSE: 38.8083 - loss: 1506.3923 - val_RMSE: 38.7142 - val_loss: 1499.0948 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 68s 26ms/step - RMSE: 38.8070 - loss: 1506.2932 - val_RMSE: 38.7142 - val_loss: 1499.0923 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 63s 24ms/step - RMSE: 38.8021 - loss: 1505.9102 - val_RMSE: 38.7140 - val_loss: 1499.0754 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 117s 27ms/step - RMSE: 43.0855 - loss: 1906.2587 - val_RMSE: 38.7267 - val_loss: 1500.1206 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9105 - loss: 1514.4437 - val_RMSE: 38.7184 - val_loss: 1499.7040 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9080 - loss: 1514.4988 - val_RMSE: 38.7158 - val_loss: 1499.8019 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8925 - loss: 1513.6099 - val_RMSE: 38.7175 - val_loss: 1500.1287 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8570 - loss: 1510.8955 - val_RMSE: 38.7136 - val_loss: 1499.6031 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8476 - loss: 1509.9430 - val_RMSE: 38.7124 - val_loss: 1499.3268 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8423 - loss: 1509.3777 - val_RMSE: 38.7118 - val_loss: 1499.1818 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8473 - loss: 1509.6743 - val_RMSE: 38.7117 - val_loss: 1499.1071 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8451 - loss: 1509.4404 - val_RMSE: 38.7103 - val_loss: 1498.9496 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8402 - loss: 1509.0154 - val_RMSE: 38.7104 - val_loss: 1498.9240 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8427 - loss: 1509.1755 - val_RMSE: 38.7087 - val_loss: 1498.7606 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8390 - loss: 1508.8665 - val_RMSE: 38.7081 - val_loss: 1498.6951 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8345 - loss: 1508.4982 - val_RMSE: 38.7088 - val_loss: 1498.7297 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8402 - loss: 1508.9176 - val_RMSE: 38.7108 - val_loss: 1498.8707 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8328 - loss: 1508.3344 - val_RMSE: 38.7076 - val_loss: 1498.6187 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8302 - loss: 1508.1289 - val_RMSE: 38.7085 - val_loss: 1498.6838 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8355 - loss: 1508.5327 - val_RMSE: 38.7080 - val_loss: 1498.6445 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8296 - loss: 1508.0712 - val_RMSE: 38.7077 - val_loss: 1498.6215 - learning_rate: 1.0000e-05\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8297 - loss: 1508.0819 - val_RMSE: 38.7077 - val_loss: 1498.6205 - learning_rate: 1.0000e-05\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8306 - loss: 1508.1505 - val_RMSE: 38.7078 - val_loss: 1498.6224 - learning_rate: 1.0000e-06\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8293 - loss: 1508.0455 - val_RMSE: 38.7079 - val_loss: 1498.6296 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8299 - loss: 1508.0964 - val_RMSE: 38.7076 - val_loss: 1498.6091 - learning_rate: 1.0000e-07\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8292 - loss: 1508.0363 - val_RMSE: 38.7081 - val_loss: 1498.6482 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8291 - loss: 1508.0310 - val_RMSE: 38.7076 - val_loss: 1498.6090 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8305 - loss: 1508.1370 - val_RMSE: 38.7077 - val_loss: 1498.6216 - learning_rate: 1.0000e-07\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 11s 6ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 06:05:34,671] Trial 3 finished with value: 38.701454162597656 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 4, 'transformer_units': 128, 'dropout_rate': 0.32999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 114s 25ms/step - RMSE: 43.1435 - loss: 1912.3213 - val_RMSE: 38.7015 - val_loss: 1498.2279 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.9314 - loss: 1516.1379 - val_RMSE: 38.6943 - val_loss: 1497.9430 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 13ms/step - RMSE: 38.9339 - loss: 1516.6235 - val_RMSE: 38.7009 - val_loss: 1498.7732 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.9153 - loss: 1515.4824 - val_RMSE: 38.6954 - val_loss: 1498.4922 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8841 - loss: 1513.0718 - val_RMSE: 38.6879 - val_loss: 1497.6763 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.8692 - loss: 1511.6859 - val_RMSE: 38.6870 - val_loss: 1497.4207 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.8696 - loss: 1511.5499 - val_RMSE: 38.6868 - val_loss: 1497.2927 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.8653 - loss: 1511.1191 - val_RMSE: 38.6867 - val_loss: 1497.2096 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8689 - loss: 1511.3248 - val_RMSE: 38.6857 - val_loss: 1497.0808 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8624 - loss: 1510.7758 - val_RMSE: 38.6853 - val_loss: 1497.0159 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8672 - loss: 1511.1150 - val_RMSE: 38.6857 - val_loss: 1497.0123 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 36s 14ms/step - RMSE: 38.8592 - loss: 1510.4672 - val_RMSE: 38.6860 - val_loss: 1497.0164 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.8670 - loss: 1511.0509 - val_RMSE: 38.6847 - val_loss: 1496.9001 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 35s 14ms/step - RMSE: 38.8603 - loss: 1510.5122 - val_RMSE: 38.6858 - val_loss: 1496.9672 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8612 - loss: 1510.5704 - val_RMSE: 38.6844 - val_loss: 1496.8457 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8588 - loss: 1510.3680 - val_RMSE: 38.6852 - val_loss: 1496.9039 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8553 - loss: 1510.0886 - val_RMSE: 38.6838 - val_loss: 1496.7844 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8563 - loss: 1510.1576 - val_RMSE: 38.6840 - val_loss: 1496.7878 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8532 - loss: 1509.9043 - val_RMSE: 38.6835 - val_loss: 1496.7439 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8521 - loss: 1509.8169 - val_RMSE: 38.6840 - val_loss: 1496.7772 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8500 - loss: 1509.6456 - val_RMSE: 38.6842 - val_loss: 1496.7865 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8509 - loss: 1509.7151 - val_RMSE: 38.6827 - val_loss: 1496.6647 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8442 - loss: 1509.1863 - val_RMSE: 38.6823 - val_loss: 1496.6378 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8501 - loss: 1509.6471 - val_RMSE: 38.6819 - val_loss: 1496.6010 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8485 - loss: 1509.5172 - val_RMSE: 38.6822 - val_loss: 1496.6163 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 123s 26ms/step - RMSE: 43.0874 - loss: 1906.9557 - val_RMSE: 38.7412 - val_loss: 1501.3207 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8807 - loss: 1512.2094 - val_RMSE: 38.7461 - val_loss: 1501.9521 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8773 - loss: 1512.2358 - val_RMSE: 38.7442 - val_loss: 1502.0736 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8416 - loss: 1509.5913 - val_RMSE: 38.7219 - val_loss: 1500.1827 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8313 - loss: 1508.6251 - val_RMSE: 38.7229 - val_loss: 1500.1093 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8353 - loss: 1508.7992 - val_RMSE: 38.7220 - val_loss: 1499.9417 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8277 - loss: 1508.1227 - val_RMSE: 38.7202 - val_loss: 1499.7432 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8231 - loss: 1507.7129 - val_RMSE: 38.7212 - val_loss: 1499.7777 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8271 - loss: 1507.9814 - val_RMSE: 38.7192 - val_loss: 1499.5903 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8260 - loss: 1507.8699 - val_RMSE: 38.7192 - val_loss: 1499.5653 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8207 - loss: 1507.4343 - val_RMSE: 38.7185 - val_loss: 1499.4913 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8225 - loss: 1507.5547 - val_RMSE: 38.7186 - val_loss: 1499.4832 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8168 - loss: 1507.0968 - val_RMSE: 38.7168 - val_loss: 1499.3308 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8162 - loss: 1507.0337 - val_RMSE: 38.7181 - val_loss: 1499.4225 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8180 - loss: 1507.1685 - val_RMSE: 38.7195 - val_loss: 1499.5259 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8098 - loss: 1506.5255 - val_RMSE: 38.7122 - val_loss: 1498.9563 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8071 - loss: 1506.3075 - val_RMSE: 38.7117 - val_loss: 1498.9125 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8066 - loss: 1506.2655 - val_RMSE: 38.7118 - val_loss: 1498.9138 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8111 - loss: 1506.6099 - val_RMSE: 38.7115 - val_loss: 1498.8809 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8070 - loss: 1506.2926 - val_RMSE: 38.7115 - val_loss: 1498.8854 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8087 - loss: 1506.4160 - val_RMSE: 38.7115 - val_loss: 1498.8835 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8091 - loss: 1506.4424 - val_RMSE: 38.7112 - val_loss: 1498.8561 - learning_rate: 1.0000e-05\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8121 - loss: 1506.6777 - val_RMSE: 38.7112 - val_loss: 1498.8525 - learning_rate: 1.0000e-05\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8110 - loss: 1506.5945 - val_RMSE: 38.7111 - val_loss: 1498.8486 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8058 - loss: 1506.1896 - val_RMSE: 38.7111 - val_loss: 1498.8445 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 110s 24ms/step - RMSE: 43.0833 - loss: 1906.1544 - val_RMSE: 38.7420 - val_loss: 1501.3306 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.9094 - loss: 1514.3882 - val_RMSE: 38.7177 - val_loss: 1499.7139 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9081 - loss: 1514.5768 - val_RMSE: 38.7518 - val_loss: 1502.6815 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8929 - loss: 1513.7024 - val_RMSE: 38.7400 - val_loss: 1501.9214 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8572 - loss: 1510.9570 - val_RMSE: 38.7119 - val_loss: 1499.5076 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8485 - loss: 1510.0437 - val_RMSE: 38.7106 - val_loss: 1499.2214 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8440 - loss: 1509.5364 - val_RMSE: 38.7105 - val_loss: 1499.1041 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8475 - loss: 1509.7103 - val_RMSE: 38.7099 - val_loss: 1498.9866 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8451 - loss: 1509.4650 - val_RMSE: 38.7095 - val_loss: 1498.9097 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8402 - loss: 1509.0345 - val_RMSE: 38.7087 - val_loss: 1498.8076 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8440 - loss: 1509.2955 - val_RMSE: 38.7089 - val_loss: 1498.8007 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8396 - loss: 1508.9308 - val_RMSE: 38.7083 - val_loss: 1498.7338 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8348 - loss: 1508.5409 - val_RMSE: 38.7094 - val_loss: 1498.8029 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8410 - loss: 1509.0067 - val_RMSE: 38.7087 - val_loss: 1498.7296 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8353 - loss: 1508.5453 - val_RMSE: 38.7075 - val_loss: 1498.6260 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8333 - loss: 1508.3806 - val_RMSE: 38.7075 - val_loss: 1498.6204 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8387 - loss: 1508.7925 - val_RMSE: 38.7073 - val_loss: 1498.5940 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8314 - loss: 1508.2211 - val_RMSE: 38.7061 - val_loss: 1498.4948 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 14ms/step - RMSE: 38.8298 - loss: 1508.0856 - val_RMSE: 38.7073 - val_loss: 1498.5839 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8288 - loss: 1508.0077 - val_RMSE: 38.7078 - val_loss: 1498.6121 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8264 - loss: 1507.8097 - val_RMSE: 38.7056 - val_loss: 1498.4427 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8253 - loss: 1507.7246 - val_RMSE: 38.7052 - val_loss: 1498.4094 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8239 - loss: 1507.6123 - val_RMSE: 38.7052 - val_loss: 1498.4077 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8239 - loss: 1507.6039 - val_RMSE: 38.7051 - val_loss: 1498.3900 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8249 - loss: 1507.6761 - val_RMSE: 38.7051 - val_loss: 1498.3849 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 06:58:27,923] Trial 4 finished with value: 38.699440002441406 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'num_transformer_heads': 2, 'transformer_units': 64, 'dropout_rate': 0.32999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 115s 26ms/step - RMSE: 45.7555 - loss: 2168.1316 - val_RMSE: 38.7151 - val_loss: 1499.0077 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.5285 - loss: 1562.6776 - val_RMSE: 38.6991 - val_loss: 1497.8905 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.3829 - loss: 1551.3192 - val_RMSE: 38.6971 - val_loss: 1497.9207 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.2354 - loss: 1539.9041 - val_RMSE: 38.6969 - val_loss: 1498.0271 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1086 - loss: 1530.0475 - val_RMSE: 38.6931 - val_loss: 1497.6881 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0863 - loss: 1528.2585 - val_RMSE: 38.6916 - val_loss: 1497.5175 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0768 - loss: 1527.4636 - val_RMSE: 38.6921 - val_loss: 1497.5164 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0539 - loss: 1525.6434 - val_RMSE: 38.6907 - val_loss: 1497.3795 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0479 - loss: 1525.1437 - val_RMSE: 38.6900 - val_loss: 1497.3008 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0396 - loss: 1524.4745 - val_RMSE: 38.6901 - val_loss: 1497.2837 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 16ms/step - RMSE: 39.0198 - loss: 1522.9083 - val_RMSE: 38.6902 - val_loss: 1497.2766 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0145 - loss: 1522.4779 - val_RMSE: 38.6895 - val_loss: 1497.2114 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.0014 - loss: 1521.4373 - val_RMSE: 38.6881 - val_loss: 1497.0848 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.9961 - loss: 1521.0112 - val_RMSE: 38.6888 - val_loss: 1497.1262 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.9817 - loss: 1519.8778 - val_RMSE: 38.6881 - val_loss: 1497.0613 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.9768 - loss: 1519.4847 - val_RMSE: 38.6870 - val_loss: 1496.9670 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9651 - loss: 1518.5574 - val_RMSE: 38.6878 - val_loss: 1497.0236 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9532 - loss: 1517.6261 - val_RMSE: 38.6872 - val_loss: 1496.9645 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.9506 - loss: 1517.4102 - val_RMSE: 38.6877 - val_loss: 1496.9958 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9383 - loss: 1516.4506 - val_RMSE: 38.6879 - val_loss: 1497.0070 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9326 - loss: 1515.9972 - val_RMSE: 38.6870 - val_loss: 1496.9302 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9297 - loss: 1515.7722 - val_RMSE: 38.6865 - val_loss: 1496.8931 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9312 - loss: 1515.8837 - val_RMSE: 38.6862 - val_loss: 1496.8665 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9303 - loss: 1515.8151 - val_RMSE: 38.6860 - val_loss: 1496.8502 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9209 - loss: 1515.0839 - val_RMSE: 38.6862 - val_loss: 1496.8690 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 123s 27ms/step - RMSE: 45.7189 - loss: 2164.6008 - val_RMSE: 38.7478 - val_loss: 1501.5664 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.4763 - loss: 1558.5763 - val_RMSE: 38.7292 - val_loss: 1500.2493 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.3425 - loss: 1548.1737 - val_RMSE: 38.7311 - val_loss: 1500.5592 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1711 - loss: 1534.8774 - val_RMSE: 38.7272 - val_loss: 1500.3629 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0583 - loss: 1526.1018 - val_RMSE: 38.7210 - val_loss: 1499.8284 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0409 - loss: 1524.6914 - val_RMSE: 38.7202 - val_loss: 1499.7159 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.0269 - loss: 1523.5452 - val_RMSE: 38.7201 - val_loss: 1499.6591 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0149 - loss: 1522.5728 - val_RMSE: 38.7184 - val_loss: 1499.5040 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9983 - loss: 1521.2456 - val_RMSE: 38.7184 - val_loss: 1499.4797 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.9966 - loss: 1521.0936 - val_RMSE: 38.7172 - val_loss: 1499.3621 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9828 - loss: 1519.9980 - val_RMSE: 38.7179 - val_loss: 1499.4021 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9686 - loss: 1518.8765 - val_RMSE: 38.7168 - val_loss: 1499.3005 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9579 - loss: 1518.0286 - val_RMSE: 38.7175 - val_loss: 1499.3401 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9451 - loss: 1517.0153 - val_RMSE: 38.7166 - val_loss: 1499.2563 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9369 - loss: 1516.3612 - val_RMSE: 38.7166 - val_loss: 1499.2494 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9196 - loss: 1515.0061 - val_RMSE: 38.7169 - val_loss: 1499.2621 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 19ms/step - RMSE: 38.9139 - loss: 1514.5591 - val_RMSE: 38.7162 - val_loss: 1499.2021 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.9066 - loss: 1513.9805 - val_RMSE: 38.7164 - val_loss: 1499.2109 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.9014 - loss: 1513.5691 - val_RMSE: 38.7161 - val_loss: 1499.1810 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.8881 - loss: 1512.5328 - val_RMSE: 38.7162 - val_loss: 1499.1799 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 50s 19ms/step - RMSE: 38.8826 - loss: 1512.0980 - val_RMSE: 38.7152 - val_loss: 1499.1049 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.8776 - loss: 1511.7020 - val_RMSE: 38.7153 - val_loss: 1499.1086 - learning_rate: 1.0000e-03\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8641 - loss: 1510.6454 - val_RMSE: 38.7155 - val_loss: 1499.1180 - learning_rate: 1.0000e-03\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.8575 - loss: 1510.1354 - val_RMSE: 38.7145 - val_loss: 1499.0352 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8588 - loss: 1510.2299 - val_RMSE: 38.7143 - val_loss: 1499.0184 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 118s 26ms/step - RMSE: 45.7002 - loss: 2162.1248 - val_RMSE: 38.7195 - val_loss: 1499.3488 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 39.5034 - loss: 1560.6990 - val_RMSE: 38.7215 - val_loss: 1499.6198 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.3608 - loss: 1549.5713 - val_RMSE: 38.7163 - val_loss: 1499.3687 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.2126 - loss: 1538.0386 - val_RMSE: 38.7228 - val_loss: 1499.8433 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 39.2021 - loss: 1537.1873 - val_RMSE: 38.7189 - val_loss: 1499.5131 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1778 - loss: 1535.2606 - val_RMSE: 38.7169 - val_loss: 1499.3527 - learning_rate: 1.0000e-04\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1736 - loss: 1534.9222 - val_RMSE: 38.7167 - val_loss: 1499.3334 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1734 - loss: 1534.9026 - val_RMSE: 38.7161 - val_loss: 1499.2797 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.1805 - loss: 1535.4563 - val_RMSE: 38.7167 - val_loss: 1499.3292 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1715 - loss: 1534.7469 - val_RMSE: 38.7160 - val_loss: 1499.2714 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1767 - loss: 1535.1536 - val_RMSE: 38.7156 - val_loss: 1499.2382 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.1627 - loss: 1534.0553 - val_RMSE: 38.7158 - val_loss: 1499.2437 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.1667 - loss: 1534.3608 - val_RMSE: 38.7154 - val_loss: 1499.2161 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.1680 - loss: 1534.4641 - val_RMSE: 38.7161 - val_loss: 1499.2670 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.1670 - loss: 1534.3833 - val_RMSE: 38.7149 - val_loss: 1499.1672 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1491 - loss: 1532.9792 - val_RMSE: 38.7148 - val_loss: 1499.1571 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1539 - loss: 1533.3547 - val_RMSE: 38.7146 - val_loss: 1499.1401 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.1601 - loss: 1533.8358 - val_RMSE: 38.7143 - val_loss: 1499.1135 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 39.1611 - loss: 1533.9114 - val_RMSE: 38.7145 - val_loss: 1499.1306 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.1487 - loss: 1532.9391 - val_RMSE: 38.7143 - val_loss: 1499.1112 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.1549 - loss: 1533.4232 - val_RMSE: 38.7144 - val_loss: 1499.1190 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.1485 - loss: 1532.9193 - val_RMSE: 38.7139 - val_loss: 1499.0756 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 39.1406 - loss: 1532.2950 - val_RMSE: 38.7139 - val_loss: 1499.0732 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.1514 - loss: 1533.1381 - val_RMSE: 38.7131 - val_loss: 1499.0117 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 39.1498 - loss: 1533.0116 - val_RMSE: 38.7137 - val_loss: 1499.0575 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 07:58:39,266] Trial 5 finished with value: 38.70474751790365 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'gelu', 'num_transformer_heads': 4, 'transformer_units': 128, 'dropout_rate': 0.39, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 111s 25ms/step - RMSE: 42.4475 - loss: 1845.1521 - val_RMSE: 38.7144 - val_loss: 1499.8320 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8458 - loss: 1510.0443 - val_RMSE: 38.6963 - val_loss: 1498.6664 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.8482 - loss: 1510.5233 - val_RMSE: 38.6972 - val_loss: 1499.0239 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8444 - loss: 1510.5272 - val_RMSE: 38.7031 - val_loss: 1499.6031 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.8118 - loss: 1507.8633 - val_RMSE: 38.6852 - val_loss: 1497.6406 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8080 - loss: 1507.0682 - val_RMSE: 38.6847 - val_loss: 1497.3075 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8027 - loss: 1506.4084 - val_RMSE: 38.6847 - val_loss: 1497.1561 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8008 - loss: 1506.1333 - val_RMSE: 38.6840 - val_loss: 1497.0272 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7933 - loss: 1505.4794 - val_RMSE: 38.6835 - val_loss: 1496.9302 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7955 - loss: 1505.6045 - val_RMSE: 38.6828 - val_loss: 1496.8416 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7970 - loss: 1505.6798 - val_RMSE: 38.6829 - val_loss: 1496.8168 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8002 - loss: 1505.9060 - val_RMSE: 38.6827 - val_loss: 1496.7836 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.7941 - loss: 1505.4130 - val_RMSE: 38.6824 - val_loss: 1496.7395 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7951 - loss: 1505.4664 - val_RMSE: 38.6827 - val_loss: 1496.7531 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7937 - loss: 1505.3501 - val_RMSE: 38.6821 - val_loss: 1496.6962 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7941 - loss: 1505.3696 - val_RMSE: 38.6821 - val_loss: 1496.6874 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7954 - loss: 1505.4630 - val_RMSE: 38.6836 - val_loss: 1496.7948 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7957 - loss: 1505.4836 - val_RMSE: 38.6825 - val_loss: 1496.7125 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7933 - loss: 1505.2955 - val_RMSE: 38.6811 - val_loss: 1496.5918 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7864 - loss: 1504.7499 - val_RMSE: 38.6808 - val_loss: 1496.5557 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7873 - loss: 1504.8098 - val_RMSE: 38.6805 - val_loss: 1496.5270 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7888 - loss: 1504.9154 - val_RMSE: 38.6804 - val_loss: 1496.5106 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7853 - loss: 1504.6434 - val_RMSE: 38.6803 - val_loss: 1496.5021 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7887 - loss: 1504.8939 - val_RMSE: 38.6801 - val_loss: 1496.4764 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7880 - loss: 1504.8342 - val_RMSE: 38.6801 - val_loss: 1496.4771 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 119s 27ms/step - RMSE: 42.3829 - loss: 1839.0455 - val_RMSE: 38.7509 - val_loss: 1502.5441 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7978 - loss: 1506.2330 - val_RMSE: 38.7444 - val_loss: 1502.3661 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8006 - loss: 1506.8879 - val_RMSE: 38.7451 - val_loss: 1502.9218 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8016 - loss: 1507.3350 - val_RMSE: 38.7529 - val_loss: 1503.6680 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7649 - loss: 1504.4216 - val_RMSE: 38.7237 - val_loss: 1500.7728 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7550 - loss: 1503.1031 - val_RMSE: 38.7243 - val_loss: 1500.4862 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7556 - loss: 1502.8591 - val_RMSE: 38.7232 - val_loss: 1500.2222 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7553 - loss: 1502.6808 - val_RMSE: 38.7209 - val_loss: 1499.9453 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7529 - loss: 1502.4088 - val_RMSE: 38.7244 - val_loss: 1500.1570 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7492 - loss: 1502.0676 - val_RMSE: 38.7239 - val_loss: 1500.0767 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7465 - loss: 1501.8169 - val_RMSE: 38.7148 - val_loss: 1499.3582 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7470 - loss: 1501.8451 - val_RMSE: 38.7148 - val_loss: 1499.3383 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7450 - loss: 1501.6754 - val_RMSE: 38.7148 - val_loss: 1499.3224 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7417 - loss: 1501.4037 - val_RMSE: 38.7147 - val_loss: 1499.3103 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7456 - loss: 1501.7014 - val_RMSE: 38.7146 - val_loss: 1499.2893 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7473 - loss: 1501.8195 - val_RMSE: 38.7148 - val_loss: 1499.3005 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7434 - loss: 1501.5094 - val_RMSE: 38.7147 - val_loss: 1499.2864 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.7426 - loss: 1501.4451 - val_RMSE: 38.7144 - val_loss: 1499.2526 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7454 - loss: 1501.6525 - val_RMSE: 38.7143 - val_loss: 1499.2401 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7448 - loss: 1501.5985 - val_RMSE: 38.7143 - val_loss: 1499.2319 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7430 - loss: 1501.4496 - val_RMSE: 38.7143 - val_loss: 1499.2231 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7407 - loss: 1501.2650 - val_RMSE: 38.7144 - val_loss: 1499.2222 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7424 - loss: 1501.3975 - val_RMSE: 38.7142 - val_loss: 1499.2080 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7424 - loss: 1501.3904 - val_RMSE: 38.7140 - val_loss: 1499.1799 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7403 - loss: 1501.2173 - val_RMSE: 38.7139 - val_loss: 1499.1743 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 115s 26ms/step - RMSE: 42.3021 - loss: 1830.5432 - val_RMSE: 38.7214 - val_loss: 1500.2247 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8304 - loss: 1508.7031 - val_RMSE: 38.7849 - val_loss: 1505.4067 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8303 - loss: 1509.0193 - val_RMSE: 38.7682 - val_loss: 1504.5594 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7944 - loss: 1506.4683 - val_RMSE: 38.7104 - val_loss: 1499.5895 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7859 - loss: 1505.3501 - val_RMSE: 38.7095 - val_loss: 1499.2378 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.7803 - loss: 1504.6820 - val_RMSE: 38.7095 - val_loss: 1499.0968 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7818 - loss: 1504.6760 - val_RMSE: 38.7092 - val_loss: 1499.0006 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7776 - loss: 1504.2859 - val_RMSE: 38.7086 - val_loss: 1498.9076 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7744 - loss: 1503.9915 - val_RMSE: 38.7084 - val_loss: 1498.8505 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7756 - loss: 1504.0525 - val_RMSE: 38.7089 - val_loss: 1498.8649 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7775 - loss: 1504.1755 - val_RMSE: 38.7073 - val_loss: 1498.7172 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7803 - loss: 1504.3685 - val_RMSE: 38.7073 - val_loss: 1498.7063 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.7707 - loss: 1503.6112 - val_RMSE: 38.7067 - val_loss: 1498.6438 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7757 - loss: 1503.9913 - val_RMSE: 38.7078 - val_loss: 1498.7180 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.7779 - loss: 1504.1486 - val_RMSE: 38.7061 - val_loss: 1498.5847 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7761 - loss: 1504.0052 - val_RMSE: 38.7064 - val_loss: 1498.5966 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7717 - loss: 1503.6576 - val_RMSE: 38.7075 - val_loss: 1498.6698 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7686 - loss: 1503.4067 - val_RMSE: 38.7062 - val_loss: 1498.5604 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7689 - loss: 1503.4172 - val_RMSE: 38.7059 - val_loss: 1498.5315 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7671 - loss: 1503.2646 - val_RMSE: 38.7055 - val_loss: 1498.4899 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7684 - loss: 1503.3551 - val_RMSE: 38.7055 - val_loss: 1498.4773 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7675 - loss: 1503.2802 - val_RMSE: 38.7056 - val_loss: 1498.4763 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7649 - loss: 1503.0704 - val_RMSE: 38.7054 - val_loss: 1498.4600 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7624 - loss: 1502.8688 - val_RMSE: 38.7052 - val_loss: 1498.4392 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7674 - loss: 1503.2544 - val_RMSE: 38.7050 - val_loss: 1498.4156 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 13s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 08:57:37,306] Trial 6 finished with value: 38.69969813028971 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.3, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 113s 26ms/step - RMSE: 44.4115 - loss: 2034.5447 - val_RMSE: 38.7035 - val_loss: 1498.1129 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.2853 - loss: 1543.5131 - val_RMSE: 38.6987 - val_loss: 1497.8732 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.2164 - loss: 1538.2460 - val_RMSE: 38.6962 - val_loss: 1497.8669 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.1339 - loss: 1531.9917 - val_RMSE: 38.6970 - val_loss: 1498.0996 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0597 - loss: 1526.3479 - val_RMSE: 38.6948 - val_loss: 1498.0144 - learning_rate: 0.0100\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.0003 - loss: 1521.7356 - val_RMSE: 38.6919 - val_loss: 1497.7168 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9883 - loss: 1520.7283 - val_RMSE: 38.6920 - val_loss: 1497.6581 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9737 - loss: 1519.5143 - val_RMSE: 38.6924 - val_loss: 1497.6260 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.9651 - loss: 1518.7964 - val_RMSE: 38.6923 - val_loss: 1497.5817 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9688 - loss: 1519.0426 - val_RMSE: 38.6914 - val_loss: 1497.4691 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.9587 - loss: 1518.2207 - val_RMSE: 38.6917 - val_loss: 1497.4679 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.9553 - loss: 1517.9292 - val_RMSE: 38.6923 - val_loss: 1497.4910 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9436 - loss: 1516.9950 - val_RMSE: 38.6907 - val_loss: 1497.3444 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9442 - loss: 1517.0242 - val_RMSE: 38.6909 - val_loss: 1497.3459 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9408 - loss: 1516.7393 - val_RMSE: 38.6908 - val_loss: 1497.3202 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9343 - loss: 1516.2159 - val_RMSE: 38.6914 - val_loss: 1497.3490 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.9229 - loss: 1515.3163 - val_RMSE: 38.6910 - val_loss: 1497.3099 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 38.9186 - loss: 1514.9668 - val_RMSE: 38.6914 - val_loss: 1497.3282 - learning_rate: 1.0000e-03\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9116 - loss: 1514.4094 - val_RMSE: 38.6902 - val_loss: 1497.2266 - learning_rate: 1.0000e-03\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9112 - loss: 1514.3723 - val_RMSE: 38.6914 - val_loss: 1497.3125 - learning_rate: 1.0000e-03\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9129 - loss: 1514.4952 - val_RMSE: 38.6910 - val_loss: 1497.2709 - learning_rate: 1.0000e-03\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9024 - loss: 1513.6681 - val_RMSE: 38.6908 - val_loss: 1497.2500 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9059 - loss: 1513.9403 - val_RMSE: 38.6907 - val_loss: 1497.2383 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9035 - loss: 1513.7535 - val_RMSE: 38.6903 - val_loss: 1497.2067 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9047 - loss: 1513.8467 - val_RMSE: 38.6902 - val_loss: 1497.1969 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 115s 25ms/step - RMSE: 44.3622 - loss: 2029.6350 - val_RMSE: 38.7290 - val_loss: 1500.0935 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.2330 - loss: 1539.4219 - val_RMSE: 38.7302 - val_loss: 1500.3285 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.1566 - loss: 1533.5837 - val_RMSE: 38.7354 - val_loss: 1500.9144 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0768 - loss: 1527.4764 - val_RMSE: 38.7230 - val_loss: 1499.9257 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0550 - loss: 1525.7434 - val_RMSE: 38.7209 - val_loss: 1499.7285 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 39.0521 - loss: 1525.4745 - val_RMSE: 38.7212 - val_loss: 1499.7164 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 39.0355 - loss: 1524.1504 - val_RMSE: 38.7213 - val_loss: 1499.7041 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 39.0313 - loss: 1523.8004 - val_RMSE: 38.7187 - val_loss: 1499.4832 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 39.0310 - loss: 1523.7611 - val_RMSE: 38.7197 - val_loss: 1499.5424 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 39.0179 - loss: 1522.7188 - val_RMSE: 38.7191 - val_loss: 1499.4773 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 39.0137 - loss: 1522.3821 - val_RMSE: 38.7189 - val_loss: 1499.4559 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9969 - loss: 1521.0582 - val_RMSE: 38.7195 - val_loss: 1499.4899 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9922 - loss: 1520.6802 - val_RMSE: 38.7185 - val_loss: 1499.3995 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9817 - loss: 1519.8529 - val_RMSE: 38.7193 - val_loss: 1499.4558 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.9750 - loss: 1519.3152 - val_RMSE: 38.7177 - val_loss: 1499.3195 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.9679 - loss: 1518.7572 - val_RMSE: 38.7185 - val_loss: 1499.3740 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9550 - loss: 1517.7489 - val_RMSE: 38.7192 - val_loss: 1499.4272 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9507 - loss: 1517.4058 - val_RMSE: 38.7186 - val_loss: 1499.3754 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9591 - loss: 1518.0565 - val_RMSE: 38.7185 - val_loss: 1499.3643 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.9565 - loss: 1517.8558 - val_RMSE: 38.7182 - val_loss: 1499.3406 - learning_rate: 1.0000e-05\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.9564 - loss: 1517.8472 - val_RMSE: 38.7181 - val_loss: 1499.3401 - learning_rate: 1.0000e-05\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9541 - loss: 1517.6646 - val_RMSE: 38.7180 - val_loss: 1499.3311 - learning_rate: 1.0000e-06\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9526 - loss: 1517.5491 - val_RMSE: 38.7181 - val_loss: 1499.3387 - learning_rate: 1.0000e-06\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9540 - loss: 1517.6628 - val_RMSE: 38.7181 - val_loss: 1499.3337 - learning_rate: 1.0000e-07\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9578 - loss: 1517.9537 - val_RMSE: 38.7182 - val_loss: 1499.3386 - learning_rate: 1.0000e-07\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 112s 25ms/step - RMSE: 44.3326 - loss: 2026.0779 - val_RMSE: 38.7319 - val_loss: 1500.3129 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.2594 - loss: 1541.4846 - val_RMSE: 38.7179 - val_loss: 1499.3530 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.1896 - loss: 1536.1431 - val_RMSE: 38.7392 - val_loss: 1501.1774 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.1068 - loss: 1529.8394 - val_RMSE: 38.7384 - val_loss: 1501.2596 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0326 - loss: 1524.1267 - val_RMSE: 38.7190 - val_loss: 1499.7012 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0253 - loss: 1523.5060 - val_RMSE: 38.7167 - val_loss: 1499.4752 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0072 - loss: 1522.0521 - val_RMSE: 38.7159 - val_loss: 1499.4034 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0092 - loss: 1522.1993 - val_RMSE: 38.7156 - val_loss: 1499.3785 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0034 - loss: 1521.7450 - val_RMSE: 38.7152 - val_loss: 1499.3414 - learning_rate: 1.0000e-05\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0030 - loss: 1521.7135 - val_RMSE: 38.7149 - val_loss: 1499.3220 - learning_rate: 1.0000e-05\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 39s 15ms/step - RMSE: 39.0185 - loss: 1522.9198 - val_RMSE: 38.7150 - val_loss: 1499.3256 - learning_rate: 1.0000e-05\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0113 - loss: 1522.3633 - val_RMSE: 38.7148 - val_loss: 1499.3162 - learning_rate: 1.0000e-05\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0095 - loss: 1522.2219 - val_RMSE: 38.7148 - val_loss: 1499.3107 - learning_rate: 1.0000e-05\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0046 - loss: 1521.8325 - val_RMSE: 38.7148 - val_loss: 1499.3109 - learning_rate: 1.0000e-05\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0069 - loss: 1522.0111 - val_RMSE: 38.7147 - val_loss: 1499.3057 - learning_rate: 1.0000e-05\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0069 - loss: 1522.0167 - val_RMSE: 38.7148 - val_loss: 1499.3093 - learning_rate: 1.0000e-05\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.9997 - loss: 1521.4526 - val_RMSE: 38.7148 - val_loss: 1499.3075 - learning_rate: 1.0000e-05\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0118 - loss: 1522.3987 - val_RMSE: 38.7147 - val_loss: 1499.3020 - learning_rate: 1.0000e-06\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0149 - loss: 1522.6351 - val_RMSE: 38.7146 - val_loss: 1499.2985 - learning_rate: 1.0000e-06\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 39.0099 - loss: 1522.2482 - val_RMSE: 38.7147 - val_loss: 1499.3003 - learning_rate: 1.0000e-06\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 39.0012 - loss: 1521.5675 - val_RMSE: 38.7147 - val_loss: 1499.3024 - learning_rate: 1.0000e-06\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0048 - loss: 1521.8497 - val_RMSE: 38.7147 - val_loss: 1499.3002 - learning_rate: 1.0000e-07\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0053 - loss: 1521.8860 - val_RMSE: 38.7148 - val_loss: 1499.3057 - learning_rate: 1.0000e-07\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0043 - loss: 1521.8080 - val_RMSE: 38.7147 - val_loss: 1499.3025 - learning_rate: 1.0000e-08\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 39.0010 - loss: 1521.5508 - val_RMSE: 38.7147 - val_loss: 1499.3065 - learning_rate: 1.0000e-08\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-17 09:53:44,757] Trial 7 finished with value: 38.707681020100914 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 96, 'dropout_rate': 0.44999999999999996, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 115s 26ms/step - RMSE: 42.6470 - loss: 1863.9602 - val_RMSE: 38.7233 - val_loss: 1500.6809 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.9045 - loss: 1514.8726 - val_RMSE: 38.7080 - val_loss: 1500.0286 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.9014 - loss: 1515.2236 - val_RMSE: 38.7096 - val_loss: 1500.5956 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 40s 15ms/step - RMSE: 38.8919 - loss: 1514.8608 - val_RMSE: 38.7099 - val_loss: 1500.8645 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 37s 14ms/step - RMSE: 38.8630 - loss: 1512.5320 - val_RMSE: 38.6867 - val_loss: 1498.2809 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 38s 15ms/step - RMSE: 38.8489 - loss: 1510.7285 - val_RMSE: 38.6854 - val_loss: 1497.7341 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8430 - loss: 1509.8857 - val_RMSE: 38.6856 - val_loss: 1497.5156 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8413 - loss: 1509.5498 - val_RMSE: 38.6851 - val_loss: 1497.3406 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8349 - loss: 1508.9290 - val_RMSE: 38.6856 - val_loss: 1497.2944 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8347 - loss: 1508.8298 - val_RMSE: 38.6849 - val_loss: 1497.1731 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8400 - loss: 1509.1870 - val_RMSE: 38.6849 - val_loss: 1497.1284 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8434 - loss: 1509.4121 - val_RMSE: 38.6844 - val_loss: 1497.0596 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.8360 - loss: 1508.8081 - val_RMSE: 38.6852 - val_loss: 1497.0940 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8354 - loss: 1508.7378 - val_RMSE: 38.6847 - val_loss: 1497.0397 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8311 - loss: 1508.3885 - val_RMSE: 38.6839 - val_loss: 1496.9625 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8309 - loss: 1508.3583 - val_RMSE: 38.6847 - val_loss: 1497.0206 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8304 - loss: 1508.3097 - val_RMSE: 38.6842 - val_loss: 1496.9631 - learning_rate: 1.0000e-03\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8300 - loss: 1508.2615 - val_RMSE: 38.6834 - val_loss: 1496.8873 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8280 - loss: 1508.0975 - val_RMSE: 38.6833 - val_loss: 1496.8713 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8256 - loss: 1507.8950 - val_RMSE: 38.6831 - val_loss: 1496.8391 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8245 - loss: 1507.7958 - val_RMSE: 38.6832 - val_loss: 1496.8417 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8258 - loss: 1507.8939 - val_RMSE: 38.6831 - val_loss: 1496.8247 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8195 - loss: 1507.3917 - val_RMSE: 38.6830 - val_loss: 1496.8035 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8257 - loss: 1507.8683 - val_RMSE: 38.6830 - val_loss: 1496.7996 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8248 - loss: 1507.7852 - val_RMSE: 38.6831 - val_loss: 1496.7948 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 18s 11ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 122s 27ms/step - RMSE: 42.6789 - loss: 1867.5491 - val_RMSE: 38.7526 - val_loss: 1502.9938 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8560 - loss: 1511.1396 - val_RMSE: 38.7444 - val_loss: 1502.8888 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8534 - loss: 1511.4731 - val_RMSE: 38.7555 - val_loss: 1504.2766 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8508 - loss: 1511.7206 - val_RMSE: 38.7391 - val_loss: 1503.2950 - learning_rate: 0.0100\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8094 - loss: 1508.5171 - val_RMSE: 38.7226 - val_loss: 1501.1578 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7924 - loss: 1506.4233 - val_RMSE: 38.7215 - val_loss: 1500.5839 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7914 - loss: 1505.9281 - val_RMSE: 38.7196 - val_loss: 1500.1873 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7950 - loss: 1505.9901 - val_RMSE: 38.7205 - val_loss: 1500.1183 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.7862 - loss: 1505.1833 - val_RMSE: 38.7220 - val_loss: 1500.1399 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7864 - loss: 1505.1119 - val_RMSE: 38.7201 - val_loss: 1499.9286 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7867 - loss: 1505.0771 - val_RMSE: 38.7208 - val_loss: 1499.9283 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.7880 - loss: 1505.1301 - val_RMSE: 38.7195 - val_loss: 1499.8014 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7860 - loss: 1504.9496 - val_RMSE: 38.7195 - val_loss: 1499.7706 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7825 - loss: 1504.6515 - val_RMSE: 38.7196 - val_loss: 1499.7528 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7845 - loss: 1504.7850 - val_RMSE: 38.7198 - val_loss: 1499.7534 - learning_rate: 1.0000e-03\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7861 - loss: 1504.8862 - val_RMSE: 38.7200 - val_loss: 1499.7610 - learning_rate: 1.0000e-03\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.7768 - loss: 1504.1593 - val_RMSE: 38.7136 - val_loss: 1499.2472 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7771 - loss: 1504.1647 - val_RMSE: 38.7135 - val_loss: 1499.2288 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 17ms/step - RMSE: 38.7808 - loss: 1504.4373 - val_RMSE: 38.7135 - val_loss: 1499.2152 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7778 - loss: 1504.1967 - val_RMSE: 38.7135 - val_loss: 1499.2048 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.7775 - loss: 1504.1656 - val_RMSE: 38.7135 - val_loss: 1499.1959 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7747 - loss: 1503.9414 - val_RMSE: 38.7134 - val_loss: 1499.1766 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.7763 - loss: 1504.0522 - val_RMSE: 38.7132 - val_loss: 1499.1594 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.7761 - loss: 1504.0264 - val_RMSE: 38.7131 - val_loss: 1499.1449 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 51s 20ms/step - RMSE: 38.7747 - loss: 1503.9128 - val_RMSE: 38.7133 - val_loss: 1499.1497 - learning_rate: 1.0000e-04\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 13s 7ms/step\n",
            "Epoch 1/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 121s 28ms/step - RMSE: 42.5061 - loss: 1849.7598 - val_RMSE: 38.7247 - val_loss: 1500.7529 - learning_rate: 0.0100\n",
            "Epoch 2/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8844 - loss: 1513.2695 - val_RMSE: 38.7218 - val_loss: 1501.0789 - learning_rate: 0.0100\n",
            "Epoch 3/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8799 - loss: 1513.4834 - val_RMSE: 38.7221 - val_loss: 1501.6064 - learning_rate: 0.0100\n",
            "Epoch 4/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 17ms/step - RMSE: 38.8444 - loss: 1510.9349 - val_RMSE: 38.7112 - val_loss: 1500.1058 - learning_rate: 1.0000e-03\n",
            "Epoch 5/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8289 - loss: 1509.1060 - val_RMSE: 38.7101 - val_loss: 1499.5858 - learning_rate: 1.0000e-03\n",
            "Epoch 6/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8265 - loss: 1508.5498 - val_RMSE: 38.7098 - val_loss: 1499.3447 - learning_rate: 1.0000e-03\n",
            "Epoch 7/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8250 - loss: 1508.2465 - val_RMSE: 38.7091 - val_loss: 1499.1617 - learning_rate: 1.0000e-03\n",
            "Epoch 8/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8208 - loss: 1507.7996 - val_RMSE: 38.7097 - val_loss: 1499.1233 - learning_rate: 1.0000e-03\n",
            "Epoch 9/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8156 - loss: 1507.3196 - val_RMSE: 38.7094 - val_loss: 1499.0496 - learning_rate: 1.0000e-03\n",
            "Epoch 10/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 41s 16ms/step - RMSE: 38.8179 - loss: 1507.4545 - val_RMSE: 38.7087 - val_loss: 1498.9669 - learning_rate: 1.0000e-03\n",
            "Epoch 11/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8191 - loss: 1507.5155 - val_RMSE: 38.7076 - val_loss: 1498.8505 - learning_rate: 1.0000e-03\n",
            "Epoch 12/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8237 - loss: 1507.8479 - val_RMSE: 38.7067 - val_loss: 1498.7649 - learning_rate: 1.0000e-03\n",
            "Epoch 13/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 42s 16ms/step - RMSE: 38.8124 - loss: 1506.9535 - val_RMSE: 38.7073 - val_loss: 1498.7961 - learning_rate: 1.0000e-03\n",
            "Epoch 14/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 43s 16ms/step - RMSE: 38.8169 - loss: 1507.2902 - val_RMSE: 38.7079 - val_loss: 1498.8285 - learning_rate: 1.0000e-03\n",
            "Epoch 15/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.8139 - loss: 1507.0425 - val_RMSE: 38.7062 - val_loss: 1498.6810 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 47s 18ms/step - RMSE: 38.8125 - loss: 1506.9236 - val_RMSE: 38.7060 - val_loss: 1498.6515 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8069 - loss: 1506.4723 - val_RMSE: 38.7058 - val_loss: 1498.6256 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 44s 17ms/step - RMSE: 38.8106 - loss: 1506.7443 - val_RMSE: 38.7055 - val_loss: 1498.5940 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8091 - loss: 1506.6151 - val_RMSE: 38.7053 - val_loss: 1498.5637 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.8070 - loss: 1506.4437 - val_RMSE: 38.7053 - val_loss: 1498.5557 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 45s 17ms/step - RMSE: 38.8082 - loss: 1506.5293 - val_RMSE: 38.7054 - val_loss: 1498.5553 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 49s 19ms/step - RMSE: 38.8086 - loss: 1506.5477 - val_RMSE: 38.7056 - val_loss: 1498.5579 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8045 - loss: 1506.2297 - val_RMSE: 38.7059 - val_loss: 1498.5764 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 48s 18ms/step - RMSE: 38.8008 - loss: 1505.9331 - val_RMSE: 38.7054 - val_loss: 1498.5397 - learning_rate: 1.0000e-05\n",
            "Epoch 25/25\n",
            "2601/2601 ━━━━━━━━━━━━━━━━━━━━ 46s 18ms/step - RMSE: 38.8063 - loss: 1506.3608 - val_RMSE: 38.7053 - val_loss: 1498.5286 - learning_rate: 1.0000e-05\n",
            "1301/1301 ━━━━━━━━━━━━━━━━━━━━ 12s 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-17 10:52:39,036] Trial 8 finished with value: 38.700538635253906 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'selu', 'num_transformer_heads': 2, 'transformer_units': 32, 'dropout_rate': 0.42, 'repeat_att': 1}. Best is trial 2 with value: 38.699286142985024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            " 709/2601 ━━━━━━━━━━━━━━━━━━━━ 54s 29ms/step - RMSE: 50.1655 - loss: 2638.2307"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-17 10:54:36,069] Trial 9 failed with parameters: {'units': 512, 'last_layer': 2, 'activation': 'relu', 'num_transformer_heads': 2, 'transformer_units': 128, 'dropout_rate': 0.48, 'repeat_att': 2} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-110-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-109-a07edfb686fe>\", line 54, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 139, in call_function\n",
            "    return function._call_flat(  # pylint: disable=protected-access\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1322, in _call_flat\n",
            "    return self._inference_function.call_preflattened(args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 216, in call_preflattened\n",
            "    flat_outputs = self.call_flat(*args)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 251, in call_flat\n",
            "    outputs = self._bound_context.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\", line 1683, in call_function\n",
            "    outputs = execute.execute(\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
            "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-02-17 10:54:36,072] Trial 9 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-109-a07edfb686fe>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     55\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 2 finished with value: 38.699286142985024\n",
        "* parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}"
      ],
      "metadata": {
        "id": "RWAsq27zwysI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9",
        "id": "PxgRBgt6wysJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RAYXidmYEsqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}