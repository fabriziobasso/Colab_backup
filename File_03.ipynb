{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Colab_backup/blob/main/File_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **S5E2: BACKPACK PRICE**"
      ],
      "metadata": {
        "id": "B4iRQtqw_Q-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: white; font-size:100%; text-align:left\">\n",
        "\n",
        "<h3 align=\"center\"><font color='#DAA520'>ðŸ’¡ About The Competition :</font></h3>\n",
        "    \n",
        "**Task**: Predict the price of backpacks based on various attributes\n",
        "\n",
        "**Dataset**: Features include brand, material, size, compartments, waterproofing, and weight capacity.Generated using a deep learning model trained on the Student Bag Price Prediction Dataset.\n",
        "\n",
        "**Exploration**: Explore differences between this dataset and the original Flood Prediction Factors dataset.\n",
        "Investigate whether incorporating the original dataset into training improves model performance.\n",
        "Utilize visualization techniques for EDA.\n",
        "The dataset is suitable for clustering analysis.\n",
        "\n",
        "**Evaluation**: Root Mean Squared Error (RMSE)..\n",
        "\n",
        "**Submission**: train.csv â€“ Training dataset with price labels.\n",
        "                test.csv â€“ Test dataset without price labels.\n",
        "                sample_submission.csv â€“ Required submission format."
      ],
      "metadata": {
        "id": "IYCroYKK2cP1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGa3SjrXsW_F"
      },
      "source": [
        "# **S4E10 - LOAN APPROVAL**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall scikit-learn\n",
        "!pip install scikit-learn==1.4"
      ],
      "metadata": {
        "id": "8fOwAMLAw7Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rYYQgW0Rmxv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qq pytorch_tabnet\n",
        "!pip install optuna\n",
        "!pip install --upgrade catboost\n",
        "!pip install optuna-integration-pytorch-tabnet\n",
        "!pip install cuml-cu11\n",
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "!pip install --upgrade category-encodersy\n",
        "!pip install optuna-integration\n",
        "!pip install colorama\n",
        "#!pip install pyfiglet\n",
        "#!pip install keras-tuner --upgrade\n",
        "#!pip install keras-nlp\n",
        "#!pip install BorutaShap\n",
        "#!pip install scikit-learn==1.2.2\n",
        "#!pip install scikit-lego\n",
        "!pip install skops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import lightgbm, xgboost, catboost\n",
        "sklearn.__version__, lightgbm.__version__, xgboost.__version__, catboost.__version__"
      ],
      "metadata": {
        "id": "crKlzXJctRD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIS1habP8JGi"
      },
      "outputs": [],
      "source": [
        "# Setup notebook\n",
        "from pathlib import Path\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pickle import load, dump\n",
        "import json\n",
        "import joblib\n",
        "#import calplot as cal\n",
        "\n",
        "# Graphic Libraries:\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg\n",
        "# Set Style\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5});\n",
        "sns.despine(left=True, bottom=True, top=False, right=False);\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "mpl.rc('axes', labelsize=12);\n",
        "plt.rc('xtick',labelsize=10);\n",
        "plt.rc('ytick',labelsize=10);\n",
        "\n",
        "mpl.rcParams['axes.spines.top'] = False;\n",
        "mpl.rcParams['axes.spines.right'] = False;\n",
        "mpl.rcParams['axes.spines.left'] = True;\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "# tool for Excel:\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "from openpyxl.styles import Border, Side, PatternFill, Font, GradientFill, Alignment\n",
        "from openpyxl.worksheet.cell_range import CellRange\n",
        "\n",
        "from openpyxl.formatting import Rule\n",
        "from openpyxl.styles import Font, PatternFill, Border\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "\n",
        "# Bloomberg\n",
        "#from xbbg import blp\n",
        "from catboost import CatBoostRegressor, Pool, CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "# Time Management\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BMonthEnd, QuarterEnd\n",
        "import datetime\n",
        "from pandas.tseries.offsets import BDay # BDay is business day, not birthday...\n",
        "import datetime as dt\n",
        "import click\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import string\n",
        "\n",
        "from ipywidgets import AppLayout\n",
        "from ipywidgets import Dropdown, Layout, HTML, AppLayout, VBox, Label, HBox, BoundedFloatText, interact, Output\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Reshape\n",
        "from keras.models import Model\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "#import keras_tuner as kt\n",
        "#from keras_tuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, cross_val_score, cross_validate, GroupKFold, GridSearchCV, RepeatedStratifiedKFold, cross_val_predict\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   QuantileTransformer,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "# metrics\n",
        "import sklearn\n",
        "#import skops.io as sio\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             root_mean_squared_error,\n",
        "                             root_mean_squared_log_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss,\n",
        "                             make_scorer)\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  HuberRegressor,\n",
        "                                  TweedieRegressor,\n",
        "                                  QuantileRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  TheilSenRegressor,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              AdaBoostClassifier,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              GradientBoostingClassifier,\n",
        "                              StackingRegressor,\n",
        "                              StackingClassifier,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              HistGradientBoostingRegressor,\n",
        "                              ExtraTreesClassifier)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
        "\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import itertools\n",
        "import warnings\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from pylab import rcParams\n",
        "import scipy.stats as ss\n",
        "\n",
        "#from category_encoders.cat_boost import CatBoostEncoder\n",
        "#from category_encoders.wrapper import PolynomialWrapper\n",
        "#from category_encoders.count import CountEncoder\n",
        "#from category_encoders import TargetEncoder\n",
        "\n",
        "import skops.io as sio\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "#import pyfiglet\n",
        "#plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkkRPWKZCkYa"
      },
      "outputs": [],
      "source": [
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.5,\n",
        "         'grid.alpha'           :0.5,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : 'black',\n",
        "         'axes.titlesize'       : 12,\n",
        "#         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "# Set Style\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "\n",
        "# import font colors\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "init(autoreset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU7oWpLHRmxy"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Connect to Colab:#\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FUNCTIONS:"
      ],
      "metadata": {
        "id": "cVSGNoaQB8fF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Plotting Functiss**"
      ],
      "metadata": {
        "id": "3uNczPcrH4iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_scatter(df, x=\"feat1\", y=\"feat2\", color_feature=None, cmap='viridis'):\n",
        "    \"\"\"\n",
        "    Generates a scatter plot with points colored based on a third feature.\n",
        "\n",
        "    Args:\n",
        "        df: Pandas DataFrame containing the data.\n",
        "        x: Name of the column to use for the x-axis.\n",
        "        y: Name of the column to use for the y-axis.\n",
        "        color_feature: Name of the column to use for coloring the points.\n",
        "                       If None, points will be a single color.\n",
        "        cmap: Colormap to use for coloring the points (e.g., 'viridis', 'plasma', 'magma', 'inferno', 'cividis').\n",
        "              See matplotlib documentation for available colormaps.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(5, 5))\n",
        "\n",
        "    if color_feature is not None:\n",
        "        # Ensure the color feature exists\n",
        "        if color_feature not in df.columns:\n",
        "            raise ValueError(f\"Color feature '{color_feature}' not found in DataFrame.\")\n",
        "\n",
        "        # Scatter plot with colors\n",
        "        scatter = plt.scatter(df[x], df[y], c=df[color_feature], cmap=cmap)\n",
        "\n",
        "        # Add a colorbar\n",
        "        cbar = plt.colorbar(scatter)\n",
        "        cbar.set_label(color_feature)  # Label the colorbar\n",
        "\n",
        "    else:\n",
        "        # Simple scatter plot (single color)\n",
        "        plt.scatter(df[x], df[y],color=\"royalblue\",alpha=0.6)\n",
        "\n",
        "    plt.xlabel(x)\n",
        "    plt.ylabel(y)\n",
        "    plt.title(\"Scatter Plot\")  # Add a title for better visualization\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RjO42zM1B8FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Dataset Management Functions**:"
      ],
      "metadata": {
        "id": "rVeb3ga2HynB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "\n",
        "    state = 42\n",
        "    n_splits = 10\n",
        "    early_stop = 200\n",
        "\n",
        "    target = 'Price'\n",
        "    train = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/train.csv')\n",
        "    test = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/test.csv')\n",
        "    submission = pd.read_csv( \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\")\n",
        "    train_org = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/training_extra.csv\")\n",
        "\n",
        "    train.Compartments = train.Compartments.astype('str')\n",
        "    test.Compartments = test.Compartments.astype('str')\n",
        "    train_org.Compartments = train_org.Compartments.astype('str')\n",
        "\n",
        "    original_data = 'Y'\n",
        "    outliers = 'N'\n",
        "    log_trf = 'N'\n",
        "    scaler_trf = 'Y'\n",
        "    feature_eng = 'N'\n",
        "    missing = 'Y'\n",
        "    force_normalization=\"N\"\n",
        "    impose_normalization=\"N\"\n",
        "    trg_enc = \"N\"\n",
        "    problem = \"Regression\"\n",
        "    metric_goal=\"RMSE\"\n",
        "    direction_=\"minimize\"\n",
        "    log_trans_cols = []\n",
        "    force_norm_cols = [\"Weight Capacity (kg)\"]\n",
        "    impose_norm_cols = []\n",
        "    trg_enc_feat = []\n",
        "\n",
        "class Preprocessing():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train = Config.train\n",
        "        self.test = Config.test\n",
        "        self.targets = Config.target\n",
        "\n",
        "        self.prp_data()\n",
        "\n",
        "    def prp_data(self):\n",
        "\n",
        "        if Config.original_data == 'Y':\n",
        "            self.train = pd.concat([self.train, Config.train_org], ignore_index=True).drop_duplicates(ignore_index=True)\n",
        "\n",
        "        self.train = self.train.drop(['id'], axis=1)\n",
        "        self.test = self.test.drop(['id'], axis=1)\n",
        "\n",
        "        self.cat_features = self.train.drop(self.targets, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "        self.num_features = self.train.drop(self.targets, axis=1).select_dtypes(exclude=['object', 'bool']).columns.tolist()\n",
        "\n",
        "        self.train = self.reduce_mem(self.train)\n",
        "        self.test = self.reduce_mem(self.test)\n",
        "        return self\n",
        "\n",
        "    def reduce_mem(self, df):\n",
        "\n",
        "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"]\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_type = df[col].dtypes\n",
        "\n",
        "            if col_type in numerics:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "\n",
        "                if \"int\" in str(col_type):\n",
        "                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)\n",
        "                else:\n",
        "                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    if c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)\n",
        "\n",
        "        return df\n",
        "\n",
        "class EDA(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_info()\n",
        "        self.heatmap()\n",
        "        self.dist_plots()\n",
        "        self.cat_feature_plots()\n",
        "        if Config.problem == 'Classification':\n",
        "          self.target_pie()\n",
        "        else:\n",
        "          self.target_dist()\n",
        "\n",
        "    def data_info(self):\n",
        "\n",
        "        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n",
        "            table_style = [{'selector': 'th:not(.index_name)',\n",
        "                            'props': [('background-color', 'slategrey'),\n",
        "                                      ('color', '#FFFFFF'),\n",
        "                                      ('font-weight', 'bold'),\n",
        "                                      ('border', '1px solid #DCDCDC'),\n",
        "                                      ('text-align', 'center')]\n",
        "                            },\n",
        "                            {'selector': 'tbody td',\n",
        "                             'props': [('border', '1px solid #DCDCDC'),\n",
        "                                       ('font-weight', 'normal')]\n",
        "                            }]\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} head\\n')\n",
        "            display(data.head().style.set_table_styles(table_style))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} info\\n'+Style.RESET_ALL)\n",
        "            display(data.info())\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} describe\\n')\n",
        "            display(data.describe().drop(index='count', columns=self.targets, errors = 'ignore').T\n",
        "                    .style.set_table_styles(table_style).format('{:.3f}'))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} missing values\\n'+Style.RESET_ALL)\n",
        "            display(data.isnull().sum())\n",
        "        return self\n",
        "\n",
        "    def heatmap(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nCorrelation Heatmap\\n')\n",
        "        plt.figure(figsize=(7,7))\n",
        "        corr = self.train.select_dtypes(exclude='object').corr(method='pearson')\n",
        "        sns.heatmap(corr, fmt = '0.2f', cmap = 'Blues', annot=True, cbar=False)\n",
        "        plt.show()\n",
        "\n",
        "    def dist_plots(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Numerical\\n\")\n",
        "        df = pd.concat([self.train[self.num_features].assign(Source = 'Train'),\n",
        "                        self.test[self.num_features].assign(Source = 'Test'),],\n",
        "                        axis=0, ignore_index = True)\n",
        "\n",
        "        fig, axes = plt.subplots(len(self.num_features), 2 ,figsize = (13, len(self.num_features) * 4),\n",
        "                                 gridspec_kw = {'hspace': 0.3,\n",
        "                                                'wspace': 0.2,\n",
        "                                                'width_ratios': [0.70, 0.30]\n",
        "                                               }\n",
        "                                )\n",
        "        for i,col in enumerate(self.num_features):\n",
        "            try:\n",
        "                ax = axes[i,0]\n",
        "            except:\n",
        "                ax = axes[i]\n",
        "            sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source',\n",
        "                        palette = ['royalblue', 'tomato'], ax = ax, alpha=0.7, linewidth = 2\n",
        "                       )\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.grid('--',alpha=0.7)\n",
        "\n",
        "            try:\n",
        "                ax = axes[i,1]\n",
        "            except:\n",
        "                ax = axes[1]\n",
        "            sns.boxplot(data = df, y = col, x=df.Source, width = 0.5,\n",
        "                        linewidth = 1, fliersize= 1,\n",
        "                        ax = ax, palette=['royalblue', 'tomato']\n",
        "                       )\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.tick_params(axis='both', which='major')\n",
        "            ax.set_xticklabels(['Train', 'Test'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def cat_feature_plots(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Categorical\\n\")\n",
        "        fig, axes = plt.subplots(len(self.cat_features), 2 ,figsize = (18, len(self.cat_features) * 6),\n",
        "                                 gridspec_kw = {'hspace': 0.5,\n",
        "                                                'wspace': 0.2,\n",
        "                                               }\n",
        "                                )\n",
        "\n",
        "        for i, col in enumerate(self.cat_features):\n",
        "\n",
        "            ax = axes[i,0]\n",
        "            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='royalblue', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Train\")\n",
        "\n",
        "            ax = axes[i,1]\n",
        "            sns.barplot(data=self.test[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='tomato', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Test\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def target_pie(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nTarget feature distribution\\n\")\n",
        "        targets = self.train[self.targets]\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        plt.pie(targets.value_counts(), labels=targets.value_counts().index, autopct='%1.2f%%', colors=palette_9)\n",
        "        plt.show()\n",
        "\n",
        "    def target_dist(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nTarget feature distribution\\n\")\n",
        "        fig, axes = plt.subplots(1, 1, figsize=(7, 5))\n",
        "        sns.histplot(self.train[self.targets], kde=True, ax=axes)\n",
        "        axes.set_title('Distribution of Price')\n",
        "        axes.set_xlabel(self.targets)\n",
        "        axes.set_ylabel('Frequency')"
      ],
      "metadata": {
        "id": "0595jA_qHZuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PuCulFRmx1"
      },
      "source": [
        "## **1.0 Dataset Version Uploads**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3odgloSjRmx4"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/train.csv',index_col=0)\n",
        "\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/test.csv',index_col=0)\n",
        "\n",
        "df_train_orig = pd.read_csv(\n",
        "    '/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/training_extra.csv'\n",
        ")\n",
        "\n",
        "df_subm = pd.read_csv(\n",
        "    \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\",index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tcWfu7npSHN"
      },
      "outputs": [],
      "source": [
        "df_train_orig = df_train_orig.dropna(subset=\"Price\")\n",
        "df_train_orig.isnull().sum(),df_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vwd0o1ph1Ai"
      },
      "outputs": [],
      "source": [
        "df_train.head()\n",
        "df_train.shape,df_test.shape,df_train_orig.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()\n",
        "df_train.Size.unique()\n",
        "\n",
        "df_train.Size = df_train.Size.replace({'Medium':2, 'Small':1, 'Large':3, np.nan:0},regex=True)"
      ],
      "metadata": {
        "id": "m7n19A3cEhfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_scatter(df=df_train,x=\"Weight Capacity (kg)\",y=\"Size\",color_feature=\"Price\")"
      ],
      "metadata": {
        "id": "9SRqXeIS0HlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.groupby(\"Size\")[[\"Weight Capacity (kg)\",\"Price\"]].agg([\"mean\",\"std\"])"
      ],
      "metadata": {
        "id": "PNezZH0_EwML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eda = EDA()"
      ],
      "metadata": {
        "id": "sEV64xg2IJAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jQmFilAvKM4"
      },
      "source": [
        "## 2.0 Data Transformation and Feature Engeneering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxSqeWx-MZHk"
      },
      "outputs": [],
      "source": [
        "class Transform(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        if self.missing == 'Y':\n",
        "            self.missing_values()\n",
        "\n",
        "        self.train_raw = self.train.copy()\n",
        "\n",
        "        if self.feature_eng == 'Y':\n",
        "            self.train = self.new_features(self.train)\n",
        "            self.test = self.new_features(self.test)\n",
        "            self.train_raw = self.new_features(self.train_raw)\n",
        "\n",
        "        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object', 'bool']).columns.tolist()\n",
        "        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "        if self.outliers == 'Y':\n",
        "            self.remove_outliers()\n",
        "\n",
        "        if self.log_trf == 'Y':\n",
        "            self.log_transformation()\n",
        "\n",
        "        if self.force_normalization == 'Y':\n",
        "            self.forced_norm_transformation()\n",
        "\n",
        "        if self.impose_normalization == 'Y':\n",
        "            self.impose_normalization_transformation()\n",
        "\n",
        "        if self.trg_enc == 'Y':\n",
        "            self.target_encoding()\n",
        "\n",
        "        if self.scaler_trf == 'Y':\n",
        "            self.scaler()\n",
        "\n",
        "        if self.outliers == 'Y' or self.log_trf == 'Y' or self.scaler_trf =='Y':\n",
        "            self.distribution()\n",
        "\n",
        "    def __call__(self):\n",
        "\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
        "        data = pd.concat([self.test, self.train])\n",
        "        self.train_enc, self.test_enc = self.encode(data)\n",
        "\n",
        "        self.cat_features_card = []\n",
        "        for f in self.cat_features:\n",
        "            self.cat_features_card.append(1 + data[f].max())\n",
        "\n",
        "        self.y = self.train[self.target]\n",
        "        self.train = self.train.drop(self.target, axis=1)\n",
        "        self.train_enc = self.train_enc.drop(self.target, axis=1)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        self.train_enc[self.num_features] = scaler.fit_transform(self.train_enc[self.num_features])\n",
        "        self.test_enc[self.num_features] = scaler.transform(self.test_enc[self.num_features])\n",
        "\n",
        "        return self.train, self.train_enc, self.y, self.test, self.test_enc, self.cat_features\n",
        "\n",
        "    def encode(self, data):\n",
        "\n",
        "        oe = OrdinalEncoder()\n",
        "        data[self.cat_features] = oe.fit_transform(data[self.cat_features]).astype('int')\n",
        "\n",
        "        train_enc = data[~data[self.target].isnull()]\n",
        "        test_enc = data[data[self.target].isnull()].drop(self.target, axis=1)\n",
        "        return train_enc, test_enc\n",
        "\n",
        "    def new_features(self, df):\n",
        "        #Replace Some entries that appears wrong in the dataset:\n",
        "\n",
        "        return df\n",
        "\n",
        "    def log_transformation(self):\n",
        "\n",
        "        self.train[self.log_trans_cols] = np.log1p(self.train[self.log_trans_cols])\n",
        "        self.test[self.log_trans_cols] = np.log1p(self.test[self.log_trans_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def forced_norm_transformation(self):\n",
        "\n",
        "        self.train[self.force_norm_cols] = np.sqrt(self.train[self.force_norm_cols]+0.1)\n",
        "        self.test[self.force_norm_cols] = np.sqrt(self.test[self.force_norm_cols]+0.1)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impose_normalization_transformation(self):\n",
        "\n",
        "        scaler = QuantileTransformer(output_distribution='normal',subsample=20_000,random_state=42)\n",
        "        self.train[self.impose_norm_cols] = scaler.fit_transform(self.train[self.impose_norm_cols])\n",
        "        self.test[self.impose_norm_cols] = scaler.transform(self.test[self.impose_norm_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def distribution(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nHistograms of distribution\\n')\n",
        "        fig, axes = plt.subplots(nrows=len(self.num_features), ncols=2, figsize=(15, len(self.num_features)*5))\n",
        "        try:\n",
        "          for (ax_r, ax_n), col in zip(axes, self.num_features):\n",
        "\n",
        "            ax_r.set_title(f'{col} ($\\mu=$ {self.train_raw[col].mean():.2f} and $\\sigma=$ {self.train_raw[col].std():.2f} )')\n",
        "            ax_r.hist(self.train_raw[col], bins=30, color='tomato',alpha=0.7)\n",
        "            ax_r.axvline(self.train_raw[col].mean(), color='r', label='Mean')\n",
        "            ax_r.axvline(self.train_raw[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_r.legend()\n",
        "\n",
        "            ax_n.set_title(f'{col} Normalized ($\\mu=$ {self.train[col].mean():.2f} and $\\sigma=$ {self.train[col].std():.2f} )')\n",
        "            ax_n.hist(self.train[col], bins=30, color='royalblue',alpha=0.7)\n",
        "            ax_n.axvline(self.train[col].mean(), color='r', label='Mean')\n",
        "            ax_n.axvline(self.train[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_n.legend()\n",
        "\n",
        "        except:\n",
        "\n",
        "          for (ax_r, ax_n), col in zip([axes], self.num_features):\n",
        "\n",
        "            ax_r.set_title(f'{col} ($\\mu=$ {self.train_raw[col].mean():.2f} and $\\sigma=$ {self.train_raw[col].std():.2f} )')\n",
        "            ax_r.hist(self.train_raw[col], bins=30, color='tomato',alpha=0.7)\n",
        "            ax_r.axvline(self.train_raw[col].mean(), color='r', label='Mean')\n",
        "            ax_r.axvline(self.train_raw[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_r.legend()\n",
        "\n",
        "            ax_n.set_title(f'{col} Normalized ($\\mu=$ {self.train[col].mean():.2f} and $\\sigma=$ {self.train[col].std():.2f} )')\n",
        "            ax_n.hist(self.train[col], bins=30, color='royalblue',alpha=0.7)\n",
        "            ax_n.axvline(self.train[col].mean(), color='r', label='Mean')\n",
        "            ax_n.axvline(self.train[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_n.legend()\n",
        "\n",
        "    def remove_outliers(self):\n",
        "        Q1 = self.train[self.targets].quantile(0.25)\n",
        "        Q3 = self.train[self.targets].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_limit = Q1 - 1.5*IQR\n",
        "        upper_limit = Q3 + 1.5*IQR\n",
        "        self.train = self.train[(self.train[self.targets] >= lower_limit) & (self.train[self.targets] <= upper_limit)]\n",
        "        self.train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def scaler(self):\n",
        "        scaler = StandardScaler()\n",
        "        self.train[self.num_features] = scaler.fit_transform(self.train[self.num_features])\n",
        "        self.test[self.num_features] = scaler.transform(self.test[self.num_features])\n",
        "        return self\n",
        "\n",
        "    def missing_values(self):\n",
        "\n",
        "        self.train = self.train.dropna(subset=self.target)\n",
        "\n",
        "        print(self.train.info())\n",
        "\n",
        "        for col in self.num_features:\n",
        "            self.train[f\"{col}_missing\"] = self.train[col].isna().astype(\"object\")\n",
        "            self.test[f\"{col}_missing\"] = self.test[col].isna().astype(\"object\")\n",
        "\n",
        "        self._cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "        self.train[self.num_features] = self.train[self.num_features].fillna(self.train[self.num_features].median())\n",
        "        self.test[self.num_features] = self.test[self.num_features].fillna(self.test[self.num_features].median())\n",
        "\n",
        "        for column in self.cat_features:\n",
        "            self.train[column] = self.train[column].fillna(\"Missing\") #fillna(self.train[column].mode()[0]) #\n",
        "            self.test[column] = self.test[column].fillna(\"Missing\") #fillna(self.test[column].mode()[0]) #\n",
        "\n",
        "        return self\n",
        "\n",
        "    def target_encoding(self):\n",
        "        te = TargetEncoder()\n",
        "        self.train[self.trg_enc_feat] = te.fit_transform(self.train[self.trg_enc_feat],self.train[self.target])\n",
        "        self.test[self.trg_enc_feat] = te.transform(self.test[self.trg_enc_feat])\n",
        "\n",
        "        for a in self.cat_features:\n",
        "            self.cat_features.remove(a)\n",
        "\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def cat_features(self):\n",
        "        return self._cat_features\n",
        "\n",
        "    @cat_features.setter\n",
        "    def cat_features(self, cat_features):\n",
        "        self._cat_features = cat_features\n",
        "\n",
        "    @property\n",
        "    def num_features(self):\n",
        "        return self._num_features\n",
        "\n",
        "    @num_features.setter\n",
        "    def num_features(self, num_features):\n",
        "        self._num_features = num_features\n",
        "\n",
        "    @property\n",
        "    def cat_features_card(self):\n",
        "        return self._cat_features_card\n",
        "\n",
        "    @cat_features_card.setter\n",
        "    def cat_features_card(self, cat_features_card):\n",
        "        self._cat_features_card = cat_features_card\n",
        "\n",
        "    @property\n",
        "    def train(self):\n",
        "        return self._train\n",
        "\n",
        "    @train.setter\n",
        "    def train(self, train):\n",
        "        self._train = train\n",
        "\n",
        "    @property\n",
        "    def direction(self):\n",
        "        return self._direction\n",
        "\n",
        "    @direction.setter\n",
        "    def direction(self, direction):\n",
        "        self._direction= direction\n",
        "\n",
        "\n",
        "class MixedDataImputer:\n",
        "    \"\"\"\n",
        "    Imputes missing values in mixed-data train and test DataFrames using\n",
        "    separate IterativeImputers for numerical and categorical features.\n",
        "\n",
        "    Args:\n",
        "      train_df: Pandas DataFrame with training data.\n",
        "      test_df: Pandas DataFrame with test data.\n",
        "      target_feature: Name of the target feature column.\n",
        "      random_state: Random state for reproducibility (default=42).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_df, test_df, target_feature=None, random_state=42):\n",
        "        super().__init__()\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.target_feature = target_feature\n",
        "        self.random_state = random_state\n",
        "        self.num_features = None\n",
        "        self.cat_features = None\n",
        "\n",
        "    def _identify_features(self):\n",
        "        \"\"\"Identifies numerical and categorical features.\"\"\"\n",
        "        self.num_features = self.train_df.select_dtypes(include=['number']).columns.tolist()\n",
        "        self.cat_features = self.train_df.select_dtypes(exclude=['number']).columns.tolist()\n",
        "        #self.num_features.remove(self.target_feature)  # Remove target from numerical features\n",
        "\n",
        "    def _impute_data(self, df):\n",
        "        \"\"\"Imputes missing values in a DataFrame.\"\"\"\n",
        "        df_num = df[self.num_features].copy()\n",
        "        df_cat = df[self.cat_features].copy()\n",
        "\n",
        "        # Impute numerical features only if there are missing values\n",
        "        if df_num.isnull().values.any():\n",
        "            num_imputer = IterativeImputer(estimator=BayesianRidge(),\n",
        "                                          random_state=self.random_state)\n",
        "            df_num_imputed = pd.DataFrame(num_imputer.fit_transform(df_num),\n",
        "                                         columns=self.num_features)\n",
        "        else:\n",
        "            df_num_imputed = df_num  # No imputation needed\n",
        "\n",
        "        # Impute categorical features only if there are missing values\n",
        "        if df_cat.isnull().values.any():\n",
        "            cat_imputer = IterativeImputer(estimator=LogisticRegression(),\n",
        "                                          initial_strategy='most_frequent',\n",
        "                                          random_state=self.random_state)\n",
        "            df_cat_imputed = pd.DataFrame(cat_imputer.fit_transform(df_cat),\n",
        "                                         columns=self.cat_features)\n",
        "\n",
        "            # Convert categorical features back to their original datatype\n",
        "            for feature in self.cat_features:\n",
        "                df_cat_imputed[feature] = df_cat_imputed[feature].astype(df[feature].dtype)\n",
        "        else:\n",
        "            df_cat_imputed = df_cat  # No imputation needed\n",
        "\n",
        "        # Concatenate the imputed DataFrames\n",
        "        df_imputed = pd.concat([df_num_imputed, df_cat_imputed], axis=1)\n",
        "\n",
        "        return df_imputed\n",
        "\n",
        "    def transform(self):\n",
        "        \"\"\"\n",
        "        Imputes missing values in both train and test DataFrames.\n",
        "\n",
        "        Returns:\n",
        "          train_df_imputed: Pandas DataFrame with imputed training data.\n",
        "          test_df_imputed: Pandas DataFrame with imputed test data.\n",
        "        \"\"\"\n",
        "        self._identify_features()\n",
        "        train_df_imputed = self._impute_data(self.train_df)\n",
        "        test_df_imputed = self._impute_data(self.test_df)\n",
        "        return train_df_imputed, test_df_imputed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = Transform()\n",
        "X, X_enc, y, test, test_enc, cat_features = t()"
      ],
      "metadata": {
        "id": "XHffgVf1IJR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.cat_features, t.cat_features_card, t.train.shape, t.direction_)\n",
        "X_enc.info()\n",
        "X_enc[\"Compartments\"].unique()"
      ],
      "metadata": {
        "id": "3HMIpp4sfN--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc[\"Weight Capacity (kg)_missing\"].value_counts()"
      ],
      "metadata": {
        "id": "89rD5acslRDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 Additional Feature Engeneering:"
      ],
      "metadata": {
        "id": "UXpYqgsBvSF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "X[\"Price\"] = y\n",
        "# Assuming your features are in a list called 'features'\n",
        "features = t.cat_features\n",
        "# Generate all combinations of 3 features\n",
        "combinations = list(itertools.combinations(features[:-1], 3))\n",
        "\n",
        "df_results = pd.DataFrame(index=combinations, columns=[\"median_min\",\"median_max\"])\n",
        "\n",
        "# Print the combinations\n",
        "for cnt, combination in tqdm(enumerate(combinations)):\n",
        "\n",
        "    grouped = X.groupby(list(combination),as_index=True)[\"Price\"].agg([\"mean\",\"median\",\"std\",\"skew\",\"count\", \"min\", \"max\"])\n",
        "    grouped = grouped.reset_index().sort_values(by=\"median\",ascending=False)\n",
        "\n",
        "    df_results.iloc[cnt,:][\"median_max\"] = grouped.iloc[0][\"median\"]\n",
        "    df_results.iloc[cnt,:][\"median_min\"] = grouped.iloc[-1][\"median\"]\n",
        "\n",
        "low_med_combos = df_results.sort_values(by=\"median_min\",ascending=False).iloc[-10:,:]#.index.tolist()\n",
        "high_med_combos = df_results.sort_values(by=\"median_max\",ascending=False).iloc[:10,:]#.index.tolist()\n",
        "\n",
        "tot_feat = set(high_med_combos.index.tolist()+low_med_combos.index.tolist())\n",
        "display(low_med_combos)\n",
        "display(high_med_combos)"
      ],
      "metadata": {
        "id": "RJH0CQbneAgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc.info()"
      ],
      "metadata": {
        "id": "mhRm4_pUln2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_enc.shape, test_enc.shape)\n",
        "X_enc.head(5)"
      ],
      "metadata": {
        "id": "u31n_LhRf_4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VERIFY SOME COMBINATION**"
      ],
      "metadata": {
        "id": "aaaZZNAHv8Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combo_1 = [\"Size\", \"Laptop Compartment\", \"Waterproof\"]\n",
        "combo_2 = [\"Brand\", \"Material\", \"Size\"]\n",
        "combo_3 = [\"Compartments\", \"Laptop Compartment\", \"Waterproof\"]\n",
        "combo_4 = [\"Brand\", \"Size\", \"Compartments\"]\n"
      ],
      "metadata": {
        "id": "lUJF8q6gNPZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_enc.copy()\n",
        "X[\"Price\"] = y\n",
        "grouped = X.groupby(combo_4,as_index=True)[\"Price\"].agg([\"mean\",\"median\",\"std\",\"skew\",\"count\", \"min\", \"max\"])\n",
        "\n",
        "grouped_test = test.groupby([\"Compartments\", \"Style\", \"Weight Capacity (kg)_missing\"],as_index=True)[\"Weight Capacity (kg)\"].agg([\"count\"])"
      ],
      "metadata": {
        "id": "P5eeeYkyulDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(21, 6))\n",
        "grouped[[\"median\",\"std\"]].plot(kind=\"bar\", ax=ax);"
      ],
      "metadata": {
        "id": "uqTQX3v8umIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped.reset_index().sort_values(by=\"mean\",ascending=False).head(10)"
      ],
      "metadata": {
        "id": "Bct8qOKj0zKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_scatter(grouped, x=\"std\", y=\"skew\", color_feature=\"mean\", cmap='viridis')"
      ],
      "metadata": {
        "id": "KiZ75Ykj1NfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class New_Features():\n",
        "\n",
        "    def __init__(self, xtrain, xtest, y, combinations, target=\"Price\"):\n",
        "\n",
        "        self.train = xtrain.copy()\n",
        "        self.test = xtest.copy()\n",
        "        self.y = y.copy()\n",
        "        self.target = target\n",
        "\n",
        "        self.x_working = self.train.copy()\n",
        "        self.x_test_working = self.test.copy()\n",
        "\n",
        "        self.x_working[self.target] = self.y\n",
        "\n",
        "        self.combinations = combinations\n",
        "\n",
        "        self.num_features = self.train.select_dtypes(include=['float']).columns.tolist()\n",
        "        self.cat_features = self.train.select_dtypes(exclude=['float']).columns.tolist()\n",
        "\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
        "        print(self.train.info())\n",
        "        #print(self.num_features, self.cat_features)\n",
        "\n",
        "    def execute(self):\n",
        "\n",
        "        # Define the conditions and corresponding values for the new column 'B'\n",
        "        conditions = [\n",
        "                      (self.x_working[self.target] <= 50),\n",
        "                      (self.x_working[self.target] > 50) & (self.x_working[self.target] < 110),\n",
        "                      (self.x_working[self.target] >= 110)\n",
        "                      ]\n",
        "\n",
        "        # Create the new column 'B' using the conditions and values\n",
        "        self.x_working['Target'] = np.select(conditions, [1,0,2])\n",
        "\n",
        "        self.train, self.test = self.create_new_features()\n",
        "\n",
        "        return self.train, self.test,\n",
        "\n",
        "    def create_new_features(self):\n",
        "\n",
        "        for combo in tqdm(self.combinations):\n",
        "          feat = self.x_working[list(combo)]\n",
        "          feat_test = self.x_test_working[list(combo)]\n",
        "          target = self.x_working['Target']\n",
        "\n",
        "          model = DecisionTreeClassifier(max_leaf_nodes=25)\n",
        "          model.fit(feat, target)\n",
        "\n",
        "          pred = model.predict(feat)\n",
        "\n",
        "          if pred.min() == pred.max():\n",
        "              pass\n",
        "          else:\n",
        "              self.train[f\"{combo[0][:3]}_{combo[1][:3]}_{combo[2][:3]}\"] = model.predict(feat).astype(\"object\")\n",
        "              self.test[f\"{combo[0][:3]}_{combo[1][:3]}_{combo[2][:3]}\"] = model.predict(feat_test).astype(\"object\")\n",
        "\n",
        "          print(f\"{combo[0][:3]}_{combo[1][:3]}_{combo[2][:3]}:\", accuracy_score(target, model.predict(feat)))\n",
        "\n",
        "        return self.train, self.test\n"
      ],
      "metadata": {
        "id": "FMuZ1hn9KPgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nf = New_Features(xtrain=X_enc, xtest=test_enc, y=y, combinations=tot_feat)\n",
        "X_enc_ext, test_enc_ext = nf.execute()"
      ],
      "metadata": {
        "id": "Oir4RpXtXji-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_enc.head()\n"
      ],
      "metadata": {
        "id": "fRLRjAqRY0Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_enc[\"Price\"] = y\n",
        "\n",
        "# X_enc_class = X_enc.copy()\n",
        "# # Assuming your features are in a list called 'features'\n",
        "# features = t.cat_features\n",
        "\n",
        "# # Generate all combinations of 3 features\n",
        "# combinations = list(itertools.combinations(features, 3))\n",
        "\n",
        "# X_enc_class[\"Target\"]=0\n",
        "\n",
        "# # Define the conditions and corresponding values for the new column 'B'\n",
        "# conditions = [\n",
        "#     (X_enc['Price'] <= 50),\n",
        "#     (X_enc['Price'] > 50) & (X_enc['Price'] < 110),\n",
        "#     (X_enc['Price'] >= 110)\n",
        "# ]\n",
        "\n",
        "# # Create the new column 'B' using the conditions and values\n",
        "# X_enc_class['Target'] = np.select(conditions, [1,0,2])\n",
        "\n",
        "\n",
        "# for combo in tqdm(tot_feat):\n",
        "#    feat = X_enc_class[list(combo)]\n",
        "#    feat_test = test_enc[list(combo)]\n",
        "#    target = X_enc_class['Target']\n",
        "\n",
        "#    model = DecisionTreeClassifier(max_leaf_nodes=25)\n",
        "#    model.fit(feat, target)\n",
        "\n",
        "#    pred = model.predict(feat)\n",
        "\n",
        "#    if pred.min() == pred.max():\n",
        "#       pass\n",
        "#    else:\n",
        "#       X_enc[f\"{combo[0][:3]}_{combo[1][:3]}_{combo[2][:3]}\"] = model.predict(feat).astype(\"object\")\n",
        "#       test_enc[f\"{combo[0][:3]}_{combo[1][:3]}_{combo[2][:3]}\"] = model.predict(feat_test).astype(\"object\")\n",
        "\n",
        "#    X_enc.drop(columns = \"Price\")\n",
        "\n",
        "\n",
        "#    print(f\"{combo[0][:3]}_{combo[1][:3]}_{combo[2][:3]}:\", accuracy_score(target, model.predict(feat)))\n",
        "\n",
        "# X_enc.drop(\"Price\",axis=1)"
      ],
      "metadata": {
        "id": "LMefol2WUhfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc_ext.shape,X_enc.shape"
      ],
      "metadata": {
        "id": "pKCJqAgw2J-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc_ext."
      ],
      "metadata": {
        "id": "Wm1U4Twd2tsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.preprocessing import TargetEncoder"
      ],
      "metadata": {
        "id": "hVmDqg7-ikgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ow76iH6FOIU4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}