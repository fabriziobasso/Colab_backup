{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Colab_backup/blob/main/rain_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **S4E10 - Rain Forecast**"
      ],
      "metadata": {
        "id": "vLrAMDbLCEyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table below shows the column names, their value formats, and their description.\n",
        "\n",
        "Index| Features      |Format             |Description\n",
        "-----|---------------|-------------------|-----------------------\n",
        "1    |Date Time      |01.01.2009 00:10:00|Date-time reference\n",
        "2    |p (mbar)       |996.52             |The pascal SI derived unit of pressure used to quantify internal pressure. Meteorological reports typically state atmospheric pressure in millibars.\n",
        "3    |T (degC)       |-8.02              |Temperature in Celsius\n",
        "4    |Tpot (K)       |265.4              |Temperature in Kelvin\n",
        "5    |Tdew (degC)    |-8.9               |Temperature in Celsius relative to humidity. Dew Point is a measure of the absolute amount of water in the air, the DP is the temperature at which the air cannot hold all the moisture in it and water condenses.\n",
        "6    |rh (%)         |93.3               |Relative Humidity is a measure of how saturated the air is with water vapor, the %RH determines the amount of water contained within collection objects.\n",
        "7    |VPmax (mbar)   |3.33               |Saturation vapor pressure\n",
        "8    |VPact (mbar)   |3.11               |Vapor pressure\n",
        "9    |VPdef (mbar)   |0.22               |Vapor pressure deficit\n",
        "10   |sh (g/kg)      |1.94               |Specific humidity\n",
        "11   |H2OC (mmol/mol)|3.12               |Water vapor concentration\n",
        "12   |rho (g/m ** 3) |1307.75            |Airtight\n",
        "13   |wv (m/s)       |1.03               |Wind speed\n",
        "14   |max. wv (m/s)  |1.75               |Maximum wind speed\n",
        "15   |wd (deg)       |152.3              |Wind direction in degrees"
      ],
      "metadata": {
        "id": "nwZZcMHfTp-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro: Problem Statement"
      ],
      "metadata": {
        "id": "I6mcoRJuAScu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"container\">\n",
        "        <p class=\"mb-2\">\n",
        "            Analyze weather data over a 5-day period to identify patterns and correlations between various meteorological factors such as pressure, temperature, humidity, and rainfall. This can help in understanding how these variables influence daily weather conditions.\n",
        "        </p>\n",
        "        <h1 class=\"mb-2\">Dataset Description</h1>\n",
        "        <p class=\"mb-2\">The dataset captures weather data over five days with the following features:</p>\n",
        "        <ul class=\"mb-2\">\n",
        "            <li class=\"mb-2\"><strong>day:</strong> Sequential day number.</li>\n",
        "            <li class=\"mb-2\"><strong>pressure (hPa):</strong> Atmospheric pressure measured in hectopascals.</li>\n",
        "            <li class=\"mb-2\"><strong>maxtemp (°C):</strong> Maximum temperature recorded on the day.</li>\n",
        "            <li class=\"mb-2\"><strong>temperature (°C):</strong> Average temperature of the day.</li>\n",
        "            <li class=\"mb-2\"><strong>mintemp (°C):</strong> Minimum temperature recorded on the day.</li>\n",
        "            <li class=\"mb-2\"><strong>dewpoint (°C):</strong> Temperature at which air becomes saturated with moisture.</li>\n",
        "            <li class=\"mb-2\"><strong>humidity (%):</strong> Relative humidity percentage.</li>\n",
        "            <li class=\"mb-2\"><strong>cloud (%):</strong> Cloud cover percentage.</li>\n",
        "            <li class=\"mb-2\"><strong>rainfall:</strong> Indicates if rainfall occurred (\"yes\"/\"no\").</li>\n",
        "            <li class=\"mb-2\"><strong>sunshine (hours):</strong> Total sunshine hours.</li>\n",
        "            <li class=\"mb-2\"><strong>winddirection (°):</strong> Direction of the wind in degrees.</li>\n",
        "            <li class=\"mb-2\"><strong>windspeed (km/h):</strong> Speed of the wind.</li>\n",
        "        </ul>\n",
        "        <p class=\"mb-2\">\n",
        "            This dataset is ideal for basic weather pattern analysis and visualization.\n",
        "        </p>\n",
        "    </div>"
      ],
      "metadata": {
        "id": "Cd0CILTwANfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Libraries"
      ],
      "metadata": {
        "id": "BHtdl8QSAalm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rYYQgW0Rmxv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# #!pip install -qq pytorch_tabnet\n",
        "!pip install optuna\n",
        "# !pip install catboost\n",
        "# #!pip install --upgrade numpy\n",
        "# #!pip install optuna-integration-pytorch-tabnet\n",
        "\n",
        "# #from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "!pip install --upgrade category-encoders\n",
        "!pip install optuna-integration\n",
        "!pip install colorama\n",
        "# #!pip install pyfiglet\n",
        "!pip install keras-tuner --upgrade\n",
        "# !pip install keras-nlp\n",
        "# !pip install BorutaShap\n",
        "# !pip install --upgrade scikit-learn\n",
        "# !pip install scikit-lego\n",
        "!pip install skops\n",
        "\n",
        "# #from pytorch_tabnet.tab_model import TabNetRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIS1habP8JGi"
      },
      "outputs": [],
      "source": [
        "# Setup notebook\n",
        "from pathlib import Path\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pickle import load, dump\n",
        "import json\n",
        "import joblib\n",
        "import skops.io as sio\n",
        "#import calplot as cal\n",
        "\n",
        "# Graphic Libraries:\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg\n",
        "# Set Style\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5});\n",
        "sns.despine(left=True, bottom=True, top=False, right=False);\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "mpl.rc('axes', labelsize=12);\n",
        "plt.rc('xtick',labelsize=10);\n",
        "plt.rc('ytick',labelsize=10);\n",
        "\n",
        "mpl.rcParams['axes.spines.top'] = False;\n",
        "mpl.rcParams['axes.spines.right'] = False;\n",
        "mpl.rcParams['axes.spines.left'] = True;\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "# tool for Excel:\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "from openpyxl.styles import Border, Side, PatternFill, Font, GradientFill, Alignment\n",
        "from openpyxl.worksheet.cell_range import CellRange\n",
        "\n",
        "from openpyxl.formatting import Rule\n",
        "from openpyxl.styles import Font, PatternFill, Border\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "\n",
        "#from catboost import CatBoostRegressor, Pool, CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "# Time Management\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BMonthEnd, QuarterEnd\n",
        "from pandas.tseries.offsets import BDay # BDay is business day, not birthday...\n",
        "import datetime as dt\n",
        "import click\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import string\n",
        "\n",
        "from ipywidgets import AppLayout\n",
        "from ipywidgets import Dropdown, Layout, HTML, AppLayout, VBox, Label, HBox, BoundedFloatText, interact, Output\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Reshape\n",
        "from keras.models import Model\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, cross_val_score, cross_validate, GroupKFold, GridSearchCV, RepeatedStratifiedKFold, cross_val_predict\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   QuantileTransformer,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "# metrics\n",
        "import sklearn\n",
        "#import skops.io as sio\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             root_mean_squared_error,\n",
        "                             root_mean_squared_log_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss,\n",
        "                             make_scorer)\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  HuberRegressor,\n",
        "                                  TweedieRegressor,\n",
        "                                  QuantileRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  TheilSenRegressor,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              AdaBoostClassifier,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              GradientBoostingClassifier,\n",
        "                              StackingRegressor,\n",
        "                              StackingClassifier,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              HistGradientBoostingRegressor,\n",
        "                              ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
        "\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import itertools\n",
        "import warnings\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "#from catboost import CatBoostRegressor\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from pylab import rcParams\n",
        "import scipy.stats as ss\n",
        "\n",
        "from category_encoders.cat_boost import CatBoostEncoder\n",
        "from category_encoders.wrapper import PolynomialWrapper\n",
        "from category_encoders.count import CountEncoder\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "#import pyfiglet\n",
        "#plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkkRPWKZCkYa"
      },
      "outputs": [],
      "source": [
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.5,\n",
        "         'grid.alpha'           :0.5,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : 'black',\n",
        "         'axes.titlesize'       : 12,\n",
        "#         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "# Set Style\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "\n",
        "# import font colors\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "init(autoreset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU7oWpLHRmxy"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "#from catboost import CatBoostRegressor\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Connect to Colab:#\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PuCulFRmx1"
      },
      "source": [
        "## 2.0 Loading and Preprocessing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3odgloSjRmx4"
      },
      "outputs": [],
      "source": [
        "df_subm = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/sample_submission.csv\", index_col=0)\n",
        "\n",
        "validation = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/X_enc_y_old_all_ext.csv\")\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/X_enc_y_new_all_ext.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/test_enc_all_ext.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_train_old.isna().sum()"
      ],
      "metadata": {
        "id": "p1h0EHi1XHE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape, validation.shape"
      ],
      "metadata": {
        "id": "SKAFYJ1SUmf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_new = pd.concat([train, test], axis=0)\n",
        "print(df_all_new.shape)\n",
        "df_all_new.head(3)"
      ],
      "metadata": {
        "id": "9Al1v9xlUXjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all_new.tail(3)"
      ],
      "metadata": {
        "id": "njLThofgUg9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 1\n",
        "\n",
        "past = 7\n",
        "future = 0\n",
        "learning_rate = 0.01\n",
        "batch_size = 64\n",
        "epochs = 21\n",
        "\n",
        "test_final = df_all_new.iloc[2190-past+1:,:]"
      ],
      "metadata": {
        "id": "Tou3cFHJdcj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train data\n",
        "x_train = train.iloc[:,:-1].values\n",
        "y_train = train.iloc[past-1:,-1].values.reshape(-1,1)\n",
        "print(x_train.shape, y_train.shape)\n",
        "# Valid Data\n",
        "x_valid = validation.iloc[:,:-1].values\n",
        "y_valid = validation.iloc[past-1:,-1].values.reshape(-1,1)\n",
        "print(x_valid.shape, y_valid.shape)\n",
        "# Test Data\n",
        "x_test = test_final.iloc[:,:-1].values\n",
        "y_test = test_final.iloc[past-1:,-1].values.reshape(-1,1)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "LKSCuiz7esqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 Dataset Management Functions"
      ],
      "metadata": {
        "id": "PNp0Vj3__8JN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 0"
      ],
      "metadata": {
        "id": "eJ4mx0jL4G-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **3.1 Train Dataset**"
      ],
      "metadata": {
        "id": "MHKxwJmkx3Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_train,\n",
        "                                                                    y_train,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=10000,\n",
        "                                                                    shuffle=True\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_train.take(1):\n",
        "    inputs, targets = batch\n",
        "\n",
        "print(\"Input shape:\", inputs.numpy().shape)\n",
        "print(\"Target shape:\", targets.numpy().shape)"
      ],
      "metadata": {
        "id": "e4G2f6EJSmZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find indices of 0s\n",
        "indices_of_zeros = np.where(targets.numpy() == 0)[0]\n",
        "\n",
        "# Find indices of 1s\n",
        "indices_of_ones = np.where(targets.numpy() == 1)[0]"
      ],
      "metadata": {
        "id": "cYLSJE9t6B6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_of_zeros.shape,indices_of_ones.shape"
      ],
      "metadata": {
        "id": "PaWo9yI_6HvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TRAIN DATASET - INFINITE SAMPLING**"
      ],
      "metadata": {
        "id": "LhsHqf7i3wPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **GOAL: Oversample the minority class**\n",
        "A related approach would be to resample the dataset by oversampling the minority class."
      ],
      "metadata": {
        "id": "SnfypQmT9HdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_features = inputs.numpy()[indices_of_ones]\n",
        "neg_features = inputs.numpy()[indices_of_zeros]\n",
        "\n",
        "pos_labels = targets.numpy()[indices_of_ones]\n",
        "neg_labels = targets.numpy()[indices_of_zeros]\n",
        "\n",
        "pos_features.shape,pos_labels.shape,neg_features.shape,neg_labels.shape"
      ],
      "metadata": {
        "id": "02FjaF-K7hBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 100000\n",
        "\n",
        "def make_ds(features, labels):\n",
        "  ds = tf.data.Dataset.from_tensor_slices((features, labels))#.cache()\n",
        "  ds = ds.shuffle(BUFFER_SIZE).repeat()\n",
        "  return ds\n",
        "\n",
        "pos_ds = make_ds(pos_features, pos_labels)\n",
        "neg_ds = make_ds(neg_features, neg_labels)"
      ],
      "metadata": {
        "id": "39H5UxeZ355b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num, (features, label) in enumerate(pos_ds.take(10)):\n",
        "  #print(\"Features:\\n\", features.numpy())\n",
        "  #print()\n",
        "  print(f\"Label sample {num}: \", label.numpy())"
      ],
      "metadata": {
        "id": "MWB3pi5k352V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_ds = tf.data.Dataset.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\n",
        "resampled_ds = resampled_ds.batch(64).prefetch(2)"
      ],
      "metadata": {
        "id": "Z_UqaVSz35zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features, label in resampled_ds.take(1):\n",
        "  print(label.numpy().mean())"
      ],
      "metadata": {
        "id": "67_V1M1182vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.2 Validation Dataset**"
      ],
      "metadata": {
        "id": "X8-dgZvdx6qA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_validation = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_valid,\n",
        "                                                                    y_valid,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=64,\n",
        "                                                                    shuffle=False\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_validation.take(1):\n",
        "    inputs, targets = batch\n",
        "\n",
        "print(\"Input shape:\", inputs.numpy().shape)\n",
        "print(\"Target shape:\", targets.numpy().shape)"
      ],
      "metadata": {
        "id": "K_W27vuOjwry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.3 Test Dataset**"
      ],
      "metadata": {
        "id": "szF4AqsCx-BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_test,\n",
        "                                                                    y_test,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=64,\n",
        "                                                                    shuffle=False\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_test.take(1):\n",
        "    inputs, targets = batch\n",
        "\n",
        "print(\"Input shape:\", inputs.numpy().shape)\n",
        "print(\"Target shape:\", targets.numpy().shape)"
      ],
      "metadata": {
        "id": "-YnsvXB_kwJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 1"
      ],
      "metadata": {
        "id": "uWEpoqFw4NsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_validation = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_valid,\n",
        "                                                                    y_valid,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=10_000,\n",
        "                                                                    shuffle=False\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_validation.take(1):\n",
        "    inputs_val, targets_val = batch\n",
        "\n",
        "print(\"Input shape:\", inputs_val.numpy().shape)\n",
        "print(\"Target shape:\", targets_val.numpy().shape)"
      ],
      "metadata": {
        "id": "BQO5xx5I4Zt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_train,\n",
        "                                                                    y_train,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=10000,\n",
        "                                                                    shuffle=True\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_train.take(1):\n",
        "    inputs_tr, targets_tr = batch\n",
        "\n",
        "print(\"Input shape:\", inputs_tr.numpy().shape)\n",
        "print(\"Target shape:\", targets_tr.numpy().shape)"
      ],
      "metadata": {
        "id": "KsVL4cvuC7rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the arrays along the first axis\n",
        "all_train = np.concatenate((inputs_val, inputs_tr), axis=0)\n",
        "all_target = np.concatenate((targets_val, targets_tr), axis=0)\n",
        "all_train.shape, all_target.shape"
      ],
      "metadata": {
        "id": "7wejRe8RFyt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train.shape[0]-365, all_train.shape"
      ],
      "metadata": {
        "id": "qg7YEzFsIDbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train data\n",
        "x_train_v1 = all_train[:2178,:,:]\n",
        "y_train_v1 = all_target[:2178,:]\n",
        "print(x_train_v1.shape, y_train_v1.shape)\n",
        "# Valid Data\n",
        "x_valid_v1 = all_train[2178:,:,:]\n",
        "y_valid_v1 = all_target[2178:,:]\n",
        "print(x_valid_v1.shape, y_valid_v1.shape)\n",
        "# Test Data\n",
        "x_test_v1 = test_final.iloc[:,:-1].values\n",
        "y_test_v1 = test_final.iloc[past-1:,-1].values.reshape(-1,1)\n",
        "print(x_test_v1.shape, y_test_v1.shape)"
      ],
      "metadata": {
        "id": "UnIewJ434RNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **3.1 Train Dataset**"
      ],
      "metadata": {
        "id": "Kb6UjR7e4NsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_v1 = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_train_v1,\n",
        "                                                                    y_train_v1,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=10000,\n",
        "                                                                    shuffle=True\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_train_v1.take(1):\n",
        "    inputs, targets = batch\n",
        "\n",
        "print(\"Input shape:\", inputs.numpy().shape)\n",
        "print(\"Target shape:\", targets.numpy().shape)"
      ],
      "metadata": {
        "id": "M4Zh1TV54NsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy arrays to TensorFlow datasets\n",
        "feature_dataset = tf.data.Dataset.from_tensor_slices(x_train_v1)\n",
        "target_dataset = tf.data.Dataset.from_tensor_slices(y_train_v1)\n",
        "\n",
        "# Combine features and target into a single dataset\n",
        "dataset_tr = tf.data.Dataset.zip((feature_dataset, target_dataset))\n",
        "\n",
        "# Shuffle and batch the dataset\n",
        "dataset_train_v1 = dataset_tr.shuffle(buffer_size=len(x_train_v1)).batch(10000)\n",
        "\n",
        "\n",
        "for batch in dataset_train_v1.take(1):\n",
        "    inputs, targets = batch\n",
        "\n",
        "print(\"Input shape:\", inputs.numpy().shape)\n",
        "print(\"Target shape:\", targets.numpy().shape)"
      ],
      "metadata": {
        "id": "xfdZP0fboIhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find indices of 0s\n",
        "indices_of_zeros = np.where(targets.numpy() == 0)[0]\n",
        "\n",
        "# Find indices of 1s\n",
        "indices_of_ones = np.where(targets.numpy() == 1)[0]"
      ],
      "metadata": {
        "id": "RZaj8zyg4NsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_of_zeros.shape,indices_of_ones.shape"
      ],
      "metadata": {
        "id": "O3EBEiaA4NsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TRAIN DATASET - INFINITE SAMPLING**"
      ],
      "metadata": {
        "id": "B0S34qMA4NsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **GOAL: Oversample the minority class**\n",
        "A related approach would be to resample the dataset by oversampling the minority class."
      ],
      "metadata": {
        "id": "a-1ByFeN4NsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_features_v1 = inputs.numpy()[indices_of_ones]\n",
        "neg_features_v1 = inputs.numpy()[indices_of_zeros]\n",
        "\n",
        "pos_labels_v1 = targets.numpy()[indices_of_ones]\n",
        "neg_labels_v1 = targets.numpy()[indices_of_zeros]\n",
        "\n",
        "pos_features_v1.shape,pos_labels_v1.shape,neg_features_v1.shape,neg_labels_v1.shape"
      ],
      "metadata": {
        "id": "QRh3h0f64NsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 100000\n",
        "\n",
        "def make_ds(features, labels):\n",
        "  ds = tf.data.Dataset.from_tensor_slices((features, labels))#.cache()\n",
        "  ds = ds.shuffle(BUFFER_SIZE).repeat()\n",
        "  return ds\n",
        "\n",
        "pos_ds_v1 = make_ds(pos_features_v1, pos_labels_v1)\n",
        "neg_ds_v1 = make_ds(neg_features_v1, neg_labels_v1)"
      ],
      "metadata": {
        "id": "ZjpWhk2V4NsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num, (features, label) in enumerate(pos_ds_v1.take(10)):\n",
        "  #print(\"Features:\\n\", features.numpy())\n",
        "  #print()\n",
        "  print(f\"Label sample {num}: \", label.numpy(), features.shape)"
      ],
      "metadata": {
        "id": "Ksn9M9OX4NsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_ds_v1 = tf.data.Dataset.sample_from_datasets([pos_ds_v1, neg_ds_v1], weights=[0.5, 0.5])\n",
        "resampled_ds_v1 = resampled_ds_v1.batch(64).prefetch(2)"
      ],
      "metadata": {
        "id": "-VGL58B44NsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features, label in resampled_ds_v1.take(1):\n",
        "  print(label.shape,label.numpy().mean())"
      ],
      "metadata": {
        "id": "7C0YPdM64NsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid_v1.shape, y_valid_v1.shape, x_valid.shape, y_valid.shape"
      ],
      "metadata": {
        "id": "QI_dE5P1NuOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.2 Validation Dataset**"
      ],
      "metadata": {
        "id": "2GUpTWaF4NsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dataset = tf.data.Dataset.from_tensor_slices(x_valid_v1)\n",
        "target_dataset = tf.data.Dataset.from_tensor_slices(y_valid_v1)\n",
        "\n",
        "# Combine features and target into a single dataset\n",
        "dataset_validation_v1 = tf.data.Dataset.zip((feature_dataset, target_dataset))\n",
        "\n",
        "# Shuffle and batch the dataset\n",
        "dataset_validation_v1 = dataset_validation_v1.batch(64).prefetch(2)\n",
        "\n",
        "for batch in dataset_validation_v1.take(1):\n",
        "    inputs, targets = batch\n",
        "\n",
        "print(\"Input shape:\", inputs.numpy().shape)\n",
        "print(\"Target shape:\", targets.numpy().shape)"
      ],
      "metadata": {
        "id": "25_iaOeX4NsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.3 Test Dataset**"
      ],
      "metadata": {
        "id": "1nAPkrT14NsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test_v1 = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_test,\n",
        "                                                                    y_test,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=64,\n",
        "                                                                    shuffle=False\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_test_v1.take(1):\n",
        "    inputs, targets = batch\n",
        "\n",
        "print(\"Input shape:\", inputs.numpy().shape)\n",
        "print(\"Target shape:\", targets.numpy().shape)"
      ],
      "metadata": {
        "id": "rSezSbjb4NsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Version 2"
      ],
      "metadata": {
        "id": "OOfdJVue-Mq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_validation_v2 = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_valid,\n",
        "                                                                    y_valid,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=10_000,\n",
        "                                                                    shuffle=False\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_validation_v2.take(1):\n",
        "    inputs_val_v2, targets_val_v2 = batch\n",
        "\n",
        "print(\"Input shape:\", inputs_val_v2.numpy().shape)\n",
        "print(\"Target shape:\", targets_val_v2.numpy().shape)"
      ],
      "metadata": {
        "id": "ln-c-my1-Mq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train_v2 = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_train,\n",
        "                                                                    y_train,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=10000,\n",
        "                                                                    shuffle=True\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_train_v2.take(1):\n",
        "    inputs_tr_v2, targets_tr_v2 = batch\n",
        "\n",
        "print(\"Input shape:\", inputs_tr_v2.numpy().shape)\n",
        "print(\"Target shape:\", targets_tr_v2.numpy().shape)"
      ],
      "metadata": {
        "id": "wqaezdmj-Mq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the arrays along the first axis\n",
        "all_train_v2 = np.concatenate((inputs_val_v2, inputs_tr_v2), axis=0)\n",
        "all_target_v2 = np.concatenate((targets_val_v2, targets_tr_v2), axis=0)\n",
        "all_train_v2.shape, all_target_v2.shape"
      ],
      "metadata": {
        "id": "i4lvOcn6-Mq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train.shape[0]-365, all_train.shape"
      ],
      "metadata": {
        "id": "HuyWBeFu-Mq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_v2, x_valid_v2, y_train_v2, y_valid_v2 = train_test_split(all_train, all_target, test_size= 0.2, random_state=42, shuffle=True, stratify=all_target)\n",
        "y_train_v2 = y_train_v2.reshape(-1,1)\n",
        "y_valid_v2 = y_valid_v2.reshape(-1,1)\n",
        "\n",
        "print(x_train_v2.shape, y_train_v2.shape)\n",
        "print(x_valid_v2.shape, y_valid_v2.shape)\n",
        "# Test Data\n",
        "x_test_v2 = test_final.iloc[:,:-1].values\n",
        "y_test_v2 = test_final.iloc[past-1:,-1].values.reshape(-1,1)\n",
        "print(x_test_v2.shape, y_test_v2.shape)"
      ],
      "metadata": {
        "id": "DsGx5gaV-Mq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train.shape"
      ],
      "metadata": {
        "id": "flRCmUd-a7Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **3.1 Train Dataset**"
      ],
      "metadata": {
        "id": "G-xOns8h-Mq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy arrays to TensorFlow datasets\n",
        "feature_dataset_v2 = tf.data.Dataset.from_tensor_slices(x_train_v2)\n",
        "target_dataset_v2 = tf.data.Dataset.from_tensor_slices(y_train_v2)\n",
        "\n",
        "# Combine features and target into a single dataset\n",
        "dataset_tr_v2 = tf.data.Dataset.zip((feature_dataset_v2, target_dataset_v2))\n",
        "\n",
        "# Shuffle and batch the dataset\n",
        "dataset_train_v2 = dataset_tr_v2.shuffle(buffer_size=len(x_train_v2)).batch(10000)\n",
        "\n",
        "\n",
        "for batch in dataset_train_v2.take(1):\n",
        "    inputs_v2, targets_v2 = batch\n",
        "\n",
        "print(\"Input shape:\", inputs_v2.numpy().shape)\n",
        "print(\"Target shape:\", targets_v2.numpy().shape)"
      ],
      "metadata": {
        "id": "KrXjgNDl-Mq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find indices of 0s\n",
        "indices_of_zeros_v2 = np.where(targets_v2.numpy() == 0)[0]\n",
        "\n",
        "# Find indices of 1s\n",
        "indices_of_ones_v2 = np.where(targets_v2.numpy() == 1)[0]"
      ],
      "metadata": {
        "id": "lvsDOtwd-Mq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_of_zeros_v2.shape,indices_of_ones_v2.shape"
      ],
      "metadata": {
        "id": "ex0BfTX2-Mq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **TRAIN DATASET - INFINITE SAMPLING**"
      ],
      "metadata": {
        "id": "A7cbIAsB-Mq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **GOAL: Oversample the minority class**\n",
        "A related approach would be to resample the dataset by oversampling the minority class."
      ],
      "metadata": {
        "id": "N9IJ0BAB-Mq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_features_v2 = inputs_v2.numpy()[indices_of_ones_v2]\n",
        "neg_features_v2 = inputs_v2.numpy()[indices_of_zeros_v2]\n",
        "\n",
        "pos_labels_v2 = targets_v2.numpy()[indices_of_ones_v2]\n",
        "neg_labels_v2 = targets_v2.numpy()[indices_of_zeros_v2]\n",
        "\n",
        "pos_features_v2.shape,pos_labels_v2.shape,neg_features_v2.shape,neg_labels_v2.shape"
      ],
      "metadata": {
        "id": "etc2c85W-Mq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 100000\n",
        "\n",
        "def make_ds(features_v2, labels_v2):\n",
        "  ds_v2 = tf.data.Dataset.from_tensor_slices((features_v2, labels_v2))#.cache()\n",
        "  ds_v2 = ds_v2.shuffle(BUFFER_SIZE).repeat()\n",
        "  return ds_v2\n",
        "\n",
        "pos_ds_v2 = make_ds(pos_features_v2, pos_labels_v2)\n",
        "neg_ds_v2 = make_ds(neg_features_v2, neg_labels_v2)"
      ],
      "metadata": {
        "id": "U5hLSHD0-Mq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num, (features, label) in enumerate(pos_ds_v2.take(10)):\n",
        "  #print(\"Features:\\n\", features.numpy())\n",
        "  #print()\n",
        "  print(f\"Label sample {num}: \", label.numpy(), features.shape)"
      ],
      "metadata": {
        "id": "MQBZwVqB-Mq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_ds_v2 = tf.data.Dataset.sample_from_datasets([pos_ds_v2, neg_ds_v2], weights=[0.5, 0.5])\n",
        "resampled_ds_v2 = resampled_ds_v2.batch(64).prefetch(2)"
      ],
      "metadata": {
        "id": "WEVCez2V-Mq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features, label in resampled_ds_v2.take(1):\n",
        "  print(label.shape,label.numpy().mean())"
      ],
      "metadata": {
        "id": "hB_US6Ow-Mq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_valid_v2.shape, y_valid_v2.shape, x_valid_v1.shape, y_valid_v1.shape"
      ],
      "metadata": {
        "id": "R-wJMoWH-Mq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.2 Validation Dataset**"
      ],
      "metadata": {
        "id": "HMrJfLLD-Mq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_dataset_v2 = tf.data.Dataset.from_tensor_slices(x_valid_v2)\n",
        "target_dataset_v2 = tf.data.Dataset.from_tensor_slices(y_valid_v2)\n",
        "\n",
        "# Combine features and target into a single dataset\n",
        "dataset_validation_v2 = tf.data.Dataset.zip((feature_dataset_v2, target_dataset_v2))\n",
        "\n",
        "# Shuffle and batch the dataset\n",
        "dataset_validation_v2 = dataset_validation_v2.batch(64).prefetch(2)\n",
        "\n",
        "for batch in dataset_validation_v2.take(1):\n",
        "    inputs_v2, targets_v2 = batch\n",
        "\n",
        "print(\"Input shape:\", inputs_v2.numpy().shape)\n",
        "print(\"Target shape:\", targets_v2.numpy().shape)"
      ],
      "metadata": {
        "id": "F0E6BXSN-Mq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3.3 Test Dataset**"
      ],
      "metadata": {
        "id": "7--EUl3w-Mq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test_v2 = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_test,\n",
        "                                                                    y_test,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=64,\n",
        "                                                                    shuffle=False\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_test_v2.take(1000):\n",
        "    inputs_v2, targets_v2 = batch\n",
        "\n",
        "print(\"Input shape:\", inputs_v2.numpy().shape)\n",
        "print(\"Target shape:\", targets_v2.numpy().shape)"
      ],
      "metadata": {
        "id": "aw5p1wUm-Mq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.0 NN Models:"
      ],
      "metadata": {
        "id": "L7M1Zy4H_oCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(history):\n",
        "  metrics = ['loss', 'auc']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
        "    plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color=colors[0], linestyle=\"--\", label='Val')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "def plot_cm(labels, predictions, threshold=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > threshold)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(threshold))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
        "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
        "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
        "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
        "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))"
      ],
      "metadata": {
        "id": "SjmHd5bvJgMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 1\n",
        "\n",
        "past = 7\n",
        "future = 0\n",
        "learning_rate = 0.01\n",
        "batch_size = 64\n",
        "epochs = 101"
      ],
      "metadata": {
        "id": "Xo3xJEX0A4H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS = [\n",
        "          keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n",
        "          keras.metrics.AUC(name='auc'),\n",
        "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "          ]"
      ],
      "metadata": {
        "id": "xwVq-GoEBLU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 LSTM v0"
      ],
      "metadata": {
        "id": "JYcGsOk3Amm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_lstm(metrics=METRICS, units_rep=64, repeat=3,  units_last=32, output_bias=0, kr_lstm=0.0001, sdo=0.3, rdo=0.3, stddev=0.05):\n",
        "\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "\n",
        "  data = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]), name=\"input_layer\")\n",
        "  x = keras.layers.GaussianNoise(stddev=stddev, name=\"noise_layer\")(data)\n",
        "\n",
        "\n",
        "  for i in range(repeat):\n",
        "    x = keras.layers.LSTM(units=units_rep,\n",
        "                          return_sequences=True,\n",
        "                          name=f\"lstm_{i}\",\n",
        "                          kernel_regularizer = keras.regularizers.l2(kr_lstm),\n",
        "                          dropout=sdo,\n",
        "                          recurrent_dropout=rdo)(x)\n",
        "\n",
        "  lstm_out = keras.layers.LSTM(units_last, name=\"lstm_final\")(x)\n",
        "\n",
        "  outputs = keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias, name=\"output\")(lstm_out)\n",
        "\n",
        "  model = keras.Model(inputs=data, outputs=outputs, name=\"LSTM_v0\")\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "\n",
        "\n",
        "  return model\n",
        "\n",
        "model= make_model_lstm()\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7ZbcOcS3fud4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_all = [resampled_ds,resampled_ds_v1,resampled_ds_v2]\n",
        "validation_ds_all = [dataset_validation,dataset_validation_v1,dataset_validation_v2]\n",
        "test_ds_all = [dataset_test,dataset_test_v1,dataset_test_v2]\n",
        "\n",
        "dss = zip(train_ds_all,validation_ds_all)"
      ],
      "metadata": {
        "id": "aN_E2-vPZnBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "ArDUspyzabHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 00"
      ],
      "metadata": {
        "id": "uv7wbc5fabHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_lstm(trial, train_data, validation_data, model=make_model_lstm, use_gpu=False, rs=42, fit_scaling=False):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units_rep': trial.suggest_categorical('units_rep', [32,64,128,256]),\n",
        "              'repeat': trial.suggest_categorical('repeat', [1,2,3,4]),\n",
        "              'units_last': trial.suggest_categorical('units_last', [32,64,128,256]),\n",
        "              'stddev': trial.suggest_float('stddev', 0.01, 0.1, step=0.01),\n",
        "              'kr_lstm': trial.suggest_float('kr_lstm', 0.001, 0.1, log=True),\n",
        "              'sdo' : trial.suggest_float('sdo', 0.20, 0.50, step=0.01),\n",
        "              'rdo' : trial.suggest_float('rdo', 0.20, 0.50, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_scores = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(**params)\n",
        "\n",
        "    for count, (train_data, validation_data) in enumerate(dss):\n",
        "      print(f\"Fold {count+1}\")\n",
        "\n",
        "      # Fit the model\n",
        "      model.fit(train_data,\n",
        "                validation_data=validation_data,\n",
        "                epochs=201,\n",
        "                steps_per_epoch=45,\n",
        "                callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                          keras.callbacks.EarlyStopping(patience=21, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                        start_from_epoch=3, mode=\"max\")]\n",
        "                )\n",
        "\n",
        "      # Make predictions on the validation set\n",
        "      y_pred = model.predict(validation_data)\n",
        "\n",
        "      # Calculate the RMSE for the current fold\n",
        "      auc_score = roc_auc_score(y_valid, y_pred)\n",
        "      auc_scores.append(auc_score)\n",
        "\n",
        "    return np.mean(auc_scores)"
      ],
      "metadata": {
        "id": "ruZMRYFGabHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=make_model_lstm, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "v-HtZZNrfPW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model_lstm, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8RwXtnsfPW4",
        "outputId": "f32d7f32-3e07-4847-bf7d-038f5e9887cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52/52 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - auc: 0.8906 - cross entropy: 0.4174 - loss: 0.4391 - prc: 0.8752 - val_auc: 0.8791 - val_cross entropy: 0.4404 - val_loss: 0.4584 - val_prc: 0.9343 - learning_rate: 5.0000e-04\n",
            "Epoch 7/201\n",
            "52/52 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - auc: 0.8844 - cross entropy: 0.4280 - loss: 0.4455 - prc: 0.8824 - val_auc: 0.8792 - val_cross entropy: 0.4652 - val_loss: 0.4822 - val_prc: 0.9334 - learning_rate: 5.0000e-04\n",
            "Epoch 8/201\n",
            "52/52 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - auc: 0.8868 - cross entropy: 0.4257 - loss: 0.4422 - prc: 0.8749 - val_auc: 0.8797 - val_cross entropy: 0.4509 - val_loss: 0.4666 - val_prc: 0.9330 - learning_rate: 2.5000e-04\n",
            "Epoch 9/201\n",
            "52/52 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - auc: 0.8910 - cross entropy: 0.4162 - loss: 0.4318 - prc: 0.8756 - val_auc: 0.8783 - val_cross entropy: 0.4652 - val_loss: 0.4804 - val_prc: 0.9317 - learning_rate: 2.5000e-04\n",
            "Epoch 10/201\n",
            "52/52 ━━━━━━━━━━━━━━━━━━━━ 2s 40ms/step - auc: 0.8936 - cross entropy: 0.4140 - loss: 0.4291 - prc: 0.8911 - val_auc: 0.8797 - val_cross entropy: 0.4580 - val_loss: 0.4730 - val_prc: 0.9332 - learning_rate: 1.2500e-04\n",
            "Epoch 11/201\n",
            "52/52 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - auc: 0.8975 - cross entropy: 0.4073 - loss: 0.4222 - prc: 0.8999 - val_auc: 0.8795 - val_cross entropy: 0.4722 - val_loss: 0.4867 - val_prc: 0.9332 - learning_rate: 1.2500e-04\n",
            "Epoch 12/201\n",
            "52/52 ━━━━━━━━━━━━━━━━━━━━ 2s 38ms/step - auc: 0.8989 - cross entropy: 0.3993 - loss: 0.4137 - prc: 0.8949 - val_auc: 0.8803 - val_cross entropy: 0.4469 - val_loss: 0.4613 - val_prc: 0.9337 - learning_rate: 6.2500e-05\n",
            "Epoch 13/201\n",
            "52/52 ━━━━━━━━━━━━━━━━━━━━ 2s 39ms/step - auc: 0.8987 - cross entropy: 0.4027 - loss: 0.4171 - prc: 0.8798 - val_auc: 0.8801 - val_cross entropy: 0.4499 - val_loss: 0.4641 - val_prc: 0.9333 - learning_rate: 6.2500e-05\n",
            "Epoch 14/201\n",
            "52/52 ━━━━━━━━━━━━━━━━━━━━ 2s 37ms/step - auc: 0.8981 - cross entropy: 0.4047 - loss: 0.4189 - prc: 0.8829 - val_auc: 0.8810 - val_cross entropy: 0.4712 - val_loss: 0.4854 - val_prc: 0.9343 - learning_rate: 3.1250e-05\n",
            "Epoch 15/201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model"
      ],
      "metadata": {
        "id": "o79SSUi3FBYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "YrgpH6iiFtFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_history = model.fit(\n",
        "                              resampled_ds,\n",
        "                              epochs=epochs,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.5),\n",
        "                                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\")],\n",
        "                              validation_data=dataset_validation\n",
        "                              )"
      ],
      "metadata": {
        "id": "NVPo4BbfFA1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history)"
      ],
      "metadata": {
        "id": "AvMLRuOjFAyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "8BGzhZ1LL-Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled = model.predict(dataset_train)\n",
        "valid_predictions_resampled = model.predict(dataset_validation)\n",
        "test_predictions_resampled = model.predict(dataset_test)"
      ],
      "metadata": {
        "id": "MVaHKXboFAvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled)"
      ],
      "metadata": {
        "id": "TzXhgUnTMCLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled)"
      ],
      "metadata": {
        "id": "y3EikvfBpb-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_resampled\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_v2_all_data.csv\")"
      ],
      "metadata": {
        "id": "kDnIrIUVFArt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 LSTM-Tab v0"
      ],
      "metadata": {
        "id": "EMZfQtbAqpUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(metrics=METRICS, units=[32,32],units_tab=[64,32], output_bias=None, gn=0.025, activation=\"relu\", do=0.3):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "\n",
        "  data = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]), name=\"input_layer\")\n",
        "  data_tabular = data[:, 6, :]\n",
        "  # LSTM Section\n",
        "  data_noised = keras.layers.GaussianNoise(stddev=gn, name=\"noise_layer\")(data)\n",
        "  lstm_out = keras.layers.LSTM(units[0], return_sequences=True, name=\"lstm_0\",)(data_noised)\n",
        "  lstm_out = keras.layers.LSTM(units[1], name=\"lstm_1\")(lstm_out)\n",
        "\n",
        "  # Tabular Section\n",
        "  tabx = keras.layers.Dense(units_tab[0], name=\"dense_0\")(data_tabular)\n",
        "  tabx = keras.layers.BatchNormalization(name=\"batch_0\")(tabx)\n",
        "  tabx = keras.layers.Activation(activation, name=\"act_0\")(tabx)\n",
        "  tabx = keras.layers.Dropout(do, name=\"do_0\")(tabx)\n",
        "  tabx = keras.layers.Dense(units_tab[1], name=\"dense_1\")(tabx)\n",
        "  tabx = keras.layers.BatchNormalization(name=\"batch_1\")(tabx)\n",
        "  tabx = keras.layers.Activation(activation, name=\"act_1\")(tabx)\n",
        "  tabx = keras.layers.Dropout(do, name=\"do_1\")(tabx)\n",
        "\n",
        "  # Concatenate\n",
        "  x = keras.layers.Concatenate(name=\"concat\")([lstm_out, tabx,data_tabular])\n",
        "  outputs = keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias, name=\"output\")(x)\n",
        "\n",
        "  model = keras.Model(inputs=data, outputs=outputs, name=\"LSTM_tab_v1\")\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "model= make_model(units=[64,32],output_bias=0)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "c21KCmDrqpUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "V9y8NofX4KTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 00"
      ],
      "metadata": {
        "id": "S6tl3QVvduz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model, use_gpu=False, rs=42, fit_scaling=False):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units': [trial.suggest_categorical('units_0', [128,64]),trial.suggest_categorical('units_1', [64,32])],\n",
        "              'units_tab': [trial.suggest_categorical('units_tab_0', [256, 128, 64]),trial.suggest_categorical('units_tab_1', [128, 64,32])],\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(**params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.3, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "VLLqEqIL4NzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=make_model, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "VT3er0dcWw34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "dYJDGyIgZGEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * Trial 21 finished with value: 0.8943167305236271\n",
        "    * parameters: {'units_0': 64, 'units_1': 64, 'units_tab_0': 64, 'units_tab_1': 32, 'activation': 'silu', 'gn': 0.09, 'do': 0.30781160547467370}.\n",
        "\n",
        " * Trial 41 finished with value: 0.8956648219100326\n",
        "    * parameters: {'units_0': 128, 'units_1': 64, 'units_tab_0': 128, 'units_tab_1': 32, 'activation': 'selu', 'gn': 0.09, 'do': 0.25}.\n",
        "\n",
        "* Best is trial 25 with value: 0.8865119909181212.\n",
        "    * {'units_0': 64,\n",
        " 'units_1': 32,\n",
        " 'units_tab_0': 256,\n",
        " 'units_tab_1': 128,\n",
        " 'activation': 'selu',\n",
        " 'gn': 0.08,\n",
        " 'do': 0.32}\n"
      ],
      "metadata": {
        "id": "0oMfqI9Urdkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 01"
      ],
      "metadata": {
        "id": "30RHzjTwd1h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model, use_gpu=False, rs=42, fit_scaling=False):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units': [trial.suggest_categorical('units_0', [128,64]),trial.suggest_categorical('units_1', [64,32])],\n",
        "              'units_tab': [trial.suggest_categorical('units_tab_0', [256, 128, 64]),trial.suggest_categorical('units_tab_1', [128, 64,32])],\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\",\"mish\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(**params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v1, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "mx-V11vQd1h4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "-owtdImCe4bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v1,validation_data=dataset_validation_v1, model_class=make_model, n_trials=111, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "5W-lPZOee4bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model"
      ],
      "metadata": {
        "id": "fKlsZrL7qpUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model 00"
      ],
      "metadata": {
        "id": "R84Pz6lneB52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model= make_model(units=[64,64], units_tab=[64,32], activation=\"silu\", gn=0.09, do=0.31, output_bias=0) # Model v2 (Score 0.85787)\n",
        "# model= make_model(units=[128,64], units_tab=[128,32], activation=\"selu\", gn=0.09, do=0.25, output_bias=0) # Model v1 (best 0.86323)\n",
        "model= make_model(units=[64,64], units_tab=[128,32], activation=\"gelu\", gn=0.09, do=0.39, output_bias=0) # Model v3 (best 0.86323)"
      ],
      "metadata": {
        "id": "ehlCfLQIrni-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "mkFnSl5iqpUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_history = model.fit(\n",
        "                              resampled_ds,\n",
        "                              epochs=epochs,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.3, monitor=\"val_auc\", min_lr=0.000001),\n",
        "                                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\")],\n",
        "                              validation_data=dataset_validation\n",
        "                              )"
      ],
      "metadata": {
        "id": "jon84oVAqpUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history)"
      ],
      "metadata": {
        "id": "VpLdQKtQqpUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model 01"
      ],
      "metadata": {
        "id": "rLVljhZmwfW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv1= make_model(units=[128,64], units_tab=[128,128], activation=\"mish\", gn=0.09, do=0.24, output_bias=0)\n",
        "\n",
        "resampled_history_v1 = model_cv1.fit(\n",
        "                              resampled_ds_v1,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3,  factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v1\n",
        "                              )\n",
        "\n",
        "model_cv1.evaluate(dataset_validation_v1)"
      ],
      "metadata": {
        "id": "j0GURFPowZaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history_v1)"
      ],
      "metadata": {
        "id": "ahX7nATlwZaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "xSdHNt2kqpUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_v0 = model.predict(dataset_train)\n",
        "valid_predictions_resampled_v0 = model.predict(dataset_validation)\n",
        "test_predictions_resampled_v0 = model.predict(dataset_test)"
      ],
      "metadata": {
        "id": "-3PLyKLlqpUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_v1 = model_cv1.predict(dataset_train)\n",
        "valid_predictions_resampled_v1 = model_cv1.predict(dataset_validation)\n",
        "test_predictions_resampled_v1 = model_cv1.predict(dataset_test)"
      ],
      "metadata": {
        "id": "kwLJ0O7W_-Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled)"
      ],
      "metadata": {
        "id": "Eegi9KDxqpUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(test_predictions_resampled_v0,test_predictions_resampled_v1)"
      ],
      "metadata": {
        "id": "jDXknGUeqpUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_resampled_v0\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_tab_v4_all_data_ext_v0.csv\")\n",
        "df_subm[\"rainfall\"] = test_predictions_resampled_v1\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_tab_v4_all_data_ext_v1.csv\")\n",
        "df_subm[\"rainfall\"] = (test_predictions_resampled_v0+test_predictions_resampled_v1)/2\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_tab_v4_all_data_ext_average.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "okTTMffeqpUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_cm(y_valid_v1, dataset_validation_v1)\n",
        "test_predictions_resampled_v0.shape, test_predictions_resampled_v1.shape"
      ],
      "metadata": {
        "id": "L6mvSi7ZBYAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = (test_predictions_resampled_cv0+test_predictions_resampled_cv1)/2\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_tab_enhanced_v0_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "DAn2sbhyBYAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ywd5mvZ9BXlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 LSTM-Tab with Transformer v0"
      ],
      "metadata": {
        "id": "k6Hx3tU7e5Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "id": "_vZJLtPpoWM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(metrics=METRICS, units=[32,32],units_tab=[64,32], output_bias=None, gn=0.025, activation=\"relu\", do=0.3, lr=5e-4):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "\n",
        "  data = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]), name=\"input_layer\")\n",
        "  data_tabular = data[:, 6, :]\n",
        "  # LSTM Section\n",
        "  data_noised = keras.layers.GaussianNoise(stddev=gn, name=\"noise_layer\")(data)\n",
        "  whole_seq_output, final_memory_state_0, final_carry_state_0 = keras.layers.LSTM(units[0], return_sequences=True, name=\"lstm_0\",return_state=True)(data_noised)\n",
        "  whole_seq_output, final_memory_state_1, final_carry_state_1 = keras.layers.LSTM(units[1], name=\"lstm_1\",return_state=True)(whole_seq_output)\n",
        "\n",
        "  print(final_memory_state_0.shape, final_carry_state_0.shape)\n",
        "  print(final_memory_state_1.shape, final_carry_state_1.shape)\n",
        "\n",
        "  # Tabular Section\n",
        "  data_tabular = keras.layers.Concatenate(name=\"concat_states\")([data_tabular,final_memory_state_0,final_carry_state_0,final_memory_state_1,final_carry_state_1])\n",
        "\n",
        "  tabx = keras.layers.Dense(units_tab[0], name=\"dense_0\")(data_tabular)\n",
        "  tabx = keras.layers.BatchNormalization(name=\"batch_0\")(tabx)\n",
        "  tabx = keras.layers.Activation(activation, name=\"act_0\")(tabx)\n",
        "  tabx = keras.layers.Dropout(do, name=\"do_0\")(tabx)\n",
        "  tabx = keras.layers.Dense(units_tab[1], name=\"dense_1\")(tabx)\n",
        "  tabx = keras.layers.BatchNormalization(name=\"batch_1\")(tabx)\n",
        "  tabx = keras.layers.Activation(activation, name=\"act_1\")(tabx)\n",
        "  tabx = keras.layers.Dropout(do, name=\"do_1\")(tabx)\n",
        "\n",
        "  # Concatenate\n",
        "  x = keras.layers.Concatenate(name=\"concat\")([whole_seq_output, tabx,data_tabular])\n",
        "  outputs = keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias, name=\"output\")(x)\n",
        "\n",
        "  model = keras.Model(inputs=data, outputs=outputs, name=\"LSTM_tab_v2\")\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "model= make_model(units=[64,32],output_bias=0)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cuiMBs9Ce5Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "fGgDX7yVpZD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "PiSyICkqe5Ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 00"
      ],
      "metadata": {
        "id": "Yu8xFMKYsEYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units': [trial.suggest_categorical('units_0', [128,64]),trial.suggest_categorical('units_1', [64,32])],\n",
        "              'units_tab': [trial.suggest_categorical('units_tab_0', [256, 128, 64]),trial.suggest_categorical('units_tab_1', [128, 64,32])],\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"mish\",\"gelu\",\"silu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(**params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=21, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "jtg0mGpZe5Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "UT5grc-Re5Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "ov29xbOFe5Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * Trial 85 finished with value: 0.8963743436923514\n",
        "    * parameters: {'units_0': 64, 'units_1': 32,  'units_tab_0': 128, 'units_tab_1': 32,\n",
        " 'activation': 'selu', 'gn': 0.1, 'do': 0.44}\n",
        "\n",
        "\n",
        "  * Trial 69 with value: 0.8962679154250035\n",
        "    * parameters: {'units_0': 64, 'units_1': 64, 'units_tab_0': 64, 'units_tab_1': 64,\n",
        " 'activation': 'gelu', 'gn': 0.05, 'do': 0.32}"
      ],
      "metadata": {
        "id": "qRfYNEnde5Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 01"
      ],
      "metadata": {
        "id": "NEz-wEChsMTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units': [trial.suggest_categorical('units_0', [128,64]),trial.suggest_categorical('units_1', [64,32])],\n",
        "              'units_tab': [trial.suggest_categorical('units_tab_0', [256, 128, 64]),trial.suggest_categorical('units_tab_1', [128, 64,32])],\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"mish\",\"gelu\",\"silu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(**params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=21, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v1, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "019oD4YMwfHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "qsngxeV8woEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v1,validation_data=dataset_validation_v1, model_class=make_model, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "UHun7z0hsOoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * Trial 91 with value: 0.902684004334417\n",
        "    * parameters: {'units_0': 128,\n",
        " 'units_1': 64,\n",
        " 'units_tab_0': 256,\n",
        " 'units_tab_1': 64,\n",
        " 'activation': 'mish',\n",
        " 'gn': 0.09,\n",
        " 'do': 0.2}"
      ],
      "metadata": {
        "id": "kj6elhtqJHhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model"
      ],
      "metadata": {
        "id": "Lz45j_MRe5Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model= make_model(units=[64,64], units_tab=[64,32], activation=\"silu\", gn=0.09, do=0.31, output_bias=0) # Model v2 (Score 0.85787)\n",
        "# model= make_model(units=[128,64], units_tab=[128,32], activation=\"selu\", gn=0.09, do=0.25, output_bias=0) # Model v1 (best 0.86323)\n",
        "# MODEL 00 'units_0': 64, 'units_1': 64, 'units_tab_0': 64, 'units_tab_1': 64, 'activation': 'gelu', 'gn': 0.05, 'do': 0.32\n",
        "model= make_model(units=[64,32], units_tab=[128,32], activation=\"selu\", gn=0.10, do=0.44, output_bias=0) # Model v3 (best 0.86323)\n",
        "model= make_model(units=[64,64], units_tab=[64,64], activation=\"gelu\", gn=0.05, do=0.32, output_bias=0) # Model v3 (best 0.86323)\n",
        "# MODEL 01\n",
        "model= make_model(units=[128,64], units_tab=[256,64], activation=\"mish\", gn=0.09, do=0.20, output_bias=0) # Model v3 (best 0.86323)"
      ],
      "metadata": {
        "id": "mPiD12PQe5Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "oTybAnP8e5Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv0= make_model(units=[64,64], units_tab=[64,64], activation=\"gelu\", gn=0.05, do=0.32, output_bias=0) # Model v3 (best 0.86323)\n",
        "\n",
        "resampled_history_v0 = model_cv0.fit(\n",
        "                              resampled_ds,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation\n",
        "                              )\n",
        "\n",
        "model_cv0.evaluate(dataset_validation)"
      ],
      "metadata": {
        "id": "ISCVyV2Xe5Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history_v0)"
      ],
      "metadata": {
        "id": "AG3VdyLte5Ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv1= make_model(units=[128,64], units_tab=[256,64], activation=\"mish\", gn=0.09, do=0.20, output_bias=0, lr=5e-4) # Model v3 (best 0.86323)\n",
        "\n",
        "resampled_history_v1 = model_cv1.fit(\n",
        "                              resampled_ds_v1,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3,  factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v1\n",
        "                              )\n",
        "\n",
        "model_cv1.evaluate(dataset_validation_v1)"
      ],
      "metadata": {
        "id": "GlPjmyfupEIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history_v1)"
      ],
      "metadata": {
        "id": "nDaW_iiypK7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "sHhM5YIMe5If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv0 = model_cv0.predict(dataset_train)\n",
        "valid_predictions_resampled_cv0 = model_cv0.predict(dataset_validation)\n",
        "test_predictions_resampled_cv0 = model_cv0.predict(dataset_test)"
      ],
      "metadata": {
        "id": "kY2nP459e5If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv1 = model_cv1.predict(dataset_train_v1)\n",
        "valid_predictions_resampled_cv1 = model_cv1.predict(dataset_validation_v1)\n",
        "test_predictions_resampled_cv1 = model_cv1.predict(dataset_test_v1)"
      ],
      "metadata": {
        "id": "yW7byOKtpbgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv1)"
      ],
      "metadata": {
        "id": "Zo9tscAkcfUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled_cv0)"
      ],
      "metadata": {
        "id": "TN8F7zTve5If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_cm(y_valid_v1, dataset_validation_v1)\n",
        "test_predictions_resampled_cv0.shape, test_predictions_resampled_cv1.shape"
      ],
      "metadata": {
        "id": "wsF-73Sre5If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = (test_predictions_resampled_cv0+test_predictions_resampled_cv1)/2\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_tab_enhanced_v0_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "C0Zu5gzke5If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 LSTM-Tab with Transformer v1 - Last Work"
      ],
      "metadata": {
        "id": "0zKke1iYQqfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "id": "4pkwqzNFQqfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_cnn_v4(input_shape, metrics=None, filters=32, kernel_size=3, kr=0.01, sdo=0.2,\n",
        "                    units_dense=128, gn=0.025, activation=\"relu\", do=0.3, strides=2):\n",
        "    \"\"\"\n",
        "    Improved 1D CNN model - Strided Convolutions instead of MaxPooling.\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['accuracy']\n",
        "\n",
        "    data = keras.layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "    data_noised = keras.layers.GaussianNoise(stddev=gn, name=\"noise_layer\")(data)\n",
        "\n",
        "    # --- Convolutional Block 1 ---\n",
        "    cnn_out = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides,  # Stride = 2\n",
        "                                    kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                    padding=\"same\", activation=None, name=\"cnn_1\")(data_noised)\n",
        "    cnn_out = keras.layers.LayerNormalization(name=\"ln_1\")(cnn_out)\n",
        "    cnn_out = keras.layers.Activation(\"relu\", name=\"act_1\")(cnn_out)\n",
        "    cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_1\")(cnn_out)\n",
        "    # NO MaxPooling\n",
        "\n",
        "    # --- Convolutional Block 2 ---\n",
        "    cnn_out = keras.layers.Conv1D(filters=filters * 2, kernel_size=kernel_size, strides=strides,  # Stride = 2\n",
        "                                    kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                    padding=\"same\", activation=None, name=\"cnn_2\")(cnn_out)\n",
        "    cnn_out = keras.layers.LayerNormalization(name=\"ln_2\")(cnn_out)\n",
        "    cnn_out = keras.layers.Activation(\"relu\", name=\"act_2\")(cnn_out)\n",
        "    cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_2\")(cnn_out)\n",
        "    # NO MaxPooling\n",
        "\n",
        "   # --- Convolutional Block 3 ---\n",
        "    cnn_out = keras.layers.Conv1D(filters=filters * 4, kernel_size=kernel_size, strides=strides,  # Stride = 2\n",
        "                                kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                padding=\"same\", activation=None, name=\"cnn_3\")(cnn_out)\n",
        "    cnn_out = keras.layers.LayerNormalization(name=\"ln_3\")(cnn_out)\n",
        "    cnn_out = keras.layers.Activation(\"relu\", name=\"act_3\")(cnn_out)\n",
        "    cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_3\")(cnn_out)\n",
        "    # NO MaxPooling\n",
        "\n",
        "    # --- Pooling and Concatenation ---\n",
        "    cnn_out_ave = keras.layers.GlobalAveragePooling1D(name=\"average_pool_final\")(cnn_out)\n",
        "    cnn_out_max = keras.layers.GlobalMaxPooling1D(name=\"max_pool_final\")(cnn_out)\n",
        "    x = keras.layers.Concatenate(name=\"concat\")([cnn_out_ave, cnn_out_max])\n",
        "\n",
        "    # --- Dense Layer ---\n",
        "    x = keras.layers.Dense(units_dense, name=\"dense_1\", kernel_regularizer=keras.regularizers.l2(kr))(x)\n",
        "    x = keras.layers.BatchNormalization(name=\"batch_dense\")(x)\n",
        "    x = keras.layers.Activation(activation, name=\"act_dense\")(x)\n",
        "    x = keras.layers.Dropout(do, name=\"do_dense\")(x)\n",
        "\n",
        "    # --- Output Layer ---\n",
        "    outputs = keras.layers.Dense(1, activation='sigmoid', name=\"output\")(x)\n",
        "\n",
        "    # --- Model Creation and Compilation ---\n",
        "    model = keras.Model(inputs=data, outputs=outputs, name=\"cnn_v4\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model= make_model_cnn_v4(input_shape=(inputs.shape[1], inputs.shape[2]),strides=2, metrics=METRICS)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "91s_Lwy-VwqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "6nv2WWAjVwqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 00"
      ],
      "metadata": {
        "id": "qBu7JccY41sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v4, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),#\n",
        "              'strides': trial.suggest_categorical('strides', [1,2]),#\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),#\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "GIJaYPxAVwqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "Y24PvcO_VwqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model_cnn_v4, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "3j8P44mtVwqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "FbdDvJ7h5SjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 66 with value: 0.8893146019582802\n",
        "    * {'kernel_size': 2, 'strides': 1, 'units_dense': 64, 'sdo': 0.43, 'kr': 0.0202, 'activation':'elu', 'gn': 0.07,'do': 0.36}\n"
      ],
      "metadata": {
        "id": "6CT5McsCVwqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 01"
      ],
      "metadata": {
        "id": "IiUV8S5b5AFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v4, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),#\n",
        "              'strides': trial.suggest_categorical('strides', [1,2]),#\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),#\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v1, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "_fKpzuIh5AFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "YcrkAlGr5AFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v1,validation_data=dataset_validation_v1, model_class=make_model_cnn_v4, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "vu07vU-n5AFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "mqGEirqR5dYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 4 with value: 0.0.9343330769888062\n",
        "    * {'kernel_size': 3, 'strides': 2, 'units_dense': 64, 'sdo': 0.37, 'kr': 0.0040221083422204585, 'activation': 'relu', 'gn': 0.10, 'do': 0.26}\n"
      ],
      "metadata": {
        "id": "z8kElCu-5AFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 02"
      ],
      "metadata": {
        "id": "85BTmxOhH1PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v4, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),#\n",
        "              'strides': trial.suggest_categorical('strides', [1,2]),#\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),#\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v2, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "F-JtHwe4H1PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "VF_Wc4PCH1PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v2,validation_data=dataset_validation_v2, model_class=make_model_cnn_v4, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "RXPBTcSuH1PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "_alVgDtjH1PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 4 with value: 0.8672805848378368\n",
        "    * {'kernel_size': 2, 'strides': 2, 'units_dense': 128, 'sdo': 0.31, 'kr': 0.003562788704053626, 'activation': 'leaky_relu', 'gn': 0.02, 'do': 0.26}\n"
      ],
      "metadata": {
        "id": "zFTC1nOnH1PF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 00"
      ],
      "metadata": {
        "id": "Qm_azkhoVwqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv0= make_model_cnn_v4(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'kernel_size': 2, 'strides': 1, 'units_dense': 64, 'sdo': 0.43, 'kr': 0.0202, 'activation':'elu', 'gn': 0.07,'do': 0.36})\n",
        "model_cv0.summary()"
      ],
      "metadata": {
        "id": "bQ8V9jAuVwqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "zU0HECZrVwqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_history_v0 = model_cv0.fit(\n",
        "                              resampled_ds,\n",
        "                              epochs=2201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001),\n",
        "                                         keras.callbacks.EarlyStopping(patience=101, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\")],\n",
        "                              validation_data=dataset_validation\n",
        "                              )"
      ],
      "metadata": {
        "id": "DUCkbZ1gVwqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history_v0)"
      ],
      "metadata": {
        "id": "GETZ5eE9VwqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "ed8MsIu0Qqfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 01"
      ],
      "metadata": {
        "id": "QeUU2U3sQqfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "9BVJtG3RQqfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv1= make_model_cnn_v4(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'kernel_size': 3, 'strides': 2, 'units_dense': 64, 'sdo': 0.37, 'kr': 0.0040221083422204585, 'activation': 'relu', 'gn': 0.10, 'do': 0.26})\n",
        "\n",
        "resampled_history_v1 = model_cv1.fit(\n",
        "                              resampled_ds_v1,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=151, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v1\n",
        "                              )\n",
        "\n",
        "model_cv1.evaluate(dataset_validation_v1)"
      ],
      "metadata": {
        "id": "4XdENnGUQqfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history_v1)"
      ],
      "metadata": {
        "id": "PMt08SdnQqf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 02"
      ],
      "metadata": {
        "id": "qWoBlbbz99B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "pwZ93Hld99B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv2= make_model_cnn_v4(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'kernel_size': 2, 'strides': 2, 'units_dense': 128, 'sdo': 0.31, 'kr': 0.003562788704053626, 'activation': 'leaky_relu','gn': 0.02,'do': 0.26})\n",
        "\n",
        "resampled_history_v2 = model_cv2.fit(\n",
        "                              resampled_ds_v2,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=151, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v2\n",
        "                              )\n",
        "\n",
        "model_cv2.evaluate(dataset_validation_v2)"
      ],
      "metadata": {
        "id": "y4K7tMxd99B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history_v2)"
      ],
      "metadata": {
        "id": "Cw8_15pe99B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "PvgyzwN6Qqf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 00**"
      ],
      "metadata": {
        "id": "NJkvQ-cc-ixv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv0 = model_cv0.predict(dataset_train)\n",
        "valid_predictions_resampled_cv0 = model_cv0.predict(dataset_validation)\n",
        "test_predictions_resampled_cv0 = model_cv0.predict(dataset_test)"
      ],
      "metadata": {
        "id": "Y7RfEeksQqf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 01**"
      ],
      "metadata": {
        "id": "dICpJUYO-l97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv1 = model_cv1.predict(dataset_train_v1)\n",
        "valid_predictions_resampled_cv1 = model_cv1.predict(dataset_validation_v1)\n",
        "test_predictions_resampled_cv1 = model_cv1.predict(dataset_test_v1)"
      ],
      "metadata": {
        "id": "yjvyksRAQqf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 02**"
      ],
      "metadata": {
        "id": "WEBL_BIX-nS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv2 = model_cv2.predict(dataset_train_v2)\n",
        "valid_predictions_resampled_cv2 = model_cv2.predict(dataset_validation_v2)\n",
        "test_predictions_resampled_cv2 = model_cv2.predict(dataset_test_v2)"
      ],
      "metadata": {
        "id": "zmGsJmcW-o3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
        "ax[0].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv1)\n",
        "ax[1].scatter(test_predictions_resampled_cv1,test_predictions_resampled_cv2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nsTw9bQ2Qqf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled_cv0)"
      ],
      "metadata": {
        "id": "9CA5EAn0Qqf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid_v1, valid_predictions_resampled_cv1)"
      ],
      "metadata": {
        "id": "2Llu8D-WBaBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid_v2, valid_predictions_resampled_cv2)"
      ],
      "metadata": {
        "id": "EXJ1RE7gBjze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_cm(y_valid_v1, dataset_validation_v1)\n",
        "test_predictions_resampled_cv0.shape, test_predictions_resampled_cv1.shape, test_predictions_resampled_cv2.shape"
      ],
      "metadata": {
        "id": "6Pix-zY_Qqf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = (test_predictions_resampled_cv0+test_predictions_resampled_cv1+test_predictions_resampled_cv2)/3\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_v3_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "co5wm71pQqf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv0\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_v3_all_data_ext_00.csv\")\n",
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv1\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_v3_all_data_ext_01.csv\")\n",
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv2\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_v3_all_data_ext_02.csv\")"
      ],
      "metadata": {
        "id": "pdpA5YeeSRBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 LSTM-Tab with Transformer v2 - Last Work"
      ],
      "metadata": {
        "id": "y6YHvUanXJR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "id": "mMYHGyUQXJR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_cnn_v5(input_shape, metrics=None, filters=32, kernel_size=3, kr=0.01, sdo=0.2,\n",
        "                    units_dense=128, gn=0.025, activation=\"relu\", do=0.3, strides=2,\n",
        "                      gru_units = 64):\n",
        "    \"\"\"\n",
        "    Improved 1D CNN model - Strided Convolutions instead of MaxPooling.\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['accuracy']\n",
        "\n",
        "    data = keras.layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "    data_noised = keras.layers.GaussianNoise(stddev=gn, name=\"noise_layer\")(data)\n",
        "    data_tabular = data_noised[:, 6, :]\n",
        "\n",
        "    ############################# Convolutional Section #############################\n",
        "    # --- Convolutional Block 1 ---\n",
        "    cnn_out = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides,  # Stride = 2\n",
        "                                    kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                    padding=\"same\", activation=None, name=\"cnn_1\")(data_noised)\n",
        "    cnn_out = keras.layers.LayerNormalization(name=\"ln_1\")(cnn_out)\n",
        "    cnn_out = keras.layers.Activation(\"relu\", name=\"act_1\")(cnn_out)\n",
        "    cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_1\")(cnn_out)\n",
        "    # NO MaxPooling\n",
        "\n",
        "    # --- Convolutional Block 2 ---\n",
        "    cnn_out = keras.layers.Conv1D(filters=filters * 2, kernel_size=kernel_size, strides=strides,  # Stride = 2\n",
        "                                    kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                    padding=\"same\", activation=None, name=\"cnn_2\")(cnn_out)\n",
        "    cnn_out = keras.layers.LayerNormalization(name=\"ln_2\")(cnn_out)\n",
        "    cnn_out = keras.layers.Activation(\"relu\", name=\"act_2\")(cnn_out)\n",
        "    cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_2\")(cnn_out)\n",
        "    # NO MaxPooling\n",
        "\n",
        "   # --- Convolutional Block 3 ---\n",
        "    cnn_out = keras.layers.Conv1D(filters=filters * 4, kernel_size=kernel_size, strides=strides,  # Stride = 2\n",
        "                                kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                padding=\"same\", activation=None, name=\"cnn_3\")(cnn_out)\n",
        "    cnn_out = keras.layers.LayerNormalization(name=\"ln_3\")(cnn_out)\n",
        "    cnn_out = keras.layers.Activation(\"relu\", name=\"act_3\")(cnn_out)\n",
        "    cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_3\")(cnn_out)\n",
        "    # NO MaxPooling\n",
        "\n",
        "    # --- Pooling and Concatenation ---\n",
        "    cnn_out_ave = keras.layers.GlobalAveragePooling1D(name=\"average_pool_final\")(cnn_out)\n",
        "    cnn_out_max = keras.layers.GlobalMaxPooling1D(name=\"max_pool_final\")(cnn_out)\n",
        "    x_conv = keras.layers.Concatenate(name=\"concat\")([cnn_out_ave, cnn_out_max])\n",
        "\n",
        "    ############################# Recursive Section #############################\n",
        "    whole_seq_output, final_memory_state_00 = keras.layers.GRU(gru_units, return_sequences=True, name=\"gru_00\",return_state=True,recurrent_dropout=do,kernel_regularizer=keras.regularizers.l2(kr))(data_noised)\n",
        "    whole_seq_output, final_memory_state_01 = keras.layers.GRU(gru_units, return_sequences=True, name=\"gru_01\",return_state=True,recurrent_dropout=do,kernel_regularizer=keras.regularizers.l2(kr))(whole_seq_output)\n",
        "    whole_seq_output, final_memory_state_10 = keras.layers.GRU(int(gru_units/2), name=\"gru_1\",return_state=True,recurrent_dropout=do,kernel_regularizer=keras.regularizers.l2(kr))(whole_seq_output)\n",
        "\n",
        "    x_states = keras.layers.Concatenate(name=\"concat_recursive_states\")([final_memory_state_00, final_memory_state_01, final_memory_state_10])\n",
        "    ############################# Final Dense Section #############################\n",
        "    # --- Dense Layer ---\n",
        "\n",
        "    x = keras.layers.Concatenate(name=\"concat_dense\")([x_conv, whole_seq_output,x_states,data_tabular])\n",
        "    x = keras.layers.Dense(units_dense, name=\"dense_1\", kernel_regularizer=keras.regularizers.l2(kr))(x)\n",
        "    x = keras.layers.BatchNormalization(name=\"batch_dense\")(x)\n",
        "    x = keras.layers.Activation(activation, name=\"act_dense\")(x)\n",
        "    x = keras.layers.Dropout(do, name=\"do_dense\")(x)\n",
        "\n",
        "    # --- Output Layer ---\n",
        "    outputs = keras.layers.Dense(1, activation='sigmoid', name=\"output\")(x)\n",
        "\n",
        "    # --- Model Creation and Compilation ---\n",
        "    model = keras.Model(inputs=data, outputs=outputs, name=\"cnn_v4\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model= make_model_cnn_v5(input_shape=(inputs.shape[1], inputs.shape[2]),strides=2, metrics=METRICS)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "eJ1RDOtiXJR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "gv-8ftc0XJR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 00"
      ],
      "metadata": {
        "id": "TzrBM-eVXJR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v5, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),#\n",
        "              'strides': trial.suggest_categorical('strides', [1,2]),#\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),#\n",
        "              'gru_units': trial.suggest_categorical('gru_units', [128, 64]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "A3zoETOLXJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "qITj_OZDXJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model_cnn_v5, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "Rt0RMs4sXJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "XkhCrc-bXJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 66 with value: 0.8993543351780899\n",
        "    * {'kernel_size': 2, 'strides': 2, 'units_dense': 256, 'gru_units': 64, 'sdo': 0.32,\n",
        "       'kr': 0.0028, 'activation': 'leaky_relu', 'gn': 0.09, 'do': 0.27}\n"
      ],
      "metadata": {
        "id": "qWWhOy4rXJR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 01"
      ],
      "metadata": {
        "id": "a8QzeR-WXJR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v5, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),#\n",
        "              'strides': trial.suggest_categorical('strides', [1,2]),#\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),#\n",
        "              'gru_units': trial.suggest_categorical('gru_units', [128, 64]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v1, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "_NvWqoEtXJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "Ldh_enRvXJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v1,validation_data=dataset_validation_v1, model_class=make_model_cnn_v5, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "vgmXZ23KXJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "ao1J7ldTXJR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 4 with value: 0.9028423172242874.\n",
        "    * {'kernel_size': 2,  'strides': 2, 'units_dense': 64, 'gru_units': 64, 'sdo': 0.4, 'kr': 0.0282, 'activation': 'elu', 'gn': 0.03,'do': 0.29}\n"
      ],
      "metadata": {
        "id": "bMfHZgnMXJR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 02"
      ],
      "metadata": {
        "id": "r0zXtpj2XJR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v5, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),#\n",
        "              'strides': trial.suggest_categorical('strides', [1,2]),#\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),#\n",
        "              'gru_units': trial.suggest_categorical('gru_units', [128, 64]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v2, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "tXH_8tpcXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "Rryii_0RXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v2,validation_data=dataset_validation_v2, model_class=make_model_cnn_v5, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "4nTGo2dvXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "DuZ9dF1tXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 4 with value: 0.8861020235066037\n",
        "    * {'kernel_size': 2, 'strides': 1, 'units_dense': 128, 'gru_units': 128, 'sdo': 0.23, 'kr': 0.0103, 'activation': 'elu', 'gn': 0.09, 'do': 0.32}\n"
      ],
      "metadata": {
        "id": "oB3Edf2hXJR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 00"
      ],
      "metadata": {
        "id": "NoAHDBN2XJR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv0= make_model_cnn_v5(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'kernel_size': 2, 'strides': 2, 'units_dense': 256, 'gru_units': 64, 'sdo': 0.32, 'kr': 0.0028, 'activation': 'leaky_relu', 'gn': 0.09, 'do': 0.27})\n",
        "model_cv0.summary()"
      ],
      "metadata": {
        "id": "YoUBV31eXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "vsCH63f8XJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_history_v0 = model_cv0.fit(\n",
        "                                      resampled_ds,\n",
        "                                      epochs=2201,\n",
        "                                      steps_per_epoch=52,\n",
        "                                      callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001),\n",
        "                                                keras.callbacks.EarlyStopping(patience=101, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                              start_from_epoch=3, mode=\"max\")],\n",
        "                                      validation_data=dataset_validation\n",
        "                                      )"
      ],
      "metadata": {
        "id": "o-h35kBoXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv0.save('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_00.keras')\n",
        "#model_cv0 = keras.models.load_model('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_00.keras')\n",
        "plot_metrics(resampled_history_v0)"
      ],
      "metadata": {
        "id": "OK6F1ACWXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "u768jeutXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 01"
      ],
      "metadata": {
        "id": "XdfAeG1GXJR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "HI1o08QGXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv1= make_model_cnn_v5(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'kernel_size': 2, 'strides': 2, 'units_dense': 64, 'gru_units': 64, 'sdo': 0.4, 'kr': 0.0282, 'activation': 'elu', 'gn': 0.03,'do': 0.29})\n",
        "\n",
        "resampled_history_v1 = model_cv1.fit(\n",
        "                              resampled_ds_v1,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=151, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v1\n",
        "                              )\n",
        "\n",
        "model_cv1.evaluate(dataset_validation_v1)"
      ],
      "metadata": {
        "id": "_1z54X6yXJR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv1.save('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_01.keras')\n",
        "#model_cv1 = keras.models.load_model('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_01.keras')\n",
        "plot_metrics(resampled_history_v1)"
      ],
      "metadata": {
        "id": "yGDOcvWNXJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 02"
      ],
      "metadata": {
        "id": "3Y6FzXTDXJR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "AejLI7ASXJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv2= make_model_cnn_v5(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'kernel_size': 2, 'strides': 1, 'units_dense': 128, 'gru_units': 128, 'sdo': 0.23, 'kr': 0.0103, 'activation': 'elu', 'gn': 0.09, 'do': 0.32})\n",
        "\n",
        "resampled_history_v2 = model_cv2.fit(\n",
        "                              resampled_ds_v2,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=151, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v2\n",
        "                              )\n",
        "\n",
        "model_cv2.evaluate(dataset_validation_v2)"
      ],
      "metadata": {
        "id": "u1FEEKTYXJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv2.save('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_02.keras')\n",
        "#model_cv2 = keras.models.load_model('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_02.keras')\n",
        "plot_metrics(resampled_history_v2)"
      ],
      "metadata": {
        "id": "-PKJAopnXJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "fkeu4-guXJR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 00**"
      ],
      "metadata": {
        "id": "u7OpnAfOXJR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv0 = model_cv0.predict(dataset_train)\n",
        "valid_predictions_resampled_cv0 = model_cv0.predict(dataset_validation)\n",
        "test_predictions_resampled_cv0 = model_cv0.predict(dataset_test)"
      ],
      "metadata": {
        "id": "mTcEnf_9XJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 01**"
      ],
      "metadata": {
        "id": "OSTeklbbXJR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv1 = model_cv1.predict(dataset_train_v1)\n",
        "valid_predictions_resampled_cv1 = model_cv1.predict(dataset_validation_v1)\n",
        "test_predictions_resampled_cv1 = model_cv1.predict(dataset_test_v1)"
      ],
      "metadata": {
        "id": "rvkuA2zwXJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 02**"
      ],
      "metadata": {
        "id": "uU9aL7V3XJR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv2 = model_cv2.predict(dataset_train_v2)\n",
        "valid_predictions_resampled_cv2 = model_cv2.predict(dataset_validation_v2)\n",
        "test_predictions_resampled_cv2 = model_cv2.predict(dataset_test_v2)"
      ],
      "metadata": {
        "id": "bjkq5r-bXJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
        "ax[0].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv1)\n",
        "ax[1].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IJXxNsrcXJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled_cv0)"
      ],
      "metadata": {
        "id": "WaRYqmS9XJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid_v1, valid_predictions_resampled_cv1)"
      ],
      "metadata": {
        "id": "GBF29wc8XJR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid_v2, valid_predictions_resampled_cv2)"
      ],
      "metadata": {
        "id": "UqxlD8aRXJR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_cm(y_valid_v1, dataset_validation_v1)\n",
        "test_predictions_resampled_cv0.shape, test_predictions_resampled_cv1.shape, test_predictions_resampled_cv2.shape"
      ],
      "metadata": {
        "id": "FykpGubkXJR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = (test_predictions_resampled_cv0+test_predictions_resampled_cv1+test_predictions_resampled_cv2)/3\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_lstm_v0_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "80B0sxpYXJR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv0\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_lstm_v0_all_data_ext_00.csv\")\n",
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv1\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_lstm_v0_all_data_ext_01.csv\")\n",
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv2\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_lstm_v0_all_data_ext_02.csv\")"
      ],
      "metadata": {
        "id": "KssyNngCXJR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 CNN-Tab with Transformer v3 - Last Work"
      ],
      "metadata": {
        "id": "ZjVYImLwj_qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "id": "Gjik7iC0j_qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_cnn_v6(input_shape, metrics=None, filters=32, kernel_size=3, kr=0.01, sdo=0.2,\n",
        "                    units_dense=128, gn=0.025, activation=\"relu\", do=0.3, strides=2):\n",
        "    \"\"\"\n",
        "    Improved 1D CNN model - Strided Convolutions instead of MaxPooling.\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['accuracy']\n",
        "\n",
        "    data = keras.layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "    data_noised = keras.layers.GaussianNoise(stddev=gn, name=\"noise_layer\")(data)\n",
        "    data_tabular = data_noised[:, 6, :]\n",
        "\n",
        "    ############################# Convolutional Section #############################\n",
        "    # --- Convolutional Block 1 ---\n",
        "    cnn_out = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides,  # Stride = 2\n",
        "                                    kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                    padding=\"same\", activation=None, name=\"cnn_1\")(data_noised)\n",
        "    cnn_out = keras.layers.LayerNormalization(name=\"ln_1\")(cnn_out)\n",
        "    cnn_out = keras.layers.Activation(\"relu\", name=\"act_1\")(cnn_out)\n",
        "    cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_1\")(cnn_out)\n",
        "    # MaxPooling\n",
        "    cnn_out_01 = keras.layers.GlobalMaxPooling1D(name=\"max_pool_1\")(cnn_out)\n",
        "\n",
        "    # --- Convolutional Block 2 ---\n",
        "    cnn_out = keras.layers.Conv1D(filters=filters * 2, kernel_size=kernel_size, strides=strides,  # Stride = 2\n",
        "                                    kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                    padding=\"same\", activation=None, name=\"cnn_2\")(cnn_out)\n",
        "    cnn_out = keras.layers.LayerNormalization(name=\"ln_2\")(cnn_out)\n",
        "    cnn_out = keras.layers.Activation(\"relu\", name=\"act_2\")(cnn_out)\n",
        "    cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_2\")(cnn_out)\n",
        "    # MaxPooling\n",
        "    cnn_out_02 = keras.layers.GlobalMaxPooling1D(name=\"max_pool_2\")(cnn_out)\n",
        "\n",
        "   # --- Convolutional Block 3 ---\n",
        "    cnn_out = keras.layers.Conv1D(filters=filters * 4, kernel_size=kernel_size, strides=strides,  # Stride = 2\n",
        "                                kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                padding=\"same\", activation=None, name=\"cnn_3\")(cnn_out)\n",
        "    cnn_out = keras.layers.LayerNormalization(name=\"ln_3\")(cnn_out)\n",
        "    cnn_out = keras.layers.Activation(\"relu\", name=\"act_3\")(cnn_out)\n",
        "    cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_3\")(cnn_out)\n",
        "    # MaxPooling\n",
        "    cnn_out_03 = keras.layers.GlobalMaxPooling1D(name=\"max_pool_3\")(cnn_out)\n",
        "\n",
        "    # --- Pooling and Concatenation ---\n",
        "    cnn_out_ave = keras.layers.GlobalAveragePooling1D(name=\"average_pool_final\")(cnn_out)\n",
        "    cnn_out_max = keras.layers.GlobalMaxPooling1D(name=\"max_pool_final\")(cnn_out)\n",
        "    x_conv = keras.layers.Concatenate(name=\"concat\")([cnn_out_ave, cnn_out_max])\n",
        "\n",
        "    x = keras.layers.Concatenate(name=\"concat_dense\")([x_conv, cnn_out_03,cnn_out_02,cnn_out_01,data_tabular])\n",
        "    x = keras.layers.Dense(units_dense, name=\"dense_1\", kernel_regularizer=keras.regularizers.l2(kr))(x)\n",
        "    x = keras.layers.BatchNormalization(name=\"batch_dense\")(x)\n",
        "    x = keras.layers.Activation(activation, name=\"act_dense\")(x)\n",
        "    x = keras.layers.Dropout(do, name=\"do_dense\")(x)\n",
        "\n",
        "    # --- Output Layer ---\n",
        "    outputs = keras.layers.Dense(1, activation='sigmoid', name=\"output\")(x)\n",
        "\n",
        "    # --- Model Creation and Compilation ---\n",
        "    model = keras.Model(inputs=data, outputs=outputs, name=\"cnn_v6\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model= make_model_cnn_v6(input_shape=(inputs.shape[1], inputs.shape[2]),strides=2, metrics=METRICS)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "j9jders5j_qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "1uHgJuRcj_qe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 00"
      ],
      "metadata": {
        "id": "2Hw3ZOsvj_qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v6, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),#\n",
        "              'strides': trial.suggest_categorical('strides', [1,2]),#\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "wyy4fIREj_qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), study_name=\"resid_cnn\")\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "P94Uhn-qj_qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model_cnn_v6, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "sU0D0CTmj_qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "dUTe8HLKj_qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 66 with value: 0.8943522066127431\n",
        "    * {'kernel_size': 3,\n",
        " 'strides': 1,\n",
        " 'units_dense': 256,\n",
        " 'sdo': 0.38,\n",
        " 'kr': 0.05689652604828707,\n",
        " 'activation': 'silu',\n",
        " 'gn': 0.060000000000000005,\n",
        " 'do': 0.23}\n"
      ],
      "metadata": {
        "id": "d5ftfGn-j_qf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 01"
      ],
      "metadata": {
        "id": "7haJ12_bj_qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v6, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),#\n",
        "              'strides': trial.suggest_categorical('strides', [1,2]),#\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v1, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "ovxdL5v8j_qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "2CfQiH7Mj_qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v1,validation_data=dataset_validation_v1, model_class=make_model_cnn_v6, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "EMS5xwZRj_qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "JZ4y8FuZj_qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 4 with value: 0.8825283018867924.\n",
        "    * {'kernel_size': 3,\n",
        " 'strides': 2,\n",
        " 'units_dense': 128,\n",
        " 'sdo': 0.30000000000000004,\n",
        " 'kr': 0.02412665758667391,\n",
        " 'activation': 'elu',\n",
        " 'gn': 0.03,\n",
        " 'do': 0.36}\n"
      ],
      "metadata": {
        "id": "IfbUaEtWj_qg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 02"
      ],
      "metadata": {
        "id": "V4BxTojVj_qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v6, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),#\n",
        "              'strides': trial.suggest_categorical('strides', [1,2]),#\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v2, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "ktSGUokFj_qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "vp7KwaOIj_qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v2,validation_data=dataset_validation_v2, model_class=make_model_cnn_v6, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "D7Zk3DQCj_qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "z63-3-4Jj_qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 4 with value: 0.8991679793206511\n",
        "    * {'kernel_size': 2,\n",
        " 'strides': 1,\n",
        " 'units_dense': 128,\n",
        " 'sdo': 0.42,\n",
        " 'kr': 0.048,\n",
        " 'activation': 'silu',\n",
        " 'gn': 0.03,\n",
        " 'do': 0.38}\n"
      ],
      "metadata": {
        "id": "0iqxsYOfj_qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 00"
      ],
      "metadata": {
        "id": "yrCO4tMmj_qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv0= make_model_cnn_v6(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'kernel_size': 3, 'strides': 1, 'units_dense': 256, 'sdo': 0.38, 'kr': 0.057, 'activation': 'silu', 'gn': 0.06, 'do': 0.23})\n",
        "model_cv0.summary()"
      ],
      "metadata": {
        "id": "tgAE2Ajoj_qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "qBNhMfmXj_qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_history_v0 = model_cv0.fit(\n",
        "                                      resampled_ds,\n",
        "                                      epochs=2201,\n",
        "                                      steps_per_epoch=52,\n",
        "                                      callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001),\n",
        "                                                keras.callbacks.EarlyStopping(patience=101, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                              start_from_epoch=3, mode=\"max\")],\n",
        "                                      validation_data=dataset_validation\n",
        "                                      )"
      ],
      "metadata": {
        "id": "kj7YtMGOj_qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv0.save('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_resnet_v0_00.keras')\n",
        "#model_cv0 = keras.models.load_model('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_00.keras')\n",
        "plot_metrics(resampled_history_v0)"
      ],
      "metadata": {
        "id": "-Mzh5OWUj_qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "4cu5YqGjj_qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 01"
      ],
      "metadata": {
        "id": "yyc-yKTGj_qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "8ScVlGxJj_qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv1= make_model_cnn_v6(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'kernel_size': 3, 'strides': 2, 'units_dense': 128, 'sdo': 0.30, 'kr': 0.024, 'activation': 'elu', 'gn': 0.03, 'do': 0.36})\n",
        "\n",
        "resampled_history_v1 = model_cv1.fit(\n",
        "                              resampled_ds_v1,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=151, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v1\n",
        "                              )\n",
        "\n",
        "model_cv1.evaluate(dataset_validation_v1)"
      ],
      "metadata": {
        "id": "AEYQ65Rbj_qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv1.save('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_resnet_v0_01.keras')\n",
        "#model_cv1 = keras.models.load_model('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_01.keras')\n",
        "plot_metrics(resampled_history_v1)"
      ],
      "metadata": {
        "id": "kEk4i3g-j_qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 02"
      ],
      "metadata": {
        "id": "BHqTpmm-j_qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "rcSEVaUGj_ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv2= make_model_cnn_v6(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'kernel_size': 2, 'strides': 1, 'units_dense': 128, 'sdo': 0.42, 'kr': 0.048, 'activation': 'silu', 'gn': 0.03, 'do': 0.38})\n",
        "\n",
        "resampled_history_v2 = model_cv2.fit(\n",
        "                              resampled_ds_v2,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=151, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v2\n",
        "                              )\n",
        "\n",
        "model_cv2.evaluate(dataset_validation_v2)"
      ],
      "metadata": {
        "id": "IRuRRpwTj_ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv2.save('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_resnet_v0_03.keras')\n",
        "#model_cv2 = keras.models.load_model('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_02.keras')\n",
        "plot_metrics(resampled_history_v2)"
      ],
      "metadata": {
        "id": "HFZPM7wEj_ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "rl7tDK3wj_ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 00**"
      ],
      "metadata": {
        "id": "BLad1cHej_qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv0 = model_cv0.predict(dataset_train)\n",
        "valid_predictions_resampled_cv0 = model_cv0.predict(dataset_validation)\n",
        "test_predictions_resampled_cv0 = model_cv0.predict(dataset_test)"
      ],
      "metadata": {
        "id": "HuHca-WIj_qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 01**"
      ],
      "metadata": {
        "id": "Mc2FBrAfj_qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv1 = model_cv1.predict(dataset_train_v1)\n",
        "valid_predictions_resampled_cv1 = model_cv1.predict(dataset_validation_v1)\n",
        "test_predictions_resampled_cv1 = model_cv1.predict(dataset_test_v1)"
      ],
      "metadata": {
        "id": "GJQs5tajj_qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 02**"
      ],
      "metadata": {
        "id": "rRecNih6j_qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv2 = model_cv2.predict(dataset_train_v2)\n",
        "valid_predictions_resampled_cv2 = model_cv2.predict(dataset_validation_v2)\n",
        "test_predictions_resampled_cv2 = model_cv2.predict(dataset_test_v2)"
      ],
      "metadata": {
        "id": "-E9KBBy3j_qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
        "ax[0].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv1)\n",
        "ax[1].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jPYDWOtIj_qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled_cv0)"
      ],
      "metadata": {
        "id": "hCog-yEwj_qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid_v1, valid_predictions_resampled_cv1)"
      ],
      "metadata": {
        "id": "ruhp45h1j_qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid_v2, valid_predictions_resampled_cv2)"
      ],
      "metadata": {
        "id": "UTTZjtu0j_qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_cm(y_valid_v1, dataset_validation_v1)\n",
        "test_predictions_resampled_cv0.shape, test_predictions_resampled_cv1.shape, test_predictions_resampled_cv2.shape"
      ],
      "metadata": {
        "id": "3oLO5Js0j_qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = (test_predictions_resampled_cv0+test_predictions_resampled_cv1+test_predictions_resampled_cv2)/3\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_resnet_v0_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "h-UbGFrEj_qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv0\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_resnet_v0_all_data_ext_00.csv\")\n",
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv1\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_resnet_v0_all_data_ext_01.csv\")\n",
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv2\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_resnet_v0_all_data_ext_02.csv\")"
      ],
      "metadata": {
        "id": "WRul-fXxj_qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 LSTM-ResNet"
      ],
      "metadata": {
        "id": "oWgKrHR6x5e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.shape"
      ],
      "metadata": {
        "id": "uTVDsmRhydVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_cnn_v7(input_shape, metrics=None, units=32, kr=0.01, sdo=0.2,\n",
        "                      units_dense=128, gn=0.025, activation=\"relu\", do=0.3, kr_gru=0.01):\n",
        "    \"\"\"\n",
        "    Improved 1D CNN model - Strided Convolutions instead of MaxPooling.\n",
        "    \"\"\"\n",
        "    if metrics is None:\n",
        "        metrics = ['accuracy']\n",
        "\n",
        "    data = keras.layers.Input(shape=input_shape, name=\"input_layer\")\n",
        "    data_noised = keras.layers.GaussianNoise(stddev=gn, name=\"noise_layer\")(data)\n",
        "    data_tabular = data_noised[:, 6, :]\n",
        "\n",
        "    ############################# RNN Section #############################\n",
        "    # ------------------ GRU Block 1 ------------------\n",
        "    gru_out = keras.layers.GRU(units, return_sequences=True, name=\"gru_1\",return_state=False, dropout=sdo,\n",
        "                               #kernel_regularizer = keras.regularizers.l2(kr_gru)\n",
        "                               )(data_noised)\n",
        "    gru_out = keras.layers.LayerNormalization(name=\"ln_1\")(gru_out)\n",
        "    # ResNet Connection\n",
        "    resnet_01 = keras.layers.Dense(units,name=\"dense_resnet_01\")(data_noised)\n",
        "    resnet_01 = keras.layers.Add(name=\"add_1\")([gru_out, resnet_01])\n",
        "\n",
        "    # Layer for Concat\n",
        "    resnet_01_flat = keras.layers.Dense(units=1, kernel_regularizer=keras.regularizers.l2(kr),name=\"dense_flat_resnet_01\")(resnet_01)\n",
        "    resnet_01_flat = keras.layers.Reshape((7,), name=\"flat_resnet_01\")(resnet_01_flat)\n",
        "    resnet_01_flat = keras.layers.BatchNormalization(name=\"batch_flat_resnet_01\")(resnet_01_flat)\n",
        "    resnet_01_flat = keras.layers.Activation(activation, name=\"act_flat_resnet_01\")(resnet_01_flat)\n",
        "\n",
        "\n",
        "    # ------------------ GRU Block 2 ------------------\n",
        "    gru_out = keras.layers.GRU(units*2, return_sequences=True, name=\"gru_2\",return_state=False, dropout=sdo,\n",
        "                               #kernel_regularizer = keras.regularizers.l2(kr_gru)\n",
        "                               )(resnet_01)\n",
        "    gru_out = keras.layers.LayerNormalization(name=\"ln_2\")(gru_out)\n",
        "    # ResNet Connection\n",
        "    resnet_02 = keras.layers.Dense(units*2,name=\"dense_resnet_02\")(resnet_01)\n",
        "    resnet_02 = keras.layers.Add(name=\"add_2\")([gru_out, resnet_02])\n",
        "\n",
        "    # Layer for Concat\n",
        "    resnet_02_flat = keras.layers.Dense(units=1, kernel_regularizer=keras.regularizers.l2(kr),name=\"dense_flat_resnet_02\")(resnet_02)\n",
        "    resnet_02_flat = keras.layers.Reshape((7,),name=\"flat_resnet_02\")(resnet_02_flat)\n",
        "    resnet_02_flat = keras.layers.BatchNormalization(name=\"batch_flat_resnet_02\")(resnet_02_flat)\n",
        "    resnet_02_flat = keras.layers.Activation(activation, name=\"act_flat_resnet_02\")(resnet_02_flat)\n",
        "\n",
        "\n",
        "    # ------------------ GRU Block 3 ------------------\n",
        "    gru_out = keras.layers.GRU(units*3, return_sequences=False, name=\"gru_3\",return_state=False, dropout=sdo,\n",
        "                               kernel_regularizer = keras.regularizers.l2(kr_gru)\n",
        "                               )(resnet_02)\n",
        "    gru_out = keras.layers.LayerNormalization(name=\"ln_3\")(gru_out)\n",
        "\n",
        "\n",
        "    # --- Pooling and Concatenation ---\n",
        "    x = keras.layers.Concatenate(name=\"concat_dense\")([resnet_01_flat, resnet_02_flat,gru_out,data_tabular])\n",
        "    x = keras.layers.Dense(units_dense, name=\"dense_1\", kernel_regularizer=keras.regularizers.l2(kr))(x)\n",
        "    x = keras.layers.BatchNormalization(name=\"batch_dense\")(x)\n",
        "    x = keras.layers.Activation(activation, name=\"act_dense\")(x)\n",
        "    x = keras.layers.Dropout(do, name=\"do_dense\")(x)\n",
        "\n",
        "    # --- Output Layer ---\n",
        "    outputs = keras.layers.Dense(1, activation='sigmoid', name=\"output\")(x)\n",
        "\n",
        "    # --- Model Creation and Compilation ---\n",
        "    model = keras.Model(inputs=data, outputs=outputs, name=\"cnn_v4\")\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
        "        loss=keras.losses.BinaryCrossentropy(),\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model= make_model_cnn_v7(input_shape=(inputs.shape[1], inputs.shape[2]),metrics=METRICS)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "QWmkG2Utydd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for a,b in resampled_ds.take(1):\n",
        "#   print(a.shape,b.shape)\n",
        "\n",
        "# model.predict(a)"
      ],
      "metadata": {
        "id": "WgMuLyZaQj7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "arXcaFa3O2Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 00"
      ],
      "metadata": {
        "id": "2-Fo_3D5O2Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v7, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),#\n",
        "              'units': trial.suggest_categorical('units', [96, 64, 32]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'kr_gru': trial.suggest_float('kr_gru', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    try:\n",
        "      # Fit the model\n",
        "      model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                        keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                      start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "      # Make predictions on the validation set\n",
        "      y_pred = model.predict(validation_data)\n",
        "\n",
        "      # Calculate the RMSE for the current fold\n",
        "      rmse_scores = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "    except ValueError as e:  # or any other relevant exception type\n",
        "      # Handle the case where the model is invalid, e.g., due to shape issues\n",
        "      print(f\"Trial failed due to ValueError: {e}\")\n",
        "      print(f\"Hyperparameters: {trial.params}\")  # Log the parameters for debugging\n",
        "      rmse_scores = float('inf')  # or any appropriate penalty value\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "4vzCRbkFO2Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "vGiL5PURO2Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model_cnn_v7, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "YtHldMecO2Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Score: 0.9018731375053213\n",
        "\n",
        "{'units_dense': 256,\n",
        " 'units': 64,\n",
        " 'sdo': 0.44,\n",
        " 'kr': 0.0019,\n",
        " 'kr_gru': 0.017,\n",
        " 'activation': 'leaky_relu',\n",
        " 'gn': 0.03,\n",
        " 'do': 0.37}"
      ],
      "metadata": {
        "id": "i9dLjK6kx4NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 01"
      ],
      "metadata": {
        "id": "7RO37nEXoJL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v7, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),#\n",
        "              'units': trial.suggest_categorical('units', [96, 64, 32]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'kr_gru': trial.suggest_float('kr_gru', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v1, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "dIL0lU8HoJL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "8wowjvv3oJL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v1,validation_data=dataset_validation_v1, model_class=make_model_cnn_v7, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "nXD90NFToJL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "R_KgpWJqoJL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best: 0.9119191919191919\n",
        "\n",
        "{'units_dense': 256,\n",
        " 'units': 96,\n",
        " 'sdo': 0.22,\n",
        " 'kr': 0.0022,\n",
        " 'kr_gru': 0.0381,\n",
        " 'activation': 'elu',\n",
        " 'gn': 0.08,\n",
        " 'do': 0.4}"
      ],
      "metadata": {
        "id": "k-8U5zFMoJL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 02"
      ],
      "metadata": {
        "id": "xAbuwwivoJL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn_v7, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),#\n",
        "              'units': trial.suggest_categorical('units', [96, 64, 32]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),#\n",
        "              'kr': trial.suggest_float('kr', 0.001, 0.1, log=True),#\n",
        "              'kr_gru': trial.suggest_float('kr_gru', 0.001, 0.1, log=True),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"elu\",\"gelu\",\"silu\",\"leaky_relu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v2, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "J7E5fmRooJL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "VRDwaD5EoJL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v2,validation_data=dataset_validation_v2, model_class=make_model_cnn_v7, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "pvHQCy8RoJL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_params"
      ],
      "metadata": {
        "id": "MJiNyJ7eoJL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best:0.8685124601155134.\n",
        "\n",
        "{'units_dense': 256,\n",
        " 'units': 96,\n",
        " 'sdo': 0.28,\n",
        " 'kr': 0.06505678914953572,\n",
        " 'kr_gru': 0.026089087169453223,\n",
        " 'activation': 'gelu',\n",
        " 'gn': 0.04,\n",
        " 'do': 0.33}"
      ],
      "metadata": {
        "id": "R7x3vFbsmZn9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 00"
      ],
      "metadata": {
        "id": "d_azH4PEQ23t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv0= make_model_cnn_v7(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'units_dense': 256,\n",
        "                                                                                                 'units': 64,\n",
        "                                                                                                 'sdo': 0.44,\n",
        "                                                                                                 'kr': 0.0019,\n",
        "                                                                                                 'kr_gru': 0.017,\n",
        "                                                                                                 'activation': 'leaky_relu',\n",
        "                                                                                                 'gn': 0.03,\n",
        "                                                                                                 'do': 0.37})\n",
        "model_cv0.summary()"
      ],
      "metadata": {
        "id": "9rZfs0cGQ23u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "__f182toQ23v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_history_v0 = model_cv0.fit(\n",
        "                                      resampled_ds,\n",
        "                                      epochs=2201,\n",
        "                                      steps_per_epoch=52,\n",
        "                                      callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001),\n",
        "                                                keras.callbacks.EarlyStopping(patience=101, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                              start_from_epoch=3, mode=\"max\")],\n",
        "                                      validation_data=dataset_validation\n",
        "                                      )"
      ],
      "metadata": {
        "id": "qqOUhjeFQ23v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv0.save('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/lstm_resnet_v0_00.keras')\n",
        "#model_cv0 = keras.models.load_model('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_00.keras')\n",
        "plot_metrics(resampled_history_v0)"
      ],
      "metadata": {
        "id": "g_58rplRQ23w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "Vx1IV3jEQ23w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 01"
      ],
      "metadata": {
        "id": "yZN7ks2VQ23w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "xKH_FYqjQ23w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv1= make_model_cnn_v7(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'units_dense': 256,\n",
        "                                                                                                 'units': 96,\n",
        "                                                                                                 'sdo': 0.22,\n",
        "                                                                                                 'kr': 0.0022,\n",
        "                                                                                                 'kr_gru': 0.0381,\n",
        "                                                                                                 'activation': 'elu',\n",
        "                                                                                                 'gn': 0.08,\n",
        "                                                                                                 'do': 0.4})\n",
        "\n",
        "resampled_history_v1 = model_cv1.fit(\n",
        "                              resampled_ds_v1,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=151, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v1\n",
        "                              )\n",
        "\n",
        "model_cv1.evaluate(dataset_validation_v1)"
      ],
      "metadata": {
        "id": "fYrsi1DdQ23w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv1.save('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/lstm_resnet_v0_01.keras')\n",
        "#model_cv1 = keras.models.load_model('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_01.keras')\n",
        "plot_metrics(resampled_history_v1)"
      ],
      "metadata": {
        "id": "Ml0lUhwVQ23x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model 02"
      ],
      "metadata": {
        "id": "YCK0SrrwQ23x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "43Sx6tLHQ23x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv2= make_model_cnn_v7(input_shape=(inputs.shape[1], inputs.shape[2]), metrics=METRICS, **{'units_dense': 256, 'units': 96, 'sdo': 0.28, 'kr': 0.0651, 'kr_gru': 0.0261, 'activation': 'gelu', 'gn': 0.04, 'do': 0.33})\n",
        "\n",
        "resampled_history_v2 = model_cv2.fit(\n",
        "                              resampled_ds_v2,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=151, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v2\n",
        "                              )\n",
        "\n",
        "model_cv2.evaluate(dataset_validation_v2)"
      ],
      "metadata": {
        "id": "bPN5rS2HQ23x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv2.save('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/lstm_resnet_v0_02.keras')\n",
        "#model_cv2 = keras.models.load_model('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/cnn_lstm_v2_02.keras')\n",
        "plot_metrics(resampled_history_v2)"
      ],
      "metadata": {
        "id": "WOO8nOooQ23y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "qGyEsMPtQ23y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 00**"
      ],
      "metadata": {
        "id": "ZwwMAensQ23y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv0 = model_cv0.predict(dataset_train)\n",
        "valid_predictions_resampled_cv0 = model_cv0.predict(dataset_validation)\n",
        "test_predictions_resampled_cv0 = model_cv0.predict(dataset_test)"
      ],
      "metadata": {
        "id": "D8WLPyKAQ23y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 01**"
      ],
      "metadata": {
        "id": "ngrb2QggQ23z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv1 = model_cv1.predict(dataset_train_v1)\n",
        "valid_predictions_resampled_cv1 = model_cv1.predict(dataset_validation_v1)\n",
        "test_predictions_resampled_cv1 = model_cv1.predict(dataset_test_v1)"
      ],
      "metadata": {
        "id": "gBbCjboCQ23z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL 02**"
      ],
      "metadata": {
        "id": "GIx9AvlvQ23z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv2 = model_cv2.predict(dataset_train_v2)\n",
        "valid_predictions_resampled_cv2 = model_cv2.predict(dataset_validation_v2)\n",
        "test_predictions_resampled_cv2 = model_cv2.predict(dataset_test_v2)"
      ],
      "metadata": {
        "id": "7ls6YxD3Q23z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
        "ax[0].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv1)\n",
        "ax[1].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EgBky_EVQ230"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled_cv0)"
      ],
      "metadata": {
        "id": "wHk2F6CMQ230"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid_v1, valid_predictions_resampled_cv1)"
      ],
      "metadata": {
        "id": "_LbqOBCfQ230"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid_v2, valid_predictions_resampled_cv2)"
      ],
      "metadata": {
        "id": "TGHJzPzzQ231"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_cm(y_valid_v1, dataset_validation_v1)\n",
        "test_predictions_resampled_cv0.shape, test_predictions_resampled_cv1.shape, test_predictions_resampled_cv2.shape"
      ],
      "metadata": {
        "id": "FM9YasdrQ232"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = (test_predictions_resampled_cv0+test_predictions_resampled_cv1+test_predictions_resampled_cv2)/3\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_resnet_v0_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "sLGxOisPQ233"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv0\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_resnet_v0_all_data_ext_00.csv\")\n",
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv1\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_resnet_v0_all_data_ext_01.csv\")\n",
        "df_subm[\"rainfall\"] = test_predictions_resampled_cv2\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_lstm_resnet_v0_all_data_ext_02.csv\")"
      ],
      "metadata": {
        "id": "JVYo23OBQ233"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J-2c_wbaL0Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mvWTGBHyL0Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vxthUTh5L0X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4c8lmlwUL0ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBlgCcFOL0vT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 CNN Classifier"
      ],
      "metadata": {
        "id": "qr93g70ish6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_cnn(metrics=METRICS, filters=64, kernel_size=2, kr=0.01, sdo=0.2,\n",
        "               units_dense=256, output_bias=None, gn=0.025, activation=\"relu\", do=0.3):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "\n",
        "  data = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]), name=\"input_layer\")\n",
        "\n",
        "  # LSTM Section\n",
        "  data_noised = keras.layers.GaussianNoise(stddev=gn, name=\"noise_layer\")(data)\n",
        "  # Cnn Layers\n",
        "  # Layer 0\n",
        "  cnn_out = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=1,\n",
        "                                kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                padding=\"same\", activation=None, name=\"cnn_0\")(data_noised)\n",
        "  cnn_out = keras.layers.LayerNormalization(name=\"ln_0\")(cnn_out)\n",
        "  cnn_out = keras.layers.Activation(\"relu\", name=\"act_0\")(cnn_out)\n",
        "  cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_0\")(cnn_out)\n",
        "  cnn_out = keras.layers.MaxPooling1D(pool_size=2, name=\"max_pool_1\")(cnn_out)\n",
        "  # Layer 1\n",
        "  cnn_out = keras.layers.Conv1D(filters=int(filters*2), kernel_size=kernel_size, strides=1,\n",
        "                                kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                padding=\"same\", activation=None, name=\"cnn_1\")(cnn_out)\n",
        "  cnn_out = keras.layers.LayerNormalization(name=\"ln_1\")(cnn_out)\n",
        "  cnn_out = keras.layers.Activation(\"relu\", name=\"act_1\")(cnn_out)\n",
        "  cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_1\")(cnn_out)\n",
        "\n",
        "  #outputs\n",
        "  cnn_out_ave = keras.layers.GlobalAveragePooling1D(name=\"average_pool_final\")(cnn_out)\n",
        "  cnn_out_max = keras.layers.GlobalMaxPooling1D(name=\"max_pool_final\")(cnn_out)\n",
        "\n",
        "  x = keras.layers.Concatenate(name=\"concat\")([cnn_out_ave, cnn_out_max])\n",
        "\n",
        "  # Dense Layer\n",
        "  x = keras.layers.Dense(units_dense, name=\"dense_1\")(x)\n",
        "  x = keras.layers.BatchNormalization(name=\"batch_dense\")(x)\n",
        "  x = keras.layers.Activation(activation, name=\"act_dense\")(x)\n",
        "  x = keras.layers.Dropout(do, name=\"do_dense\")(x)\n",
        "\n",
        "  outputs = keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias, name=\"output\")(x)\n",
        "\n",
        "  model = keras.Model(inputs=data, outputs=outputs, name=\"cnn_v0\")\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=5e-4),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "model= make_model_cnn(output_bias=0)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NAY6uqY4sh6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "nXard4tgsh60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn, use_gpu=False, rs=42, fit_scaling=False, epochs=21):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'filters': trial.suggest_categorical('filters', [16,32,64]),\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),\n",
        "              'kr': trial.suggest_float('kr', 0.01, 0.1, step=0.01),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01)\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(**params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=9, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "5fV519yush60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=make_model_cnn, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "BWA1ezH0sh60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model_cnn, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "kKLPpjbfsh60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Best is trial 66 with value: 0.895310061\n",
        "    * {'filters': 64,\n",
        " 'kernel_size': 2,\n",
        " 'units_dense': 256,\n",
        " 'do': 0.39,\n",
        " 'sdo': 0.39,\n",
        " 'kr': 0.09999999999999999,\n",
        " 'activation': 'gelu',\n",
        " 'gn': 0.09999999999999999}\n"
      ],
      "metadata": {
        "id": "JRnYccLxsh61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model"
      ],
      "metadata": {
        "id": "P8iwG5gOsh61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model= make_model(units=[64,64], units_tab=[64,32], activation=\"silu\", gn=0.09, do=0.31, output_bias=0) # Model v2 (Score 0.85787)\n",
        "# model= make_model(units=[128,64], units_tab=[128,32], activation=\"selu\", gn=0.09, do=0.25, output_bias=0) # Model v1 (best 0.86323)\n",
        "model= make_model_cnn(**{'filters': 64, 'kernel_size': 2, 'units_dense': 256, 'do': 0.39, 'sdo': 0.39, 'kr': 0.1, 'activation': 'gelu', 'gn': 0.1}) # Model v\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "bemB6o2Esh61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "1Jusoosfsh61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_history = model.fit(\n",
        "                              resampled_ds,\n",
        "                              epochs=2201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001),\n",
        "                                         keras.callbacks.EarlyStopping(patience=101, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\")],\n",
        "                              validation_data=dataset_validation\n",
        "                              )"
      ],
      "metadata": {
        "id": "Cbwc3hANsh61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history)"
      ],
      "metadata": {
        "id": "7htlvlfIsh62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "ACco_smAsh62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled = model.predict(dataset_train)\n",
        "valid_predictions_resampled = model.predict(dataset_validation)\n",
        "test_predictions_resampled = model.predict(dataset_test)"
      ],
      "metadata": {
        "id": "TW9ycNw1sh62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled)"
      ],
      "metadata": {
        "id": "QA3AYwyUsh62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled)"
      ],
      "metadata": {
        "id": "R70UQzh-sh63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_resampled\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_v2_all_data_ext.csv\")"
      ],
      "metadata": {
        "id": "zlBYIK6Lsh63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 CNN Classifier - Tab v0"
      ],
      "metadata": {
        "id": "5pNUEKwx1Pr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model_cnn(metrics=METRICS, filters=64, kernel_size=2, kr=0.01, sdo=0.2,\n",
        "                   units_dense=256, output_bias=None, gn=0.025, activation=\"relu\",\n",
        "                   do=0.3, units_tab=[128,128], lr=5e-4):\n",
        "\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "\n",
        "  data = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]), name=\"input_layer\")\n",
        "  data_tabular = data[:, 6, :]\n",
        "\n",
        "  # LSTM Section\n",
        "  data_noised = keras.layers.GaussianNoise(stddev=gn, name=\"noise_layer\")(data)\n",
        "  # Cnn Layers\n",
        "  # Layer 0\n",
        "  cnn_out = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=1,\n",
        "                                kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                padding=\"same\", activation=None, name=\"cnn_0\")(data_noised)\n",
        "  cnn_out = keras.layers.LayerNormalization(name=\"ln_cnn_0\")(cnn_out)\n",
        "  cnn_out = keras.layers.Activation(\"relu\", name=\"act_cnn_0\")(cnn_out)\n",
        "  cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_cnn_0\")(cnn_out)\n",
        "  cnn_out = keras.layers.MaxPooling1D(pool_size=2, name=\"max_pool_cnn_1\")(cnn_out)\n",
        "  # Layer 1\n",
        "  cnn_out = keras.layers.Conv1D(filters=int(filters*2), kernel_size=kernel_size, strides=1,\n",
        "                                kernel_regularizer=keras.regularizers.l2(kr),\n",
        "                                padding=\"same\", activation=None, name=\"cnn_1\")(cnn_out)\n",
        "  cnn_out = keras.layers.LayerNormalization(name=\"ln_cnn_1\")(cnn_out)\n",
        "  cnn_out = keras.layers.Activation(\"relu\", name=\"act_cnn_1\")(cnn_out)\n",
        "  cnn_out = keras.layers.SpatialDropout1D(sdo, name=\"sdo_cnn_1\")(cnn_out)\n",
        "\n",
        "  #outputs\n",
        "  cnn_out_ave = keras.layers.GlobalAveragePooling1D(name=\"average_pool_final\")(cnn_out)\n",
        "  cnn_out_max = keras.layers.GlobalMaxPooling1D(name=\"max_pool_final\")(cnn_out)\n",
        "\n",
        "  x = keras.layers.Concatenate(name=\"concat\")([cnn_out_ave, cnn_out_max])\n",
        "\n",
        "  # Dense Layer\n",
        "  x = keras.layers.Dense(units_dense, name=\"dense_conv_1\")(x)\n",
        "  x = keras.layers.BatchNormalization(name=\"batch_dense_conv\")(x)\n",
        "  x = keras.layers.Activation(activation, name=\"act_dense_conv\")(x)\n",
        "  x = keras.layers.Dropout(do, name=\"do_dense_conv\")(x)\n",
        "\n",
        "  # Tabular Section\n",
        "  tabx = keras.layers.Dense(units_tab[0], name=\"dense_0\")(data_tabular)\n",
        "  tabx = keras.layers.BatchNormalization(name=\"batch_0\")(tabx)\n",
        "  tabx = keras.layers.Activation(activation, name=\"act_0\")(tabx)\n",
        "  tabx = keras.layers.Dropout(do, name=\"do_0\")(tabx)\n",
        "  tabx = keras.layers.Dense(units_tab[1], name=\"dense_1\")(tabx)\n",
        "  tabx = keras.layers.BatchNormalization(name=\"batch_1\")(tabx)\n",
        "  tabx = keras.layers.Activation(activation, name=\"act_1\")(tabx)\n",
        "  tabx = keras.layers.Dropout(do, name=\"do_1\")(tabx)\n",
        "\n",
        "  # Final Concatenation\n",
        "  x = keras.layers.Concatenate(name=\"final_concat\")([x, tabx])\n",
        "\n",
        "  outputs = keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias, name=\"output\")(x)\n",
        "\n",
        "  model = keras.Model(inputs=data, outputs=outputs, name=\"cnn_v0\")\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "model= make_model_cnn(output_bias=0)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GluxvQWv1Pr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "06sG8aEu1Pr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model_cnn, use_gpu=False, rs=42, fit_scaling=False,epochs=51):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'filters': trial.suggest_categorical('filters', [32,64,128]),\n",
        "              'kernel_size': trial.suggest_categorical('kernel_size', [2,3]),\n",
        "              'units_dense': trial.suggest_categorical('units_dense', [256, 128, 64]),\n",
        "              'sdo': trial.suggest_float('sdo', 0.20, 0.45, step=0.01),\n",
        "              'kr': trial.suggest_float('kr', 0.01, 0.1, step=0.01),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]),\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.20, 0.45, step=0.01),\n",
        "              'units_tab': [trial.suggest_categorical('units_tab_0', [256, 128, 64]),trial.suggest_categorical('units_tab_1', [128, 64,32])],\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(**params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=epochs,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=21, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "WrHKe5ts1Pr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "eQfvhcVu1Pr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model_cnn, n_trials=151, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "nxO1X9KA1Pr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " * Best is trial 119 with value: 0.9003476656733361.\n",
        "    * parameters: {'filters': 32,\n",
        " 'kernel_size': 3,\n",
        " 'units_dense': 256,\n",
        " 'do': 0.26,\n",
        " 'sdo': 0.26,\n",
        " 'kr': 0.08,\n",
        " 'activation': 'relu',\n",
        " 'gn': 0.03,\n",
        " 'units_tab_0': 128,\n",
        " 'units_tab_1': 64}.\n",
        "\n",
        "\n",
        " * Best is trial 129 with value: 0.897686958989641.\n",
        "    * parameters: {'filters': 64, 'kernel_size': 3, 'units_dense': 64, 'sdo': 0.22, 'kr': 0.05, 'activation': 'selu', 'gn': 0.06, 'do': 0.2, 'units_tab_0': 128, 'units_tab_1': 64}\n"
      ],
      "metadata": {
        "id": "4l5tpfTM1Pr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model"
      ],
      "metadata": {
        "id": "PK2kiBhI1Pr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model= make_model(units=[64,64], units_tab=[64,32], activation=\"silu\", gn=0.09, do=0.31, output_bias=0) # Model v2 (Score 0.85787)\n",
        "# model= make_model(units=[128,64], units_tab=[128,32], activation=\"selu\", gn=0.09, do=0.25, output_bias=0) # Model v1 (best 0.86323)\n",
        "model= make_model_cnn(lr=5e-4, **{'filters': 32, 'kernel_size': 3, 'units_dense': 256, 'do': 0.26, 'sdo': 0.26, 'kr': 0.08, 'activation': 'relu', 'gn': 0.03, \"units_tab\":[128, 64]}) # auc: 0.8876\n",
        "model= make_model_cnn(lr=5e-4, **{'filters': 64, 'kernel_size': 3, 'units_dense': 64, 'do': 0.20, 'sdo': 0.22, 'kr': 0.05, 'activation': 'relu', 'gn': 0.06, \"units_tab\":[128, 64]}) # auc: 0.8853\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "aSMDIIrH1Pr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch"
      ],
      "metadata": {
        "id": "B8h7sYCc1Pr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_history = model.fit(\n",
        "                              resampled_ds,\n",
        "                              epochs=151,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", min_lr=0.00001),\n",
        "                                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\")],\n",
        "                              validation_data=dataset_validation\n",
        "                              )"
      ],
      "metadata": {
        "id": "UPMm6_EQ1Pr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history)"
      ],
      "metadata": {
        "id": "pFAl5lNe1Pr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "QNTLHgAg1Pr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled = model.predict(dataset_train)\n",
        "valid_predictions_resampled = model.predict(dataset_validation)\n",
        "test_predictions_resampled = model.predict(dataset_test)"
      ],
      "metadata": {
        "id": "KHWynjkh1Pr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled)"
      ],
      "metadata": {
        "id": "d89G8NTx1Pr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled)"
      ],
      "metadata": {
        "id": "pp4pcmqp1Pr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_resampled\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_cnn_tab_v1_all_data_ext.csv\")"
      ],
      "metadata": {
        "id": "iNEAA1CM1Pr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Temporal Transformer"
      ],
      "metadata": {
        "id": "MQmGzGpAZCdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 5000\n",
        "sequence_length = 7\n",
        "num_features = 30\n",
        "embed_dim = num_features\n",
        "head_size = 32\n",
        "num_heads = 4\n",
        "gru_units = 64\n",
        "dropout = 0.1\n",
        "\n",
        "# 2. Positional Embedding Layer\n",
        "class TimeSeriesPositionalEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = keras.layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "# 3. Transformer Encoder Layer with GRU\n",
        "def transformer_encoder_gru(inputs, head_size, num_heads, gru_units, embed_dim, dropout=0):\n",
        "    # Attention and Normalization\n",
        "    x = keras.layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(inputs, inputs)\n",
        "    x = keras.layers.Dropout(dropout)(x)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part (GRU)\n",
        "    x = keras.layers.GRU(units=gru_units, return_sequences=True)(res)\n",
        "    x = keras.layers.Dropout(dropout)(x)\n",
        "    x = keras.layers.Dense(units=embed_dim)(x) # Added Dense Layer\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "IcPjVlr90d0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(metrics=METRICS, units=[32,32], output_bias=None, gn=0.025, activation=\"relu\", do=0.3, num_transformer_layers=2,\n",
        "               head_size=32, num_heads=4, gru_units=64):\n",
        "  \"\"\"\n",
        "  Builds a transformer-based binary classification model for time series data.\n",
        "\n",
        "  Args:\n",
        "      input_shape: Tuple, the shape of the input data (sequence_length, num_features).\n",
        "      num_transformer_layers: Integer, the number of transformer encoder layers to stack.\n",
        "      head_size: Integer, the size of each attention head.\n",
        "      num_heads: Integer, the number of attention heads.\n",
        "      gru_units: Integer, the number of GRU units in the feed-forward part.\n",
        "      dropout: Float, the dropout rate.\n",
        "\n",
        "  Returns:\n",
        "      A Keras model.\n",
        "  \"\"\"\n",
        "\n",
        "  tot_obs, sequence_length, num_features = inputs.shape\n",
        "  embed_dim = inputs.shape[2]\n",
        "\n",
        "  data = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]), name=\"input_layer\")\n",
        "  data_tabular = data[:, 6, :]\n",
        "  # LSTM Section\n",
        "  data_noised = keras.layers.GaussianNoise(stddev=gn, name=\"noise_layer\")(data)\n",
        "  lstm_out = keras.layers.LSTM(units[0], return_sequences=True, name=\"lstm_0\",)(data_noised)\n",
        "  lstm_out = keras.layers.LSTM(units[1], name=\"lstm_1\")(lstm_out)\n",
        "  lstm_out = keras.layers.Dropout(do)(lstm_out)\n",
        "\n",
        "  # Tabular Section\n",
        "\n",
        "  x = TimeSeriesPositionalEmbedding(sequence_length, embed_dim)(data)\n",
        "\n",
        "  # Stack Transformer Layers\n",
        "  for _ in range(num_transformer_layers):\n",
        "      x = transformer_encoder_gru(x, head_size, num_heads, gru_units, embed_dim, dropout)\n",
        "\n",
        "  # Global Pooling and Output Layer\n",
        "  x_av = keras.layers.GlobalAveragePooling1D()(x)\n",
        "  x_av = keras.layers.Dropout(do)(x_av)\n",
        "  # Max Pooling and Output Layer\n",
        "  x_mx = keras.layers.GlobalMaxPooling1D()(x)\n",
        "  x_mx = keras.layers.Dropout(do)(x_mx)\n",
        "\n",
        "\n",
        "  # Concatenate\n",
        "  x = keras.layers.Concatenate(name=\"concat\")([lstm_out, x_av,x_mx])\n",
        "\n",
        "  outputs = keras.layers.Dense(1, activation='sigmoid',bias_initializer=output_bias, name=\"output\")(x)\n",
        "\n",
        "  model = keras.Model(inputs=data, outputs=outputs, name=\"temp_trans_v0\")\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "      loss=keras.losses.BinaryCrossentropy(),\n",
        "      metrics=metrics)\n",
        "\n",
        "  return model\n",
        "\n",
        "model= make_model(output_bias=0)\n",
        "\n",
        "# Reset the bias to zero, since this dataset is balanced.\n",
        "output_layer = model.layers[-1]\n",
        "output_layer.bias.assign([0])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "xv6NZyftZL1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "kau7Z85rZLyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optuna Optimization"
      ],
      "metadata": {
        "id": "mXb1uZ8Y-EMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 00"
      ],
      "metadata": {
        "id": "XZOoOM5z-EMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model, use_gpu=False, rs=42, fit_scaling=False, epochs=101):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units': [trial.suggest_categorical('units_0', [128,64]),trial.suggest_categorical('units_1', [64,32])],\n",
        "              'output_bias':0,\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.15, 0.45, step=0.01),\n",
        "              'num_transformer_layers': trial.suggest_categorical('num_transformer_layers', [2,3]),\n",
        "              'head_size': trial.suggest_categorical('head_size', [32,64]),\n",
        "              'num_heads': trial.suggest_categorical('num_heads', [4,8,16]),\n",
        "              'gru_units': trial.suggest_categorical('gru_units', [32,64,128])\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(**params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.3, verbose=1, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=21, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "SMYmaGSf-AGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "sVfcNnMW-AGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds,validation_data=dataset_validation, model_class=make_model, n_trials=101, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "F-3g1SAn-AGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Best is trial 91 with value: 0.8922236412657868\n",
        "  * Parameters: {'units_0': 128, 'units_1': 64, 'gn': 0.08, 'do': 0.43999999999999995, 'num_transformer_layers': 3, 'head_size': 32, 'num_heads': 4, 'gru_units': 128}"
      ],
      "metadata": {
        "id": "bIKdsUp-tKUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Optimization 01"
      ],
      "metadata": {
        "id": "mcFK_0K-tr3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_nn(trial, train_data, validation_data, model=make_model, use_gpu=False, rs=42, fit_scaling=False):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    params = {\n",
        "              'units': [trial.suggest_categorical('units_0', [128,64]),trial.suggest_categorical('units_1', [64,32])],\n",
        "              'output_bias':0,\n",
        "              'gn': trial.suggest_float('gn', 0.01, 0.1, step=0.01),\n",
        "              'do': trial.suggest_float('do', 0.15, 0.45, step=0.01),\n",
        "              'num_transformer_layers': trial.suggest_categorical('num_transformer_layers', [2,3]),\n",
        "              'head_size': trial.suggest_categorical('head_size', [32,64]),\n",
        "              'num_heads': trial.suggest_categorical('num_heads', [4,8,16]),\n",
        "              'gru_units': trial.suggest_categorical('gru_units', [32,64,128])\n",
        "              }\n",
        "\n",
        "    auc_score = []\n",
        "\n",
        "    keras.utils.set_random_seed(rs)\n",
        "    model = model_class(**params)\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(train_data,\n",
        "              validation_data=validation_data,\n",
        "              epochs=101,\n",
        "              steps_per_epoch=52,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.3, verbose=1, monitor=\"val_auc\", min_lr=0.000001, mode=\"max\"),\n",
        "                         keras.callbacks.EarlyStopping(patience=21, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                       start_from_epoch=3, mode=\"max\")]\n",
        "              )\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = model.predict(validation_data)\n",
        "\n",
        "    # Calculate the RMSE for the current fold\n",
        "    rmse_scores = roc_auc_score(y_valid_v1, y_pred)\n",
        "\n",
        "    return rmse_scores"
      ],
      "metadata": {
        "id": "caRetMQjtr3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(train_data, validation_data, model_class, n_trials, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, train_data=train_data, validation_data=validation_data, model=model_class, use_gpu=use_gpu), n_trials=n_trials)\n",
        "    return study  # Return the study object"
      ],
      "metadata": {
        "id": "_w3CUFj6tr3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_study = tune_hyperparameters(train_data=resampled_ds_v1,validation_data=dataset_validation_v1, model_class=make_model, n_trials=111, use_gpu=False)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = nn_study.best_params\n",
        "cat_params"
      ],
      "metadata": {
        "id": "8-YI3QKMtr3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit the model"
      ],
      "metadata": {
        "id": "90SOeGFZtlKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model= make_model(units=[64,64], units_tab=[64,32], activation=\"silu\", gn=0.09, do=0.31, output_bias=0) # Model v2 (Score 0.85787)\n",
        "# model= make_model(units=[128,64], units_tab=[128,32], activation=\"selu\", gn=0.09, do=0.25, output_bias=0) # Model v1 (best 0.86323)\n",
        "# MODEL 00 'units_0': 64, 'units_1': 64, 'units_tab_0': 64, 'units_tab_1': 64, 'activation': 'gelu', 'gn': 0.05, 'do': 0.32\n",
        "model= make_model(units=[64,32], units_tab=[128,32], activation=\"selu\", gn=0.10, do=0.44, output_bias=0) # Model v3 (best 0.86323)\n",
        "model= make_model(units=[64,64], units_tab=[64,64], activation=\"gelu\", gn=0.05, do=0.32, output_bias=0) # Model v3 (best 0.86323)\n",
        "# MODEL 01\n",
        "model= make_model(units=[128,64], units_tab=[256,64], activation=\"mish\", gn=0.09, do=0.20, output_bias=0) # Model v3 (best 0.86323)"
      ],
      "metadata": {
        "id": "XpXwDs_ktlKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = pos_features.shape[0]\n",
        "neg = neg_features.shape[0]\n",
        "print(pos,neg)\n",
        "resampled_steps_per_epoch = int(np.ceil(2.0*pos/batch_size))\n",
        "resampled_steps_per_epoch\n",
        "\n",
        "#{'units_0': 128, 'units_1': 64, 'gn': 0.08, 'do': 0.43999999999999995, 'num_transformer_layers': 3, 'head_size': 32, 'num_heads': 4, 'gru_units': 128}"
      ],
      "metadata": {
        "id": "ytMhAlv8tlKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model 00"
      ],
      "metadata": {
        "id": "MY5MsRiCyF5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cv0= make_model(metrics=METRICS, units=[128,64], output_bias=0, gn=0.08, do=0.44, num_transformer_layers=3,\n",
        "                        head_size=32, num_heads=4, gru_units=128) # Model v0\n",
        "\n",
        "resampled_history_v0 = model_cv0.fit(\n",
        "                              resampled_ds,\n",
        "                              epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation\n",
        "                              )\n",
        "\n",
        "model_cv0.evaluate(dataset_validation)"
      ],
      "metadata": {
        "id": "R6Wk1oIVtlKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history_v0)"
      ],
      "metadata": {
        "id": "ZRP02MSOtlKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model 01"
      ],
      "metadata": {
        "id": "-OSeoVG1FaUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "             epochs=201,\n",
        "                              steps_per_epoch=52,\n",
        "                              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, monitor=\"val_auc\", mode=\"max\", min_lr=0.000001, verbose=1),\n",
        "                                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\", verbose=1)],\n",
        "                              validation_data=dataset_validation_v1\n",
        "                              )\n",
        "\n",
        "model_cv1.evaluate(dataset_validation_v1)"
      ],
      "metadata": {
        "id": "erhTWNPxtlKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(resampled_history_v1)"
      ],
      "metadata": {
        "id": "OzljwYyEtlKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate Model:"
      ],
      "metadata": {
        "id": "NjLkbdPYtlKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv0 = model_cv0.predict(dataset_train)\n",
        "valid_predictions_resampled_cv0 = model_cv0.predict(dataset_validation)\n",
        "test_predictions_resampled_cv0 = model_cv0.predict(dataset_test)"
      ],
      "metadata": {
        "id": "FulK21U-tlKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_subm[\"rainfall\"] = test_predictions_resampled_cv0\n",
        "# df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_transformer_enhanced_v0_all_data_ext.csv\")\n",
        "df_subm_v0 = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_transformer_enhanced_v0_all_data_ext.csv\", index_col=0)\n",
        "df_subm_v0"
      ],
      "metadata": {
        "id": "OVeqOhaTy-DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_predictions_resampled_cv1 = model_cv1.predict(dataset_train_v1)\n",
        "valid_predictions_resampled_cv1 = model_cv1.predict(dataset_validation_v1)\n",
        "test_predictions_resampled_cv1 = model_cv1.predict(dataset_test_v1)"
      ],
      "metadata": {
        "id": "LC7j8hhvtlKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm_v1 = df_subm.copy()\n",
        "df_subm_v1[\"rainfall\"] = test_predictions_resampled_cv1\n",
        "df_subm_v1.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_transformer_enhanced_v1_all_data_ext.csv\")\n",
        "df_subm_v1 = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_transformer_enhanced_v1_all_data_ext.csv\", index_col=0)\n",
        "df_subm_v1"
      ],
      "metadata": {
        "id": "J8051hooJfkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(df_subm_v0,df_subm_v1)"
      ],
      "metadata": {
        "id": "Zq1lQgz_tlKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_cm(y_valid, valid_predictions_resampled_cv0)"
      ],
      "metadata": {
        "id": "MZe1914UtlKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_cm(y_valid_v1, dataset_validation_v1)\n",
        "test_predictions_resampled_cv0.shape, test_predictions_resampled_cv1.shape"
      ],
      "metadata": {
        "id": "5Yx0cFTstlKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = (df_subm_v1+df_subm_v0)/2\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_transformer_enhanced_v_ensemble_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "QmUCSEQNtlKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmZnoc4eO1_E"
      },
      "source": [
        "### **5.1 TREE BASED MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_train = pd.concat([validation, train], axis=0)\n",
        "all_train.shape"
      ],
      "metadata": {
        "id": "6tI7yzr4mLu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = all_train.drop(\"rainfall\", axis=1)\n",
        "y = all_train[\"rainfall\"]\n",
        "#x.describe().T"
      ],
      "metadata": {
        "id": "5AUd_WnlmcBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1GyNmMXO7Dw"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "sample_pos_weight = class_weights[1]/class_weights[0]\n",
        "sample_pos_weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "XbSC1edRZLvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
        "                                ('scaler', StandardScaler())])\n",
        "\n",
        "X_train_res = preprocessing.fit_transform(X_train_res)\n",
        "X_val = preprocessing.transform(X_val)"
      ],
      "metadata": {
        "id": "C0ACmqbmZLsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_res.shape, X_val.shape"
      ],
      "metadata": {
        "id": "xYzppYqtsenE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "clf_verify = GaussianNB()\n",
        "cross_val_score(clf_verify, X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()"
      ],
      "metadata": {
        "id": "AoHZJOEzRV6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Random Forest:"
      ],
      "metadata": {
        "id": "6hTZn-vksT1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "JteANcWtZLpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_rf(trial):\n",
        "    # Suggest important hyperparameters for RandomForest\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
        "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
        "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 20)\n",
        "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
        "\n",
        "    clf = RandomForestClassifier(\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(clf,X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()\n",
        "    return score\n",
        "\n",
        "study_rf = optuna.create_study(direction=\"maximize\")\n",
        "study_rf.optimize(objective_rf, n_trials=50) #adjust to 50\n",
        "best_params_rf = study_rf.best_trial.params\n",
        "print(\"Best params for RandomForest:\", best_params_rf)"
      ],
      "metadata": {
        "id": "G9VDPlO6ZLjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Best Params: {'n_estimators': 304, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
        "* Score: 0.9533470932991909"
      ],
      "metadata": {
        "id": "bReMr4zTvU94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna.visualization as vis\n",
        "from IPython.display import IFrame, display\n",
        "\n",
        "# Plot the optimization history (objective value over\n",
        "vis.plot_optimization_history(study_rf)\n",
        "#fig_history.write_html(\"optimization_history.html\")"
      ],
      "metadata": {
        "id": "8wwFqFHYZLf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the parameter importances\n",
        "plot_param_importances(study_rf)\n",
        "#fig_importances.write_html(\"parameter_importances.html\")"
      ],
      "metadata": {
        "id": "K2e_TTULZLcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "oFmpKojPwgRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_gb(trial):\n",
        "    # Suggest key hyperparameters for GradientBoostingClassifier\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 400, 650) #trial.suggest_int(\"n_estimators\", 50, 500)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.05, 0.25, log=True)\n",
        "    max_features =trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 7, 12)\n",
        "    subsample = trial.suggest_float(\"subsample\", 0.6, 0.95, step=0.025)\n",
        "\n",
        "    clf = GradientBoostingClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=n_estimators,\n",
        "        max_features=max_features,\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=max_depth,\n",
        "        subsample=subsample\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(clf,X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()\n",
        "    return score\n",
        "\n",
        "study_gb = optuna.create_study(direction=\"maximize\")\n",
        "study_gb.optimize(objective_gb, n_trials=30) #adjust to 50\n",
        "best_params_gb = study_gb.best_trial.params\n",
        "print(\"Best params for GradientBoosting:\", best_params_gb)"
      ],
      "metadata": {
        "id": "YzS8OEdgwYHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Best Params: {'n_estimators': 455, 'learning_rate': 0.1516395931910056, 'max_features': 'sqrt', 'max_depth': 9, 'subsample': 0.75}\n",
        "* Score: 0.9678502444785376"
      ],
      "metadata": {
        "id": "Z6NsoOjQ5MWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the optimization history (objective value over\n",
        "vis.plot_optimization_history(study_gb)\n",
        "#fig_history.write_html(\"optimization_history.html\")"
      ],
      "metadata": {
        "id": "4MBf5LdS5dJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the parameter importances\n",
        "plot_param_importances(study_gb)\n",
        "#fig_importances.write_html(\"parameter_importances.html\")"
      ],
      "metadata": {
        "id": "GixWFvoV5c7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBClassifier"
      ],
      "metadata": {
        "id": "ughDP3_N4jz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_xgb(trial):\n",
        "    # Suggest key hyperparameters for XGBClassifier\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
        "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
        "    gamma = trial.suggest_float('gamma', 0, 1)\n",
        "    reg_alpha = trial.suggest_float('reg_alpha', 0.00001, 1.0, log=True)\n",
        "    reg_lambda = trial.suggest_float('reg_lambda', 0.00001, 1.0, log=True)\n",
        "\n",
        "    clf = XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state=42,\n",
        "        n_estimators=n_estimators,\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=max_depth,\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        gamma=gamma,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(clf, X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()\n",
        "    return score\n",
        "study_xgb = optuna.create_study(direction=\"maximize\")\n",
        "study_xgb.optimize(objective_xgb, n_trials=71) #adjust to 50\n",
        "best_params_xgb = study_xgb.best_trial.params\n",
        "print(\"Best params for XGBoost:\", best_params_xgb)"
      ],
      "metadata": {
        "id": "A-F1hGtDwYCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Parameters: {'n_estimators': 325, 'learning_rate': 0.06278279256080053, 'max_depth': 8, 'subsample': 0.70, 'colsample_bytree': 0.87, 'gamma': 0.17, 'reg_alpha': 0.024, 'reg_lambda': 8.111e-05}\n",
        "* Score 0.9618260284102824"
      ],
      "metadata": {
        "id": "xybTrH1i7Adw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the optimization history (objective value over\n",
        "vis.plot_optimization_history(study_xgb)\n",
        "#fig_history.write_html(\"optimization_history.html\")"
      ],
      "metadata": {
        "id": "Bz467bcmwX95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the parameter importances\n",
        "plot_param_importances(study_xgb)\n",
        "#fig_importances.write_html(\"parameter_importances.html\")"
      ],
      "metadata": {
        "id": "2UYNEOkmwX4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Voting Classifier"
      ],
      "metadata": {
        "id": "9u7igxU07vks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Build final models using the best parameters from Optuna studies\n",
        "rf_final = RandomForestClassifier(class_weight='balanced', random_state=42, **best_params_rf)\n",
        "gb_final = GradientBoostingClassifier(random_state=42, **best_params_gb)\n",
        "xgb_final = XGBClassifier(use_label_encoder=False, eval_metric=\"auc\", random_state=42, **best_params_xgb)\n",
        "\n",
        "# Create a VotingClassifier with the best estimators\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf_final),\n",
        "        ('gb', gb_final),\n",
        "        ('xgb', xgb_final)\n",
        "    ],\n",
        "    voting='soft'  # or 'soft' if probability estimates are preferred\n",
        ")\n",
        "\n",
        "# Fit the VotingClassifier on your resampled training data\n",
        "voting_clf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Evaluate using cross-validation with ROC AUC as the metric.\n",
        "# If it's a binary classification, use 'roc_auc'.\n",
        "# For multi-class problems, consider using 'roc_auc_ovr' or 'roc_auc_ovo'\n",
        "from sklearn.model_selection import cross_val_score\n",
        "voting_roc_auc = cross_val_score(voting_clf, X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()\n",
        "print(\"Voting Classifier CV ROC AUC Score:\", voting_roc_auc)"
      ],
      "metadata": {
        "id": "B7SHiXd6wXoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = sio.dump(voting_clf, \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/voting_v0.skops\")\n",
        "\n",
        "unknown_types = sio.get_untrusted_types(file=\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/voting_v0.skops\")\n",
        "# investigate the contents of unknown_types, and only load if you trust\n",
        "# everything you see.\n",
        "voting_clf = sio.load(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/voting_v0.skops\", trusted=unknown_types)"
      ],
      "metadata": {
        "id": "TMpPdHQ-PJAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred_proba = voting_clf.predict_proba(X_val)[:, 1]\n",
        "print('Validation ROC AUC:', roc_auc_score(y_val, y_val_pred_proba),'\\n')\n",
        "print(classification_report(y_val, voting_clf.predict(X_val)))"
      ],
      "metadata": {
        "id": "9KtNz8suwXhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = preprocessing.transform(test)\n",
        "test_predictions_proba = voting_clf.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "VP6f5nBgwXYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_proba\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_voting_v0_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "3wkcUPdyALrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
        "ax[0].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv1)\n",
        "ax[1].scatter(test_predictions_resampled_cv2,test_predictions_proba)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xVQfVumTA7Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgygW5mLYQbS"
      },
      "source": [
        "### **5.1 TREE BASED MODELS - SKTIME**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sktime"
      ],
      "metadata": {
        "id": "KrzaVU0lyjgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test_v2 = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_test,\n",
        "                                                                    y_test,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=10000,\n",
        "                                                                    shuffle=False\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_test_v2.take(1000):\n",
        "    test_v2, test_targets_v2 = batch\n",
        "\n",
        "test_v2 = test_v2.numpy()\n",
        "test_targets_v2 = test_targets_v2.numpy()\n",
        "\n",
        "print(\"Input shape:\", test_v2.shape)\n",
        "print(\"Target shape:\", test_targets_v2.shape)"
      ],
      "metadata": {
        "id": "mS5ImiIQXj9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.classification.deep_learning import GRUClassifier\n",
        "from sktime.classification.interval_based import TimeSeriesForestClassifier"
      ],
      "metadata": {
        "id": "1pb5ideNaE6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all_train = pd.concat([validation, train], axis=0)\n",
        "all_train.shape, all_target.shape, inputs_v2.shape"
      ],
      "metadata": {
        "id": "_MlvxaKTYQbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = all_train.copy()\n",
        "y = all_target.copy()\n",
        "X_test = test_v2.copy()\n",
        "#x.describe().T\n",
        "X_test.shape"
      ],
      "metadata": {
        "id": "MBylQrB2YQbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeTMnqepYQbV"
      },
      "outputs": [],
      "source": [
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "# class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# sample_pos_weight = class_weights[1]/class_weights[0]\n",
        "# sample_pos_weight"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape)\n",
        "X_train_ = X_train.reshape(X_train.shape[0], -1)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_, y_train)\n",
        "\n",
        "X_train_res = X_train_res.reshape(X_train_res.shape[0], X_train.shape[1], X_train.shape[2])\n",
        "X_train_res.shape"
      ],
      "metadata": {
        "id": "7jMJYYJKYQbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
        "#                                 ('scaler', StandardScaler())])\n",
        "\n",
        "# X_train_res = preprocessing.fit_transform(X_train_res)\n",
        "# X_val = preprocessing.transform(X_val)"
      ],
      "metadata": {
        "id": "jfZ4jdIAYQbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_res.shape, X_val.shape"
      ],
      "metadata": {
        "id": "JzWvvZt9YQbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# clf_verify = GaussianNB()\n",
        "# cross_val_score(clf_verify, X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()"
      ],
      "metadata": {
        "id": "6xu4YEFdYQbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GRUClassifier:"
      ],
      "metadata": {
        "id": "3omriM99YQbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "PMmCTUgaYQbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_rf(trial):\n",
        "    # Suggest important hyperparameters for RandomForest\n",
        "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [64, 128, 256])\n",
        "    n_layers = trial.suggest_categorical(\"n_layers\", [2, 3, 4, 5])\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5, step=0.025)\n",
        "    fc_dropout = trial.suggest_float(\"fc_dropout\", 0.1, 0.5, step=0.025)\n",
        "    bidirectional = trial.suggest_categorical(\"bidirectional\", [True, False])\n",
        "\n",
        "    clf = GRUClassifier(\n",
        "        random_state=42,\n",
        "        hidden_dim=hidden_dim,\n",
        "        n_layers=n_layers,\n",
        "        dropout=dropout,\n",
        "        fc_dropout=fc_dropout,\n",
        "        bidirectional=bidirectional,\n",
        "        num_epochs=15,\n",
        "        batch_size=64,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(clf,X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()\n",
        "    return score\n",
        "\n",
        "study_gru = optuna.create_study(direction=\"maximize\")\n",
        "study_gru.optimize(objective_rf, n_trials=50) #adjust to 50\n",
        "best_params_gru = study_gru.best_trial.params\n",
        "print(\"Best params for GRUFCNNClassifier:\", best_params_gru)"
      ],
      "metadata": {
        "id": "4TfviAKEYQbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Best Params: {'n_estimators': 304, 'max_depth': 15, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}\n",
        "* Score: 0.9533470932991909"
      ],
      "metadata": {
        "id": "H6V0reIIYQba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna.visualization as vis\n",
        "from IPython.display import IFrame, display\n",
        "\n",
        "# Plot the optimization history (objective value over\n",
        "vis.plot_optimization_history(study_rf)\n",
        "#fig_history.write_html(\"optimization_history.html\")"
      ],
      "metadata": {
        "id": "Er7G0GERYQba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the parameter importances\n",
        "plot_param_importances(study_rf)\n",
        "#fig_importances.write_html(\"parameter_importances.html\")"
      ],
      "metadata": {
        "id": "4CFPTL_gYQba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HIVECOTEV2"
      ],
      "metadata": {
        "id": "CPiGGmBJizQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.classification.kernel_based import RocketClassifier\n",
        "from sktime.classification.ensemble import ComposableTimeSeriesForestClassifier\n",
        "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
        "from sktime.classification.hybrid import HIVECOTEV2"
      ],
      "metadata": {
        "id": "ODwsqKGHjLY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_res.shape"
      ],
      "metadata": {
        "id": "VOO41b4Q5pgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_hc2(trial):\n",
        "    # --- Hyperparameters for component classifiers ---\n",
        "    drcif_n_estimators = trial.suggest_categorical(\"drcif_n_estimators\", [50, 100, 150, 200])\n",
        "    arsenal_n_estimators = trial.suggest_categorical(\"arsenal_n_estimators\", [50, 100, 150, 200])\n",
        "    tde_n_estimators = trial.suggest_categorical(\"tde_n_estimators\", [50, 100, 150, 200])\n",
        "\n",
        "    # ShapeletTransformClassifier hyperparameters\n",
        "    stc_n_shapelet_samples = trial.suggest_int(\"stc_n_shapelet_samples\", 1000, 20000, log=True)\n",
        "    stc_max_shapelet_length_prop = trial.suggest_float(\"stc_max_shapelet_length_prop\", 0.2, 0.8)\n",
        "    stc_max_shapelets_prop = trial.suggest_float(\"stc_max_shapelets_prop\", 0.01, 0.2)\n",
        "    stc_max_shapelets = int(stc_n_shapelet_samples * stc_max_shapelets_prop)\n",
        "    stc_max_shapelet_length = None\n",
        "\n",
        "    # TemporalDictionaryEnsemble (TDE) Hyperparameters - Corrected\n",
        "    tde_n_parameter_samples = trial.suggest_int(\"tde_n_parameter_samples\", 50, 250)  # Number of parameter sets to sample\n",
        "    tde_max_ensemble_size = trial.suggest_int(\"tde_max_ensemble_size\", 25, 100)      # Max size of the ensemble\n",
        "    tde_max_win_len_prop = trial.suggest_float(\"tde_max_win_len_prop\", 0.5, 1.0)      # Max window length proportion\n",
        "\n",
        "    # Create the HIVECOTEV2 classifier with tuned parameters\n",
        "    clf = HIVECOTEV2(\n",
        "        stc_params={\n",
        "            \"n_shapelet_samples\": stc_n_shapelet_samples,\n",
        "            \"max_shapelets\": stc_max_shapelets,\n",
        "            \"max_shapelet_length\": stc_max_shapelet_length\n",
        "        },\n",
        "        drcif_params={\"n_estimators\": drcif_n_estimators},\n",
        "        arsenal_params={\"n_estimators\": arsenal_n_estimators},\n",
        "        tde_params={  # Corrected TDE parameters\n",
        "            \"n_parameter_samples\": tde_n_parameter_samples,\n",
        "            \"max_ensemble_size\": tde_max_ensemble_size,\n",
        "            \"max_win_len_prop\": tde_max_win_len_prop\n",
        "        },\n",
        "        time_limit_in_minutes=0.5,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    # Custom scoring function\n",
        "    def custom_roc_auc(estimator, X, y):\n",
        "        try:\n",
        "            if len(np.unique(y)) > 2:\n",
        "                y_pred_proba = estimator.predict_proba(X)\n",
        "                return roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
        "            else:\n",
        "                y_pred_proba = estimator.predict_proba(X)[:, 1]\n",
        "                return roc_auc_score(y, y_pred_proba)\n",
        "        except AttributeError:\n",
        "            y_pred = estimator.predict(X)\n",
        "            return roc_auc_score(y, y_pred)\n",
        "\n",
        "    y_train_str = y_train.astype(str)\n",
        "    score = cross_val_score(clf, all_train, all_target, cv=5, scoring=custom_roc_auc).mean()\n",
        "    return score\n",
        "\n",
        "# --- Optuna Optimization ---\n",
        "study_hc2 = optuna.create_study(direction=\"maximize\")\n",
        "study_hc2.optimize(objective_hc2, n_trials=31)\n",
        "best_params_hc2 = study_hc2.best_trial.params\n",
        "print(\"Best params for HIVE-COTE v2:\", best_params_hc2)"
      ],
      "metadata": {
        "id": "PRZ8fU9zizBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "from sktime.datasets import load_unit_test\n",
        "X_train, y_train = load_unit_test(return_X_y=True, split=\"train\")\n",
        "X_test, y_test = load_unit_test(return_X_y=True, split=\"test\")\n",
        "classifier = KNeighborsTimeSeriesClassifier(distance=\"euclidean\")\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "xi6P49j8iyp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "ntQTrdgjiyb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### KNeighborsTimeSeriesClassifier"
      ],
      "metadata": {
        "id": "tc1nw61mYQba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier"
      ],
      "metadata": {
        "id": "rcIF2l1Cpkr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "FtyBCqEkWAnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_ = X.reshape(X.shape[0], -1)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_, y)\n",
        "\n",
        "X_train_res = X_train_res.reshape(X_train_res.shape[0], X.shape[1], X.shape[2])\n",
        "X_train_res.shape"
      ],
      "metadata": {
        "id": "kESa24edbDhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_hc2(trial):\n",
        "    # --- Hyperparameters for component classifiers ---\n",
        "    n_neighbors = trial.suggest_categorical(\"n_neighbors\", [3, 4, 5, 6, 7])\n",
        "    weights = trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"])\n",
        "    leaf_size = trial.suggest_categorical(\"leaf_size\", [25,30,35,40,50])\n",
        "\n",
        "\n",
        "    # Create the HIVECOTEV2 classifier with tuned parameters\n",
        "    clf = KNeighborsTimeSeriesClassifier(n_neighbors=n_neighbors,distance=\"euclidean\",algorithm=\"ball_tree\",leaf_size=leaf_size,weights=weights)\n",
        "\n",
        "    # Custom scoring function\n",
        "    def custom_roc_auc(estimator, X, y):\n",
        "        try:\n",
        "            if len(np.unique(y)) > 2:\n",
        "                y_pred_proba = estimator.predict_proba(X)\n",
        "                return roc_auc_score(y, y_pred_proba, multi_class='ovr')\n",
        "            else:\n",
        "                y_pred_proba = estimator.predict_proba(X)[:, 1]\n",
        "                return roc_auc_score(y, y_pred_proba)\n",
        "        except AttributeError:\n",
        "            y_pred = estimator.predict(X)\n",
        "            return roc_auc_score(y, y_pred)\n",
        "\n",
        "    y_train_str = y_train_res.astype(str)\n",
        "    score = cross_val_score(clf, X_train_res, y_train_str, cv=cv, scoring=custom_roc_auc).mean()\n",
        "    return score\n",
        "\n",
        "# --- Optuna Optimization ---\n",
        "study_hc2 = optuna.create_study(direction=\"maximize\")\n",
        "study_hc2.optimize(objective_hc2, n_trials=31)\n",
        "best_params_hc2 = study_hc2.best_trial.params\n",
        "print(\"Best params for HIVE-COTE v2:\", best_params_hc2)"
      ],
      "metadata": {
        "id": "m8Vf83TEYQbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Best Params: {'n_estimators': 455, 'learning_rate': 0.1516395931910056, 'max_features': 'sqrt', 'max_depth': 9, 'subsample': 0.75}\n",
        "* Score: 0.9678502444785376"
      ],
      "metadata": {
        "id": "-wNCiQHoYQbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna.visualization as vis\n",
        "from IPython.display import IFrame, display"
      ],
      "metadata": {
        "id": "iXzwRUniQ2XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the parameter importances\n",
        "plot_param_importances(study_hc2)\n",
        "#fig_importances.write_html(\"parameter_importances.html\")"
      ],
      "metadata": {
        "id": "5e1LWjtOQ2XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the optimization history (objective value over\n",
        "vis.plot_optimization_history(study_hc2)\n",
        "#fig_history.write_html(\"optimization_history.html\")"
      ],
      "metadata": {
        "id": "ZE51CuQfYQbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "X_train_res, y_train_res\n",
        "y_pred = cross_val_predict(KNeighborsTimeSeriesClassifier(**{'n_neighbors': 3, 'weights': 'distance', 'leaf_size': 30}),\n",
        "                           X_train_res, y_train_res,\n",
        "                           cv=cv, method=\"predict_proba\")"
      ],
      "metadata": {
        "id": "c2pt51QcSADq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "iyTnZ8GMR_-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Fit the Model:"
      ],
      "metadata": {
        "id": "TCGR4gRlpQmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "b9KuZtrcpaSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a VotingClassifier with the best estimators\n",
        "clf = KNeighborsTimeSeriesClassifier(n_neighbors=6,distance=\"euclidean\",algorithm=\"ball_tree\",leaf_size=30,weights=\"distance\")\n",
        "\n",
        "# Fit the VotingClassifier on your resampled training data\n",
        "clf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Evaluate using cross-validation with ROC AUC as the metric.\n",
        "# If it's a binary classification, use 'roc_auc'.\n",
        "# For multi-class problems, consider using 'roc_auc_ovr' or 'roc_auc_ovo'\n",
        "from sklearn.model_selection import cross_val_score\n",
        "voting_roc_auc = cross_val_score(clf, X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()\n",
        "print(\"KNeighborsTimeSeries Classifier CV ROC AUC Score:\", voting_roc_auc)"
      ],
      "metadata": {
        "id": "140SaKDfR_6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obj = sio.dump(clf, \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/clf_KNeighbors_v0.skops\")\n",
        "\n",
        "unknown_types = sio.get_untrusted_types(file=\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/clf_KNeighbors_v0.skops\")\n",
        "# investigate the contents of unknown_types, and only load if you trust\n",
        "# everything you see.\n",
        "clf = sio.load(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/clf_KNeighbors_v0.skops\", trusted=unknown_types)"
      ],
      "metadata": {
        "id": "BnIu9PojR_1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "h_s5AHuBqv4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test = preprocessing.transform(X_test)\n",
        "test_predictions_proba = clf.predict_proba(X_test)[:, 1]\n",
        "test_predictions_proba"
      ],
      "metadata": {
        "id": "Qj-AgQTKqI7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_proba\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/clf_KNeighbors_v0_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "EoIZKdnDqI7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
        "ax[0].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv1)\n",
        "ax[1].scatter(test_predictions_resampled_cv2,test_predictions_proba)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xdZWX52pqI7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### XGBClassifier"
      ],
      "metadata": {
        "id": "Gv27Bhn0YQbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective_xgb(trial):\n",
        "    # Suggest key hyperparameters for XGBClassifier\n",
        "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 500)\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True)\n",
        "    max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n",
        "    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n",
        "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n",
        "    gamma = trial.suggest_float('gamma', 0, 1)\n",
        "    reg_alpha = trial.suggest_float('reg_alpha', 0.00001, 1.0, log=True)\n",
        "    reg_lambda = trial.suggest_float('reg_lambda', 0.00001, 1.0, log=True)\n",
        "\n",
        "    clf = XGBClassifier(\n",
        "        use_label_encoder=False,\n",
        "        eval_metric='logloss',\n",
        "        random_state=42,\n",
        "        n_estimators=n_estimators,\n",
        "        learning_rate=learning_rate,\n",
        "        max_depth=max_depth,\n",
        "        subsample=subsample,\n",
        "        colsample_bytree=colsample_bytree,\n",
        "        gamma=gamma,\n",
        "        reg_alpha=reg_alpha,\n",
        "        reg_lambda=reg_lambda\n",
        "    )\n",
        "\n",
        "    score = cross_val_score(clf, X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()\n",
        "    return score\n",
        "study_xgb = optuna.create_study(direction=\"maximize\")\n",
        "study_xgb.optimize(objective_xgb, n_trials=71) #adjust to 50\n",
        "best_params_xgb = study_xgb.best_trial.params\n",
        "print(\"Best params for XGBoost:\", best_params_xgb)"
      ],
      "metadata": {
        "id": "PIC4g-5_YQbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Parameters: {'n_estimators': 325, 'learning_rate': 0.06278279256080053, 'max_depth': 8, 'subsample': 0.70, 'colsample_bytree': 0.87, 'gamma': 0.17, 'reg_alpha': 0.024, 'reg_lambda': 8.111e-05}\n",
        "* Score 0.9618260284102824"
      ],
      "metadata": {
        "id": "DTFZ6jMAYQbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the optimization history (objective value over\n",
        "vis.plot_optimization_history(study_xgb)\n",
        "#fig_history.write_html(\"optimization_history.html\")"
      ],
      "metadata": {
        "id": "Ak70pdZLYQbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the parameter importances\n",
        "plot_param_importances(study_xgb)\n",
        "#fig_importances.write_html(\"parameter_importances.html\")"
      ],
      "metadata": {
        "id": "WwESjD2wYQbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Voting Classifier"
      ],
      "metadata": {
        "id": "MmV2DA-sYQbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Build final models using the best parameters from Optuna studies\n",
        "rf_final = RandomForestClassifier(class_weight='balanced', random_state=42, **best_params_rf)\n",
        "gb_final = GradientBoostingClassifier(random_state=42, **best_params_gb)\n",
        "xgb_final = XGBClassifier(use_label_encoder=False, eval_metric=\"auc\", random_state=42, **best_params_xgb)\n",
        "\n",
        "# Create a VotingClassifier with the best estimators\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf_final),\n",
        "        ('gb', gb_final),\n",
        "        ('xgb', xgb_final)\n",
        "    ],\n",
        "    voting='soft'  # or 'soft' if probability estimates are preferred\n",
        ")\n",
        "\n",
        "# Fit the VotingClassifier on your resampled training data\n",
        "voting_clf.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Evaluate using cross-validation with ROC AUC as the metric.\n",
        "# If it's a binary classification, use 'roc_auc'.\n",
        "# For multi-class problems, consider using 'roc_auc_ovr' or 'roc_auc_ovo'\n",
        "from sklearn.model_selection import cross_val_score\n",
        "voting_roc_auc = cross_val_score(voting_clf, X_train_res, y_train_res, cv=cv, scoring='roc_auc').mean()\n",
        "print(\"Voting Classifier CV ROC AUC Score:\", voting_roc_auc)"
      ],
      "metadata": {
        "id": "vPG7ePUeYQbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = sio.dump(voting_clf, \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/voting_v0.skops\")\n",
        "\n",
        "unknown_types = sio.get_untrusted_types(file=\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/voting_v0.skops\")\n",
        "# investigate the contents of unknown_types, and only load if you trust\n",
        "# everything you see.\n",
        "voting_clf = sio.load(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E3/voting_v0.skops\", trusted=unknown_types)"
      ],
      "metadata": {
        "id": "L_YSvE9QYQbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_pred_proba = voting_clf.predict_proba(X_val)[:, 1]\n",
        "print('Validation ROC AUC:', roc_auc_score(y_val, y_val_pred_proba),'\\n')\n",
        "print(classification_report(y_val, voting_clf.predict(X_val)))"
      ],
      "metadata": {
        "id": "d0D4ea6OYQbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = preprocessing.transform(test)\n",
        "test_predictions_proba = voting_clf.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "hjJpw1VZYQbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subm[\"rainfall\"] = test_predictions_proba\n",
        "df_subm.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E3/submission_voting_v0_all_data_ext.csv\")\n",
        "df_subm"
      ],
      "metadata": {
        "id": "sVJ2ThimYQbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10,3))\n",
        "ax[0].scatter(test_predictions_resampled_cv0,test_predictions_resampled_cv1)\n",
        "ax[1].scatter(test_predictions_resampled_cv2,test_predictions_proba)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dOy7875lYQbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 CNN Classifier"
      ],
      "metadata": {
        "id": "gFI8NU4gfwDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sktime"
      ],
      "metadata": {
        "id": "vda8e61NuY5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
        "\n",
        "METRICS = [\n",
        "          keras.metrics.BinaryCrossentropy(name='cross entropy'),  # same as model's loss\n",
        "          keras.metrics.AUC(name='auc'),\n",
        "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
        "          ]"
      ],
      "metadata": {
        "id": "GheFySZDnfze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resampled_ds = tf.data.Dataset.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\n",
        "resampled_ds = resampled_ds.batch(10_000).prefetch(2)"
      ],
      "metadata": {
        "id": "2KjAtyMBkeoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features, label in resampled_ds.take(1):\n",
        "  print(label.numpy().mean())\n",
        "  print(label.numpy().shape)\n",
        "  print(features.numpy().shape)"
      ],
      "metadata": {
        "id": "Itiww3_MkeoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_validation = keras.preprocessing.timeseries_dataset_from_array(\n",
        "                                                                    x_valid,\n",
        "                                                                    y_valid,\n",
        "                                                                    sequence_length=7,\n",
        "                                                                    sampling_rate=step,\n",
        "                                                                    batch_size=2000,\n",
        "                                                                    shuffle=False\n",
        "                                                                 )\n",
        "\n",
        "\n",
        "for batch in dataset_validation.take(1):\n",
        "    inputs, targets = batch\n",
        "\n",
        "print(\"Input shape:\", inputs.numpy().shape)\n",
        "print(\"Target shape:\", targets.numpy().shape)"
      ],
      "metadata": {
        "id": "mvKXKh8iFAlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = np.concatenate((inputs, features), axis=0)\n",
        "y = np.concatenate((targets, label), axis=0)\n",
        "train.shape, y.shape"
      ],
      "metadata": {
        "id": "cAazLXbZFAhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "call_bk = [keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.3, monitor=\"val_auc\", min_lr=0.000001),\n",
        "                                         keras.callbacks.EarlyStopping(patience=31, restore_best_weights=True, monitor=\"val_auc\",\n",
        "                                                                      start_from_epoch=3, mode=\"max\")]\n",
        "\n",
        "model = GRUFCNNClassifier( hidden_dim=128, gru_layers=2, verbose=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#CNNClassifier(n_epochs=8, batch_size=64, kernel_size=3, avg_pool_size=2, n_conv_layers=2, callbacks=call_bk,\n",
        "#                      verbose=True, loss=keras.losses.BinaryCrossentropy(), metrics=METRICS, random_state=42,\n",
        "#                      activation='sigmoid', use_bias=True, optimizer=keras.optimizers.Adam(learning_rate=1e-3), filter_sizes=None, padding='auto')"
      ],
      "metadata": {
        "id": "po3FcgrSFAer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prob_pred = model.fit_predict_proba(train, y, cv=5)"
      ],
      "metadata": {
        "id": "TH8_GPrKFAbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "class TimeSeriesPositionalEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.position_embeddings = keras.layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return inputs + embedded_positions\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention and Normalization\n",
        "    x = keras.layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(inputs, inputs)\n",
        "    x = keras.layers.Dropout(dropout)(x)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "    x = keras.layers.Dropout(dropout)(x)\n",
        "    x = keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1, activation='gelu')(x)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res\n",
        "\n",
        "# Example Usage\n",
        "sequence_length = 7 # Example sequence length\n",
        "embed_dim = 30 # Example embedding dimension.\n",
        "head_size = 32\n",
        "num_heads = 4\n",
        "ff_dim = 128\n",
        "dropout = 0.1\n",
        "\n",
        "inputs = keras.layers.Input(shape=(sequence_length, embed_dim))\n",
        "x = TimeSeriesPositionalEmbedding(sequence_length, embed_dim)(inputs)\n",
        "x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=x)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "iwQAsGz3h93w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test SKTime:"
      ],
      "metadata": {
        "id": "zayv4QQIRXxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sktime"
      ],
      "metadata": {
        "id": "MAToVO3jTKHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.datasets import load_italy_power_demand"
      ],
      "metadata": {
        "id": "TbO5PR3yRZsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Increase display width\n",
        "pd.set_option(\"display.width\", 1000)"
      ],
      "metadata": {
        "id": "0UhHHGK3RcSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Panel data - sktime data formats\n",
        "\n",
        "Panel is an abstract data type where the values are observed for:\n",
        "\n",
        "* instance, e.g., patient\n",
        "* variable, e.g., blood pressure, body temperature of the patient\n",
        "* time/index, e.g., January 12, 2023 (usually but not necessarily a time index!)\n",
        "One value X is: \"patient 'A' had blood pressure 'X' on January 12, 2023\"\n",
        "\n",
        "Time series classification, regression, clustering: slices Panel data by instance\n",
        "\n",
        "Preferred format 1: pd.DataFrame with 2-level MultiIndex, (instance, time) and columns: variables\n",
        "\n",
        "Preferred format 2: 3D np.ndarray with index (instance, variable, time)\n",
        "\n",
        "* sktime supports and recognizes multiple data formats for convenience and internal use, e.g., dask, xarray\n",
        "* abstract data type = \"scitype\"; in-memory specification = \"mtype\"\n",
        "* More information in tutorial on in-memory data representations and data loading"
      ],
      "metadata": {
        "id": "cyudcXKKTats"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.1 Preferred format 1 - pd-multiindex specification\n",
        "\n",
        "pd-multiindex = pd.DataFrame with 2-level MultiIndex, (instance, time) and columns: variables"
      ],
      "metadata": {
        "id": "SE847lLZTzVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.datasets import load_italy_power_demand\n",
        "\n",
        "# load an example time series panel in pd-multiindex mtype\n",
        "X, _ = load_italy_power_demand(return_type=\"pd-multiindex\")\n",
        "\n",
        "# renaming columns for illustrative purposes\n",
        "X.columns = [\"total_power_demand\"]\n",
        "X.index.names = [\"day_ID\", \"hour_of_day\"]"
      ],
      "metadata": {
        "id": "l6m94P00TV2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Italy power demand dataset has:\n",
        "\n",
        "* 1096 individual time series instances = single days of total power demand (mean subtracted)\n",
        "* one single variable per time series instances, total_power_demand\n",
        "    * total power demand on that day, in that hourly period\n",
        "    * Since there's only one column, it is a univariate dataset\n",
        "* individual time series are observed at 24 time (period) points (the same number for all instances)\n",
        "\n",
        "In the dataset, days are jumbled and of different scope (independent sampling).\n",
        "\n",
        "* considered independent - because hour_of_day in one sample doesn't affect hour_of_day in another\n",
        "* for task, e.g., \"identify season or weekday/weekend from pattern\""
      ],
      "metadata": {
        "id": "sWsfCArKT8rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "z_myGPjhT5_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.datasets import load_basic_motions\n",
        "\n",
        "# load an example time series panel in pd-multiindex mtype\n",
        "X, _ = load_basic_motions(return_type=\"pd-multiindex\")\n",
        "\n",
        "# renaming columns for illustrative purposes\n",
        "X.columns = [\"accel_1\", \"accel_2\", \"accel_3\", \"gyro_1\", \"gyro_2\", \"gyro_3\"]\n",
        "X.index.names = [\"trial_no\", \"timepoint\"]"
      ],
      "metadata": {
        "id": "TY4Vrnp7UK-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic motions dataset has:\n",
        "\n",
        "* 80 individual time series instances = trials = person engaging in an activity like running, badminton, etc.\n",
        "* six variables per time series instance, dim_0 to dim_5 (renamed according to the values they represent)\n",
        "  * 3 accelerometer and 3 gyrometer measurements\n",
        "  * hence a multivariate dataset\n",
        "* individual time series are observed at 100 time points (the same number for all instances)"
      ],
      "metadata": {
        "id": "Nrxol4DyUe1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The outermost index represents the instance number\n",
        "# whereas the inner index represents the index of the particular index\n",
        "# within that instance.\n",
        "X"
      ],
      "metadata": {
        "id": "MO3VEr7nUZm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select:\n",
        "# * the fourth variable (gyroscope 1)\n",
        "# * of the first instance (trial 1 = 0 in python)\n",
        "# * values at all 100 timestamps\n",
        "#\n",
        "X.loc[0, \"gyro_1\"]"
      ],
      "metadata": {
        "id": "mD9LprOwUbjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.loc[0, \"gyro_1\"].plot()"
      ],
      "metadata": {
        "id": "uw0fGi7HUxkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1.2 preferred format 2 - numpy3D"
      ],
      "metadata": {
        "id": "Wfi-zLf-ZNsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    numpy3D = 3D np.ndarray with index (instance, variable, time)\n",
        "\n",
        "instance/time index is interpreted as integer\n",
        "\n",
        "IMPORTANT: unlike pd-multiindex, this assumes:\n",
        "\n",
        "* all individual series have the same length\n",
        "\n",
        "* all individual series have the same index"
      ],
      "metadata": {
        "id": "LW4wQtAAZUSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.datasets import load_italy_power_demand\n",
        "\n",
        "# load an example time series panel in numpy mtype\n",
        "X, _ = load_italy_power_demand(return_type=\"numpy3D\")"
      ],
      "metadata": {
        "id": "xVrXvTQjU1S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (num_instances, num_variables, length)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "7TzDv09fZejX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.datasets import load_basic_motions\n",
        "\n",
        "# load an example time series panel in numpy mtype\n",
        "X, _ = load_basic_motions(return_type=\"numpy3D\")"
      ],
      "metadata": {
        "id": "VqJSo7jybZVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (num_instances, num_variables, length)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "E0x56kYGbbAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2.3 Time Series Classification - deployment vignette\n",
        "Basic deployment vignette for TSC:\n",
        "\n",
        "load/setup training data, X in a Panel (more specifically numpy3D) format, y as 1D np.ndarray\n",
        "\n",
        "load/setup new data for prediction (can be done after 3 too)\n",
        "\n",
        "specify the classifier using sklearn-like syntax\n",
        "\n",
        "fit classifier to training data, fit(X, y)\n",
        "\n",
        "predict labels on new data, predict(X_new)"
      ],
      "metadata": {
        "id": "d23cqamZbvWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# steps 1, 2 - prepare osuleaf dataset (train and new)\n",
        "from sktime.datasets import load_italy_power_demand\n",
        "\n",
        "X_train, y_train = load_italy_power_demand(split=\"train\", return_type=\"numpy3D\")\n",
        "X_new, _ = load_italy_power_demand(split=\"test\", return_type=\"numpy3D\")"
      ],
      "metadata": {
        "id": "YcBUhfK6bjtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is in numpy3D format, but could also be pd-multiindex or other\n",
        "X_train.shape, X_new.shape"
      ],
      "metadata": {
        "id": "J45agVQeb1OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y is a 1D np.ndarray of labels - same length as number of instances in X_train\n",
        "y_train.shape"
      ],
      "metadata": {
        "id": "U-bcmP52b3DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3 - specify the classifier\n",
        "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "\n",
        "# example 1 - 3-NN with simple dynamic time warping distance (requires numba)\n",
        "clf = KNeighborsTimeSeriesClassifier(n_neighbors=3)\n",
        "\n",
        "# example 2 - custom distance:\n",
        "# 3-nearest neighbour classifier with Euclidean distance (on flattened time series)\n",
        "# (requires scipy)\n",
        "from sktime.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
        "from sktime.dists_kernels import FlatDist, ScipyDist\n",
        "\n",
        "eucl_dist = FlatDist(ScipyDist())\n",
        "clf = KNeighborsTimeSeriesClassifier(n_neighbors=3, distance=eucl_dist)"
      ],
      "metadata": {
        "id": "aZriiT87b6uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all classifiers is scikit-learn / scikit-base compatible!\n",
        "# nested parameter interface via get_params, set_params\n",
        "clf.get_params()"
      ],
      "metadata": {
        "id": "xvYlcMNacCgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4 - fit/train the classifier\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ceHwNXHicFQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the classifier is now fitted\n",
        "clf.is_fitted"
      ],
      "metadata": {
        "id": "xrT5PRgWcIEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and we can inspect fitted parameters if we like\n",
        "clf.get_fitted_params()"
      ],
      "metadata": {
        "id": "ZqumVPcccKiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5 - predict labels on new data\n",
        "y_pred = clf.predict(X_new)\n",
        "y_prob = clf.predict_proba(X_new)"
      ],
      "metadata": {
        "id": "aLb_-KXMcMZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions and unique counts, for illustration\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "unique, counts"
      ],
      "metadata": {
        "id": "lnU04c2hcP-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XM0oLHoDcRKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sktime.classification.interval_based import TimeSeriesForestClassifier\n",
        "from sktime.datasets import load_arrow_head\n",
        "\n",
        "# step 1-- prepare a dataset (multivariate for demonstration)\n",
        "X_train, y_train = load_arrow_head(split=\"train\", return_type=\"numpy3D\")\n",
        "X_new, _ = load_arrow_head(split=\"test\", return_type=\"numpy3D\")"
      ],
      "metadata": {
        "id": "JhjQ_hw8cW3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "h5lpPMQ6c-iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train"
      ],
      "metadata": {
        "id": "7s9v2kjwdBF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sktime.classification.deep_learning as dl_clf\n",
        "from sktime.classification.deep_learning import GRUFCNNClassifier\n",
        "from sktime.datasets import load_unit_test\n",
        "X_train, y_train = load_unit_test(split=\"train\", return_type=\"numpy3D\")\n",
        "X_test, y_test = load_unit_test(split=\"test\", return_type=\"numpy3D\")"
      ],
      "metadata": {
        "id": "bPBRFcgzh4Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "DXVqNi4uepew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = GRUFCNNClassifier( hidden_dim=128, gru_layers=2, verbose=True)\n",
        "cnn.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "0KZdfe7feOJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iR3kK1ihfRjG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}