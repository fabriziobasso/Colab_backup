{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMYfoxyPabV49zuAXAgC3ir",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Colab_backup/blob/main/File_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quBZdpFsyyqC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGa3SjrXsW_F"
      },
      "source": [
        "# **S5E2 - Backpack Prices**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4M8cw5duVkf"
      },
      "outputs": [],
      "source": [
        "# Necessry to run LGBMRegressor\n",
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rYYQgW0Rmxv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qq pytorch_tabnet\n",
        "!pip install optuna\n",
        "!pip install catboost\n",
        "#!pip install optuna-integration-pytorch-tabnet\n",
        "\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade\n",
        "\n",
        "#from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "\n",
        "!pip install --upgrade category-encoders\n",
        "!pip install optuna-integration\n",
        "!pip install colorama\n",
        "#!pip install pyfiglet\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install keras-nlp\n",
        "!pip install BorutaShap\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install scikit-lego\n",
        "!pip install skops\n",
        "\n",
        "#from pytorch_tabnet.tab_model import TabNetRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIS1habP8JGi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "9ca7c8fd-63bb-44d0-ebd3-1ecdc8b41e52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Setup notebook\n",
        "from pathlib import Path\n",
        "import ipywidgets as widgets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pickle import load, dump\n",
        "import json\n",
        "import joblib\n",
        "#import calplot as cal\n",
        "\n",
        "# Graphic Libraries:\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.image as mpimg\n",
        "# Set Style\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5});\n",
        "sns.despine(left=True, bottom=True, top=False, right=False);\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "mpl.rc('axes', labelsize=12);\n",
        "plt.rc('xtick',labelsize=10);\n",
        "plt.rc('ytick',labelsize=10);\n",
        "\n",
        "mpl.rcParams['axes.spines.top'] = False;\n",
        "mpl.rcParams['axes.spines.right'] = False;\n",
        "mpl.rcParams['axes.spines.left'] = True;\n",
        "\n",
        "# Palette Setup\n",
        "colors = ['#FB5B68','#FFEB48','#2676A1','#FFBDB0',]\n",
        "colormap_0 = mpl.colors.LinearSegmentedColormap.from_list(\"\",colors)\n",
        "palette_1 = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "palette_2 = sns.color_palette(\"YlOrBr\", as_cmap=True)\n",
        "palette_3 = sns.light_palette(\"red\", as_cmap=True)\n",
        "palette_4 = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "palette_5 = sns.color_palette(\"rocket\", as_cmap=True)\n",
        "palette_6 = sns.color_palette(\"GnBu\", as_cmap=True)\n",
        "palette_7 = sns.color_palette(\"tab20c\", as_cmap=False)\n",
        "palette_8 = sns.color_palette(\"Set2\", as_cmap=False)\n",
        "\n",
        "palette_custom = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
        "palette_9 = sns.color_palette(palette_custom, as_cmap=False)\n",
        "\n",
        "# tool for Excel:\n",
        "from openpyxl import load_workbook, Workbook\n",
        "from openpyxl.drawing.image import Image\n",
        "from openpyxl.styles import Border, Side, PatternFill, Font, GradientFill, Alignment\n",
        "from openpyxl.worksheet.cell_range import CellRange\n",
        "\n",
        "from openpyxl.formatting import Rule\n",
        "from openpyxl.styles import Font, PatternFill, Border\n",
        "from openpyxl.styles.differential import DifferentialStyle\n",
        "\n",
        "# Bloomberg\n",
        "#from xbbg import blp\n",
        "from catboost import CatBoostRegressor, Pool, CatBoostClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from xgboost.callback import EarlyStopping\n",
        "\n",
        "import lightgbm as lgb\n",
        "from lightgbm import (LGBMRegressor,\n",
        "                      LGBMClassifier,\n",
        "                      early_stopping,\n",
        "                      record_evaluation,\n",
        "                      log_evaluation)\n",
        "\n",
        "# Time Management\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BMonthEnd, QuarterEnd\n",
        "import datetime\n",
        "from pandas.tseries.offsets import BDay # BDay is business day, not birthday...\n",
        "import datetime as dt\n",
        "import click\n",
        "import glob\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import string\n",
        "\n",
        "from ipywidgets import AppLayout\n",
        "from ipywidgets import Dropdown, Layout, HTML, AppLayout, VBox, Label, HBox, BoundedFloatText, interact, Output\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.trial import TrialState\n",
        "from optuna.visualization import plot_intermediate_values\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_contour\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Reshape\n",
        "from keras.models import Model\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch, GridSearch, BayesianOptimization\n",
        "\n",
        "#from my_func import *\n",
        "\n",
        "# preprocessing modules\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, RepeatedKFold, cross_val_score, cross_validate, GroupKFold, GridSearchCV, RepeatedStratifiedKFold, cross_val_predict\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "from sklearn.preprocessing import (LabelEncoder,\n",
        "                                   StandardScaler,\n",
        "                                   MinMaxScaler,\n",
        "                                   OrdinalEncoder,\n",
        "                                   RobustScaler,\n",
        "                                   PowerTransformer,\n",
        "                                   OneHotEncoder,\n",
        "                                   QuantileTransformer,\n",
        "                                   PolynomialFeatures)\n",
        "\n",
        "# metrics\n",
        "import sklearn\n",
        "#import skops.io as sio\n",
        "from sklearn.metrics import (mean_squared_error,\n",
        "                             root_mean_squared_error,\n",
        "                             root_mean_squared_log_error,\n",
        "                             r2_score,\n",
        "                             mean_absolute_error,\n",
        "                             mean_absolute_percentage_error,\n",
        "                             classification_report,\n",
        "                             confusion_matrix,\n",
        "                             ConfusionMatrixDisplay,\n",
        "                             multilabel_confusion_matrix,\n",
        "                             accuracy_score,\n",
        "                             roc_auc_score,\n",
        "                             auc,\n",
        "                             roc_curve,\n",
        "                             log_loss,\n",
        "                             make_scorer)\n",
        "# modeling algos\n",
        "from sklearn.linear_model import (LogisticRegression,\n",
        "                                  Lasso,\n",
        "                                  ridge_regression,\n",
        "                                  LinearRegression,\n",
        "                                  Ridge,\n",
        "                                  RidgeCV,\n",
        "                                  ElasticNet,\n",
        "                                  BayesianRidge,\n",
        "                                  HuberRegressor,\n",
        "                                  TweedieRegressor,\n",
        "                                  QuantileRegressor,\n",
        "                                  ARDRegression,\n",
        "                                  TheilSenRegressor,\n",
        "                                  PoissonRegressor,\n",
        "                                  GammaRegressor)\n",
        "\n",
        "from sklearn.ensemble import (AdaBoostRegressor,\n",
        "                              AdaBoostClassifier,\n",
        "                              RandomForestRegressor,\n",
        "                              RandomForestClassifier,\n",
        "                              VotingRegressor,\n",
        "                              GradientBoostingRegressor,\n",
        "                              GradientBoostingClassifier,\n",
        "                              StackingRegressor,\n",
        "                              StackingClassifier,\n",
        "                              HistGradientBoostingClassifier,\n",
        "                              HistGradientBoostingRegressor,\n",
        "                              ExtraTreesClassifier)\n",
        "\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
        "\n",
        "from sklearn.multioutput import RegressorChain\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import itertools\n",
        "import warnings\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from pylab import rcParams\n",
        "import scipy.stats as ss\n",
        "\n",
        "from category_encoders.cat_boost import CatBoostEncoder\n",
        "from category_encoders.wrapper import PolynomialWrapper\n",
        "from category_encoders.count import CountEncoder\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "#import pyfiglet\n",
        "#plt.style.use('fivethirtyeight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkkRPWKZCkYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64613852-da0c-4ade-fc47-4499d5a1e0ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 960x660 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.5,\n",
        "         'grid.alpha'           :0.5,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : 'black',\n",
        "         'axes.titlesize'       : 12,\n",
        "#         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "# Set Style\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "\n",
        "# import font colors\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "init(autoreset=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU7oWpLHRmxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67071728-3e9c-4af7-d62d-8a8dc3162a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from itertools import product\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Connect to Colab:#\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2PuCulFRmx1"
      },
      "source": [
        "<div style=\"text-align:center; border-radius:15px; padding:15px; margin:0; font-size:100%; font-family:Arial, sans-serif; background-color:#A8DADC; color:#1D3557; overflow:hidden; box-shadow:0 3px 6px rgba(0, 0, 0, 0.2);\">\n",
        "    <h3>Loading and Preprocessing Data for Compatibility</h3>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3odgloSjRmx4"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_enc_ext.csv\",index_col=0)\n",
        "\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/test_enc_ext.csv\",index_col=0)\n",
        "\n",
        "# df_train_orig = pd.read_csv(\n",
        "#     '/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E10/credit_risk_dataset.csv'\n",
        "# )\n",
        "\n",
        "df_subm = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\",index_col=0)\n",
        "\n",
        "# df_orig = pd.read_csv(\n",
        "#     \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S4E12/Insurance Premium Prediction Dataset.csv\",\n",
        "#      parse_dates=['Policy Start Date'],\n",
        "#     #     index_col='id',\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vwd0o1ph1Ai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c6f91d-83ac-4acf-e7b6-acf0d60580ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3994318, 31), (200000, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df_train.head()\n",
        "df_train.shape,df_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tcWfu7npSHN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "968836fb-c33a-4ad5-da09-d1b414dc5ed2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  Style  \\\n",
              "0      1         1     1             7                   2           1      3   \n",
              "1      1         0     3             1                   2           2      1   \n",
              "2      5         1     3             2                   2           1      1   \n",
              "3      3         3     3             8                   2           1      1   \n",
              "4      0         0     1             0                   2           2      1   \n",
              "\n",
              "   Color  Weight Capacity (kg)  Weight Capacity (kg)_missing  Mat_Siz_Col  \\\n",
              "0      0             -0.917722                             0            0   \n",
              "1      3              1.300573                             0            0   \n",
              "2      6             -0.196013                             0            0   \n",
              "3      3             -0.727615                             0            0   \n",
              "4      3             -0.037447                             0            0   \n",
              "\n",
              "   Siz_Lap_Col  Bra_Siz_Wat  Siz_Lap_Wat  Mat_Lap_Wat  Bra_Siz_Sty  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Bra_Lap_Wat  Siz_Com_Lap  Siz_Lap_Sty  Mat_Com_Lap  Mat_Siz_Com  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Bra_Siz_Com  Com_Lap_Wat  Bra_Siz_Lap  Bra_Mat_Siz  Siz_Com_Wat  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Siz_Com_Sty     TE_wc    skew_0    skew_1      Price  \n",
              "0            0  0.261445 -0.292388 -0.428820  112.15875  \n",
              "1            0  0.621130 -0.302957 -0.460902   68.88056  \n",
              "2            0  0.016408 -0.301780 -1.112454   39.17320  \n",
              "3            0  1.498987 -0.301780 -0.551413   80.60793  \n",
              "4            0  0.016408 -0.375870  0.519525   86.02312  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6bb5d3c-9cfe-4930-9d1d-faec26c91fa7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>Weight Capacity (kg)_missing</th>\n",
              "      <th>Mat_Siz_Col</th>\n",
              "      <th>Siz_Lap_Col</th>\n",
              "      <th>Bra_Siz_Wat</th>\n",
              "      <th>Siz_Lap_Wat</th>\n",
              "      <th>Mat_Lap_Wat</th>\n",
              "      <th>Bra_Siz_Sty</th>\n",
              "      <th>Bra_Lap_Wat</th>\n",
              "      <th>Siz_Com_Lap</th>\n",
              "      <th>Siz_Lap_Sty</th>\n",
              "      <th>Mat_Com_Lap</th>\n",
              "      <th>Mat_Siz_Com</th>\n",
              "      <th>Bra_Siz_Com</th>\n",
              "      <th>Com_Lap_Wat</th>\n",
              "      <th>Bra_Siz_Lap</th>\n",
              "      <th>Bra_Mat_Siz</th>\n",
              "      <th>Siz_Com_Wat</th>\n",
              "      <th>Siz_Com_Sty</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.917722</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.261445</td>\n",
              "      <td>-0.292388</td>\n",
              "      <td>-0.428820</td>\n",
              "      <td>112.15875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1.300573</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.621130</td>\n",
              "      <td>-0.302957</td>\n",
              "      <td>-0.460902</td>\n",
              "      <td>68.88056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.196013</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-0.301780</td>\n",
              "      <td>-1.112454</td>\n",
              "      <td>39.17320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.727615</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.498987</td>\n",
              "      <td>-0.301780</td>\n",
              "      <td>-0.551413</td>\n",
              "      <td>80.60793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.037447</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>-0.375870</td>\n",
              "      <td>0.519525</td>\n",
              "      <td>86.02312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6bb5d3c-9cfe-4930-9d1d-faec26c91fa7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e6bb5d3c-9cfe-4930-9d1d-faec26c91fa7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e6bb5d3c-9cfe-4930-9d1d-faec26c91fa7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d8987680-7402-469c-9d73-b5163bbf6f28\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8987680-7402-469c-9d73-b5163bbf6f28')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d8987680-7402-469c-9d73-b5163bbf6f28 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#df_train_orig.isna().sum()\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njvQSzYr_4BD"
      },
      "outputs": [],
      "source": [
        "def plot_scatter(x=\"Price\",y=\"TE_wc\", df=df_train):\n",
        "\n",
        "  plt.figure(figsize=(5,5))\n",
        "  plt.scatter(df[x],df[y])\n",
        "  plt.xlabel(x)\n",
        "  plt.ylabel(y)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R5dhMXeAUhG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "89ce5b8e-b373-4338-ec99-cc14760689a2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_scatter(x=\"Price\",y=\"TE_wc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pykP87axQAvp"
      },
      "outputs": [],
      "source": [
        "#df_train[\"person_emp_length\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hREKVL1iPrWR"
      },
      "outputs": [],
      "source": [
        "#df_train[df_train[\"cb_person_cred_hist_length\"]>0.75*df_train[\"person_age\"]]\n",
        "#np.round(df_train[\"cb_person_cred_hist_length\"].mean(),0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vfGRiTeLCd2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am9pd9NwKIBl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho9znWqB1KpL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFmR4Gl6JRAl"
      },
      "source": [
        "**Descriptions of Loan Data**\n",
        "\n",
        "Descriptions for the column names based on the data provided:\n",
        "\n",
        "* **id**: Unique identifier for each record.\n",
        "* **person_age**: Age of the individual, categorized into ranges.\n",
        "* **person_income**: Income of the individual, categorized into income ranges.\n",
        "* **person_home_ownership**: Homeownership status, which includes categories like 'RENT', 'MORTGAGE', etc.\n",
        "* **person_emp_length**: Employment length of the individual, categorized into ranges based on years.\n",
        "* **loan_intent**: The purpose of the loan, with categories such as 'EDUCATION', 'MEDICAL', etc.\n",
        "* **loan_grade**: The credit grade of the loan, such as 'A', 'B', etc.\n",
        "* **loan_amnt**: Loan amount, categorized into ranges.\n",
        "* **loan_int_rate**: Loan interest rate, categorized into percentage ranges.\n",
        "* **loan_percent_income**: Percentage of the individuals income that the loan represents, categorized into - ranges.\n",
        "* **cb_person_default_on_file**: Whether the person has a history of loan default, with values 'true' or 'false'.\n",
        "* **cb_person_cred_hist_length**: Length of the individuals credit history, categorized into ranges.\n",
        "* **loan_status**: with values representing whether the loan status approval( binary values)\n",
        "\n",
        "The dataset is a about loan applications, including personal, financial, and loan details. It's likely used for predicting whether a person will default on a loan, making it a binary classification problem. The goal is to figure out which applicants are at higher risk of not paying back their loans based on their age, income, employment, loan purpose, credit history, and other related information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_IkGc2Wya01"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "\n",
        "    state = 42\n",
        "    n_splits = 10\n",
        "    early_stop = 200\n",
        "\n",
        "    target = 'Price'\n",
        "    problem = \"Regression\"\n",
        "    train = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_enc_ext.csv', index_col=0)\n",
        "    test = pd.read_csv('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/test_enc_ext.csv', index_col=0)\n",
        "    submission = pd.read_csv( \"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\", index_col=0)\n",
        "    train_org = None\n",
        "\n",
        "    original_data = 'N'\n",
        "    outliers = 'N'\n",
        "    log_trf = 'N'\n",
        "    scaler_trf = 'Y'\n",
        "    feature_eng = 'Y'\n",
        "    missing = 'Y'\n",
        "    force_normalization=\"N\"\n",
        "    impose_normalization=\"Y\"\n",
        "    trg_enc = \"N\"\n",
        "    metric_goal=\"rmse\"\n",
        "    direction_=\"minimize\"\n",
        "    log_trans_cols = []\n",
        "    force_norm_cols = []\n",
        "    impose_norm_cols = [\"skew_0\",\"skew_1\"]\n",
        "    trg_enc_feat = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjPBIogOJ2mz"
      },
      "outputs": [],
      "source": [
        "class Preprocessing():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.train = Config.train\n",
        "        self.test = Config.test\n",
        "        self.targets = Config.target\n",
        "        self.problem = Config.problem\n",
        "        self.submission = Config.submission\n",
        "\n",
        "        self.prp_data()\n",
        "\n",
        "    def prp_data(self):\n",
        "\n",
        "        if Config.original_data == 'Y':\n",
        "            self.train = pd.concat([self.train, Config.train_org], ignore_index=True).drop_duplicates(ignore_index=True)\n",
        "        if 'id' in self.train.columns:\n",
        "            self.train = self.train.drop(['id'], axis=1)\n",
        "            self.test = self.test.drop(['id'], axis=1)\n",
        "\n",
        "        self.cat_features = self.train.drop(self.targets, axis=1).select_dtypes(include=['object', 'bool', 'int', 'category']).columns.tolist()\n",
        "        self.num_features = self.train.drop(self.targets, axis=1).select_dtypes(exclude=['object', 'bool', 'int', 'category']).columns.tolist()\n",
        "\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
        "\n",
        "        self.train = self.reduce_mem(self.train)\n",
        "        self.test = self.reduce_mem(self.test)\n",
        "        return self\n",
        "\n",
        "    def reduce_mem(self, df):\n",
        "\n",
        "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64', \"uint16\", \"uint32\", \"uint64\"]\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_type = df[col].dtypes\n",
        "\n",
        "            if col_type in numerics:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "\n",
        "                if \"int\" in str(col_type):\n",
        "                    if c_min >= np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min >= np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)\n",
        "                else:\n",
        "                    if c_min >= np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    if c_min >= np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)\n",
        "\n",
        "        return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICqfSaYMKIF8"
      },
      "outputs": [],
      "source": [
        "class EDA(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.data_info()\n",
        "        self.heatmap()\n",
        "        #self.dist_plots()\n",
        "        #self.cat_feature_plots()\n",
        "        #self.target_pie()\n",
        "\n",
        "    def data_info(self):\n",
        "\n",
        "        for data, label in zip([self.train, self.test], ['Train', 'Test']):\n",
        "            table_style = [{'selector': 'th:not(.index_name)',\n",
        "                            'props': [('background-color', 'slategrey'),\n",
        "                                      ('color', '#FFFFFF'),\n",
        "                                      ('font-weight', 'bold'),\n",
        "                                      ('border', '1px solid #DCDCDC'),\n",
        "                                      ('text-align', 'center')]\n",
        "                            },\n",
        "                            {'selector': 'tbody td',\n",
        "                             'props': [('border', '1px solid #DCDCDC'),\n",
        "                                       ('font-weight', 'normal')]\n",
        "                            }]\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} head\\n')\n",
        "            display(data.head().style.set_table_styles(table_style))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} info\\n'+Style.RESET_ALL)\n",
        "            display(data.info())\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} describe\\n')\n",
        "            display(data.describe().drop(index='count', columns=self.targets, errors = 'ignore').T\n",
        "                    .style.set_table_styles(table_style).format('{:.3f}'))\n",
        "\n",
        "            print(Style.BRIGHT+Fore.RED+f'\\n{label} missing values\\n'+Style.RESET_ALL)\n",
        "            display(data.isna().sum())\n",
        "        return self\n",
        "\n",
        "    def heatmap(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nCorrelation Heatmap\\n')\n",
        "        plt.figure(figsize=(7,7))\n",
        "        corr = self.train.select_dtypes(exclude=['object', 'category']).corr(method='pearson')\n",
        "        sns.heatmap(corr, fmt = '0.2f', cmap = 'Blues', annot=True, cbar=False)\n",
        "        plt.show()\n",
        "\n",
        "    def dist_plots(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Numerical\\n\")\n",
        "        df = pd.concat([self.train[self.num_features].assign(Source = 'Train'),\n",
        "                        self.test[self.num_features].assign(Source = 'Test'),],\n",
        "                        axis=0, ignore_index = True)\n",
        "\n",
        "        fig, axes = plt.subplots(len(self.num_features), 2 ,figsize = (18, len(self.num_features) * 6),\n",
        "                                 gridspec_kw = {'hspace': 0.3,\n",
        "                                                'wspace': 0.2,\n",
        "                                                'width_ratios': [0.70, 0.30]\n",
        "                                               }\n",
        "                                )\n",
        "        for i,col in enumerate(self.num_features):\n",
        "            ax = axes[i,0]\n",
        "            sns.kdeplot(data = df[[col, 'Source']], x = col, hue = 'Source',\n",
        "                        palette = ['royalblue', 'tomato'], ax = ax, alpha=0.7, linewidth = 2\n",
        "                       )\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.grid('--',alpha=0.7)\n",
        "\n",
        "            ax = axes[i,1]\n",
        "            sns.boxplot(data = df, y = col, x=df.Source, width = 0.5,\n",
        "                        linewidth = 1, fliersize= 1,\n",
        "                        ax = ax, palette=['royalblue', 'tomato']\n",
        "                       )\n",
        "            ax.set_title(f\"\\n{col}\")\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.tick_params(axis='both', which='major')\n",
        "            ax.set_xticklabels(['Train', 'Test'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def cat_feature_plots(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nDistribution analysis - Categorical\\n\")\n",
        "        fig, axes = plt.subplots(len(self.cat_features), 2 ,figsize = (18, len(self.cat_features) * 6),\n",
        "                                 gridspec_kw = {'hspace': 0.5,\n",
        "                                                'wspace': 0.2,\n",
        "                                               }\n",
        "                                )\n",
        "\n",
        "        for i, col in enumerate(self.cat_features):\n",
        "\n",
        "            ax = axes[i,0]\n",
        "            sns.barplot(data=self.train[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='royalblue', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Train\")\n",
        "\n",
        "            ax = axes[i,1]\n",
        "            sns.barplot(data=self.test[col].value_counts().nlargest(10).reset_index(), x=col, y='count', ax=ax, color='tomato', alpha=0.7)\n",
        "            ax.set(xlabel = '', ylabel = '')\n",
        "            ax.set_title(f\"\\n{col} Test\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def target_pie(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f\"\\nTarget feature distribution\\n\")\n",
        "        targets = self.train[self.targets]\n",
        "        plt.figure(figsize=(6, 6))\n",
        "        if self.problem==\"Regression\":\n",
        "          plt.hist(targets, bins=35, color='royalblue',alpha=0.7)\n",
        "        else:\n",
        "          plt.pie(targets.value_counts(), labels=targets.value_counts().index, autopct='%1.2f%%', colors=palette_9)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIy8MLVmvv3N"
      },
      "source": [
        "## 1.0 EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O1IeGiuKwDA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "873fd3dd-1023-4554-fda2-38dc0a841d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train head\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a1fa6531390>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_38790 th:not(.index_name) {\n",
              "  background-color: slategrey;\n",
              "  color: #FFFFFF;\n",
              "  font-weight: bold;\n",
              "  border: 1px solid #DCDCDC;\n",
              "  text-align: center;\n",
              "}\n",
              "#T_38790 tbody td {\n",
              "  border: 1px solid #DCDCDC;\n",
              "  font-weight: normal;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_38790\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_38790_level0_col0\" class=\"col_heading level0 col0\" >Brand</th>\n",
              "      <th id=\"T_38790_level0_col1\" class=\"col_heading level0 col1\" >Material</th>\n",
              "      <th id=\"T_38790_level0_col2\" class=\"col_heading level0 col2\" >Size</th>\n",
              "      <th id=\"T_38790_level0_col3\" class=\"col_heading level0 col3\" >Compartments</th>\n",
              "      <th id=\"T_38790_level0_col4\" class=\"col_heading level0 col4\" >Laptop Compartment</th>\n",
              "      <th id=\"T_38790_level0_col5\" class=\"col_heading level0 col5\" >Waterproof</th>\n",
              "      <th id=\"T_38790_level0_col6\" class=\"col_heading level0 col6\" >Style</th>\n",
              "      <th id=\"T_38790_level0_col7\" class=\"col_heading level0 col7\" >Color</th>\n",
              "      <th id=\"T_38790_level0_col8\" class=\"col_heading level0 col8\" >Weight Capacity (kg)</th>\n",
              "      <th id=\"T_38790_level0_col9\" class=\"col_heading level0 col9\" >Weight Capacity (kg)_missing</th>\n",
              "      <th id=\"T_38790_level0_col10\" class=\"col_heading level0 col10\" >Mat_Siz_Col</th>\n",
              "      <th id=\"T_38790_level0_col11\" class=\"col_heading level0 col11\" >Siz_Lap_Col</th>\n",
              "      <th id=\"T_38790_level0_col12\" class=\"col_heading level0 col12\" >Bra_Siz_Wat</th>\n",
              "      <th id=\"T_38790_level0_col13\" class=\"col_heading level0 col13\" >Siz_Lap_Wat</th>\n",
              "      <th id=\"T_38790_level0_col14\" class=\"col_heading level0 col14\" >Mat_Lap_Wat</th>\n",
              "      <th id=\"T_38790_level0_col15\" class=\"col_heading level0 col15\" >Bra_Siz_Sty</th>\n",
              "      <th id=\"T_38790_level0_col16\" class=\"col_heading level0 col16\" >Bra_Lap_Wat</th>\n",
              "      <th id=\"T_38790_level0_col17\" class=\"col_heading level0 col17\" >Siz_Com_Lap</th>\n",
              "      <th id=\"T_38790_level0_col18\" class=\"col_heading level0 col18\" >Siz_Lap_Sty</th>\n",
              "      <th id=\"T_38790_level0_col19\" class=\"col_heading level0 col19\" >Mat_Com_Lap</th>\n",
              "      <th id=\"T_38790_level0_col20\" class=\"col_heading level0 col20\" >Mat_Siz_Com</th>\n",
              "      <th id=\"T_38790_level0_col21\" class=\"col_heading level0 col21\" >Bra_Siz_Com</th>\n",
              "      <th id=\"T_38790_level0_col22\" class=\"col_heading level0 col22\" >Com_Lap_Wat</th>\n",
              "      <th id=\"T_38790_level0_col23\" class=\"col_heading level0 col23\" >Bra_Siz_Lap</th>\n",
              "      <th id=\"T_38790_level0_col24\" class=\"col_heading level0 col24\" >Bra_Mat_Siz</th>\n",
              "      <th id=\"T_38790_level0_col25\" class=\"col_heading level0 col25\" >Siz_Com_Wat</th>\n",
              "      <th id=\"T_38790_level0_col26\" class=\"col_heading level0 col26\" >Siz_Com_Sty</th>\n",
              "      <th id=\"T_38790_level0_col27\" class=\"col_heading level0 col27\" >TE_wc</th>\n",
              "      <th id=\"T_38790_level0_col28\" class=\"col_heading level0 col28\" >skew_0</th>\n",
              "      <th id=\"T_38790_level0_col29\" class=\"col_heading level0 col29\" >skew_1</th>\n",
              "      <th id=\"T_38790_level0_col30\" class=\"col_heading level0 col30\" >Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_38790_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_38790_row0_col0\" class=\"data row0 col0\" >1</td>\n",
              "      <td id=\"T_38790_row0_col1\" class=\"data row0 col1\" >1</td>\n",
              "      <td id=\"T_38790_row0_col2\" class=\"data row0 col2\" >1</td>\n",
              "      <td id=\"T_38790_row0_col3\" class=\"data row0 col3\" >7</td>\n",
              "      <td id=\"T_38790_row0_col4\" class=\"data row0 col4\" >2</td>\n",
              "      <td id=\"T_38790_row0_col5\" class=\"data row0 col5\" >1</td>\n",
              "      <td id=\"T_38790_row0_col6\" class=\"data row0 col6\" >3</td>\n",
              "      <td id=\"T_38790_row0_col7\" class=\"data row0 col7\" >0</td>\n",
              "      <td id=\"T_38790_row0_col8\" class=\"data row0 col8\" >-0.917722</td>\n",
              "      <td id=\"T_38790_row0_col9\" class=\"data row0 col9\" >0</td>\n",
              "      <td id=\"T_38790_row0_col10\" class=\"data row0 col10\" >0</td>\n",
              "      <td id=\"T_38790_row0_col11\" class=\"data row0 col11\" >0</td>\n",
              "      <td id=\"T_38790_row0_col12\" class=\"data row0 col12\" >0</td>\n",
              "      <td id=\"T_38790_row0_col13\" class=\"data row0 col13\" >0</td>\n",
              "      <td id=\"T_38790_row0_col14\" class=\"data row0 col14\" >0</td>\n",
              "      <td id=\"T_38790_row0_col15\" class=\"data row0 col15\" >0</td>\n",
              "      <td id=\"T_38790_row0_col16\" class=\"data row0 col16\" >0</td>\n",
              "      <td id=\"T_38790_row0_col17\" class=\"data row0 col17\" >0</td>\n",
              "      <td id=\"T_38790_row0_col18\" class=\"data row0 col18\" >0</td>\n",
              "      <td id=\"T_38790_row0_col19\" class=\"data row0 col19\" >0</td>\n",
              "      <td id=\"T_38790_row0_col20\" class=\"data row0 col20\" >0</td>\n",
              "      <td id=\"T_38790_row0_col21\" class=\"data row0 col21\" >0</td>\n",
              "      <td id=\"T_38790_row0_col22\" class=\"data row0 col22\" >0</td>\n",
              "      <td id=\"T_38790_row0_col23\" class=\"data row0 col23\" >0</td>\n",
              "      <td id=\"T_38790_row0_col24\" class=\"data row0 col24\" >0</td>\n",
              "      <td id=\"T_38790_row0_col25\" class=\"data row0 col25\" >0</td>\n",
              "      <td id=\"T_38790_row0_col26\" class=\"data row0 col26\" >0</td>\n",
              "      <td id=\"T_38790_row0_col27\" class=\"data row0 col27\" >0.261445</td>\n",
              "      <td id=\"T_38790_row0_col28\" class=\"data row0 col28\" >-0.292388</td>\n",
              "      <td id=\"T_38790_row0_col29\" class=\"data row0 col29\" >-0.428820</td>\n",
              "      <td id=\"T_38790_row0_col30\" class=\"data row0 col30\" >112.158752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38790_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_38790_row1_col0\" class=\"data row1 col0\" >1</td>\n",
              "      <td id=\"T_38790_row1_col1\" class=\"data row1 col1\" >0</td>\n",
              "      <td id=\"T_38790_row1_col2\" class=\"data row1 col2\" >3</td>\n",
              "      <td id=\"T_38790_row1_col3\" class=\"data row1 col3\" >1</td>\n",
              "      <td id=\"T_38790_row1_col4\" class=\"data row1 col4\" >2</td>\n",
              "      <td id=\"T_38790_row1_col5\" class=\"data row1 col5\" >2</td>\n",
              "      <td id=\"T_38790_row1_col6\" class=\"data row1 col6\" >1</td>\n",
              "      <td id=\"T_38790_row1_col7\" class=\"data row1 col7\" >3</td>\n",
              "      <td id=\"T_38790_row1_col8\" class=\"data row1 col8\" >1.300573</td>\n",
              "      <td id=\"T_38790_row1_col9\" class=\"data row1 col9\" >0</td>\n",
              "      <td id=\"T_38790_row1_col10\" class=\"data row1 col10\" >0</td>\n",
              "      <td id=\"T_38790_row1_col11\" class=\"data row1 col11\" >0</td>\n",
              "      <td id=\"T_38790_row1_col12\" class=\"data row1 col12\" >0</td>\n",
              "      <td id=\"T_38790_row1_col13\" class=\"data row1 col13\" >0</td>\n",
              "      <td id=\"T_38790_row1_col14\" class=\"data row1 col14\" >0</td>\n",
              "      <td id=\"T_38790_row1_col15\" class=\"data row1 col15\" >0</td>\n",
              "      <td id=\"T_38790_row1_col16\" class=\"data row1 col16\" >0</td>\n",
              "      <td id=\"T_38790_row1_col17\" class=\"data row1 col17\" >0</td>\n",
              "      <td id=\"T_38790_row1_col18\" class=\"data row1 col18\" >0</td>\n",
              "      <td id=\"T_38790_row1_col19\" class=\"data row1 col19\" >0</td>\n",
              "      <td id=\"T_38790_row1_col20\" class=\"data row1 col20\" >0</td>\n",
              "      <td id=\"T_38790_row1_col21\" class=\"data row1 col21\" >0</td>\n",
              "      <td id=\"T_38790_row1_col22\" class=\"data row1 col22\" >0</td>\n",
              "      <td id=\"T_38790_row1_col23\" class=\"data row1 col23\" >0</td>\n",
              "      <td id=\"T_38790_row1_col24\" class=\"data row1 col24\" >0</td>\n",
              "      <td id=\"T_38790_row1_col25\" class=\"data row1 col25\" >0</td>\n",
              "      <td id=\"T_38790_row1_col26\" class=\"data row1 col26\" >0</td>\n",
              "      <td id=\"T_38790_row1_col27\" class=\"data row1 col27\" >0.621130</td>\n",
              "      <td id=\"T_38790_row1_col28\" class=\"data row1 col28\" >-0.302957</td>\n",
              "      <td id=\"T_38790_row1_col29\" class=\"data row1 col29\" >-0.460902</td>\n",
              "      <td id=\"T_38790_row1_col30\" class=\"data row1 col30\" >68.880562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38790_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_38790_row2_col0\" class=\"data row2 col0\" >5</td>\n",
              "      <td id=\"T_38790_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "      <td id=\"T_38790_row2_col2\" class=\"data row2 col2\" >3</td>\n",
              "      <td id=\"T_38790_row2_col3\" class=\"data row2 col3\" >2</td>\n",
              "      <td id=\"T_38790_row2_col4\" class=\"data row2 col4\" >2</td>\n",
              "      <td id=\"T_38790_row2_col5\" class=\"data row2 col5\" >1</td>\n",
              "      <td id=\"T_38790_row2_col6\" class=\"data row2 col6\" >1</td>\n",
              "      <td id=\"T_38790_row2_col7\" class=\"data row2 col7\" >6</td>\n",
              "      <td id=\"T_38790_row2_col8\" class=\"data row2 col8\" >-0.196013</td>\n",
              "      <td id=\"T_38790_row2_col9\" class=\"data row2 col9\" >0</td>\n",
              "      <td id=\"T_38790_row2_col10\" class=\"data row2 col10\" >0</td>\n",
              "      <td id=\"T_38790_row2_col11\" class=\"data row2 col11\" >0</td>\n",
              "      <td id=\"T_38790_row2_col12\" class=\"data row2 col12\" >0</td>\n",
              "      <td id=\"T_38790_row2_col13\" class=\"data row2 col13\" >0</td>\n",
              "      <td id=\"T_38790_row2_col14\" class=\"data row2 col14\" >0</td>\n",
              "      <td id=\"T_38790_row2_col15\" class=\"data row2 col15\" >0</td>\n",
              "      <td id=\"T_38790_row2_col16\" class=\"data row2 col16\" >0</td>\n",
              "      <td id=\"T_38790_row2_col17\" class=\"data row2 col17\" >0</td>\n",
              "      <td id=\"T_38790_row2_col18\" class=\"data row2 col18\" >0</td>\n",
              "      <td id=\"T_38790_row2_col19\" class=\"data row2 col19\" >0</td>\n",
              "      <td id=\"T_38790_row2_col20\" class=\"data row2 col20\" >0</td>\n",
              "      <td id=\"T_38790_row2_col21\" class=\"data row2 col21\" >0</td>\n",
              "      <td id=\"T_38790_row2_col22\" class=\"data row2 col22\" >0</td>\n",
              "      <td id=\"T_38790_row2_col23\" class=\"data row2 col23\" >0</td>\n",
              "      <td id=\"T_38790_row2_col24\" class=\"data row2 col24\" >0</td>\n",
              "      <td id=\"T_38790_row2_col25\" class=\"data row2 col25\" >0</td>\n",
              "      <td id=\"T_38790_row2_col26\" class=\"data row2 col26\" >0</td>\n",
              "      <td id=\"T_38790_row2_col27\" class=\"data row2 col27\" >0.016408</td>\n",
              "      <td id=\"T_38790_row2_col28\" class=\"data row2 col28\" >-0.301780</td>\n",
              "      <td id=\"T_38790_row2_col29\" class=\"data row2 col29\" >-1.112454</td>\n",
              "      <td id=\"T_38790_row2_col30\" class=\"data row2 col30\" >39.173199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38790_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_38790_row3_col0\" class=\"data row3 col0\" >3</td>\n",
              "      <td id=\"T_38790_row3_col1\" class=\"data row3 col1\" >3</td>\n",
              "      <td id=\"T_38790_row3_col2\" class=\"data row3 col2\" >3</td>\n",
              "      <td id=\"T_38790_row3_col3\" class=\"data row3 col3\" >8</td>\n",
              "      <td id=\"T_38790_row3_col4\" class=\"data row3 col4\" >2</td>\n",
              "      <td id=\"T_38790_row3_col5\" class=\"data row3 col5\" >1</td>\n",
              "      <td id=\"T_38790_row3_col6\" class=\"data row3 col6\" >1</td>\n",
              "      <td id=\"T_38790_row3_col7\" class=\"data row3 col7\" >3</td>\n",
              "      <td id=\"T_38790_row3_col8\" class=\"data row3 col8\" >-0.727615</td>\n",
              "      <td id=\"T_38790_row3_col9\" class=\"data row3 col9\" >0</td>\n",
              "      <td id=\"T_38790_row3_col10\" class=\"data row3 col10\" >0</td>\n",
              "      <td id=\"T_38790_row3_col11\" class=\"data row3 col11\" >0</td>\n",
              "      <td id=\"T_38790_row3_col12\" class=\"data row3 col12\" >0</td>\n",
              "      <td id=\"T_38790_row3_col13\" class=\"data row3 col13\" >0</td>\n",
              "      <td id=\"T_38790_row3_col14\" class=\"data row3 col14\" >0</td>\n",
              "      <td id=\"T_38790_row3_col15\" class=\"data row3 col15\" >0</td>\n",
              "      <td id=\"T_38790_row3_col16\" class=\"data row3 col16\" >0</td>\n",
              "      <td id=\"T_38790_row3_col17\" class=\"data row3 col17\" >0</td>\n",
              "      <td id=\"T_38790_row3_col18\" class=\"data row3 col18\" >0</td>\n",
              "      <td id=\"T_38790_row3_col19\" class=\"data row3 col19\" >0</td>\n",
              "      <td id=\"T_38790_row3_col20\" class=\"data row3 col20\" >0</td>\n",
              "      <td id=\"T_38790_row3_col21\" class=\"data row3 col21\" >0</td>\n",
              "      <td id=\"T_38790_row3_col22\" class=\"data row3 col22\" >0</td>\n",
              "      <td id=\"T_38790_row3_col23\" class=\"data row3 col23\" >0</td>\n",
              "      <td id=\"T_38790_row3_col24\" class=\"data row3 col24\" >0</td>\n",
              "      <td id=\"T_38790_row3_col25\" class=\"data row3 col25\" >0</td>\n",
              "      <td id=\"T_38790_row3_col26\" class=\"data row3 col26\" >0</td>\n",
              "      <td id=\"T_38790_row3_col27\" class=\"data row3 col27\" >1.498987</td>\n",
              "      <td id=\"T_38790_row3_col28\" class=\"data row3 col28\" >-0.301780</td>\n",
              "      <td id=\"T_38790_row3_col29\" class=\"data row3 col29\" >-0.551413</td>\n",
              "      <td id=\"T_38790_row3_col30\" class=\"data row3 col30\" >80.607933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_38790_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_38790_row4_col0\" class=\"data row4 col0\" >0</td>\n",
              "      <td id=\"T_38790_row4_col1\" class=\"data row4 col1\" >0</td>\n",
              "      <td id=\"T_38790_row4_col2\" class=\"data row4 col2\" >1</td>\n",
              "      <td id=\"T_38790_row4_col3\" class=\"data row4 col3\" >0</td>\n",
              "      <td id=\"T_38790_row4_col4\" class=\"data row4 col4\" >2</td>\n",
              "      <td id=\"T_38790_row4_col5\" class=\"data row4 col5\" >2</td>\n",
              "      <td id=\"T_38790_row4_col6\" class=\"data row4 col6\" >1</td>\n",
              "      <td id=\"T_38790_row4_col7\" class=\"data row4 col7\" >3</td>\n",
              "      <td id=\"T_38790_row4_col8\" class=\"data row4 col8\" >-0.037447</td>\n",
              "      <td id=\"T_38790_row4_col9\" class=\"data row4 col9\" >0</td>\n",
              "      <td id=\"T_38790_row4_col10\" class=\"data row4 col10\" >0</td>\n",
              "      <td id=\"T_38790_row4_col11\" class=\"data row4 col11\" >0</td>\n",
              "      <td id=\"T_38790_row4_col12\" class=\"data row4 col12\" >0</td>\n",
              "      <td id=\"T_38790_row4_col13\" class=\"data row4 col13\" >0</td>\n",
              "      <td id=\"T_38790_row4_col14\" class=\"data row4 col14\" >0</td>\n",
              "      <td id=\"T_38790_row4_col15\" class=\"data row4 col15\" >0</td>\n",
              "      <td id=\"T_38790_row4_col16\" class=\"data row4 col16\" >0</td>\n",
              "      <td id=\"T_38790_row4_col17\" class=\"data row4 col17\" >0</td>\n",
              "      <td id=\"T_38790_row4_col18\" class=\"data row4 col18\" >0</td>\n",
              "      <td id=\"T_38790_row4_col19\" class=\"data row4 col19\" >0</td>\n",
              "      <td id=\"T_38790_row4_col20\" class=\"data row4 col20\" >0</td>\n",
              "      <td id=\"T_38790_row4_col21\" class=\"data row4 col21\" >0</td>\n",
              "      <td id=\"T_38790_row4_col22\" class=\"data row4 col22\" >0</td>\n",
              "      <td id=\"T_38790_row4_col23\" class=\"data row4 col23\" >0</td>\n",
              "      <td id=\"T_38790_row4_col24\" class=\"data row4 col24\" >0</td>\n",
              "      <td id=\"T_38790_row4_col25\" class=\"data row4 col25\" >0</td>\n",
              "      <td id=\"T_38790_row4_col26\" class=\"data row4 col26\" >0</td>\n",
              "      <td id=\"T_38790_row4_col27\" class=\"data row4 col27\" >0.016408</td>\n",
              "      <td id=\"T_38790_row4_col28\" class=\"data row4 col28\" >-0.375870</td>\n",
              "      <td id=\"T_38790_row4_col29\" class=\"data row4 col29\" >0.519525</td>\n",
              "      <td id=\"T_38790_row4_col30\" class=\"data row4 col30\" >86.023117</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train info\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 31 columns):\n",
            " #   Column                        Dtype   \n",
            "---  ------                        -----   \n",
            " 0   Brand                         category\n",
            " 1   Material                      category\n",
            " 2   Size                          category\n",
            " 3   Compartments                  category\n",
            " 4   Laptop Compartment            category\n",
            " 5   Waterproof                    category\n",
            " 6   Style                         category\n",
            " 7   Color                         category\n",
            " 8   Weight Capacity (kg)          float32 \n",
            " 9   Weight Capacity (kg)_missing  category\n",
            " 10  Mat_Siz_Col                   category\n",
            " 11  Siz_Lap_Col                   category\n",
            " 12  Bra_Siz_Wat                   category\n",
            " 13  Siz_Lap_Wat                   category\n",
            " 14  Mat_Lap_Wat                   category\n",
            " 15  Bra_Siz_Sty                   category\n",
            " 16  Bra_Lap_Wat                   category\n",
            " 17  Siz_Com_Lap                   category\n",
            " 18  Siz_Lap_Sty                   category\n",
            " 19  Mat_Com_Lap                   category\n",
            " 20  Mat_Siz_Com                   category\n",
            " 21  Bra_Siz_Com                   category\n",
            " 22  Com_Lap_Wat                   category\n",
            " 23  Bra_Siz_Lap                   category\n",
            " 24  Bra_Mat_Siz                   category\n",
            " 25  Siz_Com_Wat                   category\n",
            " 26  Siz_Com_Sty                   category\n",
            " 27  TE_wc                         float32 \n",
            " 28  skew_0                        float32 \n",
            " 29  skew_1                        float32 \n",
            " 30  Price                         float32 \n",
            "dtypes: category(26), float32(5)\n",
            "memory usage: 205.7 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train describe\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a1ecaa6f710>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_c73a3 th:not(.index_name) {\n",
              "  background-color: slategrey;\n",
              "  color: #FFFFFF;\n",
              "  font-weight: bold;\n",
              "  border: 1px solid #DCDCDC;\n",
              "  text-align: center;\n",
              "}\n",
              "#T_c73a3 tbody td {\n",
              "  border: 1px solid #DCDCDC;\n",
              "  font-weight: normal;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_c73a3\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_c73a3_level0_col0\" class=\"col_heading level0 col0\" >mean</th>\n",
              "      <th id=\"T_c73a3_level0_col1\" class=\"col_heading level0 col1\" >std</th>\n",
              "      <th id=\"T_c73a3_level0_col2\" class=\"col_heading level0 col2\" >min</th>\n",
              "      <th id=\"T_c73a3_level0_col3\" class=\"col_heading level0 col3\" >25%</th>\n",
              "      <th id=\"T_c73a3_level0_col4\" class=\"col_heading level0 col4\" >50%</th>\n",
              "      <th id=\"T_c73a3_level0_col5\" class=\"col_heading level0 col5\" >75%</th>\n",
              "      <th id=\"T_c73a3_level0_col6\" class=\"col_heading level0 col6\" >max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_c73a3_level0_row0\" class=\"row_heading level0 row0\" >Weight Capacity (kg)</th>\n",
              "      <td id=\"T_c73a3_row0_col0\" class=\"data row0 col0\" >-0.000</td>\n",
              "      <td id=\"T_c73a3_row0_col1\" class=\"data row0 col1\" >0.999</td>\n",
              "      <td id=\"T_c73a3_row0_col2\" class=\"data row0 col2\" >-1.866</td>\n",
              "      <td id=\"T_c73a3_row0_col3\" class=\"data row0 col3\" >-0.852</td>\n",
              "      <td id=\"T_c73a3_row0_col4\" class=\"data row0 col4\" >0.006</td>\n",
              "      <td id=\"T_c73a3_row0_col5\" class=\"data row0 col5\" >0.857</td>\n",
              "      <td id=\"T_c73a3_row0_col6\" class=\"data row0 col6\" >1.720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c73a3_level0_row1\" class=\"row_heading level0 row1\" >TE_wc</th>\n",
              "      <td id=\"T_c73a3_row1_col0\" class=\"data row1 col0\" >0.000</td>\n",
              "      <td id=\"T_c73a3_row1_col1\" class=\"data row1 col1\" >0.998</td>\n",
              "      <td id=\"T_c73a3_row1_col2\" class=\"data row1 col2\" >-6.904</td>\n",
              "      <td id=\"T_c73a3_row1_col3\" class=\"data row1 col3\" >-0.471</td>\n",
              "      <td id=\"T_c73a3_row1_col4\" class=\"data row1 col4\" >0.016</td>\n",
              "      <td id=\"T_c73a3_row1_col5\" class=\"data row1 col5\" >0.451</td>\n",
              "      <td id=\"T_c73a3_row1_col6\" class=\"data row1 col6\" >6.732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c73a3_level0_row2\" class=\"row_heading level0 row2\" >skew_0</th>\n",
              "      <td id=\"T_c73a3_row2_col0\" class=\"data row2 col0\" >0.000</td>\n",
              "      <td id=\"T_c73a3_row2_col1\" class=\"data row2 col1\" >0.995</td>\n",
              "      <td id=\"T_c73a3_row2_col2\" class=\"data row2 col2\" >-16.094</td>\n",
              "      <td id=\"T_c73a3_row2_col3\" class=\"data row2 col3\" >-0.302</td>\n",
              "      <td id=\"T_c73a3_row2_col4\" class=\"data row2 col4\" >-0.062</td>\n",
              "      <td id=\"T_c73a3_row2_col5\" class=\"data row2 col5\" >0.176</td>\n",
              "      <td id=\"T_c73a3_row2_col6\" class=\"data row2 col6\" >33.648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c73a3_level0_row3\" class=\"row_heading level0 row3\" >skew_1</th>\n",
              "      <td id=\"T_c73a3_row3_col0\" class=\"data row3 col0\" >0.000</td>\n",
              "      <td id=\"T_c73a3_row3_col1\" class=\"data row3 col1\" >0.995</td>\n",
              "      <td id=\"T_c73a3_row3_col2\" class=\"data row3 col2\" >-23.594</td>\n",
              "      <td id=\"T_c73a3_row3_col3\" class=\"data row3 col3\" >-0.551</td>\n",
              "      <td id=\"T_c73a3_row3_col4\" class=\"data row3 col4\" >0.048</td>\n",
              "      <td id=\"T_c73a3_row3_col5\" class=\"data row3 col5\" >0.506</td>\n",
              "      <td id=\"T_c73a3_row3_col6\" class=\"data row3 col6\" >4.795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train missing values\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Brand                           0\n",
              "Material                        0\n",
              "Size                            0\n",
              "Compartments                    0\n",
              "Laptop Compartment              0\n",
              "Waterproof                      0\n",
              "Style                           0\n",
              "Color                           0\n",
              "Weight Capacity (kg)            0\n",
              "Weight Capacity (kg)_missing    0\n",
              "Mat_Siz_Col                     0\n",
              "Siz_Lap_Col                     0\n",
              "Bra_Siz_Wat                     0\n",
              "Siz_Lap_Wat                     0\n",
              "Mat_Lap_Wat                     0\n",
              "Bra_Siz_Sty                     0\n",
              "Bra_Lap_Wat                     0\n",
              "Siz_Com_Lap                     0\n",
              "Siz_Lap_Sty                     0\n",
              "Mat_Com_Lap                     0\n",
              "Mat_Siz_Com                     0\n",
              "Bra_Siz_Com                     0\n",
              "Com_Lap_Wat                     0\n",
              "Bra_Siz_Lap                     0\n",
              "Bra_Mat_Siz                     0\n",
              "Siz_Com_Wat                     0\n",
              "Siz_Com_Sty                     0\n",
              "TE_wc                           0\n",
              "skew_0                          0\n",
              "skew_1                          0\n",
              "Price                           0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Brand</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Material</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Compartments</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Waterproof</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Style</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Color</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)_missing</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mat_Siz_Col</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Lap_Col</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Siz_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Lap_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mat_Lap_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Siz_Sty</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Lap_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Com_Lap</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Lap_Sty</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mat_Com_Lap</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mat_Siz_Com</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Siz_Com</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Com_Lap_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Siz_Lap</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Mat_Siz</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Com_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Com_Sty</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TE_wc</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Price</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test head\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a1eca99ced0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_a8c93 th:not(.index_name) {\n",
              "  background-color: slategrey;\n",
              "  color: #FFFFFF;\n",
              "  font-weight: bold;\n",
              "  border: 1px solid #DCDCDC;\n",
              "  text-align: center;\n",
              "}\n",
              "#T_a8c93 tbody td {\n",
              "  border: 1px solid #DCDCDC;\n",
              "  font-weight: normal;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_a8c93\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_a8c93_level0_col0\" class=\"col_heading level0 col0\" >Brand</th>\n",
              "      <th id=\"T_a8c93_level0_col1\" class=\"col_heading level0 col1\" >Material</th>\n",
              "      <th id=\"T_a8c93_level0_col2\" class=\"col_heading level0 col2\" >Size</th>\n",
              "      <th id=\"T_a8c93_level0_col3\" class=\"col_heading level0 col3\" >Compartments</th>\n",
              "      <th id=\"T_a8c93_level0_col4\" class=\"col_heading level0 col4\" >Laptop Compartment</th>\n",
              "      <th id=\"T_a8c93_level0_col5\" class=\"col_heading level0 col5\" >Waterproof</th>\n",
              "      <th id=\"T_a8c93_level0_col6\" class=\"col_heading level0 col6\" >Style</th>\n",
              "      <th id=\"T_a8c93_level0_col7\" class=\"col_heading level0 col7\" >Color</th>\n",
              "      <th id=\"T_a8c93_level0_col8\" class=\"col_heading level0 col8\" >Weight Capacity (kg)</th>\n",
              "      <th id=\"T_a8c93_level0_col9\" class=\"col_heading level0 col9\" >Weight Capacity (kg)_missing</th>\n",
              "      <th id=\"T_a8c93_level0_col10\" class=\"col_heading level0 col10\" >Mat_Siz_Col</th>\n",
              "      <th id=\"T_a8c93_level0_col11\" class=\"col_heading level0 col11\" >Siz_Lap_Col</th>\n",
              "      <th id=\"T_a8c93_level0_col12\" class=\"col_heading level0 col12\" >Bra_Siz_Wat</th>\n",
              "      <th id=\"T_a8c93_level0_col13\" class=\"col_heading level0 col13\" >Siz_Lap_Wat</th>\n",
              "      <th id=\"T_a8c93_level0_col14\" class=\"col_heading level0 col14\" >Mat_Lap_Wat</th>\n",
              "      <th id=\"T_a8c93_level0_col15\" class=\"col_heading level0 col15\" >Bra_Siz_Sty</th>\n",
              "      <th id=\"T_a8c93_level0_col16\" class=\"col_heading level0 col16\" >Bra_Lap_Wat</th>\n",
              "      <th id=\"T_a8c93_level0_col17\" class=\"col_heading level0 col17\" >Siz_Com_Lap</th>\n",
              "      <th id=\"T_a8c93_level0_col18\" class=\"col_heading level0 col18\" >Siz_Lap_Sty</th>\n",
              "      <th id=\"T_a8c93_level0_col19\" class=\"col_heading level0 col19\" >Mat_Com_Lap</th>\n",
              "      <th id=\"T_a8c93_level0_col20\" class=\"col_heading level0 col20\" >Mat_Siz_Com</th>\n",
              "      <th id=\"T_a8c93_level0_col21\" class=\"col_heading level0 col21\" >Bra_Siz_Com</th>\n",
              "      <th id=\"T_a8c93_level0_col22\" class=\"col_heading level0 col22\" >Com_Lap_Wat</th>\n",
              "      <th id=\"T_a8c93_level0_col23\" class=\"col_heading level0 col23\" >Bra_Siz_Lap</th>\n",
              "      <th id=\"T_a8c93_level0_col24\" class=\"col_heading level0 col24\" >Bra_Mat_Siz</th>\n",
              "      <th id=\"T_a8c93_level0_col25\" class=\"col_heading level0 col25\" >Siz_Com_Wat</th>\n",
              "      <th id=\"T_a8c93_level0_col26\" class=\"col_heading level0 col26\" >Siz_Com_Sty</th>\n",
              "      <th id=\"T_a8c93_level0_col27\" class=\"col_heading level0 col27\" >TE_wc</th>\n",
              "      <th id=\"T_a8c93_level0_col28\" class=\"col_heading level0 col28\" >skew_0</th>\n",
              "      <th id=\"T_a8c93_level0_col29\" class=\"col_heading level0 col29\" >skew_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_a8c93_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_a8c93_row0_col0\" class=\"data row0 col0\" >4</td>\n",
              "      <td id=\"T_a8c93_row0_col1\" class=\"data row0 col1\" >1</td>\n",
              "      <td id=\"T_a8c93_row0_col2\" class=\"data row0 col2\" >3</td>\n",
              "      <td id=\"T_a8c93_row0_col3\" class=\"data row0 col3\" >2</td>\n",
              "      <td id=\"T_a8c93_row0_col4\" class=\"data row0 col4\" >1</td>\n",
              "      <td id=\"T_a8c93_row0_col5\" class=\"data row0 col5\" >1</td>\n",
              "      <td id=\"T_a8c93_row0_col6\" class=\"data row0 col6\" >3</td>\n",
              "      <td id=\"T_a8c93_row0_col7\" class=\"data row0 col7\" >3</td>\n",
              "      <td id=\"T_a8c93_row0_col8\" class=\"data row0 col8\" >0.381607</td>\n",
              "      <td id=\"T_a8c93_row0_col9\" class=\"data row0 col9\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col10\" class=\"data row0 col10\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col11\" class=\"data row0 col11\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col12\" class=\"data row0 col12\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col13\" class=\"data row0 col13\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col14\" class=\"data row0 col14\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col15\" class=\"data row0 col15\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col16\" class=\"data row0 col16\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col17\" class=\"data row0 col17\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col18\" class=\"data row0 col18\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col19\" class=\"data row0 col19\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col20\" class=\"data row0 col20\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col21\" class=\"data row0 col21\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col22\" class=\"data row0 col22\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col23\" class=\"data row0 col23\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col24\" class=\"data row0 col24\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col25\" class=\"data row0 col25\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col26\" class=\"data row0 col26\" >0</td>\n",
              "      <td id=\"T_a8c93_row0_col27\" class=\"data row0 col27\" >0.414531</td>\n",
              "      <td id=\"T_a8c93_row0_col28\" class=\"data row0 col28\" >0.195748</td>\n",
              "      <td id=\"T_a8c93_row0_col29\" class=\"data row0 col29\" >0.428915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a8c93_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_a8c93_row1_col0\" class=\"data row1 col0\" >3</td>\n",
              "      <td id=\"T_a8c93_row1_col1\" class=\"data row1 col1\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col2\" class=\"data row1 col2\" >1</td>\n",
              "      <td id=\"T_a8c93_row1_col3\" class=\"data row1 col3\" >7</td>\n",
              "      <td id=\"T_a8c93_row1_col4\" class=\"data row1 col4\" >1</td>\n",
              "      <td id=\"T_a8c93_row1_col5\" class=\"data row1 col5\" >2</td>\n",
              "      <td id=\"T_a8c93_row1_col6\" class=\"data row1 col6\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col7\" class=\"data row1 col7\" >3</td>\n",
              "      <td id=\"T_a8c93_row1_col8\" class=\"data row1 col8\" >-0.637706</td>\n",
              "      <td id=\"T_a8c93_row1_col9\" class=\"data row1 col9\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col10\" class=\"data row1 col10\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col11\" class=\"data row1 col11\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col12\" class=\"data row1 col12\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col13\" class=\"data row1 col13\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col14\" class=\"data row1 col14\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col15\" class=\"data row1 col15\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col16\" class=\"data row1 col16\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col17\" class=\"data row1 col17\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col18\" class=\"data row1 col18\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col19\" class=\"data row1 col19\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col20\" class=\"data row1 col20\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col21\" class=\"data row1 col21\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col22\" class=\"data row1 col22\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col23\" class=\"data row1 col23\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col24\" class=\"data row1 col24\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col25\" class=\"data row1 col25\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col26\" class=\"data row1 col26\" >0</td>\n",
              "      <td id=\"T_a8c93_row1_col27\" class=\"data row1 col27\" >-0.095293</td>\n",
              "      <td id=\"T_a8c93_row1_col28\" class=\"data row1 col28\" >0.000521</td>\n",
              "      <td id=\"T_a8c93_row1_col29\" class=\"data row1 col29\" >0.040421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a8c93_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_a8c93_row2_col0\" class=\"data row2 col0\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col1\" class=\"data row2 col1\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col2\" class=\"data row2 col2\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col3\" class=\"data row2 col3\" >9</td>\n",
              "      <td id=\"T_a8c93_row2_col4\" class=\"data row2 col4\" >1</td>\n",
              "      <td id=\"T_a8c93_row2_col5\" class=\"data row2 col5\" >2</td>\n",
              "      <td id=\"T_a8c93_row2_col6\" class=\"data row2 col6\" >1</td>\n",
              "      <td id=\"T_a8c93_row2_col7\" class=\"data row2 col7\" >1</td>\n",
              "      <td id=\"T_a8c93_row2_col8\" class=\"data row2 col8\" >-0.889313</td>\n",
              "      <td id=\"T_a8c93_row2_col9\" class=\"data row2 col9\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col10\" class=\"data row2 col10\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col11\" class=\"data row2 col11\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col12\" class=\"data row2 col12\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col13\" class=\"data row2 col13\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col14\" class=\"data row2 col14\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col15\" class=\"data row2 col15\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col16\" class=\"data row2 col16\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col17\" class=\"data row2 col17\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col18\" class=\"data row2 col18\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col19\" class=\"data row2 col19\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col20\" class=\"data row2 col20\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col21\" class=\"data row2 col21\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col22\" class=\"data row2 col22\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col23\" class=\"data row2 col23\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col24\" class=\"data row2 col24\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col25\" class=\"data row2 col25\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col26\" class=\"data row2 col26\" >0</td>\n",
              "      <td id=\"T_a8c93_row2_col27\" class=\"data row2 col27\" >2.164135</td>\n",
              "      <td id=\"T_a8c93_row2_col28\" class=\"data row2 col28\" >-0.061768</td>\n",
              "      <td id=\"T_a8c93_row2_col29\" class=\"data row2 col29\" >1.242390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a8c93_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_a8c93_row3_col0\" class=\"data row3 col0\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col1\" class=\"data row3 col1\" >3</td>\n",
              "      <td id=\"T_a8c93_row3_col2\" class=\"data row3 col2\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col3\" class=\"data row3 col3\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col4\" class=\"data row3 col4\" >2</td>\n",
              "      <td id=\"T_a8c93_row3_col5\" class=\"data row3 col5\" >1</td>\n",
              "      <td id=\"T_a8c93_row3_col6\" class=\"data row3 col6\" >1</td>\n",
              "      <td id=\"T_a8c93_row3_col7\" class=\"data row3 col7\" >3</td>\n",
              "      <td id=\"T_a8c93_row3_col8\" class=\"data row3 col8\" >0.066921</td>\n",
              "      <td id=\"T_a8c93_row3_col9\" class=\"data row3 col9\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col10\" class=\"data row3 col10\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col11\" class=\"data row3 col11\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col12\" class=\"data row3 col12\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col13\" class=\"data row3 col13\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col14\" class=\"data row3 col14\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col15\" class=\"data row3 col15\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col16\" class=\"data row3 col16\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col17\" class=\"data row3 col17\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col18\" class=\"data row3 col18\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col19\" class=\"data row3 col19\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col20\" class=\"data row3 col20\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col21\" class=\"data row3 col21\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col22\" class=\"data row3 col22\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col23\" class=\"data row3 col23\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col24\" class=\"data row3 col24\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col25\" class=\"data row3 col25\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col26\" class=\"data row3 col26\" >0</td>\n",
              "      <td id=\"T_a8c93_row3_col27\" class=\"data row3 col27\" >-0.516258</td>\n",
              "      <td id=\"T_a8c93_row3_col28\" class=\"data row3 col28\" >-0.295283</td>\n",
              "      <td id=\"T_a8c93_row3_col29\" class=\"data row3 col29\" >0.744158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_a8c93_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_a8c93_row4_col0\" class=\"data row4 col0\" >2</td>\n",
              "      <td id=\"T_a8c93_row4_col1\" class=\"data row4 col1\" >3</td>\n",
              "      <td id=\"T_a8c93_row4_col2\" class=\"data row4 col2\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col3\" class=\"data row4 col3\" >2</td>\n",
              "      <td id=\"T_a8c93_row4_col4\" class=\"data row4 col4\" >2</td>\n",
              "      <td id=\"T_a8c93_row4_col5\" class=\"data row4 col5\" >2</td>\n",
              "      <td id=\"T_a8c93_row4_col6\" class=\"data row4 col6\" >3</td>\n",
              "      <td id=\"T_a8c93_row4_col7\" class=\"data row4 col7\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col8\" class=\"data row4 col8\" >-1.162081</td>\n",
              "      <td id=\"T_a8c93_row4_col9\" class=\"data row4 col9\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col10\" class=\"data row4 col10\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col11\" class=\"data row4 col11\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col12\" class=\"data row4 col12\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col13\" class=\"data row4 col13\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col14\" class=\"data row4 col14\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col15\" class=\"data row4 col15\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col16\" class=\"data row4 col16\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col17\" class=\"data row4 col17\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col18\" class=\"data row4 col18\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col19\" class=\"data row4 col19\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col20\" class=\"data row4 col20\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col21\" class=\"data row4 col21\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col22\" class=\"data row4 col22\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col23\" class=\"data row4 col23\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col24\" class=\"data row4 col24\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col25\" class=\"data row4 col25\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col26\" class=\"data row4 col26\" >0</td>\n",
              "      <td id=\"T_a8c93_row4_col27\" class=\"data row4 col27\" >0.016408</td>\n",
              "      <td id=\"T_a8c93_row4_col28\" class=\"data row4 col28\" >-0.426006</td>\n",
              "      <td id=\"T_a8c93_row4_col29\" class=\"data row4 col29\" >0.348637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test info\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 200000 entries, 0 to 199999\n",
            "Data columns (total 30 columns):\n",
            " #   Column                        Non-Null Count   Dtype   \n",
            "---  ------                        --------------   -----   \n",
            " 0   Brand                         200000 non-null  category\n",
            " 1   Material                      200000 non-null  category\n",
            " 2   Size                          200000 non-null  category\n",
            " 3   Compartments                  200000 non-null  category\n",
            " 4   Laptop Compartment            200000 non-null  category\n",
            " 5   Waterproof                    200000 non-null  category\n",
            " 6   Style                         200000 non-null  category\n",
            " 7   Color                         200000 non-null  category\n",
            " 8   Weight Capacity (kg)          200000 non-null  float32 \n",
            " 9   Weight Capacity (kg)_missing  200000 non-null  category\n",
            " 10  Mat_Siz_Col                   200000 non-null  category\n",
            " 11  Siz_Lap_Col                   200000 non-null  category\n",
            " 12  Bra_Siz_Wat                   200000 non-null  category\n",
            " 13  Siz_Lap_Wat                   200000 non-null  category\n",
            " 14  Mat_Lap_Wat                   200000 non-null  category\n",
            " 15  Bra_Siz_Sty                   200000 non-null  category\n",
            " 16  Bra_Lap_Wat                   200000 non-null  category\n",
            " 17  Siz_Com_Lap                   200000 non-null  category\n",
            " 18  Siz_Lap_Sty                   200000 non-null  category\n",
            " 19  Mat_Com_Lap                   200000 non-null  category\n",
            " 20  Mat_Siz_Com                   200000 non-null  category\n",
            " 21  Bra_Siz_Com                   200000 non-null  category\n",
            " 22  Com_Lap_Wat                   200000 non-null  category\n",
            " 23  Bra_Siz_Lap                   200000 non-null  category\n",
            " 24  Bra_Mat_Siz                   200000 non-null  category\n",
            " 25  Siz_Com_Wat                   200000 non-null  category\n",
            " 26  Siz_Com_Sty                   200000 non-null  category\n",
            " 27  TE_wc                         200000 non-null  float32 \n",
            " 28  skew_0                        200000 non-null  float32 \n",
            " 29  skew_1                        200000 non-null  float32 \n",
            "dtypes: category(26), float32(4)\n",
            "memory usage: 9.5 MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test describe\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a1ecadebe50>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_eb630 th:not(.index_name) {\n",
              "  background-color: slategrey;\n",
              "  color: #FFFFFF;\n",
              "  font-weight: bold;\n",
              "  border: 1px solid #DCDCDC;\n",
              "  text-align: center;\n",
              "}\n",
              "#T_eb630 tbody td {\n",
              "  border: 1px solid #DCDCDC;\n",
              "  font-weight: normal;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_eb630\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_eb630_level0_col0\" class=\"col_heading level0 col0\" >mean</th>\n",
              "      <th id=\"T_eb630_level0_col1\" class=\"col_heading level0 col1\" >std</th>\n",
              "      <th id=\"T_eb630_level0_col2\" class=\"col_heading level0 col2\" >min</th>\n",
              "      <th id=\"T_eb630_level0_col3\" class=\"col_heading level0 col3\" >25%</th>\n",
              "      <th id=\"T_eb630_level0_col4\" class=\"col_heading level0 col4\" >50%</th>\n",
              "      <th id=\"T_eb630_level0_col5\" class=\"col_heading level0 col5\" >75%</th>\n",
              "      <th id=\"T_eb630_level0_col6\" class=\"col_heading level0 col6\" >max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_eb630_level0_row0\" class=\"row_heading level0 row0\" >Weight Capacity (kg)</th>\n",
              "      <td id=\"T_eb630_row0_col0\" class=\"data row0 col0\" >-0.002</td>\n",
              "      <td id=\"T_eb630_row0_col1\" class=\"data row0 col1\" >1.000</td>\n",
              "      <td id=\"T_eb630_row0_col2\" class=\"data row0 col2\" >-1.866</td>\n",
              "      <td id=\"T_eb630_row0_col3\" class=\"data row0 col3\" >-0.852</td>\n",
              "      <td id=\"T_eb630_row0_col4\" class=\"data row0 col4\" >0.006</td>\n",
              "      <td id=\"T_eb630_row0_col5\" class=\"data row0 col5\" >0.854</td>\n",
              "      <td id=\"T_eb630_row0_col6\" class=\"data row0 col6\" >1.720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_eb630_level0_row1\" class=\"row_heading level0 row1\" >TE_wc</th>\n",
              "      <td id=\"T_eb630_row1_col0\" class=\"data row1 col0\" >0.000</td>\n",
              "      <td id=\"T_eb630_row1_col1\" class=\"data row1 col1\" >1.000</td>\n",
              "      <td id=\"T_eb630_row1_col2\" class=\"data row1 col2\" >-6.756</td>\n",
              "      <td id=\"T_eb630_row1_col3\" class=\"data row1 col3\" >-0.475</td>\n",
              "      <td id=\"T_eb630_row1_col4\" class=\"data row1 col4\" >0.016</td>\n",
              "      <td id=\"T_eb630_row1_col5\" class=\"data row1 col5\" >0.453</td>\n",
              "      <td id=\"T_eb630_row1_col6\" class=\"data row1 col6\" >6.500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_eb630_level0_row2\" class=\"row_heading level0 row2\" >skew_0</th>\n",
              "      <td id=\"T_eb630_row2_col0\" class=\"data row2 col0\" >0.003</td>\n",
              "      <td id=\"T_eb630_row2_col1\" class=\"data row2 col1\" >1.052</td>\n",
              "      <td id=\"T_eb630_row2_col2\" class=\"data row2 col2\" >-16.094</td>\n",
              "      <td id=\"T_eb630_row2_col3\" class=\"data row2 col3\" >-0.302</td>\n",
              "      <td id=\"T_eb630_row2_col4\" class=\"data row2 col4\" >-0.062</td>\n",
              "      <td id=\"T_eb630_row2_col5\" class=\"data row2 col5\" >0.176</td>\n",
              "      <td id=\"T_eb630_row2_col6\" class=\"data row2 col6\" >33.648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_eb630_level0_row3\" class=\"row_heading level0 row3\" >skew_1</th>\n",
              "      <td id=\"T_eb630_row3_col0\" class=\"data row3 col0\" >-0.001</td>\n",
              "      <td id=\"T_eb630_row3_col1\" class=\"data row3 col1\" >1.009</td>\n",
              "      <td id=\"T_eb630_row3_col2\" class=\"data row3 col2\" >-23.594</td>\n",
              "      <td id=\"T_eb630_row3_col3\" class=\"data row3 col3\" >-0.551</td>\n",
              "      <td id=\"T_eb630_row3_col4\" class=\"data row3 col4\" >0.048</td>\n",
              "      <td id=\"T_eb630_row3_col5\" class=\"data row3 col5\" >0.506</td>\n",
              "      <td id=\"T_eb630_row3_col6\" class=\"data row3 col6\" >4.795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test missing values\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Brand                           0\n",
              "Material                        0\n",
              "Size                            0\n",
              "Compartments                    0\n",
              "Laptop Compartment              0\n",
              "Waterproof                      0\n",
              "Style                           0\n",
              "Color                           0\n",
              "Weight Capacity (kg)            0\n",
              "Weight Capacity (kg)_missing    0\n",
              "Mat_Siz_Col                     0\n",
              "Siz_Lap_Col                     0\n",
              "Bra_Siz_Wat                     0\n",
              "Siz_Lap_Wat                     0\n",
              "Mat_Lap_Wat                     0\n",
              "Bra_Siz_Sty                     0\n",
              "Bra_Lap_Wat                     0\n",
              "Siz_Com_Lap                     0\n",
              "Siz_Lap_Sty                     0\n",
              "Mat_Com_Lap                     0\n",
              "Mat_Siz_Com                     0\n",
              "Bra_Siz_Com                     0\n",
              "Com_Lap_Wat                     0\n",
              "Bra_Siz_Lap                     0\n",
              "Bra_Mat_Siz                     0\n",
              "Siz_Com_Wat                     0\n",
              "Siz_Com_Sty                     0\n",
              "TE_wc                           0\n",
              "skew_0                          0\n",
              "skew_1                          0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Brand</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Material</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Compartments</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Waterproof</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Style</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Color</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)_missing</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mat_Siz_Col</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Lap_Col</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Siz_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Lap_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mat_Lap_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Siz_Sty</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Lap_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Com_Lap</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Lap_Sty</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mat_Com_Lap</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mat_Siz_Com</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Siz_Com</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Com_Lap_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Siz_Lap</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Bra_Mat_Siz</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Com_Wat</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Siz_Com_Sty</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TE_wc</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Correlation Heatmap\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 840x840 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "eda = EDA()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jQmFilAvKM4"
      },
      "source": [
        "## 2.0 Data Transformation and Feature Engeneering:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxSqeWx-MZHk"
      },
      "outputs": [],
      "source": [
        "class Transform(Config, Preprocessing):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        if self.missing == 'Y':\n",
        "            self.missing_values()\n",
        "\n",
        "        self.train_raw = self.train.copy()\n",
        "\n",
        "        if self.feature_eng == 'Y':\n",
        "            self.train = self.new_features(self.train)\n",
        "            self.test = self.new_features(self.test)\n",
        "            self.train_raw = self.new_features(self.train_raw)\n",
        "\n",
        "        self.num_features = self.train.drop(self.target, axis=1).select_dtypes(exclude=['object', 'bool', 'category']).columns.tolist()\n",
        "        self.cat_features = self.train.drop(self.target, axis=1).select_dtypes(include=['object', 'bool', 'category']).columns.tolist()\n",
        "\n",
        "        if self.outliers == 'Y':\n",
        "            self.remove_outliers()\n",
        "\n",
        "        if self.log_trf == 'Y':\n",
        "            self.log_transformation()\n",
        "\n",
        "        if self.force_normalization == 'Y':\n",
        "            self.forced_norm_transformation()\n",
        "\n",
        "        if self.impose_normalization == 'Y':\n",
        "            self.impose_normalization_transformation()\n",
        "\n",
        "        if self.trg_enc == 'Y':\n",
        "            self.target_encoding()\n",
        "\n",
        "        if self.scaler_trf == 'Y':\n",
        "            self.scaler()\n",
        "\n",
        "        if self.outliers == 'Y' or self.log_trf == 'Y' or self.scaler_trf =='Y':\n",
        "            self.distribution()\n",
        "\n",
        "    def __call__(self):\n",
        "\n",
        "        self.train[self.cat_features] = self.train[self.cat_features].astype('category')\n",
        "        self.test[self.cat_features] = self.test[self.cat_features].astype('category')\n",
        "        data = pd.concat([self.test, self.train])\n",
        "        self.train_enc, self.test_enc = self.encode(data)\n",
        "\n",
        "        self.cat_features_card = []\n",
        "        for f in self.cat_features:\n",
        "            self.cat_features_card.append(1 + data[f].max())\n",
        "\n",
        "        self.y = self.train[self.target]\n",
        "        self.train = self.train.drop(self.target, axis=1)\n",
        "        self.train_enc = self.train_enc.drop(self.target, axis=1)\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        self.train_enc[self.num_features] = scaler.fit_transform(self.train_enc[self.num_features])\n",
        "        self.test_enc[self.num_features] = scaler.transform(self.test_enc[self.num_features])\n",
        "\n",
        "        return self.train, self.train_enc, self.y, self.test, self.test_enc, self.cat_features\n",
        "\n",
        "    def encode(self, data):\n",
        "\n",
        "        oe = OrdinalEncoder()\n",
        "        data[self.cat_features] = oe.fit_transform(data[self.cat_features]).astype('int')\n",
        "\n",
        "        train_enc = data[~data[self.target].isna()]\n",
        "        test_enc = data[data[self.target].isna()].drop(self.target, axis=1)\n",
        "        return train_enc, test_enc\n",
        "\n",
        "    def new_features(self, df):\n",
        "\n",
        "        price_flags = [\"Mat_Siz_Col\",\t\"Siz_Lap_Col\",\t\"Bra_Siz_Wat\",\t\"Siz_Lap_Wat\",\t\"Mat_Lap_Wat\",\t\"Bra_Siz_Sty\",\t\"Bra_Lap_Wat\",\t\"Siz_Com_Lap\",\t\"Siz_Lap_Sty\",\n",
        "                       \"Mat_Com_Lap\",\t\"Mat_Siz_Com\",\t\"Bra_Siz_Com\",\t\"Com_Lap_Wat\",\t\"Bra_Siz_Lap\",\t\"Bra_Mat_Siz\",\t\"Siz_Com_Wat\",\t\"Siz_Com_Sty\"]\n",
        "\n",
        "        df['cheap_flag'] = df[price_flags].apply(lambda row: 1 in row.values, axis=1).astype(\"category\")\n",
        "        df['expansive_flag'] = df[price_flags].apply(lambda row: 2 in row.values, axis=1).astype(\"category\")\n",
        "\n",
        "        df = df.drop(columns=price_flags)\n",
        "        df = df.drop(columns=[\"Weight Capacity (kg)_missing\"])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def log_transformation(self):\n",
        "\n",
        "        self.train[self.log_trans_cols] = np.log1p(self.train[self.log_trans_cols])\n",
        "        self.test[self.log_trans_cols] = np.log1p(self.test[self.log_trans_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "    def forced_norm_transformation(self):\n",
        "\n",
        "        self.train[self.force_norm_cols] = np.sqrt(self.train[self.force_norm_cols]+0.1)\n",
        "        self.test[self.force_norm_cols] = np.sqrt(self.test[self.force_norm_cols]+0.1)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impose_normalization_transformation(self):\n",
        "\n",
        "        scaler = QuantileTransformer(output_distribution='normal',subsample=20_000,random_state=42)\n",
        "        self.train[self.impose_norm_cols] = scaler.fit_transform(self.train[self.impose_norm_cols])\n",
        "        self.test[self.impose_norm_cols] = scaler.transform(self.test[self.impose_norm_cols])\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def distribution(self):\n",
        "\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nHistograms of distribution\\n')\n",
        "        fig, axes = plt.subplots(nrows=len(self.num_features), ncols=2, figsize=(15, len(self.num_features)*5))\n",
        "\n",
        "        for (ax_r, ax_n), col in zip(axes, self.num_features):\n",
        "\n",
        "            ax_r.set_title(f'{col} ($\\mu=$ {self.train_raw[col].mean():.2f} and $\\sigma=$ {self.train_raw[col].std():.2f} )')\n",
        "            ax_r.hist(self.train_raw[col], bins=30, color='tomato',alpha=0.7)\n",
        "            ax_r.axvline(self.train_raw[col].mean(), color='r', label='Mean')\n",
        "            ax_r.axvline(self.train_raw[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_r.legend()\n",
        "\n",
        "            ax_n.set_title(f'{col} Normalized ($\\mu=$ {self.train[col].mean():.2f} and $\\sigma=$ {self.train[col].std():.2f} )')\n",
        "            ax_n.hist(self.train[col], bins=30, color='royalblue',alpha=0.7)\n",
        "            ax_n.axvline(self.train[col].mean(), color='r', label='Mean')\n",
        "            ax_n.axvline(self.train[col].median(), color='y', linestyle='--', label='Median')\n",
        "            ax_n.legend()\n",
        "\n",
        "    def remove_outliers(self):\n",
        "        Q1 = self.train[self.targets].quantile(0.25)\n",
        "        Q3 = self.train[self.targets].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_limit = Q1 - 1.5*IQR\n",
        "        upper_limit = Q3 + 1.5*IQR\n",
        "        self.train = self.train[(self.train[self.targets] >= lower_limit) & (self.train[self.targets] <= upper_limit)]\n",
        "        self.train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def scaler(self):\n",
        "        scaler = StandardScaler()\n",
        "        self.train[self.num_features] = scaler.fit_transform(self.train[self.num_features])\n",
        "        self.test[self.num_features] = scaler.transform(self.test[self.num_features])\n",
        "        return self\n",
        "\n",
        "    def missing_values(self):\n",
        "\n",
        "        self.train[self.num_features] = self.train[self.num_features].fillna(self.train[self.num_features].median())\n",
        "        self.test[self.num_features] = self.test[self.num_features].fillna(self.test[self.num_features].median())\n",
        "        for column in self.cat_features:\n",
        "            self.train[column] = self.train[column].fillna(self.train[column].mode()[0])\n",
        "            self.test[column] = self.test[column].fillna(self.test[column].mode()[0])\n",
        "        return self\n",
        "\n",
        "    def target_encoding(self):\n",
        "        te = TargetEncoder()\n",
        "        self.train[self.trg_enc_feat] = te.fit_transform(self.train[self.trg_enc_feat],self.train[self.target])\n",
        "        self.test[self.trg_enc_feat] = te.transform(self.test[self.trg_enc_feat])\n",
        "\n",
        "        for a in self.cat_features:\n",
        "            self.cat_features.remove(a)\n",
        "\n",
        "        return self\n",
        "\n",
        "    @property\n",
        "    def cat_features(self):\n",
        "        return self._cat_features\n",
        "\n",
        "    @cat_features.setter\n",
        "    def cat_features(self, cat_features):\n",
        "        self._cat_features = cat_features\n",
        "\n",
        "    @property\n",
        "    def num_features(self):\n",
        "        return self._num_features\n",
        "\n",
        "    @num_features.setter\n",
        "    def num_features(self, num_features):\n",
        "        self._num_features = num_features\n",
        "\n",
        "    @property\n",
        "    def cat_features_card(self):\n",
        "        return self._cat_features_card\n",
        "\n",
        "    @cat_features_card.setter\n",
        "    def cat_features_card(self, cat_features_card):\n",
        "        self._cat_features_card = cat_features_card\n",
        "\n",
        "    @property\n",
        "    def train(self):\n",
        "        return self._train\n",
        "\n",
        "    @train.setter\n",
        "    def train(self, train):\n",
        "        self._train = train\n",
        "\n",
        "    @property\n",
        "    def direction(self):\n",
        "        return self._direction\n",
        "\n",
        "    @direction.setter\n",
        "    def direction(self, direction):\n",
        "        self._direction= direction\n",
        "\n",
        "\n",
        "class MixedDataImputer:\n",
        "    \"\"\"\n",
        "    Imputes missing values in mixed-data train and test DataFrames using\n",
        "    separate IterativeImputers for numerical and categorical features.\n",
        "\n",
        "    Args:\n",
        "      train_df: Pandas DataFrame with training data.\n",
        "      test_df: Pandas DataFrame with test data.\n",
        "      target_feature: Name of the target feature column.\n",
        "      random_state: Random state for reproducibility (default=42).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_df, test_df, target_feature=None, random_state=42):\n",
        "        super().__init__()\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.target_feature = target_feature\n",
        "        self.random_state = random_state\n",
        "        self.num_features = None\n",
        "        self.cat_features = None\n",
        "\n",
        "    def _identify_features(self):\n",
        "        \"\"\"Identifies numerical and categorical features.\"\"\"\n",
        "        self.num_features = self.train_df.select_dtypes(include=['number']).columns.tolist()\n",
        "        self.cat_features = self.train_df.select_dtypes(exclude=['number']).columns.tolist()\n",
        "        #self.num_features.remove(self.target_feature)  # Remove target from numerical features\n",
        "\n",
        "    def _impute_data(self, df):\n",
        "        \"\"\"Imputes missing values in a DataFrame.\"\"\"\n",
        "        df_num = df[self.num_features].copy()\n",
        "        df_cat = df[self.cat_features].copy()\n",
        "\n",
        "        # Impute numerical features only if there are missing values\n",
        "        if df_num.isnull().values.any():\n",
        "            num_imputer = IterativeImputer(estimator=BayesianRidge(),\n",
        "                                          random_state=self.random_state)\n",
        "            df_num_imputed = pd.DataFrame(num_imputer.fit_transform(df_num),\n",
        "                                         columns=self.num_features)\n",
        "        else:\n",
        "            df_num_imputed = df_num  # No imputation needed\n",
        "\n",
        "        # Impute categorical features only if there are missing values\n",
        "        if df_cat.isnull().values.any():\n",
        "            cat_imputer = IterativeImputer(estimator=LogisticRegression(),\n",
        "                                          initial_strategy='most_frequent',\n",
        "                                          random_state=self.random_state)\n",
        "            df_cat_imputed = pd.DataFrame(cat_imputer.fit_transform(df_cat),\n",
        "                                         columns=self.cat_features)\n",
        "\n",
        "            # Convert categorical features back to their original datatype\n",
        "            for feature in self.cat_features:\n",
        "                df_cat_imputed[feature] = df_cat_imputed[feature].astype(df[feature].dtype)\n",
        "        else:\n",
        "            df_cat_imputed = df_cat  # No imputation needed\n",
        "\n",
        "        # Concatenate the imputed DataFrames\n",
        "        df_imputed = pd.concat([df_num_imputed, df_cat_imputed], axis=1)\n",
        "\n",
        "        return df_imputed\n",
        "\n",
        "    def transform(self):\n",
        "        \"\"\"\n",
        "        Imputes missing values in both train and test DataFrames.\n",
        "\n",
        "        Returns:\n",
        "          train_df_imputed: Pandas DataFrame with imputed training data.\n",
        "          test_df_imputed: Pandas DataFrame with imputed test data.\n",
        "        \"\"\"\n",
        "        self._identify_features()\n",
        "        train_df_imputed = self._impute_data(self.train_df)\n",
        "        test_df_imputed = self._impute_data(self.test_df)\n",
        "        return train_df_imputed, test_df_imputed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YimbhYB_v_G5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da90ce4d-f7c4-4e8a-e97c-8a923017e66a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Histograms of distribution\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x2400 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "t = Transform()\n",
        "X, X_enc, y, test, test_enc, cat_features = t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQFVvKjm560K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6505c3c2-72dc-4b78-bed9-109f787c080b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Brand',\n",
              "  'Material',\n",
              "  'Size',\n",
              "  'Compartments',\n",
              "  'Laptop Compartment',\n",
              "  'Waterproof',\n",
              "  'Style',\n",
              "  'Color',\n",
              "  'cheap_flag',\n",
              "  'expansive_flag'],\n",
              " [6, 5, 4, 10, 3, 3, 4, 7, 2, 2],\n",
              " (3994318, 14),\n",
              " 'minimize')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "t.cat_features, t.cat_features_card, t.train.shape, t.direction_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J1svia6wHk4"
      },
      "outputs": [],
      "source": [
        "#X.shape, X_enc.shape, test.shape, test_enc.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BsRXz1PiZfT"
      },
      "outputs": [],
      "source": [
        "#X_enc.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4StkPOujlvR"
      },
      "outputs": [],
      "source": [
        "imputer = MixedDataImputer(X_enc, test_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6vWq-ulkDQB"
      },
      "outputs": [],
      "source": [
        "train_df_imputed, test_df_imputed = imputer.transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iF1T6CCtD-B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "39ef5bbe-f47a-4052-b2c3-1e8b23846639"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Brand                   0\n",
              "Material                0\n",
              "Size                    0\n",
              "Compartments            0\n",
              "Laptop Compartment      0\n",
              "Waterproof              0\n",
              "Style                   0\n",
              "Color                   0\n",
              "Weight Capacity (kg)    0\n",
              "TE_wc                   0\n",
              "skew_0                  0\n",
              "skew_1                  0\n",
              "cheap_flag              0\n",
              "expansive_flag          0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Brand</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Material</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Size</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Compartments</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Waterproof</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Style</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Color</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TE_wc</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skew_1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheap_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>expansive_flag</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_df_imputed.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZsrhGGvPt39"
      },
      "source": [
        "## 3.0 Advanced Feature Engeneering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1kqkztpPy6m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "5288dff8-e626-4c97-d180-7c100eefdffa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "1290775      4         3     0             3                   1           1   \n",
              "3597346      0         3     0             6                   2           2   \n",
              "2985571      3         0     3             3                   1           2   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "1290775      3      3              0.398548  1.560046  1.391704  0.291707   \n",
              "3597346      1      3              0.408570 -0.170742 -1.749252  1.131928   \n",
              "2985571      2      1             -0.052275  0.159772  0.746521  0.368845   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "1290775           0               0  \n",
              "3597346           0               0  \n",
              "2985571           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-95ae21ae-97ec-41ff-9953-7e0983691e26\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1290775</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.398548</td>\n",
              "      <td>1.560046</td>\n",
              "      <td>1.391704</td>\n",
              "      <td>0.291707</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3597346</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.408570</td>\n",
              "      <td>-0.170742</td>\n",
              "      <td>-1.749252</td>\n",
              "      <td>1.131928</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2985571</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.052275</td>\n",
              "      <td>0.159772</td>\n",
              "      <td>0.746521</td>\n",
              "      <td>0.368845</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95ae21ae-97ec-41ff-9953-7e0983691e26')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-95ae21ae-97ec-41ff-9953-7e0983691e26 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-95ae21ae-97ec-41ff-9953-7e0983691e26');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1280e436-0f87-4b6c-ab3a-0e2cd1b51904\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1280e436-0f87-4b6c-ab3a-0e2cd1b51904')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1280e436-0f87-4b6c-ab3a-0e2cd1b51904 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.3985476791858673,\n          0.40856999158859253\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.5600457191467285,\n          -0.17074206471443176\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.391703724861145,\n          -1.7492516040802002\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.2917065918445587,\n          1.1319280862808228\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLNavxj3aTVe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "2b76e0b2-ab12-4dc9-db54-aa0c60c211f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "2198061      5         0     3             5                   1           1   \n",
              "263036       4         0     2             0                   1           2   \n",
              "1432852      0         0     3             0                   1           2   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "2198061      3      2             -0.867668 -1.216453  1.019273 -1.055655   \n",
              "263036       1      5             -0.710027  0.016408  2.029452  2.627789   \n",
              "1432852      0      6              0.398646 -1.549553  0.746521  1.310607   \n",
              "\n",
              "         cheap_flag  expansive_flag      Price  \n",
              "2198061           0               0  62.314899  \n",
              "263036            0               0  29.110609  \n",
              "1432852           0               0  55.473881  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94a50db0-40b3-4f0c-93a6-eb8c8a449968\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2198061</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.867668</td>\n",
              "      <td>-1.216453</td>\n",
              "      <td>1.019273</td>\n",
              "      <td>-1.055655</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>62.314899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263036</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.710027</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>2.029452</td>\n",
              "      <td>2.627789</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>29.110609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1432852</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.398646</td>\n",
              "      <td>-1.549553</td>\n",
              "      <td>0.746521</td>\n",
              "      <td>1.310607</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>55.473881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94a50db0-40b3-4f0c-93a6-eb8c8a449968')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-94a50db0-40b3-4f0c-93a6-eb8c8a449968 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-94a50db0-40b3-4f0c-93a6-eb8c8a449968');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-90693fbf-28ce-427e-a52f-0ed0ddf98311\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-90693fbf-28ce-427e-a52f-0ed0ddf98311')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-90693fbf-28ce-427e-a52f-0ed0ddf98311 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc_y\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          4,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2,\n        \"max\": 6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.8676679730415344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.2164533138275146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0192734003067017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -1.0556546449661255\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Price\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          62.31489944458008\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "X_enc_y = pd.concat([X_enc, y], axis=1)\n",
        "X_enc_y.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1eGrjSSNBTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ffc706-7e49-4c85-f2f3-150cbc1994f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 15 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float32\n",
            " 9   TE_wc                 float32\n",
            " 10  skew_0                float32\n",
            " 11  skew_1                float32\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            " 14  Price                 float32\n",
            "dtypes: float32(5), int64(10)\n",
            "memory usage: 411.4 MB\n"
          ]
        }
      ],
      "source": [
        "X_enc_y.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzFLG4OrlysF"
      },
      "outputs": [],
      "source": [
        "class plot_class():\n",
        "\n",
        "    def __init__(self,df, target_variable, features_cat,features_num):\n",
        "      self.df = df\n",
        "      self.target_variable = target_variable\n",
        "      self.features_cat = features_cat\n",
        "      self.features_num = features_num\n",
        "\n",
        "    @classmethod\n",
        "    def plot_categorical_features(cls, df, target_variable, features_cat, features_num):\n",
        "        \"\"\"\n",
        "        Plots the frequency of the target variable for each value of multiple categorical features.\n",
        "\n",
        "        Args:\n",
        "          df: Pandas DataFrame containing the data.\n",
        "          target_variable: Name of the target variable column in the DataFrame.\n",
        "          features: List of names of the categorical feature columns to plot.\n",
        "        \"\"\"\n",
        "\n",
        "        num_features = len(features_cat)\n",
        "        num_rows = (num_features + 1) // 2  # Calculate the number of rows needed\n",
        "\n",
        "        fig, axes = plt.subplots(num_rows, 2, figsize=(12, 4 * num_rows))\n",
        "        axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
        "\n",
        "        for i, feature in enumerate(features_cat):\n",
        "            cross_tab = pd.crosstab(df[feature], df[target_variable])\n",
        "            cross_tab.plot(kind='bar', stacked=False, position=0.3, width=0.4, ax=axes[i],colormap=palette_1, alpha=0.6)\n",
        "            axes[i].set_xlabel(feature)\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "            axes[i].set_title(f'Frequency of {target_variable} by {feature}')\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for i in range(num_features, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return cls(df, target_variable, features_cat, features_num)\n",
        "\n",
        "    @classmethod\n",
        "    def plot_numerical_features(cls, df, target_variable, features_cat, features_num):\n",
        "        \"\"\"\n",
        "        Generates violin plots for numerical features, showing the distribution for each target class.\n",
        "\n",
        "        Args:\n",
        "          df: Pandas DataFrame containing the data.\n",
        "          target_variable: Name of the target variable column in the DataFrame.\n",
        "          features: List of names of the numerical feature columns to plot.\n",
        "        \"\"\"\n",
        "\n",
        "        num_features = len(features_num)\n",
        "        num_rows = (num_features + 1) // 2  # Calculate the number of rows needed\n",
        "\n",
        "        fig, axes = plt.subplots(num_rows, 2, figsize=(12, 4 * num_rows))\n",
        "        axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
        "\n",
        "        for i, feature in enumerate(features_num):\n",
        "            sns.violinplot(x=target_variable, y=feature, data=df, ax=axes[i],\n",
        "                           hue=target_variable,  # Use 'hue' to color by target class\n",
        "                           palette=palette_9)\n",
        "            axes[i].set_xlabel(target_variable)\n",
        "            axes[i].set_ylabel(feature)\n",
        "            axes[i].set_title(f'Distribution of {feature} by {target_variable}')\n",
        "\n",
        "        # Hide any unused subplots\n",
        "        for i in range(num_features, len(axes)):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return cls(df, target_variable, features_cat, features_num)\n",
        "\n",
        "    def scatter_comp(self, feat_01, feat_02, hue_def):\n",
        "        \"\"\"\n",
        "        Generates a scatter plot between two features, colored by a third\n",
        "        categorical feature using Seaborn.\n",
        "\n",
        "        Args:\n",
        "          df: Pandas DataFrame containing the data.\n",
        "          x_feature: Name of the feature to plot on the x-axis.\n",
        "          y_feature: Name of the feature to plot on the y-axis.\n",
        "          color_feature: Name of the categorical feature to use for coloring.\n",
        "        \"\"\"\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        sns.scatterplot(\n",
        "            x=feat_01,\n",
        "            y=feat_02,\n",
        "            hue=hue_def,  # Use 'hue' for color encoding\n",
        "            data=self.df,\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        plt.xlabel(feat_01)\n",
        "        plt.ylabel(feat_02)\n",
        "        plt.title(f'ScatterPlot of {feat_01} vs. {feat_02} colored by {hue_def}')\n",
        "        plt.show()\n",
        "\n",
        "    def heatmap_corr(self):\n",
        "        print(Style.BRIGHT+Fore.RED+f'\\nCorrelation Heatmap\\n')\n",
        "        plt.figure(figsize=(7,7))\n",
        "        corr = self.df.select_dtypes(exclude='int').corr(method='pearson')\n",
        "        sns.heatmap(corr, fmt = '0.2f', cmap = \"Reds\", annot=True, cbar=False)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YShjpCmaPB7"
      },
      "outputs": [],
      "source": [
        "#plot_instance = plot_class.plot_categorical_features(df=X_enc_y, target_variable=\"loan_status\", features_cat=t.cat_features, features_num=t.num_features);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYJ72HImPy3N"
      },
      "outputs": [],
      "source": [
        "#plot_instance.plot_numerical_features(df=X_enc_y, target_variable=\"loan_status\", features_cat=t.cat_features, features_num=t.num_features);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thXxmDkdPy0L"
      },
      "outputs": [],
      "source": [
        "#plot_instance.scatter_comp(feat_01=\"loan_sustainability\", feat_02=\"loan_grade\", hue_def=\"loan_status\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLWYiuaaPyxD"
      },
      "outputs": [],
      "source": [
        "#plot_instance.heatmap_corr()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc.info()"
      ],
      "metadata": {
        "id": "Y20xEnaSuowD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb988cdc-1e6a-46e5-90cb-f78223cdab12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Brand                 int64  \n",
            " 1   Material              int64  \n",
            " 2   Size                  int64  \n",
            " 3   Compartments          int64  \n",
            " 4   Laptop Compartment    int64  \n",
            " 5   Waterproof            int64  \n",
            " 6   Style                 int64  \n",
            " 7   Color                 int64  \n",
            " 8   Weight Capacity (kg)  float32\n",
            " 9   TE_wc                 float32\n",
            " 10  skew_0                float32\n",
            " 11  skew_1                float32\n",
            " 12  cheap_flag            int64  \n",
            " 13  expansive_flag        int64  \n",
            "dtypes: float32(4), int64(10)\n",
            "memory usage: 396.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKAbo5tTtczL"
      },
      "outputs": [],
      "source": [
        "# X_enc_y.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_train_enc_expanded.csv\", index=False)\n",
        "# test_enc.to_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_test_enc_expanded.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc_y = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_train_enc_expanded.csv\")\n",
        "test_enc = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/X_test_enc_expanded.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "7O2s6nVcPyUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMb-65cVOuzp"
      },
      "source": [
        "## **4.0 MODELS**\n",
        "\n",
        "--------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmZnoc4eO1_E"
      },
      "source": [
        "### **4.1 TREE BASED MODELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1GyNmMXO7Dw"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
        "# class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "# sample_pos_weight = class_weights[1]/class_weights[0]\n",
        "# sample_pos_weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qrTmdIJRsR-"
      },
      "source": [
        "#### 4.1.1 CatBoostClassifier:\n",
        "\n",
        "    class CatBoostClassifier(iterations=None,\n",
        "                            learning_rate=None,\n",
        "                            depth=None,\n",
        "                            l2_leaf_reg=None,\n",
        "                            model_size_reg=None,\n",
        "                            rsm=None,\n",
        "                            loss_function=None,\n",
        "                            border_count=None,\n",
        "                            feature_border_type=None,\n",
        "                            per_float_feature_quantization=None,\n",
        "                            input_borders=None,\n",
        "                            output_borders=None,\n",
        "                            fold_permutation_block=None,\n",
        "                            od_pval=None,\n",
        "                            od_wait=None,\n",
        "                            od_type=None,\n",
        "                            nan_mode=None,\n",
        "                            counter_calc_method=None,\n",
        "                            leaf_estimation_iterations=None,\n",
        "                            leaf_estimation_method=None,\n",
        "                            thread_count=None,\n",
        "                            random_seed=None,\n",
        "                            use_best_model=None,\n",
        "                            verbose=None,\n",
        "                            logging_level=None,\n",
        "                            metric_period=None,\n",
        "                            ctr_leaf_count_limit=None,\n",
        "                            store_all_simple_ctr=None,\n",
        "                            max_ctr_complexity=None,\n",
        "                            has_time=None,\n",
        "                            allow_const_label=None,\n",
        "                            classes_count=None,\n",
        "                            class_weights=None,\n",
        "                            auto_class_weights=None,\n",
        "                            one_hot_max_size=None,\n",
        "                            random_strength=None,\n",
        "                            name=None,\n",
        "                            ignored_features=None,\n",
        "                            train_dir=None,\n",
        "                            custom_loss=None,\n",
        "                            custom_metric=None,\n",
        "                            eval_metric=None,\n",
        "                            bagging_temperature=None,\n",
        "                            save_snapshot=None,\n",
        "                            snapshot_file=None,\n",
        "                            snapshot_interval=None,\n",
        "                            fold_len_multiplier=None,\n",
        "                            used_ram_limit=None,\n",
        "                            gpu_ram_part=None,\n",
        "                            allow_writing_files=None,\n",
        "                            final_ctr_computation_mode=None,\n",
        "                            approx_on_full_history=None,\n",
        "                            boosting_type=None,\n",
        "                            simple_ctr=None,\n",
        "                            combinations_ctr=None,\n",
        "                            per_feature_ctr=None,\n",
        "                            task_type=None,\n",
        "                            device_config=None,\n",
        "                            devices=None,\n",
        "                            bootstrap_type=None,\n",
        "                            subsample=None,\n",
        "                            sampling_unit=None,\n",
        "                            dev_score_calc_obj_block_size=None,\n",
        "                            max_depth=None,\n",
        "                            n_estimators=None,\n",
        "                            num_boost_round=None,\n",
        "                            num_trees=None,\n",
        "                            colsample_bylevel=None,\n",
        "                            random_state=None,\n",
        "                            reg_lambda=None,\n",
        "                            objective=None,\n",
        "                            eta=None,\n",
        "                            max_bin=None,\n",
        "                            scale_pos_weight=None,\n",
        "                            gpu_cat_features_storage=None,\n",
        "                            data_partition=None\n",
        "                            metadata=None,\n",
        "                            early_stopping_rounds=None,\n",
        "                            cat_features=None,\n",
        "                            grow_policy=None,\n",
        "                            min_data_in_leaf=None,\n",
        "                            min_child_samples=None,\n",
        "                            max_leaves=None,\n",
        "                            num_leaves=None,\n",
        "                            score_function=None,\n",
        "                            leaf_estimation_backtracking=None,\n",
        "                            ctr_history_unit=None,\n",
        "                            monotone_constraints=None,\n",
        "                            feature_weights=None,\n",
        "                            penalties_coefficient=None,\n",
        "                            first_feature_use_penalties=None,\n",
        "                            model_shrink_rate=None,\n",
        "                            model_shrink_mode=None,\n",
        "                            langevin=None,\n",
        "                            diffusion_temperature=None,\n",
        "                            posterior_sampling=None,\n",
        "                            boost_from_average=None,\n",
        "                            text_features=None,\n",
        "                            tokenizers=None,\n",
        "                            dictionaries=None,\n",
        "                            feature_calcers=None,\n",
        "                            text_processing=None,\n",
        "                            fixed_binary_splits=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvZIUdxLzo-n"
      },
      "outputs": [],
      "source": [
        "cat_prob = {\"objective\":\"RMSE\",\"eval_metric\":\"RMSE\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJvn8jzFQUNr"
      },
      "source": [
        "##### 4.1.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDGGnrUAQUNr"
      },
      "outputs": [],
      "source": [
        "def objective_catboost(trial, X, y, n_splits, n_repeats, model=CatBoostRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=cat_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {\n",
        "        'iterations': 1000,\n",
        "        'learning_rate': 0.025, #trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'depth': trial.suggest_int('depth', 5, 9),\n",
        "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-4, 0.1, log=True),\n",
        "        \"bootstrap_type\": \"Bayesian\",\n",
        "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.5, 1.5, step=0.1),\n",
        "        'random_strength': trial.suggest_float('random_strength', 0.5, 3.5, step=0.25),\n",
        "        #'border_count': trial.suggest_int('border_count', 32, 255),\n",
        "        'cat_features': categorical_features,\n",
        "        'task_type': 'GPU' if use_gpu else 'CPU',\n",
        "        'random_seed':rs,\n",
        "        'verbose': 250,\n",
        "        'objective': metrics[\"objective\"],\n",
        "        'eval_metric': metrics[\"eval_metric\"],\n",
        "        \"od_type\":'EBS', #Early stopping hyperparmeter\n",
        "        \"od_wait\":101,\n",
        "        #\"sampling_frequency\":\"PerTreeLevel\",\n",
        "        \"use_best_model\":True,\n",
        "    }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "      kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "      kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "      kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "      kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1,1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1,1)\n",
        "\n",
        "        if fit_scaling:\n",
        "          scaler = StandardScaler()\n",
        "          X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "          X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create the Pool objects for CatBoost\n",
        "        train_pool = Pool(data=X_train, label=y_train, cat_features=categorical_features)\n",
        "        valid_pool = Pool(data=X_valid, label=y_valid, cat_features=categorical_features)\n",
        "\n",
        "        # Create the pipeline\n",
        "        model = model_class(**params)\n",
        "        # Fit the model:\n",
        "        model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=101,\n",
        "                  #callbacks=[optuna.integration.CatBoostPruningCallback(trial, \"RMSE\")]\n",
        "                  )\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "#        y_pred = np.expm1(y_pred)\n",
        "#        y_valid = np.expm1(y_valid)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4q9nTQpQUNs"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_catboost(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMaZd811QUNs"
      },
      "outputs": [],
      "source": [
        "# usage with XGBRegressor\n",
        "cat_study = tune_hyperparameters(X_enc, y, model_class=CatBoostRegressor, n_trials=31, n_splits_ = 5 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(cat_study, CatBoostRegressor, \"CatBoost_ext\")\n",
        "cat_params = cat_study.best_paramsy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 10 finished with value: 38.67948459216966\n",
        "- Parameters: {'depth': 9, 'l2_leaf_reg': 0.004177701145518355, 'bagging_temperature': 0.5, 'random_strength': 0.5}"
      ],
      "metadata": {
        "id": "c8WTOGC1vY-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-aPe-7lMjdY"
      },
      "outputs": [],
      "source": [
        "X_enc.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvqbaMJMxyqI"
      },
      "source": [
        "#### **4.2.1 LGBMRegressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2VbsJlay765"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "lgbm_prob = {\"objective\":\"regression\",\"eval_metric\":\"rmse\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOB-zCzMx9Jr"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cHDHOJMiYKK"
      },
      "outputs": [],
      "source": [
        "X_enc.info()\n",
        "t.cat_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_EHAxqPPyrU"
      },
      "outputs": [],
      "source": [
        "def objective_lgbm(trial, X, y, n_splits, n_repeats, model=LGBMRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=lgbm_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 31, 131),\n",
        "        'learning_rate': 0.02, #trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        #'max_depth': trial.suggest_int('max_depth', 5, 10),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 60),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 0.95),\n",
        "        'subsample_freq': trial.suggest_int('subsample_freq', 1, 3),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        \"reg_alpha\" :         trial.suggest_float(\"reg_alpha\", 1e-3, 1.0, log=True),\n",
        "        \"reg_lambda\" :        trial.suggest_float(\"reg_lambda\", 1e-3, 1.0, log=True),\n",
        "        \"boosting_type\":      'gbdt',\n",
        "        'n_estimators': 2501,\n",
        "        'objective': metrics[\"objective\"],\n",
        "        'device': 'gpu' if use_gpu else 'cpu',\n",
        "        'verbose': -1,\n",
        "        #'scale_pos_weight': sample_pos_weight,\n",
        "#       'categorical_feature': [2,4,5,9],\n",
        "        'random_state': rs,\n",
        "    }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1, 1)\n",
        "\n",
        "        if fit_scaling:\n",
        "            scaler = StandardScaler()\n",
        "            X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "            X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create the datasets for LightGBM\n",
        "        # d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_features)\n",
        "        # d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=categorical_features)\n",
        "\n",
        "        # Create the model\n",
        "        model = model_class(**params)\n",
        "\n",
        "        # Create the early stopping callback\n",
        "        early_stop = early_stopping(stopping_rounds=101)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], callbacks=[early_stop], eval_metric=metrics[\"eval_metric\"], categorical_feature= categorical_features)\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnC-tzJ-PyoE"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_lgbm(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mXmUXUZPyk8"
      },
      "outputs": [],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=LGBMRegressor, n_trials=31, n_splits_ = 5 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(cat_study, LGBMRegressor, \"LGBMBoost_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 28 finished with value: 38.657451168104984\n",
        "- parameters: {'num_leaves': 103, 'min_child_samples': 36, 'subsample': 0.9131771240297577, 'subsample_freq': 2, 'colsample_bytree': 0.6190291906152294, 'reg_alpha': 0.03976551748855951, 'reg_lambda': 0.2576052197300848}"
      ],
      "metadata": {
        "id": "4Ul6eVTJZNC5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md0hNqSwgfV4"
      },
      "source": [
        "#### **4.3.1 XGBClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yoxlSCCgfV5"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "xgb_prob = {'objective': \"reg:squarederror\",'eval_metric': \"rmse\"}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_enc[t.cat_features] = X_enc[t.cat_features].astype(\"category\")\n",
        "test_enc[t.cat_features] = test_enc[t.cat_features].astype(\"category\")\n",
        "test_enc.info()"
      ],
      "metadata": {
        "id": "ELHBSxDA4C95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SglmFB9QgfV5"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ROmQq9OgfV6"
      },
      "outputs": [],
      "source": [
        "def objective_xgb(trial, X, y, n_splits, n_repeats, model=XGBRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=xgb_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {\n",
        "              'n_estimators': 1000,\n",
        "              'learning_rate': 0.025, #trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "              'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "              #'max_bin': trial.suggest_int('max_bin', 255, 511),\n",
        "              'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "              'subsample': trial.suggest_float('subsample', 0.7, 0.95),\n",
        "              'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.95),\n",
        "              'gamma': trial.suggest_float('gamma', 0, 1),\n",
        "              'reg_alpha': trial.suggest_float('reg_alpha', 0.00001, 1.0, log=True),\n",
        "              'reg_lambda': trial.suggest_float('reg_lambda', 0.00001, 10, log=True),\n",
        "              'objective':  metrics[\"objective\"],  # For binary classification\n",
        "              'eval_metric': metrics[\"eval_metric\"],\n",
        "              \"early_stopping_rounds\":51,\n",
        "              'tree_method': 'gpu_hist' if use_gpu else 'hist',  # Use GPU if available\n",
        "              'random_state': rs,\n",
        "              'enable_categorical': True,\n",
        "#              'scale_pos_weight': sample_pos_weight,\n",
        "             }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1, 1)\n",
        "\n",
        "        if fit_scaling:\n",
        "            scaler = StandardScaler()\n",
        "            X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "            X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create DMatrix objects for XGBoost\n",
        "        d_train = xgb.DMatrix(X_train, label=y_train,feature_types=[\"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"q\",\"q\",\"q\",\"q\",\n",
        "                                                                    \"c\",\"c\"],enable_categorical=True)\n",
        "        d_valid = xgb.DMatrix(X_valid, label=y_valid,feature_types=[\"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"c\",\"c\",\"c\",\"c\",\n",
        "                                                                    \"q\",\"q\",\"q\",\"q\",\n",
        "                                                                    \"c\",\"c\"],enable_categorical=True)\n",
        "\n",
        "        # Create the model\n",
        "        model = model_class(**params)\n",
        "\n",
        "        # Create the early stopping callback\n",
        "        # early_stop = early_stopping(stopping_rounds=101)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train, y_train,\n",
        "                  eval_set=[(X_valid, y_valid)],\n",
        "                  verbose=False)\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1QiMlUGgfV6"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_xgb(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57BpBdEqgfV6"
      },
      "outputs": [],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=XGBRegressor, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(cat_study, XGBRegressor, \"XGB_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 18 finished with value: 38.66071701049805\n",
        "- parameters: {'max_depth': 9, 'min_child_weight': 9, 'subsample': 0.9493097301768285, 'colsample_bytree': 0.750026675584575, 'gamma': 0.5511841320880777, 'reg_alpha': 0.006948944983035811, 'reg_lambda': 0.0015661314558322087}."
      ],
      "metadata": {
        "id": "jxXsfZ_hDsYH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbThfscYTcDW"
      },
      "source": [
        "#### **4.4.1 TabNetClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u__JMQhTcDX"
      },
      "outputs": [],
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
        "tab_prob = {'eval_metric': \"rmse\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFqDr7bBZKoj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "a3f157d3-60d1-4e6a-c578-752ecc625063"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "3782983      5         3     1             7                   1           1   \n",
              "2003343      3         0     1             1                   2           2   \n",
              "3404963      4         3     3             7                   2           2   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "3782983      3      2              1.451171  0.016408  0.483511 -0.935853   \n",
              "2003343      3      0             -0.433108  0.033023 -1.185578 -0.348455   \n",
              "3404963      3      6              1.522432 -1.099254 -0.846883  0.448205   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "3782983           0               0  \n",
              "2003343           0               0  \n",
              "3404963           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24a6a99d-0672-43cf-92dd-bb5402b777ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3782983</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1.451171</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>0.483511</td>\n",
              "      <td>-0.935853</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2003343</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.433108</td>\n",
              "      <td>0.033023</td>\n",
              "      <td>-1.185578</td>\n",
              "      <td>-0.348455</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3404963</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1.522432</td>\n",
              "      <td>-1.099254</td>\n",
              "      <td>-0.846883</td>\n",
              "      <td>0.448205</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24a6a99d-0672-43cf-92dd-bb5402b777ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24a6a99d-0672-43cf-92dd-bb5402b777ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24a6a99d-0672-43cf-92dd-bb5402b777ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6f397794-e47b-42fc-85e9-13ceccbf56be\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f397794-e47b-42fc-85e9-13ceccbf56be')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6f397794-e47b-42fc-85e9-13ceccbf56be button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 7,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.4511709213256836\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.01640772819519043\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4835107922554016\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.9358525276184082\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UikYbKG4ZC24"
      },
      "outputs": [],
      "source": [
        "grouped_features = []\n",
        "feature_cols = X_enc.columns.to_list()\n",
        "\n",
        "group_1 = [\"Brand\",\t\"Material\",\t\"Size\",\t\"Compartments\",\t\"Laptop Compartment\",\t\"Waterproof\",\t\"Style\",\t\"Color\"]\n",
        "group_2 = ['cheap_flag',\t'expansive_flag']\n",
        "#group_3 = ['cb_person_default_on_file']\n",
        "\n",
        "# Iterate through each set of related columns (e.g., blood glucose, insulin, etc.)\n",
        "for colset in [group_1,group_2]:\n",
        "    group_idxs = [idx for idx, col in enumerate(feature_cols) if col in colset]\n",
        "    grouped_features.append(group_idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oh4feROUZoUr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b96d80-f3f7-4c82-e67a-f13ee845aef4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 1, 2, 3, 4, 5, 6, 7], [12, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "grouped_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWEKkZi7TcDX"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyVDQ0FjUBUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daae5a4a-771f-488d-fceb-2741fa8fa4da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 13]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "cate_feat = t.cat_features.copy()\n",
        "\n",
        "for colset in [cate_feat]:\n",
        "    cat_index_cols = [idx for idx, col in enumerate(feature_cols) if col in colset]\n",
        "\n",
        "cat_index_cols[-2:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[len(X_enc[col].unique()) for col in t.cat_features][-2:]"
      ],
      "metadata": {
        "id": "X76B9uW6BSVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3ca0d6d-d1d7-4d73-824d-4b7f3ff89c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWyN_Q_rTcDY"
      },
      "outputs": [],
      "source": [
        "def objective_tabnet(trial, X, y, n_splits, n_repeats, model=TabNetRegressor, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\", metrics=tab_prob):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'n_d': trial.suggest_categorical('n_d', [4,8,12]),\n",
        "              'n_a': trial.suggest_categorical('n_d', [4,8,12]),\n",
        "              'n_steps': trial.suggest_int('n_steps', 3, 4),\n",
        "              'gamma': trial.suggest_float('gamma', 1.01, 2),\n",
        "              'lambda_sparse':trial.suggest_float('lambda_sparse', 1e-5, 1e-1),\n",
        "              \"grouped_features\":grouped_features,\n",
        "              #'cat_idxs': cat_index_cols,\n",
        "              #'cat_dims': [len(X_enc[col].unique()) for col in categorical_features],\n",
        "              #'cat_emb_dim': [2,2,1,3,1,1,1,2,1,1],\n",
        "              'n_independent': trial.suggest_int('n_independent', 1, 3),\n",
        "              'n_shared': trial.suggest_int('n_shared', 1, 3),\n",
        "              'mask_type': trial.suggest_categorical('mask_type', ['sparsemax']),\n",
        "              'device_name': 'cuda' if use_gpu else 'cpu',\n",
        "              'seed': rs,\n",
        "              \"optimizer_fn\":torch.optim.Adam,\n",
        "              \"optimizer_params\":dict(lr=0.01),\n",
        "              \"scheduler_params\":{\"patience\":3, # how to use learning rate scheduler\n",
        "                                \"factor\":0.5,\n",
        "                                \"min_lr\":0.0001},\n",
        "              \"scheduler_fn\":torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy().reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy().reshape(-1, 1)\n",
        "\n",
        "        if fit_scaling:\n",
        "            scaler = StandardScaler()\n",
        "            X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "            X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "        # Create the model\n",
        "        model = model_class(**params)\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit(X_train.values, y_train,\n",
        "                  eval_set=[(X_valid.values, y_valid)],\n",
        "                  batch_size=1024,\n",
        "                  max_epochs=11,\n",
        "                  patience=5,\n",
        "                  eval_metric=[tab_prob[\"eval_metric\"]])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict(X_valid.values)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfvK9Qw-TcDY"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "    study.optimize(lambda trial: objective_tabnet(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=model_class, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJwRqrJRTcDY"
      },
      "outputs": [],
      "source": [
        "tab_study = tune_hyperparameters(X_enc, y, model_class=TabNetRegressor, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "save_results(tab_study, TabNetRegressor, \"tabnet_ext\")\n",
        "tab_params = tab_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jch2yojjoiZS"
      },
      "source": [
        "#### 4.5.1 Yggdrasil - RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mgl-1wLt_4I"
      },
      "outputs": [],
      "source": [
        "#!pip install ydf -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_YHewPTzMVM"
      },
      "outputs": [],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vavm8Xws3vO"
      },
      "source": [
        "##### 4.5.2 Optuna Optimization:\n",
        "\n",
        "[HyperParameters Link](https://ydf.readthedocs.io/en/stable/hyperparameters/#lambda_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eJuaeqmPyh0"
      },
      "outputs": [],
      "source": [
        "# import ydf\n",
        "\n",
        "# def objective_ydf_gbt(trial, X, y, n_splits, n_repeats, model_ = ydf.GradientBoostedTreesLearner, rs=42, fit_scaling=False, cv_strategy=\"KFold\", use_gpu=False):\n",
        "\n",
        "#     model_class = model_  # Use ydf's GradientBoostedTreesLearner\n",
        "\n",
        "#     categorical_features = t.cat_features.copy()  # Assuming 't' is defined somewhere with categorical features\n",
        "\n",
        "#     num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "#     auc_scores = []\n",
        "\n",
        "#     params = {\n",
        "#         'loss':\"BINOMIAL_LOG_LIKELIHOOD\",\n",
        "#         'num_trees': trial.suggest_int('num_trees', 100, 1000),  # Number of trees in the forest\n",
        "#         'max_depth': trial.suggest_int('max_depth', 3, 10),  # Maximum depth of the trees\n",
        "#         'min_examples': trial.suggest_int('min_examples', 1, 10),  # Minimum number of samples required to split an internal node\n",
        "#         'l1_regularization': trial.suggest_float('l1_regularization', 1e-5, 10.0, log=True),\n",
        "#         'l2_regularization': trial.suggest_float('l2_regularization', 1e-5, 10.0, log=True),\n",
        "# #         'shrinkage': trial.suggest_float('shrinkage', 0.01, 0.3, log=True),  # Similar to learning rate, but applied after each tree is trained\n",
        "#          'subsample': trial.suggest_float('subsample', 0.7, 1.0),  # Fraction of samples used for training each tree\n",
        "# # #        'max_categorical_cardinality': trial.suggest_int('max_categorical_cardinality', 10, 100),  # Maximum number of unique values for categorical features\n",
        "# #         'categorical_algorithm': trial.suggest_categorical('categorical_algorithm', ['CART', 'RANDOM']),  # Algorithm used for categorical splits\n",
        "# #         'split_axis': trial.suggest_categorical('split_axis', ['AXIS_ALIGNED', 'SPARSE_OBLIQUE']),  # How to split numerical features\n",
        "# #         'sparse_oblique_normalization': trial.suggest_categorical('sparse_oblique_normalization', ['NONE', 'STANDARD_DEVIATION', 'MIN_MAX']),  # Normalization for sparse oblique splits\n",
        "# #         'sparse_oblique_num_projections_exponent': trial.suggest_float('sparse_oblique_num_projections_exponent', 0.5, 1.0),  # Exponent for the number of projections in sparse oblique splits\n",
        "#         'random_seed': rs,  # Random seed for reproducibility\n",
        "#         'early_stopping':\"LOSS_INCREASE\",  # Enable early stopping\n",
        "#         'early_stopping_num_trees_look_ahead': 30,  # Number of trees to look ahead for early stopping\n",
        "# #        'weights': {0:1,1:sample_pos_weight},\n",
        "#         \"label\" : \"loan_status\",\n",
        "#         }\n",
        "\n",
        "\n",
        "#     if cv_strategy == 'RepKFold':\n",
        "#         kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "#     elif cv_strategy == 'KFold':\n",
        "#         kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "#     elif cv_strategy == \"StratKFold\":\n",
        "#         kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "#     elif cv_strategy == \"RepStratKFold\":\n",
        "#         kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "#     for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "#         # Split the data\n",
        "#         X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
        "#         X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
        "\n",
        "#         if fit_scaling:\n",
        "#             scaler = StandardScaler()\n",
        "#             X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
        "#             X_valid[num_cols] = scaler.transform(X_valid[num_cols])\n",
        "\n",
        "#         X_train[\"loan_status\"] = y_train.values\n",
        "#         X_valid[\"loan_status\"] = y_valid.values\n",
        "\n",
        "#         # Train the model with early stopping\n",
        "#         model = model_class(task=ydf.Task.CLASSIFICATION, **params)\n",
        "#         model.train(X_train, valid=(X_valid))  # Use ydf's fit method with early stopping\n",
        "\n",
        "#         # Make predictions\n",
        "#         y_pred = model.get_predictions(X_valid)[\"loan_status_probability_1\"]\n",
        "\n",
        "#         # Calculate AUC\n",
        "#         auc_score = roc_auc_score(y_valid, y_pred)\n",
        "#         auc_scores.append(auc_score)\n",
        "\n",
        "#     # Calculate the mean AUC\n",
        "#     key_metric = np.mean(auc_scores)\n",
        "\n",
        "#     return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbunXYe3vquN"
      },
      "outputs": [],
      "source": [
        "# # Step 2: Tuning Hyperparameters with Optuna\n",
        "# def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "#     study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=50))\n",
        "#     study.optimize(lambda trial: objective_ydf_gbt(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model_=model_class, use_gpu=use_gpu, cv_strategy=\"StratKFold\"), n_trials=n_trials)\n",
        "#     return study  # Return the study object\n",
        "\n",
        "# # Step 3: Saving Best Results and Models\n",
        "# def save_results(study, model_class, model_name):\n",
        "#     best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "#     joblib.dump(study.best_params, best_params_file)\n",
        "#     print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "#     verbose_file = f\"{model_name}_ydf_verbose.log\"\n",
        "#     with open(verbose_file, \"w\") as f:\n",
        "#         f.write(str(study.trials))\n",
        "#     print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6XXbt231ogK"
      },
      "outputs": [],
      "source": [
        "# cat_study = tune_hyperparameters(X_enc, y, model_class=ydf.GradientBoostedTreesLearner, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=False)\n",
        "# save_results(cat_study, TabNetClassifier, \"ydf_ext\")\n",
        "# cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41K1txKVzFhu"
      },
      "source": [
        "#### **4.6.1 NeuralNetwork**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGyFvBNgzFh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "f3fb4191-eaad-44ab-89fa-1aa222af44cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "3627270      1         0     3             6                   1           2   \n",
              "156426       3         4     0             6                   1           2   \n",
              "963428       4         3     1             2                   2           2   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "3627270      0      5              1.087092 -0.217410  0.746521  0.063366   \n",
              "156426       3      0             -1.030614 -0.202770  0.044250  0.216236   \n",
              "963428       0      3              1.682245 -0.364852 -1.185578 -0.018634   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "3627270           0               0  \n",
              "156426            0               0  \n",
              "963428            0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ffcc0213-618f-4ecf-be41-1bd9bba687cd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3627270</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1.087092</td>\n",
              "      <td>-0.217410</td>\n",
              "      <td>0.746521</td>\n",
              "      <td>0.063366</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156426</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>-1.030614</td>\n",
              "      <td>-0.202770</td>\n",
              "      <td>0.044250</td>\n",
              "      <td>0.216236</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963428</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.682245</td>\n",
              "      <td>-0.364852</td>\n",
              "      <td>-1.185578</td>\n",
              "      <td>-0.018634</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffcc0213-618f-4ecf-be41-1bd9bba687cd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ffcc0213-618f-4ecf-be41-1bd9bba687cd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ffcc0213-618f-4ecf-be41-1bd9bba687cd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f51a457c-ba0c-4a72-9410-c68a9c5d056f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f51a457c-ba0c-4a72-9410-c68a9c5d056f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f51a457c-ba0c-4a72-9410-c68a9c5d056f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          4,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 2,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.0870918035507202\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.2174099087715149\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7465205192565918\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.06336555629968643\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512,last_layer = 1, activation=\"relu\"):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "VF6Qd4eziUd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIf9YCb8Bdcu",
        "outputId": "19bc5409-4ff7-4850-a175-048da1fd2b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([6, 5, 4, 10, 3, 3, 4, 7, 2, 2],\n",
              " array([3., 3., 2., 4., 2., 2., 2., 3., 2., 2.]),\n",
              " 10)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Efx2uIuzFh4"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maUIASqazFh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69691a1f-0b12-4a0d-a6f2-51241dc89baf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Dtype\n",
            "---  ------              -----\n",
            " 0   Brand               int64\n",
            " 1   Material            int64\n",
            " 2   Size                int64\n",
            " 3   Compartments        int64\n",
            " 4   Laptop Compartment  int64\n",
            " 5   Waterproof          int64\n",
            " 6   Style               int64\n",
            " 7   Color               int64\n",
            " 8   cheap_flag          int64\n",
            " 9   expansive_flag      int64\n",
            "dtypes: int64(10)\n",
            "memory usage: 335.2 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Weight Capacity (kg)  float32\n",
            " 1   TE_wc                 float32\n",
            " 2   skew_0                float32\n",
            " 3   skew_1                float32\n",
            "dtypes: float32(4)\n",
            "memory usage: 91.4 MB\n"
          ]
        }
      ],
      "source": [
        "categorical_feat = t.cat_features.copy()\n",
        "numerical_feat = t.num_features.copy()\n",
        "\n",
        "X_train_cat = X_enc[categorical_feat]\n",
        "X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "X_test_cat = test_enc[categorical_feat]\n",
        "X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "X_train_cat.info()\n",
        "X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GgRZk52zFh5"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"])}\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num])\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYf3TjJVzFh6"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9KKYtiYzFh7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b70baf2-5abe-4dd1-cb1e-cf8ca45589ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-22 10:43:19,819] A new study created in memory with name: no-name-da565752-2ae0-42aa-ab90-3d713d7cd484\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-22 10:43:26,476] Trial 0 failed with parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu'} because of the following error: FailedPreconditionError().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-42-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-41-f555a01cb6c3>\", line 48, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
            "    except TypeError as e:\n",
            "tensorflow.python.framework.errors_impl.FailedPreconditionError: Graph execution error:\n",
            "\n",
            "Detected at node StatefulPartitionedCall defined at (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "\n",
            "  File \"<ipython-input-44-3ce299ed4080>\", line 1, in <cell line: 0>\n",
            "\n",
            "  File \"<ipython-input-42-f63d57ecfd44>\", line 4, in tune_hyperparameters\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "\n",
            "  File \"<ipython-input-42-f63d57ecfd44>\", line 4, in <lambda>\n",
            "\n",
            "  File \"<ipython-input-41-f555a01cb6c3>\", line 48, in objective_nn\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n",
            "\n",
            "DNN library initialization failed. Look at the errors above for more details.\n",
            "\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_10724]\n",
            "[W 2025-02-22 10:43:26,478] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-44-3ce299ed4080>\", line 1, in <cell line: 0>\n\n  File \"<ipython-input-42-f63d57ecfd44>\", line 4, in tune_hyperparameters\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"<ipython-input-42-f63d57ecfd44>\", line 4, in <lambda>\n\n  File \"<ipython-input-41-f555a01cb6c3>\", line 48, in objective_nn\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_10724]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-f555a01cb6c3>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     49\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-44-3ce299ed4080>\", line 1, in <cell line: 0>\n\n  File \"<ipython-input-42-f63d57ecfd44>\", line 4, in tune_hyperparameters\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"<ipython-input-42-f63d57ecfd44>\", line 4, in <lambda>\n\n  File \"<ipython-input-41-f555a01cb6c3>\", line 48, in objective_nn\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_10724]"
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtsmIR6j1aLi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 3 finished with value: 38.69801712036133\n",
        "* parameters: {'units': 1024, 'last_layer': 2, 'activation': 'silu'}"
      ],
      "metadata": {
        "id": "egD1bR4BVai7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kz6gSSAQlNe"
      },
      "source": [
        "#### **4.6.1 NeuralNetwork v1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "35a43236-3ea7-4305-deb0-be09e602671c",
        "id": "Vu4pcEHqQlNf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "1308799      2         1     0             5                   1           1   \n",
              "769973       2         3     0             4                   1           1   \n",
              "369845       1         1     3             1                   1           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "1308799      1      5              0.564222 -0.120751  1.391704  1.500713   \n",
              "769973       1      6             -0.641991 -0.703372  1.391704  1.500713   \n",
              "369845       3      4             -0.091990 -0.348663  1.019273  0.063366   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "1308799           0               0  \n",
              "769973            0               0  \n",
              "369845            0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9db768e-c345-436d-9f42-3964e2326712\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1308799</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.564222</td>\n",
              "      <td>-0.120751</td>\n",
              "      <td>1.391704</td>\n",
              "      <td>1.500713</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769973</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.641991</td>\n",
              "      <td>-0.703372</td>\n",
              "      <td>1.391704</td>\n",
              "      <td>1.500713</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369845</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.091990</td>\n",
              "      <td>-0.348663</td>\n",
              "      <td>1.019273</td>\n",
              "      <td>0.063366</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9db768e-c345-436d-9f42-3964e2326712')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9db768e-c345-436d-9f42-3964e2326712 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9db768e-c345-436d-9f42-3964e2326712');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2605f29a-7b10-41b8-81b1-6f4f8fa2cec9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2605f29a-7b10-41b8-81b1-6f4f8fa2cec9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2605f29a-7b10-41b8-81b1-6f4f8fa2cec9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 4,\n        \"max\": 6,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.5642222166061401\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.12075134366750717\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0192734003067017\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.06336555629968643\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512,last_layer = 1, activation=\"relu\", reg=0.001, dropout_rate=0.33):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    reshaped_features = layers.Reshape((1, -1))(x)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x, flattened_attention])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "J50ErSTRQlNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model()\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "71HFsxAZVnCN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d8dfd9f-ab15-40fc-a5e7-d6963725f939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_4              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " get_item_20 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_21 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_22 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_23 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_24 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_25 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_26 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_27 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_28 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_29 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " embedding_20 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m18\u001b[0m  get_item_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_21 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m15\u001b[0m  get_item_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_22 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m8\u001b[0m  get_item_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_23 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                           \u001b[38;5;34m40\u001b[0m  get_item_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_24 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m6\u001b[0m  get_item_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_25 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m6\u001b[0m  get_item_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_26 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m8\u001b[0m  get_item_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_27 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m21\u001b[0m  get_item_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_28 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m4\u001b[0m  get_item_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_29 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m4\u001b[0m  get_item_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_20 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_21 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_22 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_23 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_24 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_25 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_26 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_27 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_28 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_29 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " input_layer_5              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " concatenate_2              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  flatten_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      flatten_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    input_layer_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " reshape_2 (\u001b[38;5;33mReshape\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m29\u001b[0m)                        \u001b[38;5;34m0\u001b[0m  concatenate_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " attention (\u001b[38;5;33mAttention\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m29\u001b[0m)                        \u001b[38;5;34m0\u001b[0m  reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    reshape_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " flatten_30 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  attention[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " concatenate_3              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  concatenate_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      flatten_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_8 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m30,208\u001b[0m  concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " batch_normalization_6      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                      \u001b[38;5;34m2,048\u001b[0m  dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dropout (\u001b[38;5;33mDropout\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_6 \n",
              "\n",
              " dense_9 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m262,656\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " batch_normalization_7      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                      \u001b[38;5;34m2,048\u001b[0m  dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dropout_1 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_7 \n",
              "\n",
              " dense_10 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m262,656\u001b[0m  dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " batch_normalization_8      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                      \u001b[38;5;34m2,048\u001b[0m  dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dropout_2 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_8 \n",
              "\n",
              " dense_11 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m513\u001b[0m  dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_4              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " get_item_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " embedding_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>  get_item_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>  get_item_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  get_item_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>  get_item_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  get_item_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  get_item_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  get_item_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>  get_item_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  get_item_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  get_item_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " input_layer_5              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " concatenate_2              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      flatten_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    input_layer_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " reshape_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    reshape_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " flatten_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " concatenate_3              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      flatten_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,208</span>  concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " batch_normalization_6      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_6 \n",
              "\n",
              " dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " batch_normalization_7      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_7 \n",
              "\n",
              " dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " batch_normalization_8      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_8 \n",
              "\n",
              " dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span>  dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m562,307\u001b[0m (2.15 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">562,307</span> (2.15 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m559,235\u001b[0m (2.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">559,235</span> (2.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,072\u001b[0m (12.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> (12.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7bdaf28-50f6-4c7b-9818-75aa19191dd3",
        "id": "9tzW7MmLQlNg"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([6, 5, 4, 10, 3, 3, 4, 7, 2, 2],\n",
              " array([3., 3., 2., 4., 2., 2., 2., 3., 2., 2.]),\n",
              " 10)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ceVtzBwQlNg"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a87fac-94f6-4023-c880-b2082a669485",
        "id": "iQ9CoxtaQlNg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Dtype\n",
            "---  ------              -----\n",
            " 0   Brand               int64\n",
            " 1   Material            int64\n",
            " 2   Size                int64\n",
            " 3   Compartments        int64\n",
            " 4   Laptop Compartment  int64\n",
            " 5   Waterproof          int64\n",
            " 6   Style               int64\n",
            " 7   Color               int64\n",
            " 8   cheap_flag          int64\n",
            " 9   expansive_flag      int64\n",
            "dtypes: int64(10)\n",
            "memory usage: 335.2 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Weight Capacity (kg)  float32\n",
            " 1   TE_wc                 float32\n",
            " 2   skew_0                float32\n",
            " 3   skew_1                float32\n",
            "dtypes: float32(4)\n",
            "memory usage: 91.4 MB\n"
          ]
        }
      ],
      "source": [
        "categorical_feat = t.cat_features.copy()\n",
        "numerical_feat = t.num_features.copy()\n",
        "\n",
        "X_train_cat = X_enc[categorical_feat]\n",
        "X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "X_test_cat = test_enc[categorical_feat]\n",
        "X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "X_train_cat.info()\n",
        "X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tObj5kq1QlNg"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': trial.suggest_float('reg', 1e-4, 0.1, log=True),\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.50)\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num])\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sux1Xc6ZQlNh"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7529e2b3-d69b-4ddb-a44c-2f80dbe52ca4",
        "id": "gW_aoIIBQlNh"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 16:03:45,993] A new study created in memory with name: no-name-36849c32-d360-4d35-8d2a-109bfaa1d4aa\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  39s 12ms/step - RMSE: 51.3745 - loss: 2832.1123 - val_RMSE: 38.7568 - val_loss: 1519.5251 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9979 - loss: 1536.6611 - val_RMSE: 38.7666 - val_loss: 1515.6813 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8919 - loss: 1525.1195 - val_RMSE: 38.7090 - val_loss: 1509.4550 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8501 - loss: 1519.8094 - val_RMSE: 38.6949 - val_loss: 1506.4019 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8228 - loss: 1516.0771 - val_RMSE: 38.6996 - val_loss: 1505.6942 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8137 - loss: 1514.2294 - val_RMSE: 38.7109 - val_loss: 1505.5392 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8025 - loss: 1512.2858 - val_RMSE: 38.6927 - val_loss: 1503.1927 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7950 - loss: 1510.7366 - val_RMSE: 38.6923 - val_loss: 1502.1176 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7867 - loss: 1509.0934 - val_RMSE: 38.6909 - val_loss: 1500.8621 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7805 - loss: 1507.7684 - val_RMSE: 38.6896 - val_loss: 1500.1909 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7806 - loss: 1507.3898 - val_RMSE: 38.6902 - val_loss: 1500.0416 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7818 - loss: 1507.2737 - val_RMSE: 38.6917 - val_loss: 1500.2687 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7816 - loss: 1507.1475 - val_RMSE: 38.6891 - val_loss: 1499.9069 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7808 - loss: 1506.9562 - val_RMSE: 38.6876 - val_loss: 1499.5712 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7792 - loss: 1506.7211 - val_RMSE: 38.6887 - val_loss: 1499.6896 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7803 - loss: 1506.7301 - val_RMSE: 38.6895 - val_loss: 1499.5400 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7802 - loss: 1506.6393 - val_RMSE: 38.6883 - val_loss: 1499.5293 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7802 - loss: 1506.5903 - val_RMSE: 38.6864 - val_loss: 1499.0499 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7780 - loss: 1506.3221 - val_RMSE: 38.6881 - val_loss: 1499.2017 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7787 - loss: 1506.2954 - val_RMSE: 38.6896 - val_loss: 1499.3910 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7775 - loss: 1506.1781 - val_RMSE: 38.6892 - val_loss: 1499.2687 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7766 - loss: 1506.0671 - val_RMSE: 38.6860 - val_loss: 1499.0186 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7808 - loss: 1506.2799 - val_RMSE: 38.6865 - val_loss: 1498.8927 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7795 - loss: 1506.1647 - val_RMSE: 38.6855 - val_loss: 1498.7249 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7756 - loss: 1505.7499 - val_RMSE: 38.6862 - val_loss: 1498.7686 - learning_rate: 0.0010\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.3021 - loss: 2823.8208 - val_RMSE: 38.7634 - val_loss: 1520.7347 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9422 - loss: 1532.8909 - val_RMSE: 38.7831 - val_loss: 1516.9619 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8378 - loss: 1520.9048 - val_RMSE: 38.7349 - val_loss: 1511.1633 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7928 - loss: 1515.2720 - val_RMSE: 38.7321 - val_loss: 1509.1328 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7676 - loss: 1511.5394 - val_RMSE: 38.7262 - val_loss: 1507.4763 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7551 - loss: 1509.4453 - val_RMSE: 38.7316 - val_loss: 1506.8464 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7485 - loss: 1507.9105 - val_RMSE: 38.7240 - val_loss: 1505.2249 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7398 - loss: 1506.3206 - val_RMSE: 38.7215 - val_loss: 1504.0413 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7341 - loss: 1504.8588 - val_RMSE: 38.7214 - val_loss: 1503.0850 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7336 - loss: 1504.0422 - val_RMSE: 38.7209 - val_loss: 1502.7485 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7323 - loss: 1503.5555 - val_RMSE: 38.7200 - val_loss: 1502.3033 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7319 - loss: 1503.3514 - val_RMSE: 38.7201 - val_loss: 1502.2601 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7316 - loss: 1503.2905 - val_RMSE: 38.7225 - val_loss: 1502.4164 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7313 - loss: 1503.1978 - val_RMSE: 38.7182 - val_loss: 1501.9128 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7306 - loss: 1502.9703 - val_RMSE: 38.7212 - val_loss: 1502.1805 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7295 - loss: 1502.8280 - val_RMSE: 38.7195 - val_loss: 1501.8370 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7306 - loss: 1502.7605 - val_RMSE: 38.7213 - val_loss: 1501.8317 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7265 - loss: 1502.3130 - val_RMSE: 38.7208 - val_loss: 1501.7612 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7292 - loss: 1502.4355 - val_RMSE: 38.7230 - val_loss: 1501.8983 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7263 - loss: 1502.2406 - val_RMSE: 38.7213 - val_loss: 1501.7244 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7275 - loss: 1502.2611 - val_RMSE: 38.7168 - val_loss: 1501.2633 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7272 - loss: 1502.1406 - val_RMSE: 38.7202 - val_loss: 1501.5856 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7272 - loss: 1502.1337 - val_RMSE: 38.7190 - val_loss: 1501.3167 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7260 - loss: 1501.9783 - val_RMSE: 38.7203 - val_loss: 1501.3640 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7227 - loss: 1501.6710 - val_RMSE: 38.7188 - val_loss: 1501.1597 - learning_rate: 0.0010\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.3149 - loss: 2824.5806 - val_RMSE: 38.7822 - val_loss: 1522.3011 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9822 - loss: 1536.1847 - val_RMSE: 38.7577 - val_loss: 1515.6809 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8762 - loss: 1524.1930 - val_RMSE: 38.7271 - val_loss: 1510.6638 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8197 - loss: 1517.1710 - val_RMSE: 38.7419 - val_loss: 1509.5366 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8001 - loss: 1513.8381 - val_RMSE: 38.7225 - val_loss: 1507.0442 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7900 - loss: 1512.0044 - val_RMSE: 38.7213 - val_loss: 1505.7921 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7782 - loss: 1510.1273 - val_RMSE: 38.7158 - val_loss: 1504.5847 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7653 - loss: 1508.2162 - val_RMSE: 38.7130 - val_loss: 1503.3190 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7611 - loss: 1507.0109 - val_RMSE: 38.7166 - val_loss: 1503.0569 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7581 - loss: 1506.0563 - val_RMSE: 38.7155 - val_loss: 1502.3295 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7562 - loss: 1505.5942 - val_RMSE: 38.7131 - val_loss: 1502.0338 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7568 - loss: 1505.4513 - val_RMSE: 38.7114 - val_loss: 1501.6659 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7535 - loss: 1505.0535 - val_RMSE: 38.7111 - val_loss: 1501.5841 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7596 - loss: 1505.3881 - val_RMSE: 38.7163 - val_loss: 1502.1027 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7549 - loss: 1505.0312 - val_RMSE: 38.7110 - val_loss: 1501.3414 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7559 - loss: 1504.9349 - val_RMSE: 38.7133 - val_loss: 1501.6398 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7529 - loss: 1504.6545 - val_RMSE: 38.7095 - val_loss: 1501.4309 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7559 - loss: 1504.7786 - val_RMSE: 38.7117 - val_loss: 1501.1122 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7561 - loss: 1504.7273 - val_RMSE: 38.7122 - val_loss: 1501.1674 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7546 - loss: 1504.4834 - val_RMSE: 38.7121 - val_loss: 1501.1199 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7566 - loss: 1504.6086 - val_RMSE: 38.7148 - val_loss: 1501.3589 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7537 - loss: 1504.3334 - val_RMSE: 38.7112 - val_loss: 1500.9198 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7567 - loss: 1504.4890 - val_RMSE: 38.7107 - val_loss: 1500.9138 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7536 - loss: 1504.1897 - val_RMSE: 38.7084 - val_loss: 1500.6278 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7551 - loss: 1504.3167 - val_RMSE: 38.7082 - val_loss: 1500.4899 - learning_rate: 0.0010\n",
            "41608/41608  82s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 16:40:07,636] Trial 0 finished with value: 38.70439020792643 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'relu', 'reg': 0.03532288502832786, 'dropout_rate': 0.31396524201979953}. Best is trial 0 with value: 38.70439020792643.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.1264 - loss: 3079.4102 - val_RMSE: 38.7246 - val_loss: 1500.0896 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0518 - loss: 1525.5660 - val_RMSE: 38.7018 - val_loss: 1498.4287 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9228 - loss: 1515.6116 - val_RMSE: 38.6893 - val_loss: 1497.5673 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8847 - loss: 1512.7523 - val_RMSE: 38.6864 - val_loss: 1497.4307 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8597 - loss: 1510.8882 - val_RMSE: 38.6857 - val_loss: 1497.4148 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8535 - loss: 1510.4323 - val_RMSE: 38.6840 - val_loss: 1497.2799 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8469 - loss: 1509.9010 - val_RMSE: 38.6838 - val_loss: 1497.2241 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8435 - loss: 1509.5848 - val_RMSE: 38.6836 - val_loss: 1497.1499 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8411 - loss: 1509.3501 - val_RMSE: 38.6828 - val_loss: 1497.0570 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8424 - loss: 1509.4250 - val_RMSE: 38.6823 - val_loss: 1497.0051 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8404 - loss: 1509.2593 - val_RMSE: 38.6821 - val_loss: 1496.9814 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8350 - loss: 1508.8384 - val_RMSE: 38.6813 - val_loss: 1496.9292 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8436 - loss: 1509.5121 - val_RMSE: 38.6818 - val_loss: 1496.9677 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8348 - loss: 1508.8245 - val_RMSE: 38.6814 - val_loss: 1496.9409 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8363 - loss: 1508.9521 - val_RMSE: 38.6803 - val_loss: 1496.8596 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8300 - loss: 1508.4655 - val_RMSE: 38.6812 - val_loss: 1496.9338 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8321 - loss: 1508.6306 - val_RMSE: 38.6806 - val_loss: 1496.8925 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8286 - loss: 1508.3646 - val_RMSE: 38.6802 - val_loss: 1496.8665 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8292 - loss: 1508.4248 - val_RMSE: 38.6799 - val_loss: 1496.8623 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8313 - loss: 1508.6008 - val_RMSE: 38.6797 - val_loss: 1496.8492 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8260 - loss: 1508.1886 - val_RMSE: 38.6804 - val_loss: 1496.9110 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8287 - loss: 1508.4119 - val_RMSE: 38.6807 - val_loss: 1496.9382 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8261 - loss: 1508.2118 - val_RMSE: 38.6808 - val_loss: 1496.9559 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8216 - loss: 1507.8792 - val_RMSE: 38.6808 - val_loss: 1496.9628 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8245 - loss: 1508.1134 - val_RMSE: 38.6798 - val_loss: 1496.9000 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.0879 - loss: 3074.5679 - val_RMSE: 38.7391 - val_loss: 1501.2137 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 7ms/step - RMSE: 39.0029 - loss: 1521.7534 - val_RMSE: 38.7260 - val_loss: 1500.3019 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8782 - loss: 1512.1405 - val_RMSE: 38.7228 - val_loss: 1500.1617 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8270 - loss: 1508.2686 - val_RMSE: 38.7208 - val_loss: 1500.1039 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8100 - loss: 1507.0386 - val_RMSE: 38.7170 - val_loss: 1499.8690 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8010 - loss: 1506.3768 - val_RMSE: 38.7162 - val_loss: 1499.7908 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7953 - loss: 1505.9154 - val_RMSE: 38.7160 - val_loss: 1499.7277 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7952 - loss: 1505.8512 - val_RMSE: 38.7157 - val_loss: 1499.6515 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7937 - loss: 1505.6903 - val_RMSE: 38.7156 - val_loss: 1499.6105 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7909 - loss: 1505.4437 - val_RMSE: 38.7152 - val_loss: 1499.5591 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7871 - loss: 1505.1288 - val_RMSE: 38.7149 - val_loss: 1499.5374 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7876 - loss: 1505.1685 - val_RMSE: 38.7153 - val_loss: 1499.5674 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7842 - loss: 1504.9037 - val_RMSE: 38.7139 - val_loss: 1499.4578 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7823 - loss: 1504.7612 - val_RMSE: 38.7151 - val_loss: 1499.5588 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7884 - loss: 1505.2439 - val_RMSE: 38.7147 - val_loss: 1499.5333 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7809 - loss: 1504.6642 - val_RMSE: 38.7139 - val_loss: 1499.4816 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7882 - loss: 1505.2380 - val_RMSE: 38.7140 - val_loss: 1499.5017 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7826 - loss: 1504.8125 - val_RMSE: 38.7144 - val_loss: 1499.5380 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7756 - loss: 1504.2653 - val_RMSE: 38.7076 - val_loss: 1498.9706 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7759 - loss: 1504.2529 - val_RMSE: 38.7076 - val_loss: 1498.9368 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7686 - loss: 1503.6591 - val_RMSE: 38.7078 - val_loss: 1498.9263 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7749 - loss: 1504.1180 - val_RMSE: 38.7079 - val_loss: 1498.9117 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7652 - loss: 1503.3452 - val_RMSE: 38.7076 - val_loss: 1498.8704 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7685 - loss: 1503.5879 - val_RMSE: 38.7077 - val_loss: 1498.8667 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7652 - loss: 1503.3159 - val_RMSE: 38.7079 - val_loss: 1498.8663 - learning_rate: 1.0000e-04\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.0607 - loss: 3071.0916 - val_RMSE: 38.7247 - val_loss: 1500.0953 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0263 - loss: 1523.5748 - val_RMSE: 38.7151 - val_loss: 1499.4620 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8984 - loss: 1513.7181 - val_RMSE: 38.7100 - val_loss: 1499.1707 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8556 - loss: 1510.4905 - val_RMSE: 38.7084 - val_loss: 1499.1277 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8397 - loss: 1509.3270 - val_RMSE: 38.7072 - val_loss: 1499.0754 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8244 - loss: 1508.1620 - val_RMSE: 38.7072 - val_loss: 1499.0577 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8248 - loss: 1508.1646 - val_RMSE: 38.7059 - val_loss: 1498.9071 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8213 - loss: 1507.8408 - val_RMSE: 38.7053 - val_loss: 1498.8085 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8185 - loss: 1507.5780 - val_RMSE: 38.7059 - val_loss: 1498.8350 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8207 - loss: 1507.7319 - val_RMSE: 38.7045 - val_loss: 1498.7238 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8156 - loss: 1507.3353 - val_RMSE: 38.7049 - val_loss: 1498.7467 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8149 - loss: 1507.2789 - val_RMSE: 38.7043 - val_loss: 1498.7028 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8107 - loss: 1506.9546 - val_RMSE: 38.7050 - val_loss: 1498.7563 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8134 - loss: 1507.1660 - val_RMSE: 38.7049 - val_loss: 1498.7622 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8092 - loss: 1506.8434 - val_RMSE: 38.7037 - val_loss: 1498.6705 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8119 - loss: 1507.0581 - val_RMSE: 38.7040 - val_loss: 1498.7018 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8108 - loss: 1506.9803 - val_RMSE: 38.7032 - val_loss: 1498.6395 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8082 - loss: 1506.7808 - val_RMSE: 38.7036 - val_loss: 1498.6838 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8077 - loss: 1506.7535 - val_RMSE: 38.7032 - val_loss: 1498.6501 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8067 - loss: 1506.6863 - val_RMSE: 38.7030 - val_loss: 1498.6527 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7974 - loss: 1505.9706 - val_RMSE: 38.7024 - val_loss: 1498.6149 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8040 - loss: 1506.4856 - val_RMSE: 38.7032 - val_loss: 1498.6814 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8049 - loss: 1506.5691 - val_RMSE: 38.7030 - val_loss: 1498.6797 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8021 - loss: 1506.3671 - val_RMSE: 38.7018 - val_loss: 1498.5995 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8007 - loss: 1506.2587 - val_RMSE: 38.7027 - val_loss: 1498.6749 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 17:10:07,902] Trial 1 finished with value: 38.696816762288414 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00027561698595006656, 'dropout_rate': 0.4105017873627729}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  39s 12ms/step - RMSE: 51.4811 - loss: 2785.5352 - val_RMSE: 38.7200 - val_loss: 1501.3983 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 39.0050 - loss: 1523.6791 - val_RMSE: 38.7155 - val_loss: 1501.4891 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.8815 - loss: 1514.4375 - val_RMSE: 38.7056 - val_loss: 1500.8835 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8411 - loss: 1511.3909 - val_RMSE: 38.7018 - val_loss: 1500.5566 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8134 - loss: 1509.1803 - val_RMSE: 38.6988 - val_loss: 1500.1974 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8055 - loss: 1508.4283 - val_RMSE: 38.6927 - val_loss: 1499.5613 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7993 - loss: 1507.7716 - val_RMSE: 38.6914 - val_loss: 1499.2638 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7926 - loss: 1507.0441 - val_RMSE: 38.6929 - val_loss: 1499.1389 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7870 - loss: 1506.3828 - val_RMSE: 38.6878 - val_loss: 1498.5454 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7828 - loss: 1505.8733 - val_RMSE: 38.6885 - val_loss: 1498.4587 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7820 - loss: 1505.6763 - val_RMSE: 38.6873 - val_loss: 1498.2343 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7822 - loss: 1505.5815 - val_RMSE: 38.6873 - val_loss: 1498.1597 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7804 - loss: 1505.3665 - val_RMSE: 38.6853 - val_loss: 1497.9257 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7785 - loss: 1505.1427 - val_RMSE: 38.6838 - val_loss: 1497.7542 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7766 - loss: 1504.9423 - val_RMSE: 38.6827 - val_loss: 1497.6527 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7774 - loss: 1504.9865 - val_RMSE: 38.6820 - val_loss: 1497.5724 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7744 - loss: 1504.7397 - val_RMSE: 38.6823 - val_loss: 1497.5912 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7753 - loss: 1504.8101 - val_RMSE: 38.6818 - val_loss: 1497.5594 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7720 - loss: 1504.5520 - val_RMSE: 38.6817 - val_loss: 1497.5492 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7737 - loss: 1504.6752 - val_RMSE: 38.6824 - val_loss: 1497.6013 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7715 - loss: 1504.5157 - val_RMSE: 38.6819 - val_loss: 1497.5652 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7703 - loss: 1504.4159 - val_RMSE: 38.6815 - val_loss: 1497.5253 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7748 - loss: 1504.7688 - val_RMSE: 38.6811 - val_loss: 1497.5056 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7722 - loss: 1504.5754 - val_RMSE: 38.6815 - val_loss: 1497.5227 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7673 - loss: 1504.1896 - val_RMSE: 38.6817 - val_loss: 1497.5555 - learning_rate: 0.0010\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4232 - loss: 2778.6873 - val_RMSE: 38.7358 - val_loss: 1502.5631 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9488 - loss: 1519.2096 - val_RMSE: 38.7286 - val_loss: 1502.3649 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8307 - loss: 1510.3438 - val_RMSE: 38.7201 - val_loss: 1501.8761 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7850 - loss: 1506.9270 - val_RMSE: 38.7249 - val_loss: 1502.3204 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7640 - loss: 1505.3335 - val_RMSE: 38.7204 - val_loss: 1501.8672 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7538 - loss: 1504.4165 - val_RMSE: 38.7161 - val_loss: 1501.3892 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7451 - loss: 1503.5719 - val_RMSE: 38.7156 - val_loss: 1501.1223 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7413 - loss: 1503.0411 - val_RMSE: 38.7135 - val_loss: 1500.7371 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7359 - loss: 1502.4141 - val_RMSE: 38.7136 - val_loss: 1500.5466 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7355 - loss: 1502.1884 - val_RMSE: 38.7144 - val_loss: 1500.4609 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7333 - loss: 1501.8695 - val_RMSE: 38.7143 - val_loss: 1500.2827 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7309 - loss: 1501.5409 - val_RMSE: 38.7140 - val_loss: 1500.1484 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7289 - loss: 1501.2842 - val_RMSE: 38.7154 - val_loss: 1500.1740 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7294 - loss: 1501.2535 - val_RMSE: 38.7144 - val_loss: 1500.0696 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7274 - loss: 1501.0635 - val_RMSE: 38.7159 - val_loss: 1500.1901 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7267 - loss: 1501.0211 - val_RMSE: 38.7144 - val_loss: 1500.0526 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7277 - loss: 1501.0828 - val_RMSE: 38.7163 - val_loss: 1500.2013 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7225 - loss: 1500.6899 - val_RMSE: 38.7157 - val_loss: 1500.1570 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7254 - loss: 1500.9139 - val_RMSE: 38.7153 - val_loss: 1500.1416 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7226 - loss: 1500.6957 - val_RMSE: 38.7161 - val_loss: 1500.1908 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7224 - loss: 1500.6945 - val_RMSE: 38.7145 - val_loss: 1500.0940 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7145 - loss: 1500.0428 - val_RMSE: 38.7088 - val_loss: 1499.4592 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7091 - loss: 1499.4463 - val_RMSE: 38.7091 - val_loss: 1499.3541 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7079 - loss: 1499.2405 - val_RMSE: 38.7091 - val_loss: 1499.2749 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7049 - loss: 1498.9391 - val_RMSE: 38.7090 - val_loss: 1499.2125 - learning_rate: 1.0000e-04\n",
            "41608/41608  82s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4282 - loss: 2778.9778 - val_RMSE: 38.7325 - val_loss: 1502.2775 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9723 - loss: 1521.0208 - val_RMSE: 38.7222 - val_loss: 1501.8488 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8661 - loss: 1513.0850 - val_RMSE: 38.7160 - val_loss: 1501.5682 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8211 - loss: 1509.7300 - val_RMSE: 38.7168 - val_loss: 1501.6185 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7951 - loss: 1507.6799 - val_RMSE: 38.7103 - val_loss: 1501.0483 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7825 - loss: 1506.6033 - val_RMSE: 38.7076 - val_loss: 1500.6799 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7775 - loss: 1506.0386 - val_RMSE: 38.7064 - val_loss: 1500.3843 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7653 - loss: 1504.9027 - val_RMSE: 38.7060 - val_loss: 1500.1735 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7628 - loss: 1504.5100 - val_RMSE: 38.7045 - val_loss: 1499.8124 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7596 - loss: 1504.0413 - val_RMSE: 38.7045 - val_loss: 1499.6505 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7552 - loss: 1503.5354 - val_RMSE: 38.7041 - val_loss: 1499.4872 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7555 - loss: 1503.4480 - val_RMSE: 38.7037 - val_loss: 1499.3448 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7506 - loss: 1502.9628 - val_RMSE: 38.7035 - val_loss: 1499.2722 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7563 - loss: 1503.3506 - val_RMSE: 38.7031 - val_loss: 1499.1765 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7515 - loss: 1502.9250 - val_RMSE: 38.7031 - val_loss: 1499.1622 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7523 - loss: 1502.9747 - val_RMSE: 38.7021 - val_loss: 1499.0925 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7504 - loss: 1502.8300 - val_RMSE: 38.7021 - val_loss: 1499.0677 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7518 - loss: 1502.9227 - val_RMSE: 38.7015 - val_loss: 1499.0411 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7540 - loss: 1503.1014 - val_RMSE: 38.7025 - val_loss: 1499.1080 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7495 - loss: 1502.7600 - val_RMSE: 38.7018 - val_loss: 1499.0720 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7519 - loss: 1502.9512 - val_RMSE: 38.7024 - val_loss: 1499.1102 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7488 - loss: 1502.7002 - val_RMSE: 38.7018 - val_loss: 1499.0699 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7511 - loss: 1502.8854 - val_RMSE: 38.7018 - val_loss: 1499.0673 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7425 - loss: 1502.1704 - val_RMSE: 38.7008 - val_loss: 1498.8035 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7384 - loss: 1501.6848 - val_RMSE: 38.7008 - val_loss: 1498.6813 - learning_rate: 1.0000e-04\n",
            "41608/41608  83s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 17:46:47,489] Trial 2 finished with value: 38.697156270345054 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0005402789192665301, 'dropout_rate': 0.36286329232617837}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  28s 7ms/step - RMSE: 61.1220 - loss: 3901.1499 - val_RMSE: 38.7016 - val_loss: 1498.0620 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.4299 - loss: 1554.9777 - val_RMSE: 38.6945 - val_loss: 1497.5388 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.3545 - loss: 1549.0581 - val_RMSE: 38.6946 - val_loss: 1497.5609 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3389 - loss: 1547.8391 - val_RMSE: 38.6933 - val_loss: 1497.4711 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3170 - loss: 1546.1323 - val_RMSE: 38.6943 - val_loss: 1497.5634 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3075 - loss: 1545.3971 - val_RMSE: 38.6924 - val_loss: 1497.4204 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2967 - loss: 1544.5590 - val_RMSE: 38.6915 - val_loss: 1497.3578 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2819 - loss: 1543.3956 - val_RMSE: 38.6921 - val_loss: 1497.4036 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2615 - loss: 1541.7911 - val_RMSE: 38.6909 - val_loss: 1497.3091 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2421 - loss: 1540.2733 - val_RMSE: 38.6904 - val_loss: 1497.2765 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2259 - loss: 1538.9979 - val_RMSE: 38.6894 - val_loss: 1497.1974 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2156 - loss: 1538.1879 - val_RMSE: 38.6893 - val_loss: 1497.1865 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2061 - loss: 1537.4473 - val_RMSE: 38.6895 - val_loss: 1497.1979 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1827 - loss: 1535.6063 - val_RMSE: 38.6887 - val_loss: 1497.1351 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1827 - loss: 1535.6050 - val_RMSE: 38.6882 - val_loss: 1497.0979 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1665 - loss: 1534.3417 - val_RMSE: 38.6879 - val_loss: 1497.0800 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1591 - loss: 1533.7563 - val_RMSE: 38.6883 - val_loss: 1497.1042 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1412 - loss: 1532.3553 - val_RMSE: 38.6870 - val_loss: 1497.0095 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1302 - loss: 1531.4967 - val_RMSE: 38.6870 - val_loss: 1497.0079 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1210 - loss: 1530.7767 - val_RMSE: 38.6866 - val_loss: 1496.9836 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1172 - loss: 1530.4889 - val_RMSE: 38.6873 - val_loss: 1497.0319 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0976 - loss: 1528.9534 - val_RMSE: 38.6856 - val_loss: 1496.9064 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0885 - loss: 1528.2468 - val_RMSE: 38.6866 - val_loss: 1496.9849 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0750 - loss: 1527.1884 - val_RMSE: 38.6865 - val_loss: 1496.9753 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.0714 - loss: 1526.9084 - val_RMSE: 38.6863 - val_loss: 1496.9641 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  26s 7ms/step - RMSE: 61.1121 - loss: 3899.5061 - val_RMSE: 38.7294 - val_loss: 1500.2212 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3797 - loss: 1551.0195 - val_RMSE: 38.7242 - val_loss: 1499.8402 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3071 - loss: 1545.3280 - val_RMSE: 38.7235 - val_loss: 1499.7985 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2922 - loss: 1544.1672 - val_RMSE: 38.7232 - val_loss: 1499.7891 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2675 - loss: 1542.2424 - val_RMSE: 38.7241 - val_loss: 1499.8722 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2493 - loss: 1540.8242 - val_RMSE: 38.7234 - val_loss: 1499.8176 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2371 - loss: 1539.8723 - val_RMSE: 38.7225 - val_loss: 1499.7538 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2248 - loss: 1538.9126 - val_RMSE: 38.7214 - val_loss: 1499.6703 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2051 - loss: 1537.3649 - val_RMSE: 38.7217 - val_loss: 1499.6891 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1998 - loss: 1536.9500 - val_RMSE: 38.7198 - val_loss: 1499.5421 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1839 - loss: 1535.6959 - val_RMSE: 38.7200 - val_loss: 1499.5596 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1752 - loss: 1535.0199 - val_RMSE: 38.7201 - val_loss: 1499.5636 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1506 - loss: 1533.0902 - val_RMSE: 38.7186 - val_loss: 1499.4491 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1460 - loss: 1532.7303 - val_RMSE: 38.7186 - val_loss: 1499.4446 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1252 - loss: 1531.0986 - val_RMSE: 38.7181 - val_loss: 1499.4125 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1251 - loss: 1531.0919 - val_RMSE: 38.7181 - val_loss: 1499.4121 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1055 - loss: 1529.5645 - val_RMSE: 38.7177 - val_loss: 1499.3754 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1024 - loss: 1529.3180 - val_RMSE: 38.7171 - val_loss: 1499.3369 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0829 - loss: 1527.7985 - val_RMSE: 38.7170 - val_loss: 1499.3302 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0718 - loss: 1526.9307 - val_RMSE: 38.7177 - val_loss: 1499.3884 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0634 - loss: 1526.2772 - val_RMSE: 38.7171 - val_loss: 1499.3406 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0478 - loss: 1525.0576 - val_RMSE: 38.7162 - val_loss: 1499.2717 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0444 - loss: 1524.7906 - val_RMSE: 38.7165 - val_loss: 1499.2999 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0418 - loss: 1524.5929 - val_RMSE: 38.7172 - val_loss: 1499.3510 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0247 - loss: 1523.2585 - val_RMSE: 38.7163 - val_loss: 1499.2823 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  28s 7ms/step - RMSE: 61.0899 - loss: 3896.7007 - val_RMSE: 38.7189 - val_loss: 1499.4044 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3946 - loss: 1552.1942 - val_RMSE: 38.7210 - val_loss: 1499.5898 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3287 - loss: 1547.0219 - val_RMSE: 38.7218 - val_loss: 1499.6602 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3118 - loss: 1545.7069 - val_RMSE: 38.7199 - val_loss: 1499.5288 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2914 - loss: 1544.1165 - val_RMSE: 38.7237 - val_loss: 1499.8373 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2798 - loss: 1543.2173 - val_RMSE: 38.7212 - val_loss: 1499.6534 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2628 - loss: 1541.8882 - val_RMSE: 38.7151 - val_loss: 1499.1798 - learning_rate: 1.0000e-04\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2652 - loss: 1542.0763 - val_RMSE: 38.7151 - val_loss: 1499.1746 - learning_rate: 1.0000e-04\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2605 - loss: 1541.6981 - val_RMSE: 38.7142 - val_loss: 1499.0953 - learning_rate: 1.0000e-04\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2621 - loss: 1541.8195 - val_RMSE: 38.7142 - val_loss: 1499.0936 - learning_rate: 1.0000e-04\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2583 - loss: 1541.5154 - val_RMSE: 38.7140 - val_loss: 1499.0720 - learning_rate: 1.0000e-04\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2597 - loss: 1541.6219 - val_RMSE: 38.7138 - val_loss: 1499.0522 - learning_rate: 1.0000e-04\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.2608 - loss: 1541.7036 - val_RMSE: 38.7140 - val_loss: 1499.0601 - learning_rate: 1.0000e-04\n",
            "Epoch 14/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.2528 - loss: 1541.0702 - val_RMSE: 38.7140 - val_loss: 1499.0582 - learning_rate: 1.0000e-04\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2437 - loss: 1540.3527 - val_RMSE: 38.7136 - val_loss: 1499.0215 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2506 - loss: 1540.8850 - val_RMSE: 38.7131 - val_loss: 1498.9785 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2565 - loss: 1541.3456 - val_RMSE: 38.7131 - val_loss: 1498.9720 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2475 - loss: 1540.6411 - val_RMSE: 38.7129 - val_loss: 1498.9581 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2401 - loss: 1540.0510 - val_RMSE: 38.7133 - val_loss: 1498.9821 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2514 - loss: 1540.9315 - val_RMSE: 38.7132 - val_loss: 1498.9702 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2488 - loss: 1540.7273 - val_RMSE: 38.7132 - val_loss: 1498.9698 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.2469 - loss: 1540.5726 - val_RMSE: 38.7129 - val_loss: 1498.9382 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.2395 - loss: 1539.9875 - val_RMSE: 38.7130 - val_loss: 1498.9432 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2509 - loss: 1540.8840 - val_RMSE: 38.7126 - val_loss: 1498.9132 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2343 - loss: 1539.5750 - val_RMSE: 38.7129 - val_loss: 1498.9302 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 18:10:55,657] Trial 3 finished with value: 38.70516586303711 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'selu', 'reg': 0.0006168819899394228, 'dropout_rate': 0.48350841958817187}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 60.7275 - loss: 3860.7412 - val_RMSE: 38.7025 - val_loss: 1503.1106 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1660 - loss: 1538.8193 - val_RMSE: 38.6939 - val_loss: 1500.7462 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0876 - loss: 1530.9313 - val_RMSE: 38.6927 - val_loss: 1499.0325 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0727 - loss: 1528.3319 - val_RMSE: 38.6935 - val_loss: 1498.2562 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0608 - loss: 1526.7576 - val_RMSE: 38.6941 - val_loss: 1498.1229 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0604 - loss: 1526.5984 - val_RMSE: 38.6947 - val_loss: 1498.0813 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0617 - loss: 1526.6384 - val_RMSE: 38.6930 - val_loss: 1497.9167 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0472 - loss: 1525.4810 - val_RMSE: 38.6966 - val_loss: 1498.2043 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0436 - loss: 1525.1986 - val_RMSE: 38.6924 - val_loss: 1497.8715 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0335 - loss: 1524.4050 - val_RMSE: 38.6930 - val_loss: 1497.9349 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0296 - loss: 1524.1052 - val_RMSE: 38.6930 - val_loss: 1497.9077 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0229 - loss: 1523.5594 - val_RMSE: 38.6927 - val_loss: 1497.9104 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0233 - loss: 1523.6105 - val_RMSE: 38.6926 - val_loss: 1497.8959 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0091 - loss: 1522.5095 - val_RMSE: 38.6918 - val_loss: 1497.8387 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0185 - loss: 1523.2445 - val_RMSE: 38.6916 - val_loss: 1497.8258 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0086 - loss: 1522.4824 - val_RMSE: 38.6933 - val_loss: 1497.9510 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0157 - loss: 1523.0261 - val_RMSE: 38.6940 - val_loss: 1498.0188 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9957 - loss: 1521.4740 - val_RMSE: 38.6899 - val_loss: 1497.7015 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9942 - loss: 1521.3615 - val_RMSE: 38.6917 - val_loss: 1497.8342 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9895 - loss: 1520.9830 - val_RMSE: 38.6923 - val_loss: 1497.9017 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9922 - loss: 1521.2125 - val_RMSE: 38.6909 - val_loss: 1497.7697 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9860 - loss: 1520.7096 - val_RMSE: 38.6904 - val_loss: 1497.7242 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9783 - loss: 1520.1158 - val_RMSE: 38.6915 - val_loss: 1497.8085 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9729 - loss: 1519.6266 - val_RMSE: 38.6862 - val_loss: 1497.2625 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9711 - loss: 1519.3628 - val_RMSE: 38.6859 - val_loss: 1497.1476 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 60.7271 - loss: 3860.0928 - val_RMSE: 38.7317 - val_loss: 1505.3287 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1190 - loss: 1535.1005 - val_RMSE: 38.7229 - val_loss: 1502.9666 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0407 - loss: 1527.2592 - val_RMSE: 38.7253 - val_loss: 1501.5563 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0226 - loss: 1524.4153 - val_RMSE: 38.7230 - val_loss: 1500.5271 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0112 - loss: 1522.8704 - val_RMSE: 38.7265 - val_loss: 1500.5741 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0022 - loss: 1522.0120 - val_RMSE: 38.7238 - val_loss: 1500.3422 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9977 - loss: 1521.6361 - val_RMSE: 38.7243 - val_loss: 1500.3431 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9938 - loss: 1521.3024 - val_RMSE: 38.7228 - val_loss: 1500.2301 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9847 - loss: 1520.5884 - val_RMSE: 38.7241 - val_loss: 1500.3109 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9884 - loss: 1520.8776 - val_RMSE: 38.7239 - val_loss: 1500.3004 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9819 - loss: 1520.3705 - val_RMSE: 38.7220 - val_loss: 1500.1422 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9818 - loss: 1520.3409 - val_RMSE: 38.7224 - val_loss: 1500.1669 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9704 - loss: 1519.4506 - val_RMSE: 38.7217 - val_loss: 1500.1550 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9708 - loss: 1519.5157 - val_RMSE: 38.7195 - val_loss: 1499.9707 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9610 - loss: 1518.7417 - val_RMSE: 38.7219 - val_loss: 1500.1582 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9624 - loss: 1518.8473 - val_RMSE: 38.7227 - val_loss: 1500.2067 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9570 - loss: 1518.4193 - val_RMSE: 38.7220 - val_loss: 1500.1650 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9550 - loss: 1518.2831 - val_RMSE: 38.7208 - val_loss: 1500.0781 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9483 - loss: 1517.7571 - val_RMSE: 38.7223 - val_loss: 1500.2198 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9406 - loss: 1517.1356 - val_RMSE: 38.7152 - val_loss: 1499.5250 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.9350 - loss: 1516.5638 - val_RMSE: 38.7150 - val_loss: 1499.4087 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9327 - loss: 1516.2960 - val_RMSE: 38.7147 - val_loss: 1499.3195 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9325 - loss: 1516.2141 - val_RMSE: 38.7146 - val_loss: 1499.2625 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9412 - loss: 1516.8430 - val_RMSE: 38.7141 - val_loss: 1499.1801 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9320 - loss: 1516.0990 - val_RMSE: 38.7147 - val_loss: 1499.2025 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 60.7024 - loss: 3856.9219 - val_RMSE: 38.7191 - val_loss: 1504.3219 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1440 - loss: 1537.0243 - val_RMSE: 38.7173 - val_loss: 1502.4677 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0631 - loss: 1528.9379 - val_RMSE: 38.7180 - val_loss: 1500.9121 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0463 - loss: 1526.1890 - val_RMSE: 38.7190 - val_loss: 1500.1782 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0397 - loss: 1525.0461 - val_RMSE: 38.7199 - val_loss: 1500.0298 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0356 - loss: 1524.5862 - val_RMSE: 38.7168 - val_loss: 1499.7716 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0341 - loss: 1524.4611 - val_RMSE: 38.7176 - val_loss: 1499.8107 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0218 - loss: 1523.4763 - val_RMSE: 38.7176 - val_loss: 1499.7853 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.0164 - loss: 1523.0387 - val_RMSE: 38.7149 - val_loss: 1499.5848 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0189 - loss: 1523.2378 - val_RMSE: 38.7157 - val_loss: 1499.6538 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0105 - loss: 1522.5756 - val_RMSE: 38.7169 - val_loss: 1499.7473 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0035 - loss: 1522.0476 - val_RMSE: 38.7141 - val_loss: 1499.5364 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0007 - loss: 1521.8361 - val_RMSE: 38.7135 - val_loss: 1499.4797 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9916 - loss: 1521.1102 - val_RMSE: 38.7138 - val_loss: 1499.5151 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9875 - loss: 1520.7906 - val_RMSE: 38.7129 - val_loss: 1499.4524 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.9831 - loss: 1520.4603 - val_RMSE: 38.7127 - val_loss: 1499.4526 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 6ms/step - RMSE: 38.9856 - loss: 1520.6663 - val_RMSE: 38.7125 - val_loss: 1499.4307 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9780 - loss: 1520.0653 - val_RMSE: 38.7113 - val_loss: 1499.3521 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9685 - loss: 1519.3345 - val_RMSE: 38.7128 - val_loss: 1499.4357 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9715 - loss: 1519.5533 - val_RMSE: 38.7142 - val_loss: 1499.5631 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9706 - loss: 1519.5002 - val_RMSE: 38.7133 - val_loss: 1499.4885 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9655 - loss: 1519.0929 - val_RMSE: 38.7113 - val_loss: 1499.3341 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9578 - loss: 1518.5057 - val_RMSE: 38.7128 - val_loss: 1499.4435 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9586 - loss: 1518.5446 - val_RMSE: 38.7120 - val_loss: 1499.4027 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 38.9454 - loss: 1517.5496 - val_RMSE: 38.7118 - val_loss: 1499.3850 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 18:35:07,326] Trial 4 finished with value: 38.70415115356445 and parameters: {'units': 128, 'last_layer': 1, 'activation': 'selu', 'reg': 0.019199229166119592, 'dropout_rate': 0.3341626635520698}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  29s 8ms/step - RMSE: 57.1019 - loss: 3422.7141 - val_RMSE: 38.7026 - val_loss: 1498.1483 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0765 - loss: 1527.2397 - val_RMSE: 38.6906 - val_loss: 1497.2533 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9670 - loss: 1518.7303 - val_RMSE: 38.6889 - val_loss: 1497.1610 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9329 - loss: 1516.1066 - val_RMSE: 38.6860 - val_loss: 1496.9572 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9313 - loss: 1516.0071 - val_RMSE: 38.6864 - val_loss: 1497.0135 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9215 - loss: 1515.2563 - val_RMSE: 38.6849 - val_loss: 1496.9028 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9178 - loss: 1514.9745 - val_RMSE: 38.6852 - val_loss: 1496.9235 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9195 - loss: 1515.1115 - val_RMSE: 38.6845 - val_loss: 1496.8754 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9177 - loss: 1514.9722 - val_RMSE: 38.6844 - val_loss: 1496.8666 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9107 - loss: 1514.4308 - val_RMSE: 38.6840 - val_loss: 1496.8417 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9166 - loss: 1514.8933 - val_RMSE: 38.6827 - val_loss: 1496.7443 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9068 - loss: 1514.1381 - val_RMSE: 38.6840 - val_loss: 1496.8505 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9096 - loss: 1514.3577 - val_RMSE: 38.6829 - val_loss: 1496.7690 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9073 - loss: 1514.1802 - val_RMSE: 38.6824 - val_loss: 1496.7325 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8974 - loss: 1513.4181 - val_RMSE: 38.6822 - val_loss: 1496.7258 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8920 - loss: 1512.9998 - val_RMSE: 38.6818 - val_loss: 1496.6971 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8913 - loss: 1512.9496 - val_RMSE: 38.6827 - val_loss: 1496.7736 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8928 - loss: 1513.0688 - val_RMSE: 38.6816 - val_loss: 1496.6895 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8956 - loss: 1513.2979 - val_RMSE: 38.6812 - val_loss: 1496.6664 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8945 - loss: 1513.2184 - val_RMSE: 38.6820 - val_loss: 1496.7319 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8896 - loss: 1512.8368 - val_RMSE: 38.6813 - val_loss: 1496.6841 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8804 - loss: 1512.1338 - val_RMSE: 38.6815 - val_loss: 1496.7079 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8909 - loss: 1512.9537 - val_RMSE: 38.6813 - val_loss: 1496.7025 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8837 - loss: 1512.4005 - val_RMSE: 38.6808 - val_loss: 1496.6716 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8773 - loss: 1511.9095 - val_RMSE: 38.6795 - val_loss: 1496.5598 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  28s 8ms/step - RMSE: 57.0798 - loss: 3419.6045 - val_RMSE: 38.7378 - val_loss: 1500.8750 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0223 - loss: 1523.0117 - val_RMSE: 38.7196 - val_loss: 1499.5078 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9216 - loss: 1515.1971 - val_RMSE: 38.7167 - val_loss: 1499.3125 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8893 - loss: 1512.7181 - val_RMSE: 38.7177 - val_loss: 1499.4166 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8841 - loss: 1512.3414 - val_RMSE: 38.7161 - val_loss: 1499.3121 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8756 - loss: 1511.6926 - val_RMSE: 38.7160 - val_loss: 1499.3086 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8671 - loss: 1511.0376 - val_RMSE: 38.7160 - val_loss: 1499.3159 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8607 - loss: 1510.5386 - val_RMSE: 38.7155 - val_loss: 1499.2704 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8628 - loss: 1510.6973 - val_RMSE: 38.7143 - val_loss: 1499.1808 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8610 - loss: 1510.5599 - val_RMSE: 38.7147 - val_loss: 1499.2192 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8595 - loss: 1510.4478 - val_RMSE: 38.7150 - val_loss: 1499.2386 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8596 - loss: 1510.4637 - val_RMSE: 38.7143 - val_loss: 1499.1863 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8578 - loss: 1510.3207 - val_RMSE: 38.7141 - val_loss: 1499.1774 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8555 - loss: 1510.1471 - val_RMSE: 38.7145 - val_loss: 1499.2074 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8551 - loss: 1510.1213 - val_RMSE: 38.7144 - val_loss: 1499.2085 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8446 - loss: 1509.3044 - val_RMSE: 38.7138 - val_loss: 1499.1660 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8508 - loss: 1509.7930 - val_RMSE: 38.7140 - val_loss: 1499.1906 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8441 - loss: 1509.2769 - val_RMSE: 38.7136 - val_loss: 1499.1621 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8407 - loss: 1509.0232 - val_RMSE: 38.7133 - val_loss: 1499.1454 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8394 - loss: 1508.9252 - val_RMSE: 38.7122 - val_loss: 1499.0658 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8354 - loss: 1508.6171 - val_RMSE: 38.7126 - val_loss: 1499.1012 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8336 - loss: 1508.4851 - val_RMSE: 38.7137 - val_loss: 1499.1908 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8339 - loss: 1508.5144 - val_RMSE: 38.7134 - val_loss: 1499.1732 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8360 - loss: 1508.6816 - val_RMSE: 38.7133 - val_loss: 1499.1696 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8341 - loss: 1508.5431 - val_RMSE: 38.7138 - val_loss: 1499.2173 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  30s 8ms/step - RMSE: 57.0575 - loss: 3416.6711 - val_RMSE: 38.7173 - val_loss: 1499.2837 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0492 - loss: 1525.1086 - val_RMSE: 38.7125 - val_loss: 1498.9554 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9459 - loss: 1517.0900 - val_RMSE: 38.7113 - val_loss: 1498.8917 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9137 - loss: 1514.6139 - val_RMSE: 38.7098 - val_loss: 1498.8010 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9009 - loss: 1513.6399 - val_RMSE: 38.7103 - val_loss: 1498.8569 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9018 - loss: 1513.7206 - val_RMSE: 38.7085 - val_loss: 1498.7189 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8966 - loss: 1513.3174 - val_RMSE: 38.7087 - val_loss: 1498.7360 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8899 - loss: 1512.7970 - val_RMSE: 38.7084 - val_loss: 1498.7219 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8896 - loss: 1512.7843 - val_RMSE: 38.7071 - val_loss: 1498.6261 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8919 - loss: 1512.9684 - val_RMSE: 38.7069 - val_loss: 1498.6093 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8831 - loss: 1512.2891 - val_RMSE: 38.7073 - val_loss: 1498.6445 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8795 - loss: 1512.0112 - val_RMSE: 38.7059 - val_loss: 1498.5452 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8859 - loss: 1512.5170 - val_RMSE: 38.7068 - val_loss: 1498.6179 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8774 - loss: 1511.8562 - val_RMSE: 38.7053 - val_loss: 1498.5110 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8771 - loss: 1511.8365 - val_RMSE: 38.7070 - val_loss: 1498.6456 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8774 - loss: 1511.8636 - val_RMSE: 38.7069 - val_loss: 1498.6376 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8718 - loss: 1511.4375 - val_RMSE: 38.7057 - val_loss: 1498.5535 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8679 - loss: 1511.1353 - val_RMSE: 38.7062 - val_loss: 1498.5947 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8715 - loss: 1511.4213 - val_RMSE: 38.7051 - val_loss: 1498.5200 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8597 - loss: 1510.5067 - val_RMSE: 38.7030 - val_loss: 1498.3457 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8619 - loss: 1510.6716 - val_RMSE: 38.7028 - val_loss: 1498.3179 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8558 - loss: 1510.1830 - val_RMSE: 38.7027 - val_loss: 1498.3052 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8602 - loss: 1510.5157 - val_RMSE: 38.7024 - val_loss: 1498.2743 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8590 - loss: 1510.4141 - val_RMSE: 38.7024 - val_loss: 1498.2650 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8598 - loss: 1510.4696 - val_RMSE: 38.7024 - val_loss: 1498.2576 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 19:01:01,290] Trial 5 finished with value: 38.698568979899086 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0003050914095911242, 'dropout_rate': 0.3691098556240708}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 53.8575 - loss: 3059.3425 - val_RMSE: 38.7152 - val_loss: 1507.5746 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9927 - loss: 1528.3422 - val_RMSE: 38.7115 - val_loss: 1504.0999 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8892 - loss: 1517.3347 - val_RMSE: 38.6953 - val_loss: 1500.9766 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8553 - loss: 1513.1832 - val_RMSE: 38.6935 - val_loss: 1500.1469 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8347 - loss: 1510.9808 - val_RMSE: 38.6920 - val_loss: 1499.5760 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8232 - loss: 1509.6844 - val_RMSE: 38.6922 - val_loss: 1499.2373 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8176 - loss: 1508.8828 - val_RMSE: 38.6911 - val_loss: 1498.8298 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8125 - loss: 1508.2145 - val_RMSE: 38.6886 - val_loss: 1498.3792 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8081 - loss: 1507.6248 - val_RMSE: 38.6897 - val_loss: 1498.3115 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8099 - loss: 1507.6558 - val_RMSE: 38.6882 - val_loss: 1498.1571 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8085 - loss: 1507.5332 - val_RMSE: 38.6884 - val_loss: 1498.1942 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8061 - loss: 1507.3561 - val_RMSE: 38.6905 - val_loss: 1498.3774 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8115 - loss: 1507.7490 - val_RMSE: 38.6881 - val_loss: 1498.1760 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8065 - loss: 1507.3190 - val_RMSE: 38.6868 - val_loss: 1497.9678 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8087 - loss: 1507.4465 - val_RMSE: 38.6867 - val_loss: 1497.9766 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8052 - loss: 1507.1985 - val_RMSE: 38.6868 - val_loss: 1497.9678 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8047 - loss: 1507.1356 - val_RMSE: 38.6862 - val_loss: 1497.9454 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8029 - loss: 1507.0101 - val_RMSE: 38.6856 - val_loss: 1497.8439 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8067 - loss: 1507.2838 - val_RMSE: 38.6853 - val_loss: 1497.8370 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8067 - loss: 1507.2637 - val_RMSE: 38.6855 - val_loss: 1497.8129 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8023 - loss: 1506.9001 - val_RMSE: 38.6869 - val_loss: 1497.9243 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8065 - loss: 1507.2102 - val_RMSE: 38.6866 - val_loss: 1497.8113 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8034 - loss: 1506.9292 - val_RMSE: 38.6845 - val_loss: 1497.6653 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8026 - loss: 1506.8529 - val_RMSE: 38.6852 - val_loss: 1497.7114 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8042 - loss: 1506.9939 - val_RMSE: 38.6852 - val_loss: 1497.6940 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 53.8064 - loss: 3052.9604 - val_RMSE: 38.7544 - val_loss: 1510.3811 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9437 - loss: 1524.3280 - val_RMSE: 38.7247 - val_loss: 1504.9838 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8440 - loss: 1513.6884 - val_RMSE: 38.7227 - val_loss: 1503.0750 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8007 - loss: 1508.8926 - val_RMSE: 38.7240 - val_loss: 1502.5308 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7808 - loss: 1506.8301 - val_RMSE: 38.7199 - val_loss: 1501.8140 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7740 - loss: 1505.9149 - val_RMSE: 38.7200 - val_loss: 1501.4786 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7600 - loss: 1504.4626 - val_RMSE: 38.7191 - val_loss: 1500.9484 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7632 - loss: 1504.3308 - val_RMSE: 38.7216 - val_loss: 1500.9006 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7595 - loss: 1503.8173 - val_RMSE: 38.7205 - val_loss: 1500.6738 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7589 - loss: 1503.6686 - val_RMSE: 38.7216 - val_loss: 1500.7496 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7594 - loss: 1503.6960 - val_RMSE: 38.7215 - val_loss: 1500.7327 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7553 - loss: 1503.3489 - val_RMSE: 38.7217 - val_loss: 1500.7219 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7560 - loss: 1503.3950 - val_RMSE: 38.7235 - val_loss: 1500.8422 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7553 - loss: 1503.3322 - val_RMSE: 38.7235 - val_loss: 1500.8344 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7506 - loss: 1502.7841 - val_RMSE: 38.7132 - val_loss: 1499.5673 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7451 - loss: 1501.9807 - val_RMSE: 38.7126 - val_loss: 1499.3402 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7482 - loss: 1502.0668 - val_RMSE: 38.7125 - val_loss: 1499.2322 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7474 - loss: 1501.9209 - val_RMSE: 38.7125 - val_loss: 1499.1783 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7433 - loss: 1501.5502 - val_RMSE: 38.7120 - val_loss: 1499.1029 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7472 - loss: 1501.8118 - val_RMSE: 38.7122 - val_loss: 1499.0840 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7429 - loss: 1501.4557 - val_RMSE: 38.7126 - val_loss: 1499.0936 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7471 - loss: 1501.7670 - val_RMSE: 38.7117 - val_loss: 1499.0170 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7415 - loss: 1501.3187 - val_RMSE: 38.7115 - val_loss: 1498.9858 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7465 - loss: 1501.6902 - val_RMSE: 38.7115 - val_loss: 1498.9705 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7439 - loss: 1501.4828 - val_RMSE: 38.7117 - val_loss: 1498.9807 - learning_rate: 1.0000e-04\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 53.7987 - loss: 3051.8386 - val_RMSE: 38.7279 - val_loss: 1508.4514 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9722 - loss: 1526.7030 - val_RMSE: 38.7203 - val_loss: 1504.9979 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8738 - loss: 1516.3914 - val_RMSE: 38.7164 - val_loss: 1502.9056 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8325 - loss: 1511.6307 - val_RMSE: 38.7183 - val_loss: 1502.1237 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8120 - loss: 1509.2515 - val_RMSE: 38.7140 - val_loss: 1501.3054 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7954 - loss: 1507.5232 - val_RMSE: 38.7117 - val_loss: 1500.7601 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7919 - loss: 1506.9127 - val_RMSE: 38.7106 - val_loss: 1500.2843 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7884 - loss: 1506.2535 - val_RMSE: 38.7103 - val_loss: 1500.0157 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7853 - loss: 1505.7999 - val_RMSE: 38.7094 - val_loss: 1499.8024 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7876 - loss: 1505.8838 - val_RMSE: 38.7098 - val_loss: 1499.8402 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7848 - loss: 1505.6862 - val_RMSE: 38.7084 - val_loss: 1499.7268 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7852 - loss: 1505.6782 - val_RMSE: 38.7083 - val_loss: 1499.6956 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7850 - loss: 1505.6604 - val_RMSE: 38.7080 - val_loss: 1499.6895 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7833 - loss: 1505.5394 - val_RMSE: 38.7077 - val_loss: 1499.6508 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7832 - loss: 1505.4836 - val_RMSE: 38.7086 - val_loss: 1499.6971 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7820 - loss: 1505.3712 - val_RMSE: 38.7103 - val_loss: 1499.8469 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7840 - loss: 1505.5438 - val_RMSE: 38.7079 - val_loss: 1499.5906 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7810 - loss: 1505.2521 - val_RMSE: 38.7072 - val_loss: 1499.5189 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7804 - loss: 1505.2148 - val_RMSE: 38.7066 - val_loss: 1499.4785 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7805 - loss: 1505.2145 - val_RMSE: 38.7088 - val_loss: 1499.5863 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7753 - loss: 1504.7609 - val_RMSE: 38.7076 - val_loss: 1499.5492 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7810 - loss: 1505.2401 - val_RMSE: 38.7068 - val_loss: 1499.4810 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7782 - loss: 1505.0005 - val_RMSE: 38.7074 - val_loss: 1499.4962 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7807 - loss: 1505.1736 - val_RMSE: 38.7080 - val_loss: 1499.5042 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7725 - loss: 1504.3693 - val_RMSE: 38.7038 - val_loss: 1498.7878 - learning_rate: 1.0000e-04\n",
            "41608/41608  86s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 19:31:47,179] Trial 6 finished with value: 38.700243631998696 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'reg': 0.007472176524756258, 'dropout_rate': 0.30743712269143775}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  39s 12ms/step - RMSE: 51.5286 - loss: 2852.2712 - val_RMSE: 38.7185 - val_loss: 1518.3163 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  25s 10ms/step - RMSE: 39.0282 - loss: 1540.8573 - val_RMSE: 38.7080 - val_loss: 1513.8391 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.9052 - loss: 1528.6173 - val_RMSE: 38.7091 - val_loss: 1511.3335 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8609 - loss: 1523.0334 - val_RMSE: 38.7366 - val_loss: 1511.8179 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8331 - loss: 1518.8773 - val_RMSE: 38.7156 - val_loss: 1508.7775 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8215 - loss: 1516.3958 - val_RMSE: 38.6994 - val_loss: 1505.7628 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8113 - loss: 1514.0636 - val_RMSE: 38.6978 - val_loss: 1503.7344 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8048 - loss: 1511.7982 - val_RMSE: 38.6993 - val_loss: 1502.6901 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7997 - loss: 1510.1787 - val_RMSE: 38.6985 - val_loss: 1502.1903 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7967 - loss: 1509.6781 - val_RMSE: 38.6974 - val_loss: 1502.1272 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7966 - loss: 1509.6090 - val_RMSE: 38.6967 - val_loss: 1501.5555 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7998 - loss: 1509.6238 - val_RMSE: 38.6971 - val_loss: 1501.6257 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7970 - loss: 1509.3464 - val_RMSE: 38.6937 - val_loss: 1501.2955 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7967 - loss: 1509.2089 - val_RMSE: 38.6942 - val_loss: 1501.3264 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7950 - loss: 1509.0024 - val_RMSE: 38.6969 - val_loss: 1501.2295 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7964 - loss: 1508.9362 - val_RMSE: 38.6936 - val_loss: 1501.1515 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7949 - loss: 1508.8281 - val_RMSE: 38.6933 - val_loss: 1500.7833 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7960 - loss: 1508.8257 - val_RMSE: 38.6913 - val_loss: 1500.4706 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7932 - loss: 1508.4797 - val_RMSE: 38.6929 - val_loss: 1500.7047 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7957 - loss: 1508.7478 - val_RMSE: 38.6936 - val_loss: 1500.6428 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7914 - loss: 1508.3126 - val_RMSE: 38.6910 - val_loss: 1500.4664 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7905 - loss: 1508.2111 - val_RMSE: 38.6927 - val_loss: 1500.4425 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7962 - loss: 1508.5892 - val_RMSE: 38.6912 - val_loss: 1500.2487 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7959 - loss: 1508.5150 - val_RMSE: 38.6902 - val_loss: 1500.0854 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7910 - loss: 1508.0262 - val_RMSE: 38.6916 - val_loss: 1500.1575 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  40s 12ms/step - RMSE: 51.4598 - loss: 2845.1838 - val_RMSE: 38.7767 - val_loss: 1525.3945 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.9769 - loss: 1538.4990 - val_RMSE: 38.7701 - val_loss: 1519.0621 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.8559 - loss: 1524.8116 - val_RMSE: 38.7531 - val_loss: 1514.3655 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8036 - loss: 1517.7728 - val_RMSE: 38.7422 - val_loss: 1511.1298 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7790 - loss: 1513.6190 - val_RMSE: 38.7304 - val_loss: 1508.8058 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7692 - loss: 1511.3303 - val_RMSE: 38.7288 - val_loss: 1507.1094 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7558 - loss: 1508.8678 - val_RMSE: 38.7251 - val_loss: 1505.3943 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7500 - loss: 1507.0267 - val_RMSE: 38.7246 - val_loss: 1503.9808 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7463 - loss: 1505.6868 - val_RMSE: 38.7226 - val_loss: 1503.6935 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7475 - loss: 1505.5291 - val_RMSE: 38.7233 - val_loss: 1503.5381 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7466 - loss: 1505.4153 - val_RMSE: 38.7257 - val_loss: 1503.6134 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7472 - loss: 1505.2638 - val_RMSE: 38.7216 - val_loss: 1502.9609 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7447 - loss: 1504.9309 - val_RMSE: 38.7237 - val_loss: 1502.9507 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7468 - loss: 1504.9750 - val_RMSE: 38.7254 - val_loss: 1503.3693 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7451 - loss: 1504.7506 - val_RMSE: 38.7264 - val_loss: 1503.3263 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7434 - loss: 1504.7736 - val_RMSE: 38.7219 - val_loss: 1502.7815 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7466 - loss: 1504.7899 - val_RMSE: 38.7261 - val_loss: 1502.9895 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7403 - loss: 1504.2426 - val_RMSE: 38.7247 - val_loss: 1502.9430 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7440 - loss: 1504.5675 - val_RMSE: 38.7308 - val_loss: 1503.5018 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7422 - loss: 1504.3862 - val_RMSE: 38.7229 - val_loss: 1502.6877 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7412 - loss: 1504.2197 - val_RMSE: 38.7232 - val_loss: 1502.7212 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7417 - loss: 1504.2346 - val_RMSE: 38.7234 - val_loss: 1502.6967 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7410 - loss: 1504.0951 - val_RMSE: 38.7241 - val_loss: 1502.6064 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7413 - loss: 1504.0226 - val_RMSE: 38.7300 - val_loss: 1503.1766 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7385 - loss: 1503.8560 - val_RMSE: 38.7266 - val_loss: 1502.8417 - learning_rate: 0.0010\n",
            "41608/41608  83s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4708 - loss: 2845.7400 - val_RMSE: 38.7702 - val_loss: 1526.5149 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  25s 9ms/step - RMSE: 39.0170 - loss: 1543.0101 - val_RMSE: 38.7751 - val_loss: 1519.9900 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.8921 - loss: 1528.1085 - val_RMSE: 38.7474 - val_loss: 1514.3889 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.8314 - loss: 1520.3462 - val_RMSE: 38.7392 - val_loss: 1512.0295 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.8094 - loss: 1517.1302 - val_RMSE: 38.7303 - val_loss: 1509.6882 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7958 - loss: 1514.3269 - val_RMSE: 38.7269 - val_loss: 1507.7281 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7864 - loss: 1511.8748 - val_RMSE: 38.7228 - val_loss: 1505.3118 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7760 - loss: 1509.1525 - val_RMSE: 38.7221 - val_loss: 1504.1284 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7733 - loss: 1508.0520 - val_RMSE: 38.7229 - val_loss: 1503.9752 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7738 - loss: 1508.0132 - val_RMSE: 38.7251 - val_loss: 1504.2043 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7701 - loss: 1507.5835 - val_RMSE: 38.7185 - val_loss: 1503.3129 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7716 - loss: 1507.5648 - val_RMSE: 38.7164 - val_loss: 1503.1106 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7676 - loss: 1507.1177 - val_RMSE: 38.7166 - val_loss: 1502.8873 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7738 - loss: 1507.5548 - val_RMSE: 38.7151 - val_loss: 1502.6533 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7697 - loss: 1507.0375 - val_RMSE: 38.7156 - val_loss: 1502.8741 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7717 - loss: 1507.1680 - val_RMSE: 38.7158 - val_loss: 1502.8152 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7686 - loss: 1506.8170 - val_RMSE: 38.7149 - val_loss: 1502.3252 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7709 - loss: 1506.8314 - val_RMSE: 38.7137 - val_loss: 1502.1184 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7728 - loss: 1506.8254 - val_RMSE: 38.7131 - val_loss: 1501.9169 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7694 - loss: 1506.5668 - val_RMSE: 38.7127 - val_loss: 1501.9113 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7718 - loss: 1506.6975 - val_RMSE: 38.7137 - val_loss: 1502.3365 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7687 - loss: 1506.4891 - val_RMSE: 38.7136 - val_loss: 1502.0699 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7701 - loss: 1506.5562 - val_RMSE: 38.7130 - val_loss: 1501.8527 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7700 - loss: 1506.3962 - val_RMSE: 38.7123 - val_loss: 1501.7814 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  25s 10ms/step - RMSE: 38.7699 - loss: 1506.3728 - val_RMSE: 38.7127 - val_loss: 1501.7281 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 20:09:20,682] Trial 7 finished with value: 38.710269927978516 and parameters: {'units': 1024, 'last_layer': 1, 'activation': 'silu', 'reg': 0.03866576727475806, 'dropout_rate': 0.37177793999597164}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 61.1209 - loss: 3902.0278 - val_RMSE: 38.6977 - val_loss: 1498.7308 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.4198 - loss: 1555.1478 - val_RMSE: 38.6921 - val_loss: 1498.2742 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.3436 - loss: 1549.0868 - val_RMSE: 38.6923 - val_loss: 1498.1431 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3325 - loss: 1548.0499 - val_RMSE: 38.6910 - val_loss: 1497.8624 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3114 - loss: 1546.2186 - val_RMSE: 38.6920 - val_loss: 1497.8009 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3005 - loss: 1545.2455 - val_RMSE: 38.6901 - val_loss: 1497.5774 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2899 - loss: 1544.3424 - val_RMSE: 38.6894 - val_loss: 1497.4891 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2768 - loss: 1543.2798 - val_RMSE: 38.6902 - val_loss: 1497.5203 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2560 - loss: 1541.6190 - val_RMSE: 38.6900 - val_loss: 1497.4908 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2385 - loss: 1540.2305 - val_RMSE: 38.6893 - val_loss: 1497.4287 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2215 - loss: 1538.8915 - val_RMSE: 38.6894 - val_loss: 1497.4237 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2103 - loss: 1538.0042 - val_RMSE: 38.6880 - val_loss: 1497.3157 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2023 - loss: 1537.3796 - val_RMSE: 38.6885 - val_loss: 1497.3528 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1786 - loss: 1535.5190 - val_RMSE: 38.6880 - val_loss: 1497.3240 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1777 - loss: 1535.4551 - val_RMSE: 38.6873 - val_loss: 1497.2700 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1629 - loss: 1534.2987 - val_RMSE: 38.6869 - val_loss: 1497.2450 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1550 - loss: 1533.6946 - val_RMSE: 38.6869 - val_loss: 1497.2510 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1376 - loss: 1532.3247 - val_RMSE: 38.6865 - val_loss: 1497.2130 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1267 - loss: 1531.4701 - val_RMSE: 38.6856 - val_loss: 1497.1497 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1183 - loss: 1530.8130 - val_RMSE: 38.6859 - val_loss: 1497.1729 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1132 - loss: 1530.4182 - val_RMSE: 38.6866 - val_loss: 1497.2325 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0954 - loss: 1529.0322 - val_RMSE: 38.6846 - val_loss: 1497.0836 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0857 - loss: 1528.2788 - val_RMSE: 38.6863 - val_loss: 1497.2079 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0722 - loss: 1527.2186 - val_RMSE: 38.6859 - val_loss: 1497.1895 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.0694 - loss: 1527.0153 - val_RMSE: 38.6851 - val_loss: 1497.1338 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 61.1169 - loss: 3901.1719 - val_RMSE: 38.7299 - val_loss: 1501.2250 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.3685 - loss: 1551.0978 - val_RMSE: 38.7243 - val_loss: 1500.7673 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.3001 - loss: 1545.6677 - val_RMSE: 38.7220 - val_loss: 1500.4536 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.2836 - loss: 1544.2148 - val_RMSE: 38.7220 - val_loss: 1500.2710 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2606 - loss: 1542.2256 - val_RMSE: 38.7228 - val_loss: 1500.1848 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2435 - loss: 1540.7621 - val_RMSE: 38.7218 - val_loss: 1500.0316 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2310 - loss: 1539.7133 - val_RMSE: 38.7199 - val_loss: 1499.8373 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2181 - loss: 1538.6545 - val_RMSE: 38.7215 - val_loss: 1499.9443 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1986 - loss: 1537.1063 - val_RMSE: 38.7201 - val_loss: 1499.8127 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1931 - loss: 1536.6681 - val_RMSE: 38.7186 - val_loss: 1499.6941 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1786 - loss: 1535.5302 - val_RMSE: 38.7184 - val_loss: 1499.6852 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1704 - loss: 1534.8855 - val_RMSE: 38.7184 - val_loss: 1499.6836 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1468 - loss: 1533.0361 - val_RMSE: 38.7167 - val_loss: 1499.5507 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1420 - loss: 1532.6622 - val_RMSE: 38.7175 - val_loss: 1499.6100 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1194 - loss: 1530.8885 - val_RMSE: 38.7176 - val_loss: 1499.6178 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1203 - loss: 1530.9589 - val_RMSE: 38.7181 - val_loss: 1499.6541 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1024 - loss: 1529.5548 - val_RMSE: 38.7178 - val_loss: 1499.6268 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0984 - loss: 1529.2516 - val_RMSE: 38.7168 - val_loss: 1499.5583 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0806 - loss: 1527.8538 - val_RMSE: 38.7145 - val_loss: 1499.3529 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0761 - loss: 1527.4706 - val_RMSE: 38.7143 - val_loss: 1499.3093 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0769 - loss: 1527.5074 - val_RMSE: 38.7143 - val_loss: 1499.2874 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0698 - loss: 1526.9324 - val_RMSE: 38.7139 - val_loss: 1499.2341 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0740 - loss: 1527.2351 - val_RMSE: 38.7137 - val_loss: 1499.1998 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0821 - loss: 1527.8506 - val_RMSE: 38.7136 - val_loss: 1499.1775 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.0721 - loss: 1527.0569 - val_RMSE: 38.7139 - val_loss: 1499.1780 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  27s 7ms/step - RMSE: 61.0877 - loss: 3897.3528 - val_RMSE: 38.7163 - val_loss: 1500.1722 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3868 - loss: 1552.5505 - val_RMSE: 38.7195 - val_loss: 1500.3879 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3191 - loss: 1547.1432 - val_RMSE: 38.7178 - val_loss: 1500.1010 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.3036 - loss: 1545.7640 - val_RMSE: 38.7175 - val_loss: 1499.9011 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2857 - loss: 1544.1846 - val_RMSE: 38.7180 - val_loss: 1499.7913 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2750 - loss: 1543.2133 - val_RMSE: 38.7164 - val_loss: 1499.5973 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2562 - loss: 1541.6792 - val_RMSE: 38.7169 - val_loss: 1499.5927 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2457 - loss: 1540.8234 - val_RMSE: 38.7190 - val_loss: 1499.7355 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.2291 - loss: 1539.4998 - val_RMSE: 38.7172 - val_loss: 1499.5797 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.2189 - loss: 1538.6761 - val_RMSE: 38.7179 - val_loss: 1499.6318 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.2034 - loss: 1537.4597 - val_RMSE: 38.7166 - val_loss: 1499.5310 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  14s 6ms/step - RMSE: 39.1938 - loss: 1536.7150 - val_RMSE: 38.7150 - val_loss: 1499.4080 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1823 - loss: 1535.8115 - val_RMSE: 38.7155 - val_loss: 1499.4434 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1630 - loss: 1534.2987 - val_RMSE: 38.7165 - val_loss: 1499.5266 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1458 - loss: 1532.9509 - val_RMSE: 38.7150 - val_loss: 1499.4058 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1392 - loss: 1532.4392 - val_RMSE: 38.7137 - val_loss: 1499.3174 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1346 - loss: 1532.0847 - val_RMSE: 38.7145 - val_loss: 1499.3782 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1174 - loss: 1530.7379 - val_RMSE: 38.7114 - val_loss: 1499.1495 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.1018 - loss: 1529.5291 - val_RMSE: 38.7135 - val_loss: 1499.3093 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0977 - loss: 1529.2113 - val_RMSE: 38.7136 - val_loss: 1499.3247 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0894 - loss: 1528.5648 - val_RMSE: 38.7141 - val_loss: 1499.3652 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0776 - loss: 1527.6477 - val_RMSE: 38.7120 - val_loss: 1499.2026 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0636 - loss: 1526.5555 - val_RMSE: 38.7128 - val_loss: 1499.2717 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0640 - loss: 1526.5757 - val_RMSE: 38.7080 - val_loss: 1498.8640 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  14s 5ms/step - RMSE: 39.0491 - loss: 1525.3817 - val_RMSE: 38.7079 - val_loss: 1498.8315 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 20:33:31,551] Trial 8 finished with value: 38.70230611165365 and parameters: {'units': 128, 'last_layer': 2, 'activation': 'silu', 'reg': 0.0032489164477933394, 'dropout_rate': 0.4814024944816824}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4691 - loss: 2813.0303 - val_RMSE: 38.7235 - val_loss: 1517.4996 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 39.0040 - loss: 1536.6215 - val_RMSE: 38.7503 - val_loss: 1512.3701 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8962 - loss: 1523.0516 - val_RMSE: 38.7205 - val_loss: 1507.7740 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8496 - loss: 1517.4974 - val_RMSE: 38.6963 - val_loss: 1504.5991 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8223 - loss: 1514.1731 - val_RMSE: 38.7003 - val_loss: 1503.9467 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8110 - loss: 1512.2694 - val_RMSE: 38.6946 - val_loss: 1502.6302 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8040 - loss: 1510.9678 - val_RMSE: 38.6917 - val_loss: 1501.7863 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7975 - loss: 1509.7816 - val_RMSE: 38.6893 - val_loss: 1500.8617 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7912 - loss: 1508.5957 - val_RMSE: 38.6903 - val_loss: 1500.2661 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7859 - loss: 1507.5677 - val_RMSE: 38.6894 - val_loss: 1499.7230 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7856 - loss: 1507.2086 - val_RMSE: 38.6885 - val_loss: 1499.5718 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7874 - loss: 1507.2902 - val_RMSE: 38.6893 - val_loss: 1499.5602 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7864 - loss: 1507.1146 - val_RMSE: 38.6900 - val_loss: 1499.6432 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7859 - loss: 1507.0447 - val_RMSE: 38.6875 - val_loss: 1499.2913 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7856 - loss: 1506.9264 - val_RMSE: 38.6874 - val_loss: 1499.1938 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7859 - loss: 1506.8579 - val_RMSE: 38.6902 - val_loss: 1499.4567 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7851 - loss: 1506.7717 - val_RMSE: 38.6876 - val_loss: 1499.1912 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7860 - loss: 1506.8245 - val_RMSE: 38.6871 - val_loss: 1498.9998 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7827 - loss: 1506.4755 - val_RMSE: 38.6865 - val_loss: 1499.0103 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7840 - loss: 1506.6088 - val_RMSE: 38.6871 - val_loss: 1498.9474 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7831 - loss: 1506.4626 - val_RMSE: 38.6861 - val_loss: 1498.9922 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7809 - loss: 1506.2688 - val_RMSE: 38.6863 - val_loss: 1498.8827 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7857 - loss: 1506.5446 - val_RMSE: 38.6860 - val_loss: 1498.8080 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7847 - loss: 1506.4434 - val_RMSE: 38.6857 - val_loss: 1498.7134 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7805 - loss: 1506.0798 - val_RMSE: 38.6848 - val_loss: 1498.5958 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 12ms/step - RMSE: 51.4111 - loss: 2806.1509 - val_RMSE: 38.7396 - val_loss: 1519.3008 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9547 - loss: 1533.0541 - val_RMSE: 38.7394 - val_loss: 1511.6337 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8421 - loss: 1519.1315 - val_RMSE: 38.7240 - val_loss: 1508.8429 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7945 - loss: 1513.7356 - val_RMSE: 38.7302 - val_loss: 1507.1317 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7689 - loss: 1510.0515 - val_RMSE: 38.7255 - val_loss: 1506.1013 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7578 - loss: 1508.3307 - val_RMSE: 38.7252 - val_loss: 1505.1339 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7468 - loss: 1506.6418 - val_RMSE: 38.7224 - val_loss: 1504.1670 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7428 - loss: 1505.5044 - val_RMSE: 38.7185 - val_loss: 1502.8888 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7390 - loss: 1504.3309 - val_RMSE: 38.7176 - val_loss: 1502.1860 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7386 - loss: 1503.7543 - val_RMSE: 38.7167 - val_loss: 1501.8513 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7377 - loss: 1503.4658 - val_RMSE: 38.7186 - val_loss: 1501.9520 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7370 - loss: 1503.3743 - val_RMSE: 38.7165 - val_loss: 1501.5751 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7363 - loss: 1503.2263 - val_RMSE: 38.7191 - val_loss: 1501.6782 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7369 - loss: 1503.2773 - val_RMSE: 38.7176 - val_loss: 1501.6855 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7351 - loss: 1503.0836 - val_RMSE: 38.7167 - val_loss: 1501.6775 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7352 - loss: 1503.0073 - val_RMSE: 38.7156 - val_loss: 1501.3649 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7375 - loss: 1503.0364 - val_RMSE: 38.7189 - val_loss: 1501.4536 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7302 - loss: 1502.4960 - val_RMSE: 38.7179 - val_loss: 1501.4189 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7340 - loss: 1502.7595 - val_RMSE: 38.7202 - val_loss: 1501.5583 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7334 - loss: 1502.6182 - val_RMSE: 38.7164 - val_loss: 1501.3289 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7323 - loss: 1502.5583 - val_RMSE: 38.7170 - val_loss: 1501.1899 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7317 - loss: 1502.4124 - val_RMSE: 38.7171 - val_loss: 1501.1547 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7316 - loss: 1502.3152 - val_RMSE: 38.7164 - val_loss: 1501.0695 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7316 - loss: 1502.2948 - val_RMSE: 38.7190 - val_loss: 1501.3092 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7293 - loss: 1502.1042 - val_RMSE: 38.7187 - val_loss: 1501.1826 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  38s 11ms/step - RMSE: 51.4084 - loss: 2805.2761 - val_RMSE: 38.7452 - val_loss: 1519.3495 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.9788 - loss: 1534.6334 - val_RMSE: 38.7431 - val_loss: 1511.3177 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8782 - loss: 1521.4064 - val_RMSE: 38.7219 - val_loss: 1508.1727 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8237 - loss: 1515.6943 - val_RMSE: 38.7163 - val_loss: 1506.0165 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.8020 - loss: 1512.4397 - val_RMSE: 38.7247 - val_loss: 1505.6578 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7893 - loss: 1510.4745 - val_RMSE: 38.7166 - val_loss: 1504.1571 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7827 - loss: 1509.2094 - val_RMSE: 38.7275 - val_loss: 1504.4454 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7702 - loss: 1507.6050 - val_RMSE: 38.7200 - val_loss: 1503.1456 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7669 - loss: 1506.6617 - val_RMSE: 38.7115 - val_loss: 1501.7999 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7634 - loss: 1505.8108 - val_RMSE: 38.7134 - val_loss: 1501.6399 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7605 - loss: 1505.3065 - val_RMSE: 38.7114 - val_loss: 1501.3373 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7616 - loss: 1505.2423 - val_RMSE: 38.7121 - val_loss: 1501.3223 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7588 - loss: 1504.9919 - val_RMSE: 38.7111 - val_loss: 1501.2677 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7652 - loss: 1505.4694 - val_RMSE: 38.7097 - val_loss: 1501.0770 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7603 - loss: 1505.0127 - val_RMSE: 38.7105 - val_loss: 1501.0117 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  25s 9ms/step - RMSE: 38.7616 - loss: 1505.0355 - val_RMSE: 38.7092 - val_loss: 1500.8599 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7586 - loss: 1504.7434 - val_RMSE: 38.7085 - val_loss: 1500.8516 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7612 - loss: 1504.9330 - val_RMSE: 38.7078 - val_loss: 1500.6390 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7636 - loss: 1505.0391 - val_RMSE: 38.7111 - val_loss: 1500.8312 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7594 - loss: 1504.6670 - val_RMSE: 38.7079 - val_loss: 1500.6687 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7616 - loss: 1504.7908 - val_RMSE: 38.7088 - val_loss: 1500.7115 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7595 - loss: 1504.6287 - val_RMSE: 38.7071 - val_loss: 1500.4790 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7611 - loss: 1504.7207 - val_RMSE: 38.7093 - val_loss: 1500.6246 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7601 - loss: 1504.5687 - val_RMSE: 38.7085 - val_loss: 1500.5195 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  24s 9ms/step - RMSE: 38.7592 - loss: 1504.5013 - val_RMSE: 38.7053 - val_loss: 1500.2549 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 21:10:13,385] Trial 9 finished with value: 38.702955881754555 and parameters: {'units': 1024, 'last_layer': 2, 'activation': 'relu', 'reg': 0.013130181575743027, 'dropout_rate': 0.3554543298268401}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.1922 - loss: 3086.4292 - val_RMSE: 38.7158 - val_loss: 1499.1244 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0593 - loss: 1525.8591 - val_RMSE: 38.6948 - val_loss: 1497.5507 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9279 - loss: 1515.6602 - val_RMSE: 38.6919 - val_loss: 1497.3785 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8905 - loss: 1512.8024 - val_RMSE: 38.6877 - val_loss: 1497.1183 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8687 - loss: 1511.1748 - val_RMSE: 38.6868 - val_loss: 1497.1005 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8609 - loss: 1510.6158 - val_RMSE: 38.6852 - val_loss: 1497.0114 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8567 - loss: 1510.3160 - val_RMSE: 38.6847 - val_loss: 1496.9912 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8530 - loss: 1510.0344 - val_RMSE: 38.6842 - val_loss: 1496.9512 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8508 - loss: 1509.8704 - val_RMSE: 38.6842 - val_loss: 1496.9491 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8524 - loss: 1509.9918 - val_RMSE: 38.6838 - val_loss: 1496.9176 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8510 - loss: 1509.8839 - val_RMSE: 38.6831 - val_loss: 1496.8740 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8456 - loss: 1509.4684 - val_RMSE: 38.6831 - val_loss: 1496.8713 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8522 - loss: 1509.9856 - val_RMSE: 38.6828 - val_loss: 1496.8525 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8437 - loss: 1509.3285 - val_RMSE: 38.6823 - val_loss: 1496.8154 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8455 - loss: 1509.4724 - val_RMSE: 38.6819 - val_loss: 1496.7974 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8390 - loss: 1508.9779 - val_RMSE: 38.6824 - val_loss: 1496.8373 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8423 - loss: 1509.2363 - val_RMSE: 38.6816 - val_loss: 1496.7876 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8362 - loss: 1508.7734 - val_RMSE: 38.6815 - val_loss: 1496.7871 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8393 - loss: 1509.0171 - val_RMSE: 38.6809 - val_loss: 1496.7478 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8401 - loss: 1509.0957 - val_RMSE: 38.6809 - val_loss: 1496.7489 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8337 - loss: 1508.6005 - val_RMSE: 38.6807 - val_loss: 1496.7427 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8368 - loss: 1508.8475 - val_RMSE: 38.6807 - val_loss: 1496.7518 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8340 - loss: 1508.6398 - val_RMSE: 38.6811 - val_loss: 1496.7921 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8302 - loss: 1508.3550 - val_RMSE: 38.6803 - val_loss: 1496.7354 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8317 - loss: 1508.4766 - val_RMSE: 38.6804 - val_loss: 1496.7581 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  35s 10ms/step - RMSE: 54.1436 - loss: 3080.3943 - val_RMSE: 38.7303 - val_loss: 1500.2467 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0116 - loss: 1522.1278 - val_RMSE: 38.7203 - val_loss: 1499.5223 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8858 - loss: 1512.3817 - val_RMSE: 38.7207 - val_loss: 1499.6080 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8341 - loss: 1508.4161 - val_RMSE: 38.7191 - val_loss: 1499.5466 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8198 - loss: 1507.3671 - val_RMSE: 38.7180 - val_loss: 1499.5077 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8091 - loss: 1506.5831 - val_RMSE: 38.7178 - val_loss: 1499.5234 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8052 - loss: 1506.3085 - val_RMSE: 38.7166 - val_loss: 1499.4503 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8063 - loss: 1506.4032 - val_RMSE: 38.7163 - val_loss: 1499.4252 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8051 - loss: 1506.3087 - val_RMSE: 38.7159 - val_loss: 1499.3994 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8004 - loss: 1505.9484 - val_RMSE: 38.7166 - val_loss: 1499.4600 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7960 - loss: 1505.6138 - val_RMSE: 38.7155 - val_loss: 1499.3750 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7963 - loss: 1505.6345 - val_RMSE: 38.7158 - val_loss: 1499.4049 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7946 - loss: 1505.5087 - val_RMSE: 38.7156 - val_loss: 1499.3896 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7929 - loss: 1505.3820 - val_RMSE: 38.7163 - val_loss: 1499.4552 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7978 - loss: 1505.7725 - val_RMSE: 38.7165 - val_loss: 1499.4700 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7894 - loss: 1505.1289 - val_RMSE: 38.7151 - val_loss: 1499.3717 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7985 - loss: 1505.8342 - val_RMSE: 38.7150 - val_loss: 1499.3671 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7911 - loss: 1505.2671 - val_RMSE: 38.7168 - val_loss: 1499.5156 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7890 - loss: 1505.1118 - val_RMSE: 38.7155 - val_loss: 1499.4302 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7938 - loss: 1505.4945 - val_RMSE: 38.7148 - val_loss: 1499.3796 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7852 - loss: 1504.8323 - val_RMSE: 38.7155 - val_loss: 1499.4423 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7886 - loss: 1505.1125 - val_RMSE: 38.7149 - val_loss: 1499.4056 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7758 - loss: 1504.1195 - val_RMSE: 38.7083 - val_loss: 1498.8820 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7787 - loss: 1504.3354 - val_RMSE: 38.7083 - val_loss: 1498.8636 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7756 - loss: 1504.0765 - val_RMSE: 38.7084 - val_loss: 1498.8582 - learning_rate: 1.0000e-04\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.1229 - loss: 3077.8313 - val_RMSE: 38.7183 - val_loss: 1499.3210 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0365 - loss: 1524.0742 - val_RMSE: 38.7155 - val_loss: 1499.1550 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9029 - loss: 1513.7166 - val_RMSE: 38.7126 - val_loss: 1498.9885 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8642 - loss: 1510.7618 - val_RMSE: 38.7109 - val_loss: 1498.9165 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8493 - loss: 1509.6608 - val_RMSE: 38.7089 - val_loss: 1498.8103 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8330 - loss: 1508.4442 - val_RMSE: 38.7079 - val_loss: 1498.7695 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8338 - loss: 1508.5392 - val_RMSE: 38.7067 - val_loss: 1498.6917 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8303 - loss: 1508.2820 - val_RMSE: 38.7054 - val_loss: 1498.5994 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8276 - loss: 1508.0770 - val_RMSE: 38.7066 - val_loss: 1498.6935 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8292 - loss: 1508.2025 - val_RMSE: 38.7056 - val_loss: 1498.6188 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8237 - loss: 1507.7791 - val_RMSE: 38.7057 - val_loss: 1498.6217 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8252 - loss: 1507.8901 - val_RMSE: 38.7043 - val_loss: 1498.5242 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8211 - loss: 1507.5760 - val_RMSE: 38.7046 - val_loss: 1498.5520 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8231 - loss: 1507.7375 - val_RMSE: 38.7047 - val_loss: 1498.5641 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8180 - loss: 1507.3539 - val_RMSE: 38.7037 - val_loss: 1498.4885 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8185 - loss: 1507.3956 - val_RMSE: 38.7051 - val_loss: 1498.6013 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8159 - loss: 1507.1997 - val_RMSE: 38.7038 - val_loss: 1498.5093 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8164 - loss: 1507.2449 - val_RMSE: 38.7041 - val_loss: 1498.5470 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8155 - loss: 1507.1851 - val_RMSE: 38.7036 - val_loss: 1498.5156 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8159 - loss: 1507.2233 - val_RMSE: 38.7038 - val_loss: 1498.5430 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8013 - loss: 1506.0947 - val_RMSE: 38.7021 - val_loss: 1498.3947 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8046 - loss: 1506.3385 - val_RMSE: 38.7021 - val_loss: 1498.3796 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8066 - loss: 1506.4810 - val_RMSE: 38.7019 - val_loss: 1498.3558 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8034 - loss: 1506.2188 - val_RMSE: 38.7019 - val_loss: 1498.3466 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8010 - loss: 1506.0227 - val_RMSE: 38.7022 - val_loss: 1498.3558 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 21:41:02,537] Trial 10 finished with value: 38.69698842366537 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.00011431075362835225, 'dropout_rate': 0.42901303537903984}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.1763 - loss: 3084.7754 - val_RMSE: 38.7122 - val_loss: 1498.8872 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0579 - loss: 1525.7869 - val_RMSE: 38.6963 - val_loss: 1497.7144 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9301 - loss: 1515.8787 - val_RMSE: 38.6895 - val_loss: 1497.2484 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8883 - loss: 1512.6915 - val_RMSE: 38.6871 - val_loss: 1497.1299 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8667 - loss: 1511.0721 - val_RMSE: 38.6863 - val_loss: 1497.1219 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8596 - loss: 1510.5698 - val_RMSE: 38.6845 - val_loss: 1497.0142 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8551 - loss: 1510.2463 - val_RMSE: 38.6841 - val_loss: 1496.9896 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8522 - loss: 1510.0212 - val_RMSE: 38.6837 - val_loss: 1496.9536 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8491 - loss: 1509.7754 - val_RMSE: 38.6842 - val_loss: 1496.9885 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8509 - loss: 1509.9172 - val_RMSE: 38.6830 - val_loss: 1496.8949 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8494 - loss: 1509.7966 - val_RMSE: 38.6829 - val_loss: 1496.8917 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8449 - loss: 1509.4520 - val_RMSE: 38.6821 - val_loss: 1496.8319 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8514 - loss: 1509.9597 - val_RMSE: 38.6825 - val_loss: 1496.8707 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8418 - loss: 1509.2164 - val_RMSE: 38.6818 - val_loss: 1496.8170 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8447 - loss: 1509.4473 - val_RMSE: 38.6820 - val_loss: 1496.8372 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8360 - loss: 1508.7780 - val_RMSE: 38.6815 - val_loss: 1496.8143 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8408 - loss: 1509.1644 - val_RMSE: 38.6812 - val_loss: 1496.7994 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8347 - loss: 1508.6938 - val_RMSE: 38.6811 - val_loss: 1496.7985 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8380 - loss: 1508.9614 - val_RMSE: 38.6806 - val_loss: 1496.7693 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8388 - loss: 1509.0323 - val_RMSE: 38.6807 - val_loss: 1496.7883 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8322 - loss: 1508.5310 - val_RMSE: 38.6801 - val_loss: 1496.7471 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8353 - loss: 1508.7817 - val_RMSE: 38.6809 - val_loss: 1496.8159 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8327 - loss: 1508.5896 - val_RMSE: 38.6803 - val_loss: 1496.7786 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8292 - loss: 1508.3246 - val_RMSE: 38.6802 - val_loss: 1496.7853 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8297 - loss: 1508.3719 - val_RMSE: 38.6802 - val_loss: 1496.7921 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.1315 - loss: 3079.1267 - val_RMSE: 38.7343 - val_loss: 1500.5990 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0113 - loss: 1522.1523 - val_RMSE: 38.7212 - val_loss: 1499.6415 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8836 - loss: 1512.2633 - val_RMSE: 38.7194 - val_loss: 1499.5706 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8322 - loss: 1508.3304 - val_RMSE: 38.7190 - val_loss: 1499.5975 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8196 - loss: 1507.4176 - val_RMSE: 38.7179 - val_loss: 1499.5709 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8081 - loss: 1506.5762 - val_RMSE: 38.7179 - val_loss: 1499.6046 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8043 - loss: 1506.3021 - val_RMSE: 38.7167 - val_loss: 1499.5193 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8049 - loss: 1506.3502 - val_RMSE: 38.7165 - val_loss: 1499.4906 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8040 - loss: 1506.2754 - val_RMSE: 38.7158 - val_loss: 1499.4307 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7997 - loss: 1505.9366 - val_RMSE: 38.7163 - val_loss: 1499.4681 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7959 - loss: 1505.6387 - val_RMSE: 38.7157 - val_loss: 1499.4236 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7956 - loss: 1505.6202 - val_RMSE: 38.7166 - val_loss: 1499.4957 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7939 - loss: 1505.4872 - val_RMSE: 38.7155 - val_loss: 1499.4136 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7920 - loss: 1505.3468 - val_RMSE: 38.7160 - val_loss: 1499.4601 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7968 - loss: 1505.7231 - val_RMSE: 38.7159 - val_loss: 1499.4611 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7884 - loss: 1505.0842 - val_RMSE: 38.7145 - val_loss: 1499.3617 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7973 - loss: 1505.7848 - val_RMSE: 38.7144 - val_loss: 1499.3713 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7897 - loss: 1505.2036 - val_RMSE: 38.7165 - val_loss: 1499.5397 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7874 - loss: 1505.0391 - val_RMSE: 38.7155 - val_loss: 1499.4724 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7922 - loss: 1505.4133 - val_RMSE: 38.7143 - val_loss: 1499.3831 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7838 - loss: 1504.7717 - val_RMSE: 38.7160 - val_loss: 1499.5247 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7827 - loss: 1504.6860 - val_RMSE: 38.7090 - val_loss: 1498.9630 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7739 - loss: 1503.9841 - val_RMSE: 38.7090 - val_loss: 1498.9510 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7759 - loss: 1504.1246 - val_RMSE: 38.7091 - val_loss: 1498.9390 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7736 - loss: 1503.9362 - val_RMSE: 38.7090 - val_loss: 1498.9218 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.1161 - loss: 3077.0850 - val_RMSE: 38.7296 - val_loss: 1500.2301 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0351 - loss: 1524.0101 - val_RMSE: 38.7174 - val_loss: 1499.3510 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.9040 - loss: 1513.8521 - val_RMSE: 38.7120 - val_loss: 1499.0028 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8647 - loss: 1510.8666 - val_RMSE: 38.7098 - val_loss: 1498.9033 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8472 - loss: 1509.5715 - val_RMSE: 38.7080 - val_loss: 1498.8170 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8318 - loss: 1508.4305 - val_RMSE: 38.7075 - val_loss: 1498.8112 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8333 - loss: 1508.5674 - val_RMSE: 38.7064 - val_loss: 1498.7351 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8289 - loss: 1508.2319 - val_RMSE: 38.7056 - val_loss: 1498.6703 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8265 - loss: 1508.0457 - val_RMSE: 38.7068 - val_loss: 1498.7614 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8273 - loss: 1508.0999 - val_RMSE: 38.7049 - val_loss: 1498.6068 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8230 - loss: 1507.7583 - val_RMSE: 38.7052 - val_loss: 1498.6263 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8242 - loss: 1507.8521 - val_RMSE: 38.7044 - val_loss: 1498.5568 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8205 - loss: 1507.5604 - val_RMSE: 38.7046 - val_loss: 1498.5757 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8217 - loss: 1507.6604 - val_RMSE: 38.7041 - val_loss: 1498.5426 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8172 - loss: 1507.3151 - val_RMSE: 38.7041 - val_loss: 1498.5443 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8179 - loss: 1507.3704 - val_RMSE: 38.7048 - val_loss: 1498.6072 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8159 - loss: 1507.2201 - val_RMSE: 38.7037 - val_loss: 1498.5337 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8161 - loss: 1507.2452 - val_RMSE: 38.7041 - val_loss: 1498.5703 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8152 - loss: 1507.1847 - val_RMSE: 38.7041 - val_loss: 1498.5776 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8151 - loss: 1507.1897 - val_RMSE: 38.7039 - val_loss: 1498.5745 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8042 - loss: 1506.3496 - val_RMSE: 38.7029 - val_loss: 1498.5033 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8093 - loss: 1506.7581 - val_RMSE: 38.7029 - val_loss: 1498.5149 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8109 - loss: 1506.8901 - val_RMSE: 38.7049 - val_loss: 1498.6849 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8066 - loss: 1506.5692 - val_RMSE: 38.7028 - val_loss: 1498.5244 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8050 - loss: 1506.4512 - val_RMSE: 38.7046 - val_loss: 1498.6814 - learning_rate: 0.0010\n",
            "41608/41608  87s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 22:11:45,903] Trial 11 finished with value: 38.69793955485026 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.00013595283372657457, 'dropout_rate': 0.42664745909121726}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.1909 - loss: 3086.3071 - val_RMSE: 38.7219 - val_loss: 1499.5839 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0627 - loss: 1526.1074 - val_RMSE: 38.7008 - val_loss: 1497.9996 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9299 - loss: 1515.8036 - val_RMSE: 38.6926 - val_loss: 1497.4127 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8894 - loss: 1512.7048 - val_RMSE: 38.6882 - val_loss: 1497.1348 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8687 - loss: 1511.1492 - val_RMSE: 38.6869 - val_loss: 1497.0865 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8607 - loss: 1510.5780 - val_RMSE: 38.6852 - val_loss: 1496.9894 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8561 - loss: 1510.2482 - val_RMSE: 38.6846 - val_loss: 1496.9614 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8528 - loss: 1510.0039 - val_RMSE: 38.6841 - val_loss: 1496.9238 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8510 - loss: 1509.8691 - val_RMSE: 38.6844 - val_loss: 1496.9553 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8513 - loss: 1509.8944 - val_RMSE: 38.6832 - val_loss: 1496.8602 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8503 - loss: 1509.8154 - val_RMSE: 38.6829 - val_loss: 1496.8400 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8455 - loss: 1509.4471 - val_RMSE: 38.6823 - val_loss: 1496.8037 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8515 - loss: 1509.9181 - val_RMSE: 38.6827 - val_loss: 1496.8345 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8427 - loss: 1509.2408 - val_RMSE: 38.6825 - val_loss: 1496.8235 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8455 - loss: 1509.4712 - val_RMSE: 38.6821 - val_loss: 1496.8068 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8378 - loss: 1508.8770 - val_RMSE: 38.6818 - val_loss: 1496.7957 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8417 - loss: 1509.1940 - val_RMSE: 38.6819 - val_loss: 1496.8142 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8355 - loss: 1508.7183 - val_RMSE: 38.6814 - val_loss: 1496.7753 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8384 - loss: 1508.9492 - val_RMSE: 38.6812 - val_loss: 1496.7728 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8389 - loss: 1508.9973 - val_RMSE: 38.6809 - val_loss: 1496.7520 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8337 - loss: 1508.5984 - val_RMSE: 38.6815 - val_loss: 1496.8038 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8367 - loss: 1508.8386 - val_RMSE: 38.6809 - val_loss: 1496.7734 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8330 - loss: 1508.5625 - val_RMSE: 38.6815 - val_loss: 1496.8279 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8283 - loss: 1508.2123 - val_RMSE: 38.6804 - val_loss: 1496.7528 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8310 - loss: 1508.4293 - val_RMSE: 38.6806 - val_loss: 1496.7800 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.1367 - loss: 3079.6467 - val_RMSE: 38.7284 - val_loss: 1500.0900 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0146 - loss: 1522.3511 - val_RMSE: 38.7226 - val_loss: 1499.6888 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8868 - loss: 1512.4427 - val_RMSE: 38.7224 - val_loss: 1499.7280 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8360 - loss: 1508.5499 - val_RMSE: 38.7198 - val_loss: 1499.5858 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8210 - loss: 1507.4440 - val_RMSE: 38.7187 - val_loss: 1499.5490 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8095 - loss: 1506.6018 - val_RMSE: 38.7178 - val_loss: 1499.5184 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8057 - loss: 1506.3345 - val_RMSE: 38.7168 - val_loss: 1499.4583 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8066 - loss: 1506.4176 - val_RMSE: 38.7169 - val_loss: 1499.4635 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8046 - loss: 1506.2675 - val_RMSE: 38.7159 - val_loss: 1499.3970 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8005 - loss: 1505.9498 - val_RMSE: 38.7159 - val_loss: 1499.3976 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7961 - loss: 1505.6115 - val_RMSE: 38.7156 - val_loss: 1499.3722 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7963 - loss: 1505.6316 - val_RMSE: 38.7160 - val_loss: 1499.4141 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7947 - loss: 1505.5127 - val_RMSE: 38.7152 - val_loss: 1499.3586 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7917 - loss: 1505.2908 - val_RMSE: 38.7159 - val_loss: 1499.4211 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7969 - loss: 1505.6969 - val_RMSE: 38.7156 - val_loss: 1499.3998 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7891 - loss: 1505.0994 - val_RMSE: 38.7148 - val_loss: 1499.3491 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7971 - loss: 1505.7301 - val_RMSE: 38.7149 - val_loss: 1499.3639 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7901 - loss: 1505.1948 - val_RMSE: 38.7159 - val_loss: 1499.4548 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7880 - loss: 1505.0436 - val_RMSE: 38.7151 - val_loss: 1499.4009 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7919 - loss: 1505.3596 - val_RMSE: 38.7139 - val_loss: 1499.3234 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7841 - loss: 1504.7611 - val_RMSE: 38.7154 - val_loss: 1499.4457 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7879 - loss: 1505.0723 - val_RMSE: 38.7150 - val_loss: 1499.4261 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7787 - loss: 1504.3616 - val_RMSE: 38.7134 - val_loss: 1499.3108 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7829 - loss: 1504.6980 - val_RMSE: 38.7155 - val_loss: 1499.4840 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7800 - loss: 1504.4830 - val_RMSE: 38.7150 - val_loss: 1499.4509 - learning_rate: 0.0010\n",
            "41608/41608  88s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  35s 10ms/step - RMSE: 54.1216 - loss: 3077.6299 - val_RMSE: 38.7213 - val_loss: 1499.5381 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0330 - loss: 1523.7867 - val_RMSE: 38.7167 - val_loss: 1499.2307 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9011 - loss: 1513.5540 - val_RMSE: 38.7164 - val_loss: 1499.2573 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8663 - loss: 1510.9023 - val_RMSE: 38.7096 - val_loss: 1498.7858 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8485 - loss: 1509.5791 - val_RMSE: 38.7089 - val_loss: 1498.7866 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8327 - loss: 1508.3979 - val_RMSE: 38.7080 - val_loss: 1498.7520 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8338 - loss: 1508.5079 - val_RMSE: 38.7068 - val_loss: 1498.6699 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8295 - loss: 1508.1863 - val_RMSE: 38.7056 - val_loss: 1498.5801 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8265 - loss: 1507.9617 - val_RMSE: 38.7069 - val_loss: 1498.6877 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8288 - loss: 1508.1416 - val_RMSE: 38.7049 - val_loss: 1498.5419 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8235 - loss: 1507.7312 - val_RMSE: 38.7052 - val_loss: 1498.5679 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8241 - loss: 1507.7897 - val_RMSE: 38.7044 - val_loss: 1498.5088 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8202 - loss: 1507.4939 - val_RMSE: 38.7045 - val_loss: 1498.5292 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8221 - loss: 1507.6453 - val_RMSE: 38.7043 - val_loss: 1498.5215 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8177 - loss: 1507.3112 - val_RMSE: 38.7035 - val_loss: 1498.4585 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8180 - loss: 1507.3434 - val_RMSE: 38.7045 - val_loss: 1498.5477 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8152 - loss: 1507.1343 - val_RMSE: 38.7039 - val_loss: 1498.5127 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8156 - loss: 1507.1738 - val_RMSE: 38.7040 - val_loss: 1498.5319 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8153 - loss: 1507.1606 - val_RMSE: 38.7039 - val_loss: 1498.5289 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8149 - loss: 1507.1396 - val_RMSE: 38.7036 - val_loss: 1498.5215 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8005 - loss: 1506.0233 - val_RMSE: 38.7016 - val_loss: 1498.3496 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8047 - loss: 1506.3372 - val_RMSE: 38.7015 - val_loss: 1498.3258 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8054 - loss: 1506.3799 - val_RMSE: 38.7018 - val_loss: 1498.3412 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8020 - loss: 1506.1045 - val_RMSE: 38.7020 - val_loss: 1498.3456 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8001 - loss: 1505.9463 - val_RMSE: 38.7021 - val_loss: 1498.3409 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 22:42:27,728] Trial 12 finished with value: 38.6991933186849 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.00010782612784245901, 'dropout_rate': 0.4285140854773215}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.1502 - loss: 3083.9399 - val_RMSE: 38.7238 - val_loss: 1502.3201 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0558 - loss: 1528.2102 - val_RMSE: 38.7006 - val_loss: 1500.6951 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9289 - loss: 1518.4215 - val_RMSE: 38.6945 - val_loss: 1500.1030 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8872 - loss: 1514.9609 - val_RMSE: 38.6906 - val_loss: 1499.3574 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8636 - loss: 1512.6492 - val_RMSE: 38.6900 - val_loss: 1498.7699 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8584 - loss: 1511.7175 - val_RMSE: 38.6868 - val_loss: 1498.0895 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8532 - loss: 1510.9292 - val_RMSE: 38.6867 - val_loss: 1497.8519 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8509 - loss: 1510.5573 - val_RMSE: 38.6867 - val_loss: 1497.7715 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8484 - loss: 1510.3207 - val_RMSE: 38.6872 - val_loss: 1497.8181 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8505 - loss: 1510.4972 - val_RMSE: 38.6856 - val_loss: 1497.6705 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8495 - loss: 1510.3989 - val_RMSE: 38.6861 - val_loss: 1497.7012 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8441 - loss: 1509.9744 - val_RMSE: 38.6853 - val_loss: 1497.6494 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8522 - loss: 1510.5988 - val_RMSE: 38.6851 - val_loss: 1497.6385 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8436 - loss: 1509.9357 - val_RMSE: 38.6844 - val_loss: 1497.5726 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8468 - loss: 1510.1835 - val_RMSE: 38.6846 - val_loss: 1497.6105 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8382 - loss: 1509.5328 - val_RMSE: 38.6849 - val_loss: 1497.6105 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8420 - loss: 1509.8026 - val_RMSE: 38.6848 - val_loss: 1497.5953 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8376 - loss: 1509.4633 - val_RMSE: 38.6846 - val_loss: 1497.5886 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8404 - loss: 1509.6797 - val_RMSE: 38.6843 - val_loss: 1497.5726 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8381 - loss: 1509.4515 - val_RMSE: 38.6804 - val_loss: 1497.1021 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8307 - loss: 1508.7223 - val_RMSE: 38.6801 - val_loss: 1496.9619 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8349 - loss: 1508.9396 - val_RMSE: 38.6800 - val_loss: 1496.8700 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8315 - loss: 1508.6008 - val_RMSE: 38.6799 - val_loss: 1496.8066 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8273 - loss: 1508.2185 - val_RMSE: 38.6799 - val_loss: 1496.7548 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8304 - loss: 1508.4109 - val_RMSE: 38.6798 - val_loss: 1496.7188 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.1101 - loss: 3078.8408 - val_RMSE: 38.7350 - val_loss: 1503.1906 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0073 - loss: 1524.4459 - val_RMSE: 38.7260 - val_loss: 1502.7195 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8818 - loss: 1514.7937 - val_RMSE: 38.7244 - val_loss: 1502.4152 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8312 - loss: 1510.6010 - val_RMSE: 38.7204 - val_loss: 1501.6827 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8173 - loss: 1509.0682 - val_RMSE: 38.7194 - val_loss: 1501.1160 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.8059 - loss: 1507.6998 - val_RMSE: 38.7174 - val_loss: 1500.5309 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8009 - loss: 1506.9116 - val_RMSE: 38.7179 - val_loss: 1500.3008 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8024 - loss: 1506.8149 - val_RMSE: 38.7184 - val_loss: 1500.2615 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8020 - loss: 1506.7490 - val_RMSE: 38.7167 - val_loss: 1500.1276 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  18s 7ms/step - RMSE: 38.7985 - loss: 1506.4796 - val_RMSE: 38.7174 - val_loss: 1500.1863 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7952 - loss: 1506.2120 - val_RMSE: 38.7168 - val_loss: 1500.1390 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7960 - loss: 1506.2762 - val_RMSE: 38.7179 - val_loss: 1500.2194 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7935 - loss: 1506.0697 - val_RMSE: 38.7177 - val_loss: 1500.1886 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7912 - loss: 1505.8981 - val_RMSE: 38.7173 - val_loss: 1500.1777 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7929 - loss: 1505.9857 - val_RMSE: 38.7108 - val_loss: 1499.4858 - learning_rate: 1.0000e-04\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7831 - loss: 1505.0587 - val_RMSE: 38.7106 - val_loss: 1499.3463 - learning_rate: 1.0000e-04\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7908 - loss: 1505.5411 - val_RMSE: 38.7104 - val_loss: 1499.2404 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7851 - loss: 1505.0199 - val_RMSE: 38.7103 - val_loss: 1499.1709 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7834 - loss: 1504.8270 - val_RMSE: 38.7102 - val_loss: 1499.1156 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7876 - loss: 1505.1053 - val_RMSE: 38.7101 - val_loss: 1499.0645 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7805 - loss: 1504.5116 - val_RMSE: 38.7100 - val_loss: 1499.0281 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7879 - loss: 1505.0587 - val_RMSE: 38.7100 - val_loss: 1499.0044 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7777 - loss: 1504.2455 - val_RMSE: 38.7097 - val_loss: 1498.9623 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7810 - loss: 1504.4775 - val_RMSE: 38.7098 - val_loss: 1498.9447 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7811 - loss: 1504.4702 - val_RMSE: 38.7099 - val_loss: 1498.9431 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 9ms/step - RMSE: 54.0903 - loss: 3076.3555 - val_RMSE: 38.7258 - val_loss: 1502.4470 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0317 - loss: 1526.3021 - val_RMSE: 38.7141 - val_loss: 1501.6918 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8995 - loss: 1516.0549 - val_RMSE: 38.7132 - val_loss: 1501.3813 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8609 - loss: 1512.7275 - val_RMSE: 38.7093 - val_loss: 1500.5996 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8458 - loss: 1511.0698 - val_RMSE: 38.7084 - val_loss: 1500.0768 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8304 - loss: 1509.4550 - val_RMSE: 38.7083 - val_loss: 1499.7263 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8311 - loss: 1509.1948 - val_RMSE: 38.7080 - val_loss: 1499.5142 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8273 - loss: 1508.7548 - val_RMSE: 38.7072 - val_loss: 1499.4109 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8249 - loss: 1508.5374 - val_RMSE: 38.7077 - val_loss: 1499.4397 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8273 - loss: 1508.7076 - val_RMSE: 38.7063 - val_loss: 1499.3219 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8222 - loss: 1508.2985 - val_RMSE: 38.7066 - val_loss: 1499.3531 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8228 - loss: 1508.3646 - val_RMSE: 38.7061 - val_loss: 1499.3060 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8187 - loss: 1508.0330 - val_RMSE: 38.7056 - val_loss: 1499.2594 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8215 - loss: 1508.2578 - val_RMSE: 38.7055 - val_loss: 1499.2800 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8172 - loss: 1507.9354 - val_RMSE: 38.7065 - val_loss: 1499.3347 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8195 - loss: 1508.0999 - val_RMSE: 38.7066 - val_loss: 1499.3313 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8181 - loss: 1507.9783 - val_RMSE: 38.7055 - val_loss: 1499.2605 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8172 - loss: 1507.9121 - val_RMSE: 38.7051 - val_loss: 1499.2234 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8162 - loss: 1507.8428 - val_RMSE: 38.7050 - val_loss: 1499.2141 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8162 - loss: 1507.8494 - val_RMSE: 38.7051 - val_loss: 1499.2327 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8076 - loss: 1507.1770 - val_RMSE: 38.7049 - val_loss: 1499.1859 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8135 - loss: 1507.6168 - val_RMSE: 38.7052 - val_loss: 1499.2134 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8145 - loss: 1507.6886 - val_RMSE: 38.7053 - val_loss: 1499.2268 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8116 - loss: 1507.4708 - val_RMSE: 38.7042 - val_loss: 1499.1322 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8108 - loss: 1507.3915 - val_RMSE: 38.7045 - val_loss: 1499.1766 - learning_rate: 0.0010\n",
            "41608/41608  88s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 23:12:45,140] Trial 13 finished with value: 38.698099772135414 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.001629847097661793, 'dropout_rate': 0.41742826609863065}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.2372 - loss: 3091.4924 - val_RMSE: 38.7170 - val_loss: 1499.5424 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0687 - loss: 1526.9362 - val_RMSE: 38.6961 - val_loss: 1498.0338 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9385 - loss: 1516.8846 - val_RMSE: 38.6910 - val_loss: 1497.7511 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8992 - loss: 1513.9355 - val_RMSE: 38.6866 - val_loss: 1497.4983 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8761 - loss: 1512.2119 - val_RMSE: 38.6863 - val_loss: 1497.5071 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8708 - loss: 1511.8153 - val_RMSE: 38.6842 - val_loss: 1497.3075 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8647 - loss: 1511.2953 - val_RMSE: 38.6851 - val_loss: 1497.3179 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8628 - loss: 1511.0874 - val_RMSE: 38.6839 - val_loss: 1497.1840 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8612 - loss: 1510.9324 - val_RMSE: 38.6839 - val_loss: 1497.1731 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8610 - loss: 1510.9070 - val_RMSE: 38.6827 - val_loss: 1497.0681 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8606 - loss: 1510.8702 - val_RMSE: 38.6826 - val_loss: 1497.0814 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8537 - loss: 1510.3533 - val_RMSE: 38.6827 - val_loss: 1497.0885 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8609 - loss: 1510.9081 - val_RMSE: 38.6823 - val_loss: 1497.0564 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8497 - loss: 1510.0367 - val_RMSE: 38.6825 - val_loss: 1497.0775 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8548 - loss: 1510.4388 - val_RMSE: 38.6821 - val_loss: 1497.0465 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8466 - loss: 1509.8090 - val_RMSE: 38.6821 - val_loss: 1497.0631 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8509 - loss: 1510.1534 - val_RMSE: 38.6814 - val_loss: 1497.0200 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8459 - loss: 1509.7778 - val_RMSE: 38.6812 - val_loss: 1497.0015 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8472 - loss: 1509.8832 - val_RMSE: 38.6807 - val_loss: 1496.9803 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8497 - loss: 1510.0814 - val_RMSE: 38.6802 - val_loss: 1496.9426 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8437 - loss: 1509.6224 - val_RMSE: 38.6799 - val_loss: 1496.9320 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8465 - loss: 1509.8555 - val_RMSE: 38.6801 - val_loss: 1496.9612 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8435 - loss: 1509.6307 - val_RMSE: 38.6798 - val_loss: 1496.9349 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8385 - loss: 1509.2467 - val_RMSE: 38.6803 - val_loss: 1496.9849 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8394 - loss: 1509.3248 - val_RMSE: 38.6805 - val_loss: 1497.0117 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.1874 - loss: 3085.2871 - val_RMSE: 38.7435 - val_loss: 1501.5917 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0258 - loss: 1523.5741 - val_RMSE: 38.7222 - val_loss: 1500.0571 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8945 - loss: 1513.4518 - val_RMSE: 38.7207 - val_loss: 1500.0442 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8429 - loss: 1509.5470 - val_RMSE: 38.7189 - val_loss: 1499.9951 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8265 - loss: 1508.3622 - val_RMSE: 38.7181 - val_loss: 1499.9818 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8174 - loss: 1507.6851 - val_RMSE: 38.7175 - val_loss: 1499.9148 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8162 - loss: 1507.5602 - val_RMSE: 38.7162 - val_loss: 1499.7758 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8160 - loss: 1507.5010 - val_RMSE: 38.7161 - val_loss: 1499.7245 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8146 - loss: 1507.3451 - val_RMSE: 38.7155 - val_loss: 1499.6410 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8090 - loss: 1506.8844 - val_RMSE: 38.7154 - val_loss: 1499.6195 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8048 - loss: 1506.5414 - val_RMSE: 38.7142 - val_loss: 1499.5243 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8069 - loss: 1506.7064 - val_RMSE: 38.7149 - val_loss: 1499.5750 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8034 - loss: 1506.4352 - val_RMSE: 38.7136 - val_loss: 1499.4709 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8015 - loss: 1506.2897 - val_RMSE: 38.7140 - val_loss: 1499.5062 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8063 - loss: 1506.6606 - val_RMSE: 38.7149 - val_loss: 1499.5800 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7974 - loss: 1505.9839 - val_RMSE: 38.7136 - val_loss: 1499.4976 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8075 - loss: 1506.7770 - val_RMSE: 38.7134 - val_loss: 1499.4849 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8002 - loss: 1506.2155 - val_RMSE: 38.7139 - val_loss: 1499.5319 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7929 - loss: 1505.6433 - val_RMSE: 38.7074 - val_loss: 1498.9861 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7955 - loss: 1505.8116 - val_RMSE: 38.7071 - val_loss: 1498.9315 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7869 - loss: 1505.1101 - val_RMSE: 38.7070 - val_loss: 1498.9012 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7926 - loss: 1505.5201 - val_RMSE: 38.7071 - val_loss: 1498.8779 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7827 - loss: 1504.7317 - val_RMSE: 38.7071 - val_loss: 1498.8577 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7861 - loss: 1504.9731 - val_RMSE: 38.7070 - val_loss: 1498.8345 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7846 - loss: 1504.8422 - val_RMSE: 38.7071 - val_loss: 1498.8226 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.1822 - loss: 3084.4521 - val_RMSE: 38.7287 - val_loss: 1500.4498 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0468 - loss: 1525.2246 - val_RMSE: 38.7150 - val_loss: 1499.4998 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9117 - loss: 1514.7979 - val_RMSE: 38.7124 - val_loss: 1499.4175 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8732 - loss: 1511.9169 - val_RMSE: 38.7109 - val_loss: 1499.3944 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8579 - loss: 1510.8069 - val_RMSE: 38.7093 - val_loss: 1499.3021 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8406 - loss: 1509.4832 - val_RMSE: 38.7079 - val_loss: 1499.1726 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8435 - loss: 1509.6738 - val_RMSE: 38.7063 - val_loss: 1498.9808 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8394 - loss: 1509.2943 - val_RMSE: 38.7061 - val_loss: 1498.9274 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8361 - loss: 1509.0034 - val_RMSE: 38.7069 - val_loss: 1498.9799 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8384 - loss: 1509.1742 - val_RMSE: 38.7051 - val_loss: 1498.8281 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8324 - loss: 1508.6997 - val_RMSE: 38.7050 - val_loss: 1498.8247 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8331 - loss: 1508.7588 - val_RMSE: 38.7041 - val_loss: 1498.7686 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8314 - loss: 1508.6329 - val_RMSE: 38.7047 - val_loss: 1498.8119 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8320 - loss: 1508.6835 - val_RMSE: 38.7041 - val_loss: 1498.7722 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8266 - loss: 1508.2732 - val_RMSE: 38.7042 - val_loss: 1498.7761 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8270 - loss: 1508.3079 - val_RMSE: 38.7041 - val_loss: 1498.7855 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8237 - loss: 1508.0518 - val_RMSE: 38.7029 - val_loss: 1498.6909 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8243 - loss: 1508.1083 - val_RMSE: 38.7026 - val_loss: 1498.6742 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8245 - loss: 1508.1329 - val_RMSE: 38.7028 - val_loss: 1498.6964 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8236 - loss: 1508.0685 - val_RMSE: 38.7029 - val_loss: 1498.7174 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8144 - loss: 1507.3624 - val_RMSE: 38.7021 - val_loss: 1498.6572 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8181 - loss: 1507.6537 - val_RMSE: 38.7016 - val_loss: 1498.6260 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8195 - loss: 1507.7714 - val_RMSE: 38.7024 - val_loss: 1498.7084 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8168 - loss: 1507.5803 - val_RMSE: 38.7018 - val_loss: 1498.6704 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8151 - loss: 1507.4535 - val_RMSE: 38.7036 - val_loss: 1498.8120 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-15 23:43:24,680] Trial 14 finished with value: 38.69705327351888 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002984783533919333, 'dropout_rate': 0.44795270691730493}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  29s 8ms/step - RMSE: 57.1659 - loss: 3430.8306 - val_RMSE: 38.6983 - val_loss: 1498.8441 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0924 - loss: 1529.5322 - val_RMSE: 38.6906 - val_loss: 1498.3064 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9839 - loss: 1521.0792 - val_RMSE: 38.6887 - val_loss: 1498.0994 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9557 - loss: 1518.7845 - val_RMSE: 38.6870 - val_loss: 1497.7704 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9515 - loss: 1518.2507 - val_RMSE: 38.6875 - val_loss: 1497.5935 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9452 - loss: 1517.5596 - val_RMSE: 38.6861 - val_loss: 1497.3618 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9426 - loss: 1517.2594 - val_RMSE: 38.6857 - val_loss: 1497.2886 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9435 - loss: 1517.3000 - val_RMSE: 38.6861 - val_loss: 1497.2948 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9410 - loss: 1517.0885 - val_RMSE: 38.6850 - val_loss: 1497.2120 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9345 - loss: 1516.5826 - val_RMSE: 38.6854 - val_loss: 1497.2504 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9393 - loss: 1516.9591 - val_RMSE: 38.6850 - val_loss: 1497.2163 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9306 - loss: 1516.2841 - val_RMSE: 38.6846 - val_loss: 1497.1846 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9304 - loss: 1516.2693 - val_RMSE: 38.6846 - val_loss: 1497.1831 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9299 - loss: 1516.2289 - val_RMSE: 38.6845 - val_loss: 1497.1815 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9198 - loss: 1515.4485 - val_RMSE: 38.6844 - val_loss: 1497.1842 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9143 - loss: 1515.0242 - val_RMSE: 38.6838 - val_loss: 1497.1359 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9117 - loss: 1514.8230 - val_RMSE: 38.6841 - val_loss: 1497.1598 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9161 - loss: 1515.1650 - val_RMSE: 38.6838 - val_loss: 1497.1333 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9151 - loss: 1515.0920 - val_RMSE: 38.6829 - val_loss: 1497.0654 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9146 - loss: 1515.0533 - val_RMSE: 38.6834 - val_loss: 1497.1141 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9108 - loss: 1514.7668 - val_RMSE: 38.6832 - val_loss: 1497.1071 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9013 - loss: 1514.0417 - val_RMSE: 38.6830 - val_loss: 1497.0922 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9106 - loss: 1514.7650 - val_RMSE: 38.6829 - val_loss: 1497.0931 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9035 - loss: 1514.2114 - val_RMSE: 38.6821 - val_loss: 1497.0345 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9028 - loss: 1514.1614 - val_RMSE: 38.6825 - val_loss: 1497.0570 - learning_rate: 0.0010\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  29s 8ms/step - RMSE: 57.1443 - loss: 3427.8079 - val_RMSE: 38.7307 - val_loss: 1501.3450 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0459 - loss: 1525.8882 - val_RMSE: 38.7199 - val_loss: 1500.5787 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9423 - loss: 1517.8477 - val_RMSE: 38.7193 - val_loss: 1500.4846 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9128 - loss: 1515.4645 - val_RMSE: 38.7176 - val_loss: 1500.1639 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9063 - loss: 1514.7551 - val_RMSE: 38.7175 - val_loss: 1499.9242 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8980 - loss: 1513.8986 - val_RMSE: 38.7176 - val_loss: 1499.8086 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8881 - loss: 1513.0250 - val_RMSE: 38.7172 - val_loss: 1499.7290 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8806 - loss: 1512.4054 - val_RMSE: 38.7163 - val_loss: 1499.6388 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8878 - loss: 1512.9453 - val_RMSE: 38.7157 - val_loss: 1499.5846 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8858 - loss: 1512.7778 - val_RMSE: 38.7162 - val_loss: 1499.6216 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8813 - loss: 1512.4236 - val_RMSE: 38.7162 - val_loss: 1499.6226 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8831 - loss: 1512.5712 - val_RMSE: 38.7156 - val_loss: 1499.5880 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8788 - loss: 1512.2426 - val_RMSE: 38.7153 - val_loss: 1499.5529 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8746 - loss: 1511.9155 - val_RMSE: 38.7152 - val_loss: 1499.5577 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8779 - loss: 1512.1763 - val_RMSE: 38.7160 - val_loss: 1499.6110 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8675 - loss: 1511.3635 - val_RMSE: 38.7155 - val_loss: 1499.5750 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8725 - loss: 1511.7579 - val_RMSE: 38.7157 - val_loss: 1499.5924 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8656 - loss: 1511.2235 - val_RMSE: 38.7159 - val_loss: 1499.6100 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8563 - loss: 1510.4905 - val_RMSE: 38.7099 - val_loss: 1499.0966 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8563 - loss: 1510.4412 - val_RMSE: 38.7098 - val_loss: 1499.0450 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8536 - loss: 1510.1877 - val_RMSE: 38.7096 - val_loss: 1498.9941 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8530 - loss: 1510.1133 - val_RMSE: 38.7095 - val_loss: 1498.9586 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8553 - loss: 1510.2628 - val_RMSE: 38.7095 - val_loss: 1498.9364 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8559 - loss: 1510.2888 - val_RMSE: 38.7094 - val_loss: 1498.9080 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8575 - loss: 1510.3870 - val_RMSE: 38.7093 - val_loss: 1498.8811 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  28s 8ms/step - RMSE: 57.1254 - loss: 3425.2498 - val_RMSE: 38.7194 - val_loss: 1500.4795 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.0774 - loss: 1528.3647 - val_RMSE: 38.7150 - val_loss: 1500.2128 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9620 - loss: 1519.3999 - val_RMSE: 38.7112 - val_loss: 1499.8650 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9355 - loss: 1517.2517 - val_RMSE: 38.7109 - val_loss: 1499.6720 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9240 - loss: 1516.1539 - val_RMSE: 38.7116 - val_loss: 1499.4846 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9239 - loss: 1515.9294 - val_RMSE: 38.7100 - val_loss: 1499.2283 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9173 - loss: 1515.3032 - val_RMSE: 38.7097 - val_loss: 1499.1447 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9120 - loss: 1514.8485 - val_RMSE: 38.7105 - val_loss: 1499.2045 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9127 - loss: 1514.8962 - val_RMSE: 38.7092 - val_loss: 1499.0884 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9147 - loss: 1515.0438 - val_RMSE: 38.7078 - val_loss: 1498.9818 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9059 - loss: 1514.3590 - val_RMSE: 38.7077 - val_loss: 1498.9750 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9007 - loss: 1513.9552 - val_RMSE: 38.7064 - val_loss: 1498.8713 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9057 - loss: 1514.3368 - val_RMSE: 38.7081 - val_loss: 1499.0177 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8994 - loss: 1513.8552 - val_RMSE: 38.7072 - val_loss: 1498.9406 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8990 - loss: 1513.8284 - val_RMSE: 38.7070 - val_loss: 1498.9335 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8983 - loss: 1513.7806 - val_RMSE: 38.7086 - val_loss: 1499.0530 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8926 - loss: 1513.3330 - val_RMSE: 38.7070 - val_loss: 1498.9297 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8849 - loss: 1512.7251 - val_RMSE: 38.7036 - val_loss: 1498.6143 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8889 - loss: 1512.9840 - val_RMSE: 38.7033 - val_loss: 1498.5455 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8807 - loss: 1512.3025 - val_RMSE: 38.7033 - val_loss: 1498.5125 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8824 - loss: 1512.4025 - val_RMSE: 38.7030 - val_loss: 1498.4615 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8788 - loss: 1512.0936 - val_RMSE: 38.7029 - val_loss: 1498.4274 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8841 - loss: 1512.4811 - val_RMSE: 38.7028 - val_loss: 1498.3962 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8813 - loss: 1512.2443 - val_RMSE: 38.7027 - val_loss: 1498.3696 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.8824 - loss: 1512.3110 - val_RMSE: 38.7028 - val_loss: 1498.3608 - learning_rate: 1.0000e-04\n",
            "41608/41608  84s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 00:09:14,427] Trial 15 finished with value: 38.698187510172524 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.0016250296084432089, 'dropout_rate': 0.39540101245549514}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.2782 - loss: 3095.8782 - val_RMSE: 38.7169 - val_loss: 1499.3932 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0832 - loss: 1527.9094 - val_RMSE: 38.6975 - val_loss: 1497.9794 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.9419 - loss: 1516.9744 - val_RMSE: 38.6904 - val_loss: 1497.5193 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9033 - loss: 1514.0696 - val_RMSE: 38.6883 - val_loss: 1497.4456 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8823 - loss: 1512.5110 - val_RMSE: 38.6860 - val_loss: 1497.3236 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8766 - loss: 1512.1173 - val_RMSE: 38.6845 - val_loss: 1497.2179 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8708 - loss: 1511.6632 - val_RMSE: 38.6846 - val_loss: 1497.2097 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8680 - loss: 1511.4290 - val_RMSE: 38.6840 - val_loss: 1497.1393 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8663 - loss: 1511.2745 - val_RMSE: 38.6831 - val_loss: 1497.0627 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8658 - loss: 1511.2291 - val_RMSE: 38.6831 - val_loss: 1497.0541 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8670 - loss: 1511.3171 - val_RMSE: 38.6825 - val_loss: 1497.0161 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8589 - loss: 1510.6921 - val_RMSE: 38.6816 - val_loss: 1496.9396 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8655 - loss: 1511.1991 - val_RMSE: 38.6816 - val_loss: 1496.9424 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8557 - loss: 1510.4449 - val_RMSE: 38.6815 - val_loss: 1496.9385 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8595 - loss: 1510.7480 - val_RMSE: 38.6817 - val_loss: 1496.9653 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8524 - loss: 1510.2029 - val_RMSE: 38.6811 - val_loss: 1496.9233 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8558 - loss: 1510.4786 - val_RMSE: 38.6810 - val_loss: 1496.9232 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8501 - loss: 1510.0358 - val_RMSE: 38.6805 - val_loss: 1496.8965 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8526 - loss: 1510.2360 - val_RMSE: 38.6808 - val_loss: 1496.9279 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8541 - loss: 1510.3625 - val_RMSE: 38.6808 - val_loss: 1496.9269 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8481 - loss: 1509.9030 - val_RMSE: 38.6810 - val_loss: 1496.9478 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8498 - loss: 1510.0400 - val_RMSE: 38.6798 - val_loss: 1496.8708 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8489 - loss: 1509.9806 - val_RMSE: 38.6802 - val_loss: 1496.9056 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8435 - loss: 1509.5713 - val_RMSE: 38.6800 - val_loss: 1496.8999 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8443 - loss: 1509.6378 - val_RMSE: 38.6797 - val_loss: 1496.8862 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.2253 - loss: 3089.3152 - val_RMSE: 38.7393 - val_loss: 1501.1278 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0370 - loss: 1524.3060 - val_RMSE: 38.7239 - val_loss: 1500.0225 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8952 - loss: 1513.3470 - val_RMSE: 38.7231 - val_loss: 1500.0548 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8459 - loss: 1509.6072 - val_RMSE: 38.7212 - val_loss: 1499.9995 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8323 - loss: 1508.6357 - val_RMSE: 38.7192 - val_loss: 1499.8962 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8250 - loss: 1508.1107 - val_RMSE: 38.7164 - val_loss: 1499.6849 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8238 - loss: 1508.0110 - val_RMSE: 38.7161 - val_loss: 1499.6388 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8235 - loss: 1507.9568 - val_RMSE: 38.7160 - val_loss: 1499.6080 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8220 - loss: 1507.8206 - val_RMSE: 38.7148 - val_loss: 1499.5004 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8152 - loss: 1507.2802 - val_RMSE: 38.7156 - val_loss: 1499.5515 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8102 - loss: 1506.8842 - val_RMSE: 38.7149 - val_loss: 1499.5077 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8129 - loss: 1507.0994 - val_RMSE: 38.7151 - val_loss: 1499.5249 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8108 - loss: 1506.9407 - val_RMSE: 38.7147 - val_loss: 1499.4971 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8068 - loss: 1506.6337 - val_RMSE: 38.7154 - val_loss: 1499.5533 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8133 - loss: 1507.1470 - val_RMSE: 38.7147 - val_loss: 1499.5066 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8031 - loss: 1506.3556 - val_RMSE: 38.7146 - val_loss: 1499.5016 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8121 - loss: 1507.0630 - val_RMSE: 38.7143 - val_loss: 1499.4923 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8042 - loss: 1506.4629 - val_RMSE: 38.7149 - val_loss: 1499.5441 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8039 - loss: 1506.4456 - val_RMSE: 38.7144 - val_loss: 1499.5140 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8069 - loss: 1506.6926 - val_RMSE: 38.7124 - val_loss: 1499.3735 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8012 - loss: 1506.2574 - val_RMSE: 38.7141 - val_loss: 1499.5081 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8050 - loss: 1506.5593 - val_RMSE: 38.7130 - val_loss: 1499.4319 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7932 - loss: 1505.6488 - val_RMSE: 38.7122 - val_loss: 1499.3729 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7973 - loss: 1505.9739 - val_RMSE: 38.7138 - val_loss: 1499.5082 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7952 - loss: 1505.8251 - val_RMSE: 38.7139 - val_loss: 1499.5271 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.2234 - loss: 3088.7781 - val_RMSE: 38.7237 - val_loss: 1499.9156 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0555 - loss: 1525.7467 - val_RMSE: 38.7137 - val_loss: 1499.2261 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9162 - loss: 1514.9709 - val_RMSE: 38.7104 - val_loss: 1499.0614 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8789 - loss: 1512.1599 - val_RMSE: 38.7106 - val_loss: 1499.1636 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8631 - loss: 1511.0133 - val_RMSE: 38.7078 - val_loss: 1499.0028 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8446 - loss: 1509.6190 - val_RMSE: 38.7086 - val_loss: 1499.0797 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8489 - loss: 1509.9587 - val_RMSE: 38.7080 - val_loss: 1499.0126 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8443 - loss: 1509.5802 - val_RMSE: 38.7058 - val_loss: 1498.8291 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8415 - loss: 1509.3489 - val_RMSE: 38.7067 - val_loss: 1498.8939 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8449 - loss: 1509.6073 - val_RMSE: 38.7060 - val_loss: 1498.8324 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8396 - loss: 1509.1887 - val_RMSE: 38.7053 - val_loss: 1498.7786 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8383 - loss: 1509.0919 - val_RMSE: 38.7043 - val_loss: 1498.7078 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8376 - loss: 1509.0490 - val_RMSE: 38.7045 - val_loss: 1498.7247 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8368 - loss: 1508.9845 - val_RMSE: 38.7033 - val_loss: 1498.6464 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8324 - loss: 1508.6527 - val_RMSE: 38.7039 - val_loss: 1498.6877 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8318 - loss: 1508.6100 - val_RMSE: 38.7045 - val_loss: 1498.7450 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8299 - loss: 1508.4669 - val_RMSE: 38.7028 - val_loss: 1498.6188 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8290 - loss: 1508.3986 - val_RMSE: 38.7031 - val_loss: 1498.6448 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8292 - loss: 1508.4265 - val_RMSE: 38.7026 - val_loss: 1498.6185 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8287 - loss: 1508.3964 - val_RMSE: 38.7026 - val_loss: 1498.6252 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8188 - loss: 1507.6318 - val_RMSE: 38.7022 - val_loss: 1498.6003 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8250 - loss: 1508.1219 - val_RMSE: 38.7025 - val_loss: 1498.6249 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8240 - loss: 1508.0503 - val_RMSE: 38.7038 - val_loss: 1498.7491 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8210 - loss: 1507.8329 - val_RMSE: 38.7033 - val_loss: 1498.7197 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8191 - loss: 1507.6973 - val_RMSE: 38.7045 - val_loss: 1498.8224 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 00:39:38,632] Trial 16 finished with value: 38.69933573404948 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00021575974293938053, 'dropout_rate': 0.45990474878515086}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 9ms/step - RMSE: 54.1107 - loss: 3078.6514 - val_RMSE: 38.7144 - val_loss: 1500.0889 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0504 - loss: 1526.2701 - val_RMSE: 38.6957 - val_loss: 1498.8293 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.9274 - loss: 1516.8407 - val_RMSE: 38.6920 - val_loss: 1498.6339 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8814 - loss: 1513.3367 - val_RMSE: 38.6914 - val_loss: 1498.5654 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8604 - loss: 1511.6453 - val_RMSE: 38.6913 - val_loss: 1498.4082 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8519 - loss: 1510.8185 - val_RMSE: 38.6897 - val_loss: 1498.0935 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8459 - loss: 1510.1533 - val_RMSE: 38.6900 - val_loss: 1497.9260 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8447 - loss: 1509.8809 - val_RMSE: 38.6898 - val_loss: 1497.7896 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8418 - loss: 1509.5629 - val_RMSE: 38.6898 - val_loss: 1497.7340 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8426 - loss: 1509.5859 - val_RMSE: 38.6886 - val_loss: 1497.6273 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8438 - loss: 1509.6635 - val_RMSE: 38.6879 - val_loss: 1497.5635 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8385 - loss: 1509.2489 - val_RMSE: 38.6880 - val_loss: 1497.5776 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8459 - loss: 1509.8237 - val_RMSE: 38.6875 - val_loss: 1497.5248 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8377 - loss: 1509.1807 - val_RMSE: 38.6866 - val_loss: 1497.4628 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8398 - loss: 1509.3450 - val_RMSE: 38.6866 - val_loss: 1497.4578 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8321 - loss: 1508.7526 - val_RMSE: 38.6872 - val_loss: 1497.5121 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8345 - loss: 1508.9409 - val_RMSE: 38.6865 - val_loss: 1497.4546 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8314 - loss: 1508.6998 - val_RMSE: 38.6860 - val_loss: 1497.4100 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8344 - loss: 1508.9286 - val_RMSE: 38.6860 - val_loss: 1497.4125 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8368 - loss: 1509.1191 - val_RMSE: 38.6865 - val_loss: 1497.4607 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8315 - loss: 1508.7134 - val_RMSE: 38.6851 - val_loss: 1497.3623 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8340 - loss: 1508.9172 - val_RMSE: 38.6855 - val_loss: 1497.3853 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8323 - loss: 1508.7795 - val_RMSE: 38.6854 - val_loss: 1497.3750 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8294 - loss: 1508.5457 - val_RMSE: 38.6843 - val_loss: 1497.2958 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8321 - loss: 1508.7620 - val_RMSE: 38.6847 - val_loss: 1497.3243 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.0633 - loss: 3072.5645 - val_RMSE: 38.7467 - val_loss: 1502.5869 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0063 - loss: 1522.8361 - val_RMSE: 38.7270 - val_loss: 1501.2798 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8792 - loss: 1513.1250 - val_RMSE: 38.7233 - val_loss: 1501.0939 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8261 - loss: 1509.0717 - val_RMSE: 38.7261 - val_loss: 1501.2937 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8118 - loss: 1507.9106 - val_RMSE: 38.7216 - val_loss: 1500.8225 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8011 - loss: 1506.9298 - val_RMSE: 38.7217 - val_loss: 1500.6202 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7958 - loss: 1506.3091 - val_RMSE: 38.7217 - val_loss: 1500.4171 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7957 - loss: 1506.1190 - val_RMSE: 38.7217 - val_loss: 1500.2946 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7959 - loss: 1506.0269 - val_RMSE: 38.7207 - val_loss: 1500.1533 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7920 - loss: 1505.6758 - val_RMSE: 38.7215 - val_loss: 1500.1971 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7896 - loss: 1505.4692 - val_RMSE: 38.7210 - val_loss: 1500.1378 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7907 - loss: 1505.5469 - val_RMSE: 38.7209 - val_loss: 1500.1379 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7869 - loss: 1505.2430 - val_RMSE: 38.7204 - val_loss: 1500.0864 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7865 - loss: 1505.2190 - val_RMSE: 38.7201 - val_loss: 1500.0696 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7896 - loss: 1505.4656 - val_RMSE: 38.7204 - val_loss: 1500.0969 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7845 - loss: 1505.0621 - val_RMSE: 38.7193 - val_loss: 1500.0079 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7900 - loss: 1505.4958 - val_RMSE: 38.7196 - val_loss: 1500.0474 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7857 - loss: 1505.1703 - val_RMSE: 38.7194 - val_loss: 1500.0381 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7837 - loss: 1505.0179 - val_RMSE: 38.7198 - val_loss: 1500.0641 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7879 - loss: 1505.3464 - val_RMSE: 38.7195 - val_loss: 1500.0469 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7807 - loss: 1504.7926 - val_RMSE: 38.7185 - val_loss: 1499.9778 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7869 - loss: 1505.2788 - val_RMSE: 38.7182 - val_loss: 1499.9569 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7769 - loss: 1504.5190 - val_RMSE: 38.7170 - val_loss: 1499.8768 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7826 - loss: 1504.9630 - val_RMSE: 38.7189 - val_loss: 1500.0370 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7790 - loss: 1504.6909 - val_RMSE: 38.7182 - val_loss: 1499.9637 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.0401 - loss: 3069.6545 - val_RMSE: 38.7322 - val_loss: 1501.4784 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0275 - loss: 1524.5115 - val_RMSE: 38.7156 - val_loss: 1500.4202 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8948 - loss: 1514.3752 - val_RMSE: 38.7129 - val_loss: 1500.3600 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8564 - loss: 1511.5105 - val_RMSE: 38.7133 - val_loss: 1500.3911 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8421 - loss: 1510.3601 - val_RMSE: 38.7132 - val_loss: 1500.2396 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8246 - loss: 1508.8284 - val_RMSE: 38.7114 - val_loss: 1499.8651 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8245 - loss: 1508.5780 - val_RMSE: 38.7111 - val_loss: 1499.6149 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8208 - loss: 1508.0903 - val_RMSE: 38.7102 - val_loss: 1499.4279 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8183 - loss: 1507.7866 - val_RMSE: 38.7104 - val_loss: 1499.3872 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8217 - loss: 1508.0056 - val_RMSE: 38.7099 - val_loss: 1499.3264 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8168 - loss: 1507.6061 - val_RMSE: 38.7093 - val_loss: 1499.2693 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8163 - loss: 1507.5669 - val_RMSE: 38.7083 - val_loss: 1499.1895 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8126 - loss: 1507.2797 - val_RMSE: 38.7077 - val_loss: 1499.1591 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8144 - loss: 1507.4257 - val_RMSE: 38.7079 - val_loss: 1499.1654 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8109 - loss: 1507.1516 - val_RMSE: 38.7080 - val_loss: 1499.1681 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8133 - loss: 1507.3391 - val_RMSE: 38.7086 - val_loss: 1499.2253 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8124 - loss: 1507.2797 - val_RMSE: 38.7071 - val_loss: 1499.1300 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8103 - loss: 1507.1340 - val_RMSE: 38.7064 - val_loss: 1499.0674 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8095 - loss: 1507.0614 - val_RMSE: 38.7062 - val_loss: 1499.0457 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8099 - loss: 1507.0938 - val_RMSE: 38.7066 - val_loss: 1499.0726 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8032 - loss: 1506.5662 - val_RMSE: 38.7069 - val_loss: 1499.0944 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8087 - loss: 1506.9945 - val_RMSE: 38.7069 - val_loss: 1499.1034 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8084 - loss: 1506.9818 - val_RMSE: 38.7070 - val_loss: 1499.1201 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8069 - loss: 1506.8638 - val_RMSE: 38.7055 - val_loss: 1498.9998 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8038 - loss: 1506.6199 - val_RMSE: 38.7055 - val_loss: 1498.9918 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 01:10:24,832] Trial 17 finished with value: 38.70278676350912 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'selu', 'reg': 0.0007138024821391923, 'dropout_rate': 0.3957879794222315}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.2398 - loss: 3091.4988 - val_RMSE: 38.7193 - val_loss: 1499.3783 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0753 - loss: 1527.0886 - val_RMSE: 38.6997 - val_loss: 1497.9049 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9403 - loss: 1516.5986 - val_RMSE: 38.6907 - val_loss: 1497.2632 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8992 - loss: 1513.4532 - val_RMSE: 38.6871 - val_loss: 1497.0455 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8772 - loss: 1511.8029 - val_RMSE: 38.6865 - val_loss: 1497.0535 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8716 - loss: 1511.4158 - val_RMSE: 38.6844 - val_loss: 1496.9219 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8660 - loss: 1511.0156 - val_RMSE: 38.6848 - val_loss: 1496.9785 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8637 - loss: 1510.8527 - val_RMSE: 38.6832 - val_loss: 1496.8630 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8611 - loss: 1510.6633 - val_RMSE: 38.6837 - val_loss: 1496.9169 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8606 - loss: 1510.6301 - val_RMSE: 38.6834 - val_loss: 1496.8917 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8621 - loss: 1510.7542 - val_RMSE: 38.6827 - val_loss: 1496.8439 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8531 - loss: 1510.0630 - val_RMSE: 38.6815 - val_loss: 1496.7584 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8593 - loss: 1510.5491 - val_RMSE: 38.6813 - val_loss: 1496.7513 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8500 - loss: 1509.8315 - val_RMSE: 38.6814 - val_loss: 1496.7617 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8534 - loss: 1510.1022 - val_RMSE: 38.6808 - val_loss: 1496.7251 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8466 - loss: 1509.5798 - val_RMSE: 38.6808 - val_loss: 1496.7343 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8502 - loss: 1509.8713 - val_RMSE: 38.6806 - val_loss: 1496.7273 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8449 - loss: 1509.4708 - val_RMSE: 38.6802 - val_loss: 1496.7064 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8474 - loss: 1509.6674 - val_RMSE: 38.6800 - val_loss: 1496.6989 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8484 - loss: 1509.7550 - val_RMSE: 38.6805 - val_loss: 1496.7426 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8421 - loss: 1509.2701 - val_RMSE: 38.6798 - val_loss: 1496.6989 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8444 - loss: 1509.4558 - val_RMSE: 38.6793 - val_loss: 1496.6677 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8428 - loss: 1509.3433 - val_RMSE: 38.6793 - val_loss: 1496.6768 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8369 - loss: 1508.8928 - val_RMSE: 38.6798 - val_loss: 1496.7220 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8367 - loss: 1508.8910 - val_RMSE: 38.6808 - val_loss: 1496.8113 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 9ms/step - RMSE: 54.1941 - loss: 3085.7605 - val_RMSE: 38.7357 - val_loss: 1500.6461 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0281 - loss: 1523.3992 - val_RMSE: 38.7228 - val_loss: 1499.6948 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8910 - loss: 1512.7607 - val_RMSE: 38.7218 - val_loss: 1499.6647 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8400 - loss: 1508.8491 - val_RMSE: 38.7194 - val_loss: 1499.5392 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8278 - loss: 1507.9591 - val_RMSE: 38.7175 - val_loss: 1499.4386 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8202 - loss: 1507.4189 - val_RMSE: 38.7162 - val_loss: 1499.3790 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8173 - loss: 1507.2258 - val_RMSE: 38.7161 - val_loss: 1499.3939 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8177 - loss: 1507.2728 - val_RMSE: 38.7159 - val_loss: 1499.3864 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8163 - loss: 1507.1754 - val_RMSE: 38.7154 - val_loss: 1499.3591 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8100 - loss: 1506.6975 - val_RMSE: 38.7156 - val_loss: 1499.3827 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8045 - loss: 1506.2773 - val_RMSE: 38.7145 - val_loss: 1499.3005 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8075 - loss: 1506.5176 - val_RMSE: 38.7149 - val_loss: 1499.3445 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8045 - loss: 1506.2889 - val_RMSE: 38.7140 - val_loss: 1499.2806 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8017 - loss: 1506.0815 - val_RMSE: 38.7149 - val_loss: 1499.3608 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8077 - loss: 1506.5504 - val_RMSE: 38.7143 - val_loss: 1499.3213 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7976 - loss: 1505.7821 - val_RMSE: 38.7140 - val_loss: 1499.3085 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8077 - loss: 1506.5778 - val_RMSE: 38.7136 - val_loss: 1499.2871 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7984 - loss: 1505.8611 - val_RMSE: 38.7150 - val_loss: 1499.3993 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7949 - loss: 1505.5833 - val_RMSE: 38.7088 - val_loss: 1498.9016 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7973 - loss: 1505.7582 - val_RMSE: 38.7086 - val_loss: 1498.8751 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7887 - loss: 1505.0756 - val_RMSE: 38.7084 - val_loss: 1498.8481 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7929 - loss: 1505.3882 - val_RMSE: 38.7085 - val_loss: 1498.8446 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7830 - loss: 1504.6169 - val_RMSE: 38.7085 - val_loss: 1498.8344 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7883 - loss: 1505.0145 - val_RMSE: 38.7087 - val_loss: 1498.8383 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7853 - loss: 1504.7732 - val_RMSE: 38.7086 - val_loss: 1498.8279 - learning_rate: 1.0000e-04\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.1967 - loss: 3085.7881 - val_RMSE: 38.7236 - val_loss: 1499.7102 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0499 - loss: 1525.0978 - val_RMSE: 38.7167 - val_loss: 1499.2147 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9143 - loss: 1514.5691 - val_RMSE: 38.7121 - val_loss: 1498.9109 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8742 - loss: 1511.5024 - val_RMSE: 38.7099 - val_loss: 1498.7930 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8603 - loss: 1510.4794 - val_RMSE: 38.7098 - val_loss: 1498.8401 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8412 - loss: 1509.0469 - val_RMSE: 38.7078 - val_loss: 1498.7269 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8449 - loss: 1509.3674 - val_RMSE: 38.7070 - val_loss: 1498.6874 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8395 - loss: 1508.9677 - val_RMSE: 38.7060 - val_loss: 1498.6287 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8370 - loss: 1508.7920 - val_RMSE: 38.7064 - val_loss: 1498.6707 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8387 - loss: 1508.9312 - val_RMSE: 38.7050 - val_loss: 1498.5713 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8339 - loss: 1508.5682 - val_RMSE: 38.7044 - val_loss: 1498.5265 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8343 - loss: 1508.6080 - val_RMSE: 38.7036 - val_loss: 1498.4749 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8323 - loss: 1508.4562 - val_RMSE: 38.7049 - val_loss: 1498.5798 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8315 - loss: 1508.3954 - val_RMSE: 38.7042 - val_loss: 1498.5349 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8267 - loss: 1508.0354 - val_RMSE: 38.7035 - val_loss: 1498.4894 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8270 - loss: 1508.0687 - val_RMSE: 38.7038 - val_loss: 1498.5228 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8238 - loss: 1507.8303 - val_RMSE: 38.7027 - val_loss: 1498.4489 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8239 - loss: 1507.8403 - val_RMSE: 38.7034 - val_loss: 1498.5072 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8246 - loss: 1507.9081 - val_RMSE: 38.7026 - val_loss: 1498.4523 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8228 - loss: 1507.7750 - val_RMSE: 38.7033 - val_loss: 1498.5188 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8126 - loss: 1506.9957 - val_RMSE: 38.7020 - val_loss: 1498.4283 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8176 - loss: 1507.3906 - val_RMSE: 38.7019 - val_loss: 1498.4310 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8181 - loss: 1507.4351 - val_RMSE: 38.7036 - val_loss: 1498.5718 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8156 - loss: 1507.2567 - val_RMSE: 38.7021 - val_loss: 1498.4636 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8133 - loss: 1507.0874 - val_RMSE: 38.7045 - val_loss: 1498.6572 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 01:40:46,962] Trial 18 finished with value: 38.69798787434896 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00010652411608589064, 'dropout_rate': 0.4509993110355638}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  28s 8ms/step - RMSE: 57.1954 - loss: 3473.9302 - val_RMSE: 38.7020 - val_loss: 1515.2155 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  15s 6ms/step - RMSE: 39.1167 - loss: 1543.0260 - val_RMSE: 38.6971 - val_loss: 1502.5275 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9993 - loss: 1525.3372 - val_RMSE: 38.6965 - val_loss: 1500.6035 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9703 - loss: 1521.5928 - val_RMSE: 38.6932 - val_loss: 1499.4594 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9657 - loss: 1520.6129 - val_RMSE: 38.6957 - val_loss: 1499.6681 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9617 - loss: 1520.3317 - val_RMSE: 38.6932 - val_loss: 1499.3479 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9611 - loss: 1520.2102 - val_RMSE: 38.6935 - val_loss: 1499.3765 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  15s 6ms/step - RMSE: 38.9617 - loss: 1520.2311 - val_RMSE: 38.6924 - val_loss: 1499.2362 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9583 - loss: 1519.9635 - val_RMSE: 38.6926 - val_loss: 1499.3032 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9523 - loss: 1519.4067 - val_RMSE: 38.6930 - val_loss: 1499.1931 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9564 - loss: 1519.7504 - val_RMSE: 38.6916 - val_loss: 1499.0542 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9477 - loss: 1519.0076 - val_RMSE: 38.6935 - val_loss: 1499.2684 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9493 - loss: 1519.1442 - val_RMSE: 38.6910 - val_loss: 1499.0090 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9479 - loss: 1519.0330 - val_RMSE: 38.6913 - val_loss: 1498.9174 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9377 - loss: 1518.1302 - val_RMSE: 38.6906 - val_loss: 1498.8359 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9322 - loss: 1517.6932 - val_RMSE: 38.6895 - val_loss: 1498.8109 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9295 - loss: 1517.5000 - val_RMSE: 38.6908 - val_loss: 1498.8977 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9335 - loss: 1517.8086 - val_RMSE: 38.6901 - val_loss: 1498.8118 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9331 - loss: 1517.7023 - val_RMSE: 38.6900 - val_loss: 1498.9238 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9332 - loss: 1517.8079 - val_RMSE: 38.6907 - val_loss: 1498.9600 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9290 - loss: 1517.4254 - val_RMSE: 38.6906 - val_loss: 1498.8538 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9144 - loss: 1515.8588 - val_RMSE: 38.6861 - val_loss: 1497.5668 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9223 - loss: 1515.8263 - val_RMSE: 38.6860 - val_loss: 1497.3218 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9161 - loss: 1515.1467 - val_RMSE: 38.6856 - val_loss: 1497.1844 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9166 - loss: 1515.0980 - val_RMSE: 38.6852 - val_loss: 1497.1039 - learning_rate: 1.0000e-04\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  30s 8ms/step - RMSE: 57.1779 - loss: 3471.2241 - val_RMSE: 38.7458 - val_loss: 1518.5850 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0603 - loss: 1538.6108 - val_RMSE: 38.7352 - val_loss: 1505.8210 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9562 - loss: 1522.1332 - val_RMSE: 38.7293 - val_loss: 1503.0566 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9251 - loss: 1518.0636 - val_RMSE: 38.7242 - val_loss: 1501.8026 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9206 - loss: 1517.1541 - val_RMSE: 38.7234 - val_loss: 1501.9583 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9142 - loss: 1516.7419 - val_RMSE: 38.7213 - val_loss: 1501.7061 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9061 - loss: 1516.0527 - val_RMSE: 38.7223 - val_loss: 1501.6149 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8986 - loss: 1515.3788 - val_RMSE: 38.7211 - val_loss: 1501.5238 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9041 - loss: 1515.7919 - val_RMSE: 38.7224 - val_loss: 1501.5063 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9031 - loss: 1515.6868 - val_RMSE: 38.7253 - val_loss: 1501.7135 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8989 - loss: 1515.2574 - val_RMSE: 38.7205 - val_loss: 1501.5234 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9019 - loss: 1515.5226 - val_RMSE: 38.7202 - val_loss: 1501.2913 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8955 - loss: 1514.9628 - val_RMSE: 38.7188 - val_loss: 1501.3521 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8919 - loss: 1514.7085 - val_RMSE: 38.7213 - val_loss: 1501.3600 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8957 - loss: 1514.9402 - val_RMSE: 38.7224 - val_loss: 1501.4587 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8848 - loss: 1514.0964 - val_RMSE: 38.7210 - val_loss: 1501.2278 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8892 - loss: 1514.3898 - val_RMSE: 38.7200 - val_loss: 1501.2106 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8819 - loss: 1513.8618 - val_RMSE: 38.7201 - val_loss: 1501.2452 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8764 - loss: 1513.3997 - val_RMSE: 38.7203 - val_loss: 1501.2402 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8771 - loss: 1513.4261 - val_RMSE: 38.7196 - val_loss: 1501.1266 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8731 - loss: 1513.1027 - val_RMSE: 38.7211 - val_loss: 1501.2502 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8713 - loss: 1512.9431 - val_RMSE: 38.7221 - val_loss: 1501.3231 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8722 - loss: 1512.9980 - val_RMSE: 38.7187 - val_loss: 1501.0776 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8708 - loss: 1512.8766 - val_RMSE: 38.7192 - val_loss: 1501.1128 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8710 - loss: 1512.8568 - val_RMSE: 38.7192 - val_loss: 1501.0691 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  28s 8ms/step - RMSE: 57.1586 - loss: 3468.8442 - val_RMSE: 38.7308 - val_loss: 1518.0854 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  16s 6ms/step - RMSE: 39.0940 - loss: 1542.0193 - val_RMSE: 38.7218 - val_loss: 1504.8708 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9738 - loss: 1523.6556 - val_RMSE: 38.7192 - val_loss: 1502.2158 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9475 - loss: 1519.7352 - val_RMSE: 38.7162 - val_loss: 1501.1461 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9395 - loss: 1518.5706 - val_RMSE: 38.7153 - val_loss: 1501.1970 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9397 - loss: 1518.6503 - val_RMSE: 38.7170 - val_loss: 1501.2604 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9350 - loss: 1518.2765 - val_RMSE: 38.7143 - val_loss: 1500.9779 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9308 - loss: 1517.8375 - val_RMSE: 38.7154 - val_loss: 1501.0334 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9315 - loss: 1517.8890 - val_RMSE: 38.7131 - val_loss: 1500.8451 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9334 - loss: 1517.9983 - val_RMSE: 38.7131 - val_loss: 1500.8263 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9250 - loss: 1517.3690 - val_RMSE: 38.7133 - val_loss: 1500.6941 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9186 - loss: 1516.7828 - val_RMSE: 38.7143 - val_loss: 1500.9939 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9247 - loss: 1517.2903 - val_RMSE: 38.7143 - val_loss: 1500.7407 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9185 - loss: 1516.7043 - val_RMSE: 38.7149 - val_loss: 1501.0273 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9173 - loss: 1516.6989 - val_RMSE: 38.7129 - val_loss: 1500.6477 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9170 - loss: 1516.5720 - val_RMSE: 38.7133 - val_loss: 1500.6489 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9109 - loss: 1516.0662 - val_RMSE: 38.7128 - val_loss: 1500.7288 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9071 - loss: 1515.8297 - val_RMSE: 38.7115 - val_loss: 1500.4503 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9111 - loss: 1516.0710 - val_RMSE: 38.7126 - val_loss: 1500.6029 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9004 - loss: 1515.2291 - val_RMSE: 38.7126 - val_loss: 1500.6469 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9030 - loss: 1515.4309 - val_RMSE: 38.7118 - val_loss: 1500.5255 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8961 - loss: 1514.8849 - val_RMSE: 38.7118 - val_loss: 1500.3914 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.9016 - loss: 1515.2128 - val_RMSE: 38.7141 - val_loss: 1500.5745 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8976 - loss: 1514.9122 - val_RMSE: 38.7143 - val_loss: 1500.6284 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  16s 6ms/step - RMSE: 38.8964 - loss: 1514.8342 - val_RMSE: 38.7157 - val_loss: 1500.7217 - learning_rate: 0.0010\n",
            "41608/41608  85s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 02:07:06,390] Trial 19 finished with value: 38.706722259521484 and parameters: {'units': 256, 'last_layer': 1, 'activation': 'gelu', 'reg': 0.08811299737907308, 'dropout_rate': 0.4052062072468139}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.0384 - loss: 3071.1411 - val_RMSE: 38.7142 - val_loss: 1500.9333 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0361 - loss: 1526.0258 - val_RMSE: 38.6967 - val_loss: 1499.7330 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9130 - loss: 1516.5133 - val_RMSE: 38.6891 - val_loss: 1499.0563 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8749 - loss: 1513.4211 - val_RMSE: 38.6889 - val_loss: 1498.7920 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8519 - loss: 1511.3660 - val_RMSE: 38.6881 - val_loss: 1498.4183 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8443 - loss: 1510.4503 - val_RMSE: 38.6856 - val_loss: 1497.9375 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8365 - loss: 1509.5820 - val_RMSE: 38.6860 - val_loss: 1497.7517 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8354 - loss: 1509.2983 - val_RMSE: 38.6856 - val_loss: 1497.6060 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8336 - loss: 1509.0789 - val_RMSE: 38.6861 - val_loss: 1497.6204 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8335 - loss: 1509.0549 - val_RMSE: 38.6848 - val_loss: 1497.5051 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8340 - loss: 1509.0795 - val_RMSE: 38.6846 - val_loss: 1497.4790 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8298 - loss: 1508.7493 - val_RMSE: 38.6844 - val_loss: 1497.4545 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8362 - loss: 1509.2292 - val_RMSE: 38.6840 - val_loss: 1497.4305 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8289 - loss: 1508.6766 - val_RMSE: 38.6843 - val_loss: 1497.4526 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8312 - loss: 1508.8406 - val_RMSE: 38.6832 - val_loss: 1497.3586 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8247 - loss: 1508.3398 - val_RMSE: 38.6833 - val_loss: 1497.3785 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8261 - loss: 1508.4633 - val_RMSE: 38.6837 - val_loss: 1497.4025 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8226 - loss: 1508.1925 - val_RMSE: 38.6833 - val_loss: 1497.3867 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8254 - loss: 1508.4210 - val_RMSE: 38.6833 - val_loss: 1497.3918 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8276 - loss: 1508.5923 - val_RMSE: 38.6823 - val_loss: 1497.3270 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8223 - loss: 1508.1713 - val_RMSE: 38.6819 - val_loss: 1497.2715 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8249 - loss: 1508.3746 - val_RMSE: 38.6822 - val_loss: 1497.2911 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8235 - loss: 1508.2632 - val_RMSE: 38.6819 - val_loss: 1497.2640 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8207 - loss: 1508.0426 - val_RMSE: 38.6826 - val_loss: 1497.3433 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8220 - loss: 1508.1719 - val_RMSE: 38.6823 - val_loss: 1497.3188 - learning_rate: 0.0010\n",
            "41608/41608  86s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.0019 - loss: 3066.4866 - val_RMSE: 38.7354 - val_loss: 1502.5933 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9869 - loss: 1522.1981 - val_RMSE: 38.7256 - val_loss: 1502.0167 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8696 - loss: 1513.2081 - val_RMSE: 38.7239 - val_loss: 1501.8569 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8212 - loss: 1509.3451 - val_RMSE: 38.7208 - val_loss: 1501.3527 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8023 - loss: 1507.5884 - val_RMSE: 38.7185 - val_loss: 1500.8442 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7922 - loss: 1506.4725 - val_RMSE: 38.7183 - val_loss: 1500.5333 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7862 - loss: 1505.7325 - val_RMSE: 38.7167 - val_loss: 1500.1764 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7869 - loss: 1505.5648 - val_RMSE: 38.7172 - val_loss: 1500.0515 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7854 - loss: 1505.3170 - val_RMSE: 38.7173 - val_loss: 1500.0198 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7815 - loss: 1504.9998 - val_RMSE: 38.7173 - val_loss: 1500.0129 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7798 - loss: 1504.8539 - val_RMSE: 38.7163 - val_loss: 1499.9261 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7812 - loss: 1504.9542 - val_RMSE: 38.7168 - val_loss: 1499.9580 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7781 - loss: 1504.7061 - val_RMSE: 38.7181 - val_loss: 1500.0417 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7781 - loss: 1504.7037 - val_RMSE: 38.7178 - val_loss: 1500.0302 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7808 - loss: 1504.9240 - val_RMSE: 38.7188 - val_loss: 1500.1210 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7756 - loss: 1504.5253 - val_RMSE: 38.7173 - val_loss: 1500.0052 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7756 - loss: 1504.4865 - val_RMSE: 38.7097 - val_loss: 1499.2786 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7702 - loss: 1503.9390 - val_RMSE: 38.7095 - val_loss: 1499.1625 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7666 - loss: 1503.5729 - val_RMSE: 38.7092 - val_loss: 1499.0795 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7697 - loss: 1503.7505 - val_RMSE: 38.7092 - val_loss: 1499.0214 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7644 - loss: 1503.2910 - val_RMSE: 38.7092 - val_loss: 1498.9847 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7707 - loss: 1503.7433 - val_RMSE: 38.7091 - val_loss: 1498.9496 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7616 - loss: 1503.0100 - val_RMSE: 38.7090 - val_loss: 1498.9152 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7664 - loss: 1503.3531 - val_RMSE: 38.7090 - val_loss: 1498.8895 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7642 - loss: 1503.1687 - val_RMSE: 38.7090 - val_loss: 1498.8744 - learning_rate: 1.0000e-04\n",
            "41608/41608  87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 53.9871 - loss: 3064.5459 - val_RMSE: 38.7325 - val_loss: 1502.3867 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0128 - loss: 1524.2484 - val_RMSE: 38.7143 - val_loss: 1501.1544 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8919 - loss: 1514.9514 - val_RMSE: 38.7147 - val_loss: 1501.1276 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8487 - loss: 1511.4740 - val_RMSE: 38.7109 - val_loss: 1500.5569 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8346 - loss: 1510.0759 - val_RMSE: 38.7098 - val_loss: 1500.1423 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8176 - loss: 1508.4363 - val_RMSE: 38.7084 - val_loss: 1499.7786 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8156 - loss: 1508.0280 - val_RMSE: 38.7078 - val_loss: 1499.5171 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8111 - loss: 1507.4880 - val_RMSE: 38.7059 - val_loss: 1499.2349 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8094 - loss: 1507.2377 - val_RMSE: 38.7074 - val_loss: 1499.2845 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8119 - loss: 1507.3927 - val_RMSE: 38.7058 - val_loss: 1499.1450 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8076 - loss: 1507.0417 - val_RMSE: 38.7066 - val_loss: 1499.1943 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8069 - loss: 1506.9802 - val_RMSE: 38.7049 - val_loss: 1499.0840 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8039 - loss: 1506.7445 - val_RMSE: 38.7058 - val_loss: 1499.1261 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8066 - loss: 1506.9498 - val_RMSE: 38.7050 - val_loss: 1499.0845 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8027 - loss: 1506.6494 - val_RMSE: 38.7054 - val_loss: 1499.1024 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8048 - loss: 1506.8075 - val_RMSE: 38.7062 - val_loss: 1499.1653 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8049 - loss: 1506.8127 - val_RMSE: 38.7051 - val_loss: 1499.0726 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8022 - loss: 1506.6107 - val_RMSE: 38.7046 - val_loss: 1499.0479 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8015 - loss: 1506.5698 - val_RMSE: 38.7045 - val_loss: 1499.0398 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8015 - loss: 1506.5397 - val_RMSE: 38.7049 - val_loss: 1499.0450 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7939 - loss: 1505.9456 - val_RMSE: 38.7041 - val_loss: 1498.9883 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8000 - loss: 1506.4276 - val_RMSE: 38.7045 - val_loss: 1499.0416 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8001 - loss: 1506.4540 - val_RMSE: 38.7044 - val_loss: 1499.0538 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7984 - loss: 1506.3348 - val_RMSE: 38.7046 - val_loss: 1499.0408 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7958 - loss: 1506.1191 - val_RMSE: 38.7032 - val_loss: 1498.9443 - learning_rate: 0.0010\n",
            "41608/41608  87s 2ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-02-16 02:38:07,287] Trial 20 finished with value: 38.698177337646484 and parameters: {'units': 512, 'last_layer': 2, 'activation': 'gelu', 'reg': 0.001253520376146537, 'dropout_rate': 0.383273235469179}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  32s 9ms/step - RMSE: 54.2278 - loss: 3090.4089 - val_RMSE: 38.7251 - val_loss: 1500.1178 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0667 - loss: 1526.7211 - val_RMSE: 38.6989 - val_loss: 1498.1919 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.9381 - loss: 1516.7971 - val_RMSE: 38.6926 - val_loss: 1497.8243 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8963 - loss: 1513.6533 - val_RMSE: 38.6882 - val_loss: 1497.5773 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8748 - loss: 1512.0690 - val_RMSE: 38.6857 - val_loss: 1497.4275 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8683 - loss: 1511.5898 - val_RMSE: 38.6845 - val_loss: 1497.3188 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8640 - loss: 1511.2229 - val_RMSE: 38.6848 - val_loss: 1497.2889 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8605 - loss: 1510.9011 - val_RMSE: 38.6833 - val_loss: 1497.1279 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8586 - loss: 1510.7120 - val_RMSE: 38.6836 - val_loss: 1497.1306 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8591 - loss: 1510.7355 - val_RMSE: 38.6826 - val_loss: 1497.0344 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8588 - loss: 1510.6996 - val_RMSE: 38.6817 - val_loss: 1496.9689 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8521 - loss: 1510.1796 - val_RMSE: 38.6818 - val_loss: 1496.9781 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8587 - loss: 1510.7000 - val_RMSE: 38.6816 - val_loss: 1496.9597 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8497 - loss: 1509.9961 - val_RMSE: 38.6817 - val_loss: 1496.9636 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8527 - loss: 1510.2360 - val_RMSE: 38.6812 - val_loss: 1496.9463 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8459 - loss: 1509.7155 - val_RMSE: 38.6816 - val_loss: 1496.9846 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8490 - loss: 1509.9684 - val_RMSE: 38.6807 - val_loss: 1496.9202 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8438 - loss: 1509.5717 - val_RMSE: 38.6810 - val_loss: 1496.9476 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8468 - loss: 1509.8046 - val_RMSE: 38.6806 - val_loss: 1496.9320 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8482 - loss: 1509.9277 - val_RMSE: 38.6807 - val_loss: 1496.9377 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8413 - loss: 1509.3867 - val_RMSE: 38.6804 - val_loss: 1496.9171 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8460 - loss: 1509.7573 - val_RMSE: 38.6805 - val_loss: 1496.9319 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8430 - loss: 1509.5291 - val_RMSE: 38.6805 - val_loss: 1496.9321 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8369 - loss: 1509.0646 - val_RMSE: 38.6803 - val_loss: 1496.9240 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8379 - loss: 1509.1442 - val_RMSE: 38.6798 - val_loss: 1496.9020 - learning_rate: 0.0010\n",
            "41608/41608  87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  31s 9ms/step - RMSE: 54.1753 - loss: 3083.8904 - val_RMSE: 38.7344 - val_loss: 1500.8442 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  19s 7ms/step - RMSE: 39.0260 - loss: 1523.5448 - val_RMSE: 38.7225 - val_loss: 1500.0184 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8898 - loss: 1513.0251 - val_RMSE: 38.7197 - val_loss: 1499.8984 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8396 - loss: 1509.2179 - val_RMSE: 38.7181 - val_loss: 1499.8555 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8256 - loss: 1508.2124 - val_RMSE: 38.7181 - val_loss: 1499.9015 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8166 - loss: 1507.5430 - val_RMSE: 38.7169 - val_loss: 1499.8066 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8138 - loss: 1507.3083 - val_RMSE: 38.7160 - val_loss: 1499.6951 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8150 - loss: 1507.3573 - val_RMSE: 38.7165 - val_loss: 1499.6984 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8134 - loss: 1507.2026 - val_RMSE: 38.7148 - val_loss: 1499.5537 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8068 - loss: 1506.6768 - val_RMSE: 38.7147 - val_loss: 1499.5270 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8034 - loss: 1506.4026 - val_RMSE: 38.7146 - val_loss: 1499.5198 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8047 - loss: 1506.5060 - val_RMSE: 38.7145 - val_loss: 1499.5148 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8013 - loss: 1506.2509 - val_RMSE: 38.7147 - val_loss: 1499.5470 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.8000 - loss: 1506.1589 - val_RMSE: 38.7144 - val_loss: 1499.5309 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8043 - loss: 1506.4999 - val_RMSE: 38.7144 - val_loss: 1499.5460 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7966 - loss: 1505.9175 - val_RMSE: 38.7138 - val_loss: 1499.5031 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8050 - loss: 1506.5771 - val_RMSE: 38.7140 - val_loss: 1499.5243 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.7977 - loss: 1506.0137 - val_RMSE: 38.7148 - val_loss: 1499.5895 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  19s 7ms/step - RMSE: 38.7950 - loss: 1505.8059 - val_RMSE: 38.7138 - val_loss: 1499.5175 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8011 - loss: 1506.2845 - val_RMSE: 38.7130 - val_loss: 1499.4683 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.7932 - loss: 1505.6808 - val_RMSE: 38.7146 - val_loss: 1499.5977 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7970 - loss: 1505.9814 - val_RMSE: 38.7146 - val_loss: 1499.5991 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7873 - loss: 1505.2322 - val_RMSE: 38.7132 - val_loss: 1499.4913 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7893 - loss: 1505.3969 - val_RMSE: 38.7138 - val_loss: 1499.5492 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7899 - loss: 1505.4431 - val_RMSE: 38.7137 - val_loss: 1499.5426 - learning_rate: 0.0010\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.1695 - loss: 3083.0068 - val_RMSE: 38.7236 - val_loss: 1500.0024 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0481 - loss: 1525.2650 - val_RMSE: 38.7175 - val_loss: 1499.6311 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9107 - loss: 1514.6559 - val_RMSE: 38.7133 - val_loss: 1499.4006 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8713 - loss: 1511.6849 - val_RMSE: 38.7095 - val_loss: 1499.1990 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8561 - loss: 1510.5833 - val_RMSE: 38.7087 - val_loss: 1499.1779 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8389 - loss: 1509.2760 - val_RMSE: 38.7076 - val_loss: 1499.0699 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8417 - loss: 1509.4633 - val_RMSE: 38.7064 - val_loss: 1498.9381 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8383 - loss: 1509.1564 - val_RMSE: 38.7057 - val_loss: 1498.8528 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8345 - loss: 1508.8416 - val_RMSE: 38.7069 - val_loss: 1498.9324 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8368 - loss: 1509.0072 - val_RMSE: 38.7053 - val_loss: 1498.8063 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8312 - loss: 1508.5721 - val_RMSE: 38.7051 - val_loss: 1498.7968 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8313 - loss: 1508.5808 - val_RMSE: 38.7041 - val_loss: 1498.7208 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8297 - loss: 1508.4636 - val_RMSE: 38.7046 - val_loss: 1498.7661 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8301 - loss: 1508.4952 - val_RMSE: 38.7042 - val_loss: 1498.7338 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8247 - loss: 1508.0795 - val_RMSE: 38.7038 - val_loss: 1498.7101 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8252 - loss: 1508.1283 - val_RMSE: 38.7034 - val_loss: 1498.6868 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8228 - loss: 1507.9518 - val_RMSE: 38.7039 - val_loss: 1498.7324 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8224 - loss: 1507.9238 - val_RMSE: 38.7029 - val_loss: 1498.6663 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8237 - loss: 1508.0353 - val_RMSE: 38.7023 - val_loss: 1498.6287 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8227 - loss: 1507.9691 - val_RMSE: 38.7027 - val_loss: 1498.6659 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8133 - loss: 1507.2393 - val_RMSE: 38.7020 - val_loss: 1498.6255 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8179 - loss: 1507.6034 - val_RMSE: 38.7018 - val_loss: 1498.6021 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8183 - loss: 1507.6455 - val_RMSE: 38.7034 - val_loss: 1498.7487 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8161 - loss: 1507.4789 - val_RMSE: 38.7021 - val_loss: 1498.6378 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8130 - loss: 1507.2428 - val_RMSE: 38.7036 - val_loss: 1498.7642 - learning_rate: 0.0010\n",
            "41608/41608  90s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 03:10:03,649] Trial 21 finished with value: 38.69902038574219 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00026956737552824854, 'dropout_rate': 0.4450751108802483}. Best is trial 1 with value: 38.696816762288414.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.3208 - loss: 3100.5127 - val_RMSE: 38.7158 - val_loss: 1499.3030 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0905 - loss: 1528.4830 - val_RMSE: 38.6945 - val_loss: 1497.7473 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9464 - loss: 1517.3270 - val_RMSE: 38.6909 - val_loss: 1497.5587 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9080 - loss: 1514.4309 - val_RMSE: 38.6876 - val_loss: 1497.3884 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8864 - loss: 1512.8289 - val_RMSE: 38.6866 - val_loss: 1497.3728 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8823 - loss: 1512.5542 - val_RMSE: 38.6843 - val_loss: 1497.2012 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8777 - loss: 1512.1973 - val_RMSE: 38.6838 - val_loss: 1497.1503 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8734 - loss: 1511.8530 - val_RMSE: 38.6831 - val_loss: 1497.0818 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8727 - loss: 1511.7786 - val_RMSE: 38.6836 - val_loss: 1497.1160 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8721 - loss: 1511.7355 - val_RMSE: 38.6828 - val_loss: 1497.0492 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8731 - loss: 1511.8104 - val_RMSE: 38.6819 - val_loss: 1496.9794 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8648 - loss: 1511.1669 - val_RMSE: 38.6822 - val_loss: 1497.0123 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8720 - loss: 1511.7264 - val_RMSE: 38.6823 - val_loss: 1497.0066 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8605 - loss: 1510.8334 - val_RMSE: 38.6821 - val_loss: 1497.0029 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8672 - loss: 1511.3625 - val_RMSE: 38.6815 - val_loss: 1496.9563 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8572 - loss: 1510.5892 - val_RMSE: 38.6814 - val_loss: 1496.9607 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8613 - loss: 1510.9116 - val_RMSE: 38.6809 - val_loss: 1496.9263 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8557 - loss: 1510.4866 - val_RMSE: 38.6811 - val_loss: 1496.9486 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8564 - loss: 1510.5496 - val_RMSE: 38.6810 - val_loss: 1496.9546 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8597 - loss: 1510.8116 - val_RMSE: 38.6814 - val_loss: 1496.9923 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8535 - loss: 1510.3367 - val_RMSE: 38.6811 - val_loss: 1496.9855 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8554 - loss: 1510.5006 - val_RMSE: 38.6810 - val_loss: 1496.9847 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8487 - loss: 1509.9783 - val_RMSE: 38.6790 - val_loss: 1496.7957 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8426 - loss: 1509.4738 - val_RMSE: 38.6788 - val_loss: 1496.7542 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8437 - loss: 1509.5366 - val_RMSE: 38.6787 - val_loss: 1496.7229 - learning_rate: 1.0000e-04\n",
            "41608/41608  91s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.2604 - loss: 3093.1023 - val_RMSE: 38.7338 - val_loss: 1500.6934 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0424 - loss: 1524.7195 - val_RMSE: 38.7224 - val_loss: 1499.8884 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8997 - loss: 1513.6796 - val_RMSE: 38.7198 - val_loss: 1499.7743 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8526 - loss: 1510.0967 - val_RMSE: 38.7192 - val_loss: 1499.8121 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8367 - loss: 1508.9478 - val_RMSE: 38.7175 - val_loss: 1499.7349 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8296 - loss: 1508.4362 - val_RMSE: 38.7173 - val_loss: 1499.7438 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8281 - loss: 1508.3344 - val_RMSE: 38.7166 - val_loss: 1499.6716 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8283 - loss: 1508.3250 - val_RMSE: 38.7163 - val_loss: 1499.6329 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8280 - loss: 1508.2902 - val_RMSE: 38.7150 - val_loss: 1499.5267 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8199 - loss: 1507.6559 - val_RMSE: 38.7147 - val_loss: 1499.4972 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8151 - loss: 1507.2803 - val_RMSE: 38.7138 - val_loss: 1499.4309 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8192 - loss: 1507.6010 - val_RMSE: 38.7140 - val_loss: 1499.4523 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8156 - loss: 1507.3221 - val_RMSE: 38.7140 - val_loss: 1499.4518 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8118 - loss: 1507.0330 - val_RMSE: 38.7149 - val_loss: 1499.5189 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8178 - loss: 1507.5011 - val_RMSE: 38.7144 - val_loss: 1499.4840 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8098 - loss: 1506.8849 - val_RMSE: 38.7141 - val_loss: 1499.4712 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8145 - loss: 1507.2468 - val_RMSE: 38.7084 - val_loss: 1498.9991 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8040 - loss: 1506.4032 - val_RMSE: 38.7081 - val_loss: 1498.9456 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8022 - loss: 1506.2330 - val_RMSE: 38.7079 - val_loss: 1498.9111 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8066 - loss: 1506.5564 - val_RMSE: 38.7078 - val_loss: 1498.8834 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8003 - loss: 1506.0463 - val_RMSE: 38.7078 - val_loss: 1498.8658 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8064 - loss: 1506.5037 - val_RMSE: 38.7078 - val_loss: 1498.8503 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7963 - loss: 1505.7012 - val_RMSE: 38.7078 - val_loss: 1498.8289 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7990 - loss: 1505.8971 - val_RMSE: 38.7077 - val_loss: 1498.8143 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.7989 - loss: 1505.8805 - val_RMSE: 38.7078 - val_loss: 1498.8075 - learning_rate: 1.0000e-04\n",
            "41608/41608  90s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  36s 10ms/step - RMSE: 54.2571 - loss: 3092.4207 - val_RMSE: 38.7283 - val_loss: 1500.2661 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0618 - loss: 1526.2323 - val_RMSE: 38.7137 - val_loss: 1499.2183 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9238 - loss: 1515.5464 - val_RMSE: 38.7104 - val_loss: 1499.0369 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8829 - loss: 1512.4448 - val_RMSE: 38.7091 - val_loss: 1499.0220 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8683 - loss: 1511.3890 - val_RMSE: 38.7083 - val_loss: 1499.0106 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8521 - loss: 1510.1748 - val_RMSE: 38.7077 - val_loss: 1498.9742 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8557 - loss: 1510.4541 - val_RMSE: 38.7066 - val_loss: 1498.8805 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8502 - loss: 1510.0190 - val_RMSE: 38.7059 - val_loss: 1498.8292 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8468 - loss: 1509.7466 - val_RMSE: 38.7065 - val_loss: 1498.8591 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8500 - loss: 1509.9894 - val_RMSE: 38.7048 - val_loss: 1498.7224 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8453 - loss: 1509.6240 - val_RMSE: 38.7048 - val_loss: 1498.7218 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8445 - loss: 1509.5668 - val_RMSE: 38.7035 - val_loss: 1498.6393 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8431 - loss: 1509.4688 - val_RMSE: 38.7042 - val_loss: 1498.6980 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8429 - loss: 1509.4601 - val_RMSE: 38.7042 - val_loss: 1498.7042 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8383 - loss: 1509.1041 - val_RMSE: 38.7028 - val_loss: 1498.6055 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8362 - loss: 1508.9556 - val_RMSE: 38.7037 - val_loss: 1498.6831 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8330 - loss: 1508.7144 - val_RMSE: 38.7025 - val_loss: 1498.5972 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8339 - loss: 1508.7906 - val_RMSE: 38.7024 - val_loss: 1498.5980 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8357 - loss: 1508.9348 - val_RMSE: 38.7022 - val_loss: 1498.5911 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8339 - loss: 1508.8019 - val_RMSE: 38.7027 - val_loss: 1498.6316 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8238 - loss: 1508.0271 - val_RMSE: 38.7012 - val_loss: 1498.5250 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8275 - loss: 1508.3156 - val_RMSE: 38.7011 - val_loss: 1498.5302 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8277 - loss: 1508.3470 - val_RMSE: 38.7032 - val_loss: 1498.7075 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8252 - loss: 1508.1738 - val_RMSE: 38.7020 - val_loss: 1498.6212 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8236 - loss: 1508.0454 - val_RMSE: 38.7028 - val_loss: 1498.6865 - learning_rate: 0.0010\n",
            "41608/41608  88s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 03:42:28,918] Trial 22 finished with value: 38.69640350341797 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002133354415296107, 'dropout_rate': 0.47090232042440616}. Best is trial 22 with value: 38.69640350341797.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.3926 - loss: 3108.2227 - val_RMSE: 38.7105 - val_loss: 1498.8347 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.1030 - loss: 1529.3966 - val_RMSE: 38.6939 - val_loss: 1497.6128 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9579 - loss: 1518.1373 - val_RMSE: 38.6908 - val_loss: 1497.4556 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9221 - loss: 1515.4268 - val_RMSE: 38.6872 - val_loss: 1497.2570 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9022 - loss: 1513.9501 - val_RMSE: 38.6860 - val_loss: 1497.2203 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8956 - loss: 1513.4907 - val_RMSE: 38.6842 - val_loss: 1497.1089 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8931 - loss: 1513.3197 - val_RMSE: 38.6840 - val_loss: 1497.1019 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8883 - loss: 1512.9509 - val_RMSE: 38.6836 - val_loss: 1497.0714 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8847 - loss: 1512.6777 - val_RMSE: 38.6841 - val_loss: 1497.1112 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8871 - loss: 1512.8662 - val_RMSE: 38.6830 - val_loss: 1497.0245 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8866 - loss: 1512.8291 - val_RMSE: 38.6824 - val_loss: 1496.9918 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8792 - loss: 1512.2565 - val_RMSE: 38.6822 - val_loss: 1496.9862 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8865 - loss: 1512.8339 - val_RMSE: 38.6813 - val_loss: 1496.9243 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8736 - loss: 1511.8363 - val_RMSE: 38.6814 - val_loss: 1496.9330 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8801 - loss: 1512.3489 - val_RMSE: 38.6810 - val_loss: 1496.9075 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8719 - loss: 1511.7194 - val_RMSE: 38.6807 - val_loss: 1496.8986 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8725 - loss: 1511.7711 - val_RMSE: 38.6801 - val_loss: 1496.8505 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8668 - loss: 1511.3331 - val_RMSE: 38.6802 - val_loss: 1496.8661 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8687 - loss: 1511.4895 - val_RMSE: 38.6802 - val_loss: 1496.8788 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8725 - loss: 1511.7947 - val_RMSE: 38.6797 - val_loss: 1496.8425 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8640 - loss: 1511.1376 - val_RMSE: 38.6804 - val_loss: 1496.9043 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8661 - loss: 1511.3091 - val_RMSE: 38.6798 - val_loss: 1496.8618 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8657 - loss: 1511.2834 - val_RMSE: 38.6800 - val_loss: 1496.8942 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8597 - loss: 1510.8302 - val_RMSE: 38.6799 - val_loss: 1496.8971 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8606 - loss: 1510.9110 - val_RMSE: 38.6797 - val_loss: 1496.8875 - learning_rate: 0.0010\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.3526 - loss: 3103.1470 - val_RMSE: 38.7315 - val_loss: 1500.4688 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0578 - loss: 1525.8635 - val_RMSE: 38.7208 - val_loss: 1499.7095 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9140 - loss: 1514.7240 - val_RMSE: 38.7194 - val_loss: 1499.6794 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8650 - loss: 1510.9910 - val_RMSE: 38.7180 - val_loss: 1499.6410 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8501 - loss: 1509.9091 - val_RMSE: 38.7170 - val_loss: 1499.6246 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8459 - loss: 1509.6292 - val_RMSE: 38.7166 - val_loss: 1499.6102 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8440 - loss: 1509.4971 - val_RMSE: 38.7168 - val_loss: 1499.6326 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8422 - loss: 1509.3621 - val_RMSE: 38.7165 - val_loss: 1499.6129 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8419 - loss: 1509.3339 - val_RMSE: 38.7151 - val_loss: 1499.5034 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8336 - loss: 1508.6904 - val_RMSE: 38.7153 - val_loss: 1499.5222 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8306 - loss: 1508.4633 - val_RMSE: 38.7148 - val_loss: 1499.4825 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8329 - loss: 1508.6409 - val_RMSE: 38.7149 - val_loss: 1499.4979 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8289 - loss: 1508.3391 - val_RMSE: 38.7149 - val_loss: 1499.5079 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8249 - loss: 1508.0361 - val_RMSE: 38.7153 - val_loss: 1499.5428 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8320 - loss: 1508.5900 - val_RMSE: 38.7147 - val_loss: 1499.5044 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8220 - loss: 1507.8259 - val_RMSE: 38.7139 - val_loss: 1499.4518 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8304 - loss: 1508.4857 - val_RMSE: 38.7136 - val_loss: 1499.4414 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8219 - loss: 1507.8411 - val_RMSE: 38.7153 - val_loss: 1499.5741 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8208 - loss: 1507.7557 - val_RMSE: 38.7149 - val_loss: 1499.5541 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8251 - loss: 1508.1027 - val_RMSE: 38.7133 - val_loss: 1499.4435 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8171 - loss: 1507.4922 - val_RMSE: 38.7144 - val_loss: 1499.5375 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8227 - loss: 1507.9366 - val_RMSE: 38.7145 - val_loss: 1499.5503 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8077 - loss: 1506.7759 - val_RMSE: 38.7085 - val_loss: 1499.0688 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8092 - loss: 1506.8654 - val_RMSE: 38.7085 - val_loss: 1499.0408 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8088 - loss: 1506.8107 - val_RMSE: 38.7085 - val_loss: 1499.0178 - learning_rate: 1.0000e-04\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  34s 10ms/step - RMSE: 54.3420 - loss: 3101.5876 - val_RMSE: 38.7273 - val_loss: 1500.1343 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0831 - loss: 1527.8464 - val_RMSE: 38.7149 - val_loss: 1499.2526 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9375 - loss: 1516.5575 - val_RMSE: 38.7110 - val_loss: 1499.0278 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8972 - loss: 1513.5001 - val_RMSE: 38.7095 - val_loss: 1498.9880 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8800 - loss: 1512.2302 - val_RMSE: 38.7090 - val_loss: 1499.0065 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8669 - loss: 1511.2573 - val_RMSE: 38.7081 - val_loss: 1498.9573 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8720 - loss: 1511.6740 - val_RMSE: 38.7076 - val_loss: 1498.9209 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8660 - loss: 1511.2126 - val_RMSE: 38.7062 - val_loss: 1498.8158 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8617 - loss: 1510.8754 - val_RMSE: 38.7073 - val_loss: 1498.8992 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8653 - loss: 1511.1545 - val_RMSE: 38.7055 - val_loss: 1498.7643 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8594 - loss: 1510.7045 - val_RMSE: 38.7061 - val_loss: 1498.8151 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8592 - loss: 1510.6851 - val_RMSE: 38.7047 - val_loss: 1498.7052 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8546 - loss: 1510.3394 - val_RMSE: 38.7050 - val_loss: 1498.7295 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8552 - loss: 1510.3861 - val_RMSE: 38.7035 - val_loss: 1498.6238 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8519 - loss: 1510.1377 - val_RMSE: 38.7040 - val_loss: 1498.6747 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8495 - loss: 1509.9532 - val_RMSE: 38.7043 - val_loss: 1498.6893 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8494 - loss: 1509.9414 - val_RMSE: 38.7031 - val_loss: 1498.6071 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8474 - loss: 1509.7963 - val_RMSE: 38.7037 - val_loss: 1498.6588 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8482 - loss: 1509.8666 - val_RMSE: 38.7033 - val_loss: 1498.6365 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8471 - loss: 1509.7942 - val_RMSE: 38.7037 - val_loss: 1498.6721 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8384 - loss: 1509.1168 - val_RMSE: 38.7041 - val_loss: 1498.7097 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8426 - loss: 1509.4498 - val_RMSE: 38.7027 - val_loss: 1498.6167 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8352 - loss: 1508.8816 - val_RMSE: 38.7016 - val_loss: 1498.5007 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8333 - loss: 1508.7089 - val_RMSE: 38.7016 - val_loss: 1498.4830 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8312 - loss: 1508.5260 - val_RMSE: 38.7016 - val_loss: 1498.4552 - learning_rate: 1.0000e-04\n",
            "41608/41608  92s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 04:14:41,087] Trial 23 finished with value: 38.69656880696615 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00018285344196860296, 'dropout_rate': 0.4958852821505541}. Best is trial 22 with value: 38.69640350341797.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  35s 10ms/step - RMSE: 54.3742 - loss: 3106.7458 - val_RMSE: 38.7131 - val_loss: 1499.6084 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.1008 - loss: 1529.8162 - val_RMSE: 38.6954 - val_loss: 1498.3958 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9560 - loss: 1518.6693 - val_RMSE: 38.6901 - val_loss: 1498.1415 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9177 - loss: 1515.8304 - val_RMSE: 38.6869 - val_loss: 1497.9764 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8996 - loss: 1514.4752 - val_RMSE: 38.6867 - val_loss: 1497.9011 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8926 - loss: 1513.8458 - val_RMSE: 38.6844 - val_loss: 1497.5876 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8902 - loss: 1513.5194 - val_RMSE: 38.6845 - val_loss: 1497.4938 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8858 - loss: 1513.0938 - val_RMSE: 38.6845 - val_loss: 1497.4396 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8831 - loss: 1512.8446 - val_RMSE: 38.6848 - val_loss: 1497.4486 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8842 - loss: 1512.9211 - val_RMSE: 38.6838 - val_loss: 1497.3635 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8854 - loss: 1513.0110 - val_RMSE: 38.6829 - val_loss: 1497.3047 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8781 - loss: 1512.4506 - val_RMSE: 38.6824 - val_loss: 1497.2651 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8855 - loss: 1513.0215 - val_RMSE: 38.6829 - val_loss: 1497.3037 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8733 - loss: 1512.0736 - val_RMSE: 38.6833 - val_loss: 1497.3385 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8791 - loss: 1512.5287 - val_RMSE: 38.6826 - val_loss: 1497.2965 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8707 - loss: 1511.8906 - val_RMSE: 38.6824 - val_loss: 1497.2834 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8723 - loss: 1512.0159 - val_RMSE: 38.6818 - val_loss: 1497.2421 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8676 - loss: 1511.6558 - val_RMSE: 38.6817 - val_loss: 1497.2469 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8686 - loss: 1511.7430 - val_RMSE: 38.6813 - val_loss: 1497.2260 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8722 - loss: 1512.0331 - val_RMSE: 38.6811 - val_loss: 1497.2048 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8653 - loss: 1511.4901 - val_RMSE: 38.6810 - val_loss: 1497.2039 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8659 - loss: 1511.5535 - val_RMSE: 38.6806 - val_loss: 1497.1833 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8669 - loss: 1511.6404 - val_RMSE: 38.6802 - val_loss: 1497.1604 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8607 - loss: 1511.1592 - val_RMSE: 38.6815 - val_loss: 1497.2582 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8617 - loss: 1511.2428 - val_RMSE: 38.6810 - val_loss: 1497.2234 - learning_rate: 0.0010\n",
            "41608/41608  89s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.3294 - loss: 3101.0984 - val_RMSE: 38.7362 - val_loss: 1501.3876 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0573 - loss: 1526.4099 - val_RMSE: 38.7201 - val_loss: 1500.3046 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9122 - loss: 1515.2542 - val_RMSE: 38.7197 - val_loss: 1500.4049 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8611 - loss: 1511.3992 - val_RMSE: 38.7180 - val_loss: 1500.3362 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8472 - loss: 1510.3560 - val_RMSE: 38.7171 - val_loss: 1500.2311 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8411 - loss: 1509.8203 - val_RMSE: 38.7167 - val_loss: 1500.0773 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8396 - loss: 1509.5865 - val_RMSE: 38.7171 - val_loss: 1500.0164 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8389 - loss: 1509.4502 - val_RMSE: 38.7165 - val_loss: 1499.9355 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8384 - loss: 1509.3826 - val_RMSE: 38.7153 - val_loss: 1499.8303 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8317 - loss: 1508.8485 - val_RMSE: 38.7155 - val_loss: 1499.8304 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8283 - loss: 1508.5773 - val_RMSE: 38.7146 - val_loss: 1499.7714 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  22s 8ms/step - RMSE: 38.8303 - loss: 1508.7450 - val_RMSE: 38.7148 - val_loss: 1499.7863 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8264 - loss: 1508.4324 - val_RMSE: 38.7155 - val_loss: 1499.8446 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8240 - loss: 1508.2527 - val_RMSE: 38.7147 - val_loss: 1499.7885 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8291 - loss: 1508.6553 - val_RMSE: 38.7144 - val_loss: 1499.7731 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8208 - loss: 1508.0199 - val_RMSE: 38.7144 - val_loss: 1499.7777 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8248 - loss: 1508.3191 - val_RMSE: 38.7086 - val_loss: 1499.2599 - learning_rate: 1.0000e-04\n",
            "Epoch 18/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8143 - loss: 1507.4417 - val_RMSE: 38.7083 - val_loss: 1499.1759 - learning_rate: 1.0000e-04\n",
            "Epoch 19/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8137 - loss: 1507.3374 - val_RMSE: 38.7080 - val_loss: 1499.1064 - learning_rate: 1.0000e-04\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8183 - loss: 1507.6465 - val_RMSE: 38.7079 - val_loss: 1499.0591 - learning_rate: 1.0000e-04\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8108 - loss: 1507.0297 - val_RMSE: 38.7079 - val_loss: 1499.0247 - learning_rate: 1.0000e-04\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8181 - loss: 1507.5647 - val_RMSE: 38.7078 - val_loss: 1498.9885 - learning_rate: 1.0000e-04\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8066 - loss: 1506.6410 - val_RMSE: 38.7078 - val_loss: 1498.9668 - learning_rate: 1.0000e-04\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8099 - loss: 1506.8774 - val_RMSE: 38.7076 - val_loss: 1498.9280 - learning_rate: 1.0000e-04\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8096 - loss: 1506.8282 - val_RMSE: 38.7078 - val_loss: 1498.9246 - learning_rate: 1.0000e-04\n",
            "41608/41608  95s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.3186 - loss: 3099.4873 - val_RMSE: 38.7199 - val_loss: 1500.1370 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.0769 - loss: 1527.9541 - val_RMSE: 38.7163 - val_loss: 1500.0200 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9302 - loss: 1516.6616 - val_RMSE: 38.7110 - val_loss: 1499.7379 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8936 - loss: 1513.9352 - val_RMSE: 38.7093 - val_loss: 1499.6638 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8770 - loss: 1512.6691 - val_RMSE: 38.7089 - val_loss: 1499.5791 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8645 - loss: 1511.6184 - val_RMSE: 38.7080 - val_loss: 1499.3773 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8692 - loss: 1511.8556 - val_RMSE: 38.7076 - val_loss: 1499.2562 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8627 - loss: 1511.2736 - val_RMSE: 38.7066 - val_loss: 1499.1486 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8591 - loss: 1510.9698 - val_RMSE: 38.7068 - val_loss: 1499.1526 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8616 - loss: 1511.1550 - val_RMSE: 38.7061 - val_loss: 1499.0885 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8573 - loss: 1510.8187 - val_RMSE: 38.7054 - val_loss: 1499.0260 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8564 - loss: 1510.7313 - val_RMSE: 38.7052 - val_loss: 1499.0048 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8546 - loss: 1510.5918 - val_RMSE: 38.7054 - val_loss: 1499.0148 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8536 - loss: 1510.5154 - val_RMSE: 38.7049 - val_loss: 1498.9822 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8512 - loss: 1510.3356 - val_RMSE: 38.7045 - val_loss: 1498.9658 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8495 - loss: 1510.2122 - val_RMSE: 38.7049 - val_loss: 1498.9963 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8469 - loss: 1510.0051 - val_RMSE: 38.7044 - val_loss: 1498.9785 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8467 - loss: 1510.0103 - val_RMSE: 38.7047 - val_loss: 1498.9928 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8482 - loss: 1510.1261 - val_RMSE: 38.7041 - val_loss: 1498.9618 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8469 - loss: 1510.0453 - val_RMSE: 38.7041 - val_loss: 1498.9664 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8389 - loss: 1509.4177 - val_RMSE: 38.7027 - val_loss: 1498.8628 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8409 - loss: 1509.5818 - val_RMSE: 38.7025 - val_loss: 1498.8490 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8395 - loss: 1509.4774 - val_RMSE: 38.7042 - val_loss: 1498.9983 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8377 - loss: 1509.3455 - val_RMSE: 38.7028 - val_loss: 1498.8794 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8365 - loss: 1509.2500 - val_RMSE: 38.7032 - val_loss: 1498.9156 - learning_rate: 0.0010\n",
            "41608/41608  88s 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-16 04:47:00,751] Trial 24 finished with value: 38.69732538859049 and parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0005025556166064429, 'dropout_rate': 0.48984751751659333}. Best is trial 22 with value: 38.69640350341797.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "2601/2601  36s 10ms/step - RMSE: 54.4057 - loss: 3109.7021 - val_RMSE: 38.7136 - val_loss: 1499.1035 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  20s 8ms/step - RMSE: 39.1025 - loss: 1529.3859 - val_RMSE: 38.6958 - val_loss: 1497.7939 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9640 - loss: 1518.6431 - val_RMSE: 38.6897 - val_loss: 1497.4073 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.9243 - loss: 1515.6382 - val_RMSE: 38.6868 - val_loss: 1497.2644 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9051 - loss: 1514.2286 - val_RMSE: 38.6861 - val_loss: 1497.2644 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8989 - loss: 1513.7876 - val_RMSE: 38.6842 - val_loss: 1497.1451 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8961 - loss: 1513.5814 - val_RMSE: 38.6843 - val_loss: 1497.1528 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8911 - loss: 1513.1978 - val_RMSE: 38.6838 - val_loss: 1497.1125 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8882 - loss: 1512.9745 - val_RMSE: 38.6836 - val_loss: 1497.1016 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8899 - loss: 1513.1068 - val_RMSE: 38.6831 - val_loss: 1497.0591 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8898 - loss: 1513.0931 - val_RMSE: 38.6827 - val_loss: 1497.0402 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8824 - loss: 1512.5287 - val_RMSE: 38.6820 - val_loss: 1496.9803 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8896 - loss: 1513.0913 - val_RMSE: 38.6819 - val_loss: 1496.9784 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8778 - loss: 1512.1780 - val_RMSE: 38.6817 - val_loss: 1496.9678 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8827 - loss: 1512.5575 - val_RMSE: 38.6816 - val_loss: 1496.9596 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8761 - loss: 1512.0461 - val_RMSE: 38.6818 - val_loss: 1496.9812 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.8759 - loss: 1512.0347 - val_RMSE: 38.6814 - val_loss: 1496.9561 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8700 - loss: 1511.5802 - val_RMSE: 38.6813 - val_loss: 1496.9491 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8735 - loss: 1511.8597 - val_RMSE: 38.6813 - val_loss: 1496.9619 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8761 - loss: 1512.0709 - val_RMSE: 38.6808 - val_loss: 1496.9270 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8669 - loss: 1511.3622 - val_RMSE: 38.6802 - val_loss: 1496.8854 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "2601/2601  20s 7ms/step - RMSE: 38.8698 - loss: 1511.5920 - val_RMSE: 38.6809 - val_loss: 1496.9432 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8692 - loss: 1511.5511 - val_RMSE: 38.6814 - val_loss: 1496.9891 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8635 - loss: 1511.1118 - val_RMSE: 38.6807 - val_loss: 1496.9469 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "2601/2601  20s 8ms/step - RMSE: 38.8641 - loss: 1511.1655 - val_RMSE: 38.6801 - val_loss: 1496.9037 - learning_rate: 0.0010\n",
            "41608/41608  87s 2ms/step\n",
            "Epoch 1/25\n",
            "2601/2601  33s 10ms/step - RMSE: 54.3674 - loss: 3104.8064 - val_RMSE: 38.7339 - val_loss: 1500.6754 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "2601/2601  21s 8ms/step - RMSE: 39.0589 - loss: 1525.9745 - val_RMSE: 38.7232 - val_loss: 1499.9159 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "2601/2601  21s 8ms/step - RMSE: 38.9153 - loss: 1514.8567 - val_RMSE: 38.7201 - val_loss: 1499.7589 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "1923/2601  4s 7ms/step - RMSE: 38.8621 - loss: 1510.7944"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-16 04:59:10,475] Trial 25 failed with parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.00019662830318865977, 'dropout_rate': 0.49978399797646733} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-54-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-53-bcff8c4fc905>\", line 51, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "    logs = self.train_function(iterator)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "    opt_outputs = multi_step_on_iterator(iterator)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n",
            "    result = self._call(*args, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 878, in _call\n",
            "    results = tracing_compilation.call_function(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 132, in call_function\n",
            "    function = trace_function(\n",
            "               ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n",
            "    concrete_function = _maybe_define_function(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 239, in _maybe_define_function\n",
            "    concrete_function = tracing_options.function_cache.lookup(\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\", line 50, in lookup\n",
            "    return self._primary[(context, dispatch_type)]\n",
            "           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\", line 456, in __hash__\n",
            "    return hash((tuple(self.parameters.items()), tuple(self.captures.items())))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\", line 893, in __hash__\n",
            "    def __hash__(self):\n",
            "    \n",
            "KeyboardInterrupt\n",
            "[W 2025-02-16 04:59:10,478] Trial 25 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-bcff8c4fc905>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     52\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   function = trace_function(\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     concrete_function = tracing_options.function_cache.lookup(\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_func_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_cache.py\u001b[0m in \u001b[0;36mlookup\u001b[0;34m(self, function_type, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mdispatch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdispatch_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_primary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    891\u001b[0m         type(self).__name__, self.shape, repr(self.dtype), repr(self.name))\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 22 finished with value: 38.69640350341797\n",
        "* parameters: {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002133354415296107, 'dropout_rate': 0.47090232042440616}. Best is trial 22 with value: 38.69640350341797."
      ],
      "metadata": {
        "id": "dwbBHeboznPK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9",
        "id": "ayypCUhQQlNh"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QaE2SucWPmo"
      },
      "source": [
        "#### **4.6.3 NeuralNetwork v2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "c3ed30f5-0209-4614-9592-ddf036843226",
        "id": "A9WMnacSWPms"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "56569        3         4     3             8                   2           1   \n",
              "1183394      1         3     3             7                   2           1   \n",
              "855407       4         1     1             9                   2           1   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "56569        0      0              0.727470 -0.518096 -0.606337 -0.709550   \n",
              "1183394      0      6              1.159509  0.950699 -0.606337 -0.613824   \n",
              "855407       0      6             -0.842369 -0.192151 -0.168518 -0.018634   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "56569             0               0  \n",
              "1183394           0               0  \n",
              "855407            0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c7de07e6-6e6c-482b-8cd1-0aea0acc187a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>56569</th>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.727470</td>\n",
              "      <td>-0.518096</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>-0.709550</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1183394</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1.159509</td>\n",
              "      <td>0.950699</td>\n",
              "      <td>-0.606337</td>\n",
              "      <td>-0.613824</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855407</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.842369</td>\n",
              "      <td>-0.192151</td>\n",
              "      <td>-0.168518</td>\n",
              "      <td>-0.018634</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c7de07e6-6e6c-482b-8cd1-0aea0acc187a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c7de07e6-6e6c-482b-8cd1-0aea0acc187a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c7de07e6-6e6c-482b-8cd1-0aea0acc187a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e19e85c6-6dda-4ad2-a0de-7574ea2a2428\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e19e85c6-6dda-4ad2-a0de-7574ea2a2428')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e19e85c6-6dda-4ad2-a0de-7574ea2a2428 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 7,\n        \"max\": 9,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          8,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.7274695038795471\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.5180957317352295\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          -0.16851840913295746\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.7095502018928528\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512, last_layer=1, activation=\"relu\",repeat_att=2,dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001): # Reduced transformer_units\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    transformer_outputs = [] # List to store transformer outputs for each categorical feature\n",
        "\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:, j])\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        embs.append(x)\n",
        "\n",
        "        # Reshape for Transformer (batch_size, 1, features) - Crucial!\n",
        "        reshaped_embedding = layers.Reshape((1, -1))(x)\n",
        "\n",
        "        # Transformer Layer for each categorical feature\n",
        "        for q in list(range(repeat_att)):\n",
        "          if q == 0:\n",
        "            attention_output = reshaped_embedding\n",
        "\n",
        "          attention_output_ = layers.MultiHeadAttention(num_heads=num_transformer_heads, key_dim=transformer_units,name=f\"mh_{j}_{q}\")(attention_output, attention_output)\n",
        "          attention_output_ = layers.LayerNormalization(name=f\"mh_ln1_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "          attention_output_ = layers.Dense(reshaped_embedding.shape[-1], activation=activation,name=f\"mh_dense_{j}_{q}\")(attention_output_)\n",
        "          attention_output = layers.LayerNormalization(name=f\"mh_ln2_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "\n",
        "        transformer_outputs.append(layers.Flatten()(attention_output)) # Store flattened transformer output\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    x_orig = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    reshaped_features = layers.Reshape((1, -1))(x_orig)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x_orig, flattened_attention])\n",
        "\n",
        "    # Concatenate Transformer outputs and numerical features\n",
        "    all_features = layers.Concatenate(axis=-1)(transformer_outputs + [x])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(all_features)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    #x = layers.Concatenate(axis=-1)([x_orig, x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "T7x-clb6WPms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model()\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "g0C5K7thWPmt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b5d6d28-be45-4164-e081-f9f16e782d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer_6              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " get_item_30 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_31 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_32 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_33 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_34 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_35 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_36 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_37 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_38 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " get_item_39 (\u001b[38;5;33mGetItem\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " embedding_30 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m18\u001b[0m  get_item_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_31 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m15\u001b[0m  get_item_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_32 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m8\u001b[0m  get_item_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_33 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                           \u001b[38;5;34m40\u001b[0m  get_item_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_34 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m6\u001b[0m  get_item_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_35 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m6\u001b[0m  get_item_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_36 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m8\u001b[0m  get_item_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_37 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m21\u001b[0m  get_item_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_38 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m4\u001b[0m  get_item_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding_39 (\u001b[38;5;33mEmbedding\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m4\u001b[0m  get_item_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_30 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_32 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_34 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_36 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_38 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_40 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_42 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_44 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_46 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " flatten_48 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dropout_9 (\u001b[38;5;33mDropout\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_12 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_15 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_18 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_21 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_24 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_27 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_30 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_33 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dropout_36 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  flatten_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_18 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " reshape_19 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_20 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_21 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_22 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_23 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_24 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_25 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_26 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_27 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  dropout_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_0_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                     \u001b[38;5;34m3,843\u001b[0m  reshape_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_1_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                     \u001b[38;5;34m3,843\u001b[0m  reshape_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_2_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  reshape_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_3_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                     \u001b[38;5;34m4,868\u001b[0m  reshape_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_4_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  reshape_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_5_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  reshape_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_6_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  reshape_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_7_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                     \u001b[38;5;34m3,843\u001b[0m  reshape_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_8_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  reshape_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_9_0                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  reshape_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               reshape_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " add_21 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_0_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_25 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_29 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_33 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_3_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_37 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_4_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_41 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_5_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_45 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_6_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_49 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_7_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_53 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_8_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_57 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_9_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " mh_ln1_0_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_1_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_2_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_29[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_3_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                         \u001b[38;5;34m8\u001b[0m  add_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_4_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_5_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_6_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_7_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_8_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_53[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_9_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_57[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_dense_0_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                        \u001b[38;5;34m12\u001b[0m  mh_ln1_0_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_1_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                        \u001b[38;5;34m12\u001b[0m  mh_ln1_1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_2_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_3_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                        \u001b[38;5;34m20\u001b[0m  mh_ln1_3_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_4_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_4_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_5_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_5_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_6_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_6_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_7_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                        \u001b[38;5;34m12\u001b[0m  mh_ln1_7_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_8_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_8_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_9_0 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_9_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " add_22 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_0_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_26 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_30 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_34 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_3_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_38 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_4_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_42 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_5_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_46 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_6_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_50 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_7_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_54 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_8_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_58 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  reshape_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_9_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " mh_ln2_0_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_1_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_2_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_3_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                         \u001b[38;5;34m8\u001b[0m  add_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_4_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_38[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_5_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_6_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_46[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_7_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_8_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_54[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_9_0                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_58[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_0_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                     \u001b[38;5;34m3,843\u001b[0m  mh_ln2_0_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_0_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_1_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                     \u001b[38;5;34m3,843\u001b[0m  mh_ln2_1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_2_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  mh_ln2_2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_3_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                     \u001b[38;5;34m4,868\u001b[0m  mh_ln2_3_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_3_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_4_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  mh_ln2_4_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_4_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_5_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  mh_ln2_5_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_5_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_6_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  mh_ln2_6_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_6_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_7_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                     \u001b[38;5;34m3,843\u001b[0m  mh_ln2_7_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_7_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_8_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  mh_ln2_8_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_8_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_9_1                     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                     \u001b[38;5;34m2,818\u001b[0m  mh_ln2_9_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mMultiHeadAttention\u001b[0m)                                               mh_ln2_9_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " add_23 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_0_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_0_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_27 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_1_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_31 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_2_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_35 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_3_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_3_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_39 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_4_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_4_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_43 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_5_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_5_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_47 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_6_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_6_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_51 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_7_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_7_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_55 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_8_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_8_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " add_59 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_9_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_9_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              "\n",
              " input_layer_7              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " mh_ln1_0_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_1_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_2_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_3_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                         \u001b[38;5;34m8\u001b[0m  add_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_4_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_5_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_6_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_7_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_8_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_55[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln1_9_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_59[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " concatenate_3              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      dropout_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    dropout_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    dropout_27[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    dropout_30[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    dropout_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    dropout_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    input_layer_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " mh_dense_0_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                        \u001b[38;5;34m12\u001b[0m  mh_ln1_0_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_1_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                        \u001b[38;5;34m12\u001b[0m  mh_ln1_1_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_2_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_2_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_3_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                        \u001b[38;5;34m20\u001b[0m  mh_ln1_3_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_4_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_4_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_5_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_5_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_6_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_6_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_7_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                        \u001b[38;5;34m12\u001b[0m  mh_ln1_7_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_8_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_8_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_dense_9_1 (\u001b[38;5;33mDense\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  mh_ln1_9_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " reshape_28 (\u001b[38;5;33mReshape\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m29\u001b[0m)                        \u001b[38;5;34m0\u001b[0m  concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " add_24 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_0_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_0_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_28 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_1_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_1_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_32 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_2_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_2_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_36 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_3_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_3_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_40 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_4_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_4_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_44 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_5_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_5_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_48 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_6_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_6_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_52 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_7_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_7_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_56 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_8_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_8_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " add_60 (\u001b[38;5;33mAdd\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  mh_ln2_9_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    mh_dense_9_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " attention_9 (\u001b[38;5;33mAttention\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m29\u001b[0m)                        \u001b[38;5;34m0\u001b[0m  reshape_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    reshape_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " mh_ln2_0_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_1_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_28[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_2_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_3_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)                         \u001b[38;5;34m8\u001b[0m  add_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_4_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_5_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_44[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_6_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_7_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m3\u001b[0m)                         \u001b[38;5;34m6\u001b[0m  add_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_8_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_56[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " mh_ln2_9_1                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m4\u001b[0m  add_60[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           \n",
              " (\u001b[38;5;33mLayerNormalization\u001b[0m)                                                                      \n",
              "\n",
              " flatten_50 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  attention_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_31 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_0_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten_33 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_1_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten_35 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_2_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten_37 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_3_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten_39 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_4_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten_41 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_5_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten_43 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_6_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten_45 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_7_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten_47 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_8_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten_49 (\u001b[38;5;33mFlatten\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  mh_ln2_9_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " concatenate_4              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  concatenate_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      flatten_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " concatenate_5              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  flatten_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      flatten_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_43[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_45[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    flatten_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      \n",
              "                                                                    concatenate_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " dense_21 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                     \u001b[38;5;34m43,008\u001b[0m  concatenate_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " batch_normalization        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                      \u001b[38;5;34m2,048\u001b[0m  dense_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dropout_39 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_22 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m262,656\u001b[0m  dropout_39[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " batch_normalization_1      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                      \u001b[38;5;34m2,048\u001b[0m  dense_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dropout_40 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_1 \n",
              "\n",
              " dense_23 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                    \u001b[38;5;34m262,656\u001b[0m  dropout_40[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " batch_normalization_2      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                      \u001b[38;5;34m2,048\u001b[0m  dense_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dropout_41 (\u001b[38;5;33mDropout\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  batch_normalization_2 \n",
              "\n",
              " dense_24 (\u001b[38;5;33mDense\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m513\u001b[0m  dropout_41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer_6              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " get_item_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " get_item_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " embedding_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>  get_item_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>  get_item_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  get_item_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>  get_item_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  get_item_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  get_item_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  get_item_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>  get_item_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  get_item_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  get_item_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " flatten_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dropout_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " reshape_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_0_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span>  reshape_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_1_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span>  reshape_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_2_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  reshape_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_3_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,868</span>  reshape_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_4_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  reshape_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_5_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  reshape_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_6_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  reshape_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_7_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span>  reshape_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_8_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  reshape_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_9_0                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  reshape_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               reshape_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " add_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_0_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_3_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_4_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_5_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_6_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_7_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_8_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_9_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " mh_ln1_0_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_1_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_2_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_29[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_3_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  add_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_4_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_5_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_6_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_7_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_8_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_53[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_9_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_57[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_dense_0_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>  mh_ln1_0_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_1_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>  mh_ln1_1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_2_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_3_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>  mh_ln1_3_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_4_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_4_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_5_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_5_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_6_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_6_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_7_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>  mh_ln1_7_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_8_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_8_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_9_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_9_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " add_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_0_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_3_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_4_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_5_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_6_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_7_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_8_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_9_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " mh_ln2_0_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_1_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_2_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_3_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  add_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_4_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_38[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_5_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_6_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_46[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_7_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_8_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_54[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_9_0                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_58[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_0_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span>  mh_ln2_0_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_0_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_1_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span>  mh_ln2_1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_2_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  mh_ln2_2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_3_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,868</span>  mh_ln2_3_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_3_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_4_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  mh_ln2_4_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_4_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_5_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  mh_ln2_5_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_5_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_6_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  mh_ln2_6_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_6_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_7_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,843</span>  mh_ln2_7_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_7_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_8_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  mh_ln2_8_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_8_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_9_1                     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,818</span>  mh_ln2_9_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttention</span>)                                               mh_ln2_9_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " add_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_0_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_0_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_1_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_2_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_3_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_3_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_4_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_4_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_5_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_5_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_6_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_6_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_7_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_7_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_8_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_8_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " add_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_9_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_9_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              "\n",
              " input_layer_7              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " mh_ln1_0_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_1_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_2_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_3_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  add_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_4_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_5_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_6_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_7_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_8_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_55[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln1_9_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_59[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " concatenate_3              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      dropout_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    dropout_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    dropout_27[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    dropout_30[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    dropout_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    dropout_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " mh_dense_0_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>  mh_ln1_0_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_1_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>  mh_ln1_1_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_2_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_2_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_3_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>  mh_ln1_3_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_4_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_4_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_5_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_5_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_6_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_6_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_7_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>  mh_ln1_7_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_8_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_8_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_dense_9_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  mh_ln1_9_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " reshape_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " add_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_0_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_0_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_1_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_1_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_2_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_2_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_3_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_3_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_4_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_4_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_5_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_5_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_6_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_6_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_7_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_7_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_8_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_8_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " add_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_9_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    mh_dense_9_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " attention_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  reshape_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    reshape_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " mh_ln2_0_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_1_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_28[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_2_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_3_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  add_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_4_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_5_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_44[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_6_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_7_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  add_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_8_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_56[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " mh_ln2_9_1                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  add_60[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                                                                      \n",
              "\n",
              " flatten_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  attention_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_0_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_1_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_2_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_3_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_4_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_5_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten_43 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_6_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_7_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_8_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  mh_ln2_9_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " concatenate_4              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      flatten_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " concatenate_5              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      flatten_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_43[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_45[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    flatten_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      \n",
              "                                                                    concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">43,008</span>  concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " batch_normalization        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  dense_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dropout_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  dropout_39[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " batch_normalization_1      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  dense_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dropout_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_1 \n",
              "\n",
              " dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span>  dropout_40[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " batch_normalization_2      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span>  dense_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dropout_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  batch_normalization_2 \n",
              "\n",
              " dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span>  dropout_41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m642,101\u001b[0m (2.45 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">642,101</span> (2.45 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m639,029\u001b[0m (2.44 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">639,029</span> (2.44 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,072\u001b[0m (12.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,072</span> (12.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(mod_test, show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"TB\")"
      ],
      "metadata": {
        "id": "xsUPc7xAb4zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "id": "8cmiDeT2WPmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D61nTPd2WPmu"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d69Nrn1WWPmu"
      },
      "outputs": [],
      "source": [
        "# categorical_feat = t.cat_features.copy()\n",
        "# numerical_feat = t.num_features.copy()\n",
        "\n",
        "# X_train_cat = X_enc[categorical_feat]\n",
        "# X_train_num = X_enc[numerical_feat]\n",
        "\n",
        "# X_test_cat = test_enc[categorical_feat]\n",
        "# X_test_num = test_enc[numerical_feat]\n",
        "\n",
        "# X_train_cat.info()\n",
        "# X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsrSslecWPmu"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "#(units=512, last_layer=1, activation=\"relu\", dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.001)\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [128,256,512,1024]),\n",
        "              'last_layer': trial.suggest_int('last_layer', 1,2),\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': 0.0001, #trial.suggest_float('reg', 1e-4, 0.1, log=True),\n",
        "              \"num_transformer_heads\": trial.suggest_int(\"num_transformer_heads\", 2, 4),\n",
        "              \"transformer_units\": trial.suggest_int(\"transformer_units\", 32, 128,step=32),\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.51,step=0.03),\n",
        "              'repeat_att': trial.suggest_categorical('repeat_att', [1,2]),\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num], batch_size=1024)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHngrYKwWPmu"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e882d5f-238a-4aff-c66b-e09c7c0a7d58",
        "id": "ghy_gPSMWPmv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-22 01:50:31,686] A new study created in memory with name: no-name-61b1de81-aab7-4218-b7a7-82e990bda90b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-22 01:51:05,809] Trial 0 failed with parameters: {'units': 128, 'last_layer': 2, 'activation': 'selu', 'num_transformer_heads': 2, 'transformer_units': 32, 'dropout_rate': 0.42, 'repeat_att': 1} because of the following error: FailedPreconditionError().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-57-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-56-a07edfb686fe>\", line 54, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
            "    except TypeError as e:\n",
            "tensorflow.python.framework.errors_impl.FailedPreconditionError: Graph execution error:\n",
            "\n",
            "Detected at node StatefulPartitionedCall defined at (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "\n",
            "  File \"<ipython-input-58-3ce299ed4080>\", line 1, in <cell line: 0>\n",
            "\n",
            "  File \"<ipython-input-57-f63d57ecfd44>\", line 4, in tune_hyperparameters\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "\n",
            "  File \"<ipython-input-57-f63d57ecfd44>\", line 4, in <lambda>\n",
            "\n",
            "  File \"<ipython-input-56-a07edfb686fe>\", line 54, in objective_nn\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n",
            "\n",
            "DNN library initialization failed. Look at the errors above for more details.\n",
            "\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_63339]\n",
            "[W 2025-02-22 01:51:05,810] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-58-3ce299ed4080>\", line 1, in <cell line: 0>\n\n  File \"<ipython-input-57-f63d57ecfd44>\", line 4, in tune_hyperparameters\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"<ipython-input-57-f63d57ecfd44>\", line 4, in <lambda>\n\n  File \"<ipython-input-56-a07edfb686fe>\", line 54, in objective_nn\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_63339]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-57-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-a07edfb686fe>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     55\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-58-3ce299ed4080>\", line 1, in <cell line: 0>\n\n  File \"<ipython-input-57-f63d57ecfd44>\", line 4, in tune_hyperparameters\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"<ipython-input-57-f63d57ecfd44>\", line 4, in <lambda>\n\n  File \"<ipython-input-56-a07edfb686fe>\", line 54, in objective_nn\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_63339]"
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 2 finished with value: 38.699286142985024\n",
        "* parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}"
      ],
      "metadata": {
        "id": "FU7j6-OHWPmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9",
        "id": "u4XgZIPLWPmv"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jReIEHz6wysG"
      },
      "source": [
        "#### **4.6.4 NeuralNetwork v3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "021deaba-7343-4bc6-f0f7-4f71d06277cb",
        "id": "QfBbW0LUwysH"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Brand  Material  Size  Compartments  Laptop Compartment  Waterproof  \\\n",
              "1058652      4         3     1             3                   2           1   \n",
              "3496582      5         3     0             4                   2           1   \n",
              "1182715      5         0     1             0                   2           2   \n",
              "\n",
              "         Style  Color  Weight Capacity (kg)     TE_wc    skew_0    skew_1  \\\n",
              "1058652      3      2              0.836865 -0.717359 -0.168518 -0.018634   \n",
              "3496582      1      0              1.425016  0.204416 -0.385691 -1.621354   \n",
              "1182715      1      5             -0.441901  0.510029 -1.185578 -2.060967   \n",
              "\n",
              "         cheap_flag  expansive_flag  \n",
              "1058652           0               0  \n",
              "3496582           0               0  \n",
              "1182715           0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a5fe9026-e88b-4e5b-8ee4-646c80163da7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Brand</th>\n",
              "      <th>Material</th>\n",
              "      <th>Size</th>\n",
              "      <th>Compartments</th>\n",
              "      <th>Laptop Compartment</th>\n",
              "      <th>Waterproof</th>\n",
              "      <th>Style</th>\n",
              "      <th>Color</th>\n",
              "      <th>Weight Capacity (kg)</th>\n",
              "      <th>TE_wc</th>\n",
              "      <th>skew_0</th>\n",
              "      <th>skew_1</th>\n",
              "      <th>cheap_flag</th>\n",
              "      <th>expansive_flag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1058652</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0.836865</td>\n",
              "      <td>-0.717359</td>\n",
              "      <td>-0.168518</td>\n",
              "      <td>-0.018634</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3496582</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.425016</td>\n",
              "      <td>0.204416</td>\n",
              "      <td>-0.385691</td>\n",
              "      <td>-1.621354</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1182715</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.441901</td>\n",
              "      <td>0.510029</td>\n",
              "      <td>-1.185578</td>\n",
              "      <td>-2.060967</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5fe9026-e88b-4e5b-8ee4-646c80163da7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a5fe9026-e88b-4e5b-8ee4-646c80163da7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a5fe9026-e88b-4e5b-8ee4-646c80163da7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4eed8661-fcca-40f3-9799-e33a70f37361\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4eed8661-fcca-40f3-9799-e33a70f37361')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4eed8661-fcca-40f3-9799-e33a70f37361 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"X_enc\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Brand\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Material\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Compartments\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Laptop Compartment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Waterproof\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Style\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight Capacity (kg)\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.8368653655052185\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TE_wc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.7173587679862976\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_0\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.16851840913295746\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skew_1\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          -0.018634038046002388\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cheap_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expansive_flag\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "X_enc.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class exitesqueeze_layer(layers.Layer):\n",
        "    def __init__(self, exite_units,dropout_rate,activation,reg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.exite_units = exite_units\n",
        "        self.activation=activation\n",
        "        self.reg=reg\n",
        "\n",
        "        self.reshaped_0 = layers.Reshape((-1, 1))\n",
        "        self.reshaped_1 = layers.Reshape((-1, ))\n",
        "\n",
        "        self.exite = layers.Dense(self.exite_units, activation=self.activation)\n",
        "        self.squeeze = layers.Dense(1, activation=\"linear\",kernel_regularizer=keras.regularizers.l2(reg))\n",
        "        self.lnorm_00 = layers.LayerNormalization()\n",
        "        self.lnorm_01 = layers.LayerNormalization()\n",
        "        self.drop = layers.Dropout(rate=dropout_rate)\n",
        "        self.attention = layers.Attention()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.reshaped_0(inputs)\n",
        "        x = self.exite(x)\n",
        "        att_out = self.attention([x,x])\n",
        "        att_out = self.lnorm_00(att_out)\n",
        "        x = layers.add([x, att_out])\n",
        "        x = self.squeeze(x)\n",
        "        x = self.reshaped_1(x)\n",
        "\n",
        "        x = layers.multiply([x, inputs])\n",
        "\n",
        "        x = self.lnorm_01(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Remove build warnings\n",
        "    def build(self):\n",
        "        self.built = True"
      ],
      "metadata": {
        "id": "UJ5OQf-rDnO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512,exite_units=64, last_layer = 1, activation=\"relu\", reg=0.001, dropout_rate=0.33):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(t.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(t.cat_features_card[j], int(np.ceil(np.sqrt(t.cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(t.num_features),))\n",
        "\n",
        "    x_0 = layers.Concatenate(axis=-1, name=\"input_concat\")(embs+[x_input_nums])\n",
        "\n",
        "    es_0 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_0_concat\")([x_0,es_0])\n",
        "    x = layers.BatchNormalization(name=\"se_0_bn\")(x)\n",
        "\n",
        "    es_1 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_1_concat\")([x,es_1])\n",
        "    x = layers.BatchNormalization(name=\"se_1_bn\")(x)\n",
        "\n",
        "    es_2 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_2_concat\")([x,es_2])\n",
        "    x = layers.BatchNormalization(name=\"se_2_bn\")(x)\n",
        "\n",
        "    x_0 = layers.Dense(units, name=\"dense_0\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_0\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_0\")(x_0)\n",
        "\n",
        "    x_0 = layers.Dense(int(units/last_layer), name=\"dense_1\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_1\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_1\")(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([x_0,x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "avZAgIvUwysH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model(units=64,exite_units=16, last_layer = 2, activation=\"relu\", reg=0.001, dropout_rate=0.33)\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "cjfFlBj5wysI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2409238b-5b32-4e30-eee0-adde281bc81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                      \n",
              "\n",
              " get_item (\u001b[38;5;33mGetItem\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " get_item_1 (\u001b[38;5;33mGetItem\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " get_item_2 (\u001b[38;5;33mGetItem\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " get_item_3 (\u001b[38;5;33mGetItem\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " get_item_4 (\u001b[38;5;33mGetItem\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " get_item_5 (\u001b[38;5;33mGetItem\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " get_item_6 (\u001b[38;5;33mGetItem\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " get_item_7 (\u001b[38;5;33mGetItem\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " get_item_8 (\u001b[38;5;33mGetItem\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " get_item_9 (\u001b[38;5;33mGetItem\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m)                               \u001b[38;5;34m0\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " embedding (\u001b[38;5;33mEmbedding\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m18\u001b[0m  get_item[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n",
              " embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m15\u001b[0m  get_item_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m8\u001b[0m  get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                           \u001b[38;5;34m40\u001b[0m  get_item_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m6\u001b[0m  get_item_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m6\u001b[0m  get_item_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m8\u001b[0m  get_item_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " embedding_7 (\u001b[38;5;33mEmbedding\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m21\u001b[0m  get_item_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " embedding_8 (\u001b[38;5;33mEmbedding\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m4\u001b[0m  get_item_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " embedding_9 (\u001b[38;5;33mEmbedding\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m4\u001b[0m  get_item_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " flatten (\u001b[38;5;33mFlatten\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
              "\n",
              " flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_7 (\u001b[38;5;33mFlatten\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  embedding_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " input_layer_1              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  -                      \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)                                                                              \n",
              "\n",
              " input_concat               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                                      flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    flatten_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    flatten_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    flatten_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    flatten_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    flatten_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    flatten_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    flatten_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              "                                                                    input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
              "\n",
              " exitesqueeze_layer         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)                         \u001b[38;5;34m139\u001b[0m  input_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              " (\u001b[38;5;33mexitesqueeze_layer\u001b[0m)                                                                      \n",
              "\n",
              " se_0_concat (\u001b[38;5;33mConcatenate\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  input_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    \n",
              "                                                                    exitesqueeze_layer[\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " se_0_bn                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)                         \u001b[38;5;34m232\u001b[0m  se_0_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_0 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                       \u001b[38;5;34m1,920\u001b[0m  input_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " exitesqueeze_layer_1       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)                         \u001b[38;5;34m197\u001b[0m  se_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              " (\u001b[38;5;33mexitesqueeze_layer\u001b[0m)                                                                      \n",
              "\n",
              " bn_0 (\u001b[38;5;33mBatchNormalization\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                         \u001b[38;5;34m256\u001b[0m  dense_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " se_1_concat (\u001b[38;5;33mConcatenate\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m116\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  se_0_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n",
              "                                                                    exitesqueeze_layer_1[\u001b[38;5;34m\u001b[0m \n",
              "\n",
              " do_0 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  bn_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
              "\n",
              " se_1_bn                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m116\u001b[0m)                        \u001b[38;5;34m464\u001b[0m  se_1_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                       \u001b[38;5;34m2,080\u001b[0m  do_0[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
              "\n",
              " exitesqueeze_layer_2       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m116\u001b[0m)                        \u001b[38;5;34m313\u001b[0m  se_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              " (\u001b[38;5;33mexitesqueeze_layer\u001b[0m)                                                                      \n",
              "\n",
              " bn_1 (\u001b[38;5;33mBatchNormalization\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                         \u001b[38;5;34m128\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " se_2_concat (\u001b[38;5;33mConcatenate\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  se_1_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],         \n",
              "                                                                    exitesqueeze_layer_2[\u001b[38;5;34m\u001b[0m \n",
              "\n",
              " do_1 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  bn_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
              "\n",
              " se_2_bn                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m232\u001b[0m)                        \u001b[38;5;34m928\u001b[0m  se_2_concat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " concatenate (\u001b[38;5;33mConcatenate\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m264\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  do_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],            \n",
              "                                                                    se_2_bn[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          \n",
              "\n",
              " dense_6 (\u001b[38;5;33mDense\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m265\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              "\n",
              " get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " get_item_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " get_item_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " get_item_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " get_item_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " get_item_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " get_item_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " get_item_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " get_item_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " get_item_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>  get_item[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              " embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>  get_item_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>  get_item_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  get_item_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>  get_item_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>  get_item_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " embedding_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>  get_item_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " embedding_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  get_item_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " embedding_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>  get_item_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
              "\n",
              " flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  embedding_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " input_layer_1              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                                              \n",
              "\n",
              " input_concat               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                                      flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    flatten_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    flatten_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    flatten_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    flatten_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    flatten_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    flatten_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    flatten_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              "                                                                    input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
              "\n",
              " exitesqueeze_layer         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">139</span>  input_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">exitesqueeze_layer</span>)                                                                      \n",
              "\n",
              " se_0_concat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  input_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    \n",
              "                                                                    exitesqueeze_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " se_0_bn                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>  se_0_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920</span>  input_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " exitesqueeze_layer_1       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>  se_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">exitesqueeze_layer</span>)                                                                      \n",
              "\n",
              " bn_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  dense_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " se_1_concat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">116</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  se_0_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n",
              "                                                                    exitesqueeze_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "\n",
              " do_0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  bn_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
              "\n",
              " se_1_bn                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">116</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">464</span>  se_1_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span>  do_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
              "\n",
              " exitesqueeze_layer_2       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">116</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">313</span>  se_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">exitesqueeze_layer</span>)                                                                      \n",
              "\n",
              " bn_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " se_2_concat (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  se_1_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],         \n",
              "                                                                    exitesqueeze_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "\n",
              " do_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  bn_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
              "\n",
              " se_2_bn                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">928</span>  se_2_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  do_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],            \n",
              "                                                                    se_2_bn[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          \n",
              "\n",
              " dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">265</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,052\u001b[0m (27.55 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,052</span> (27.55 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,048\u001b[0m (23.62 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,048</span> (23.62 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,004\u001b[0m (3.92 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,004</span> (3.92 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(mod_test, show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"TB\")"
      ],
      "metadata": {
        "id": "gxWhVnRkwysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "id": "hTJyGENowysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "697CMLBjwysI"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujnWEIfcwysI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2def89d5-c7f8-46ce-e2a1-202871b32395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 10 columns):\n",
            " #   Column              Dtype\n",
            "---  ------              -----\n",
            " 0   Brand               int32\n",
            " 1   Material            int32\n",
            " 2   Size                int32\n",
            " 3   Compartments        int32\n",
            " 4   Laptop Compartment  int32\n",
            " 5   Waterproof          int32\n",
            " 6   Style               int32\n",
            " 7   Color               int32\n",
            " 8   cheap_flag          int32\n",
            " 9   expansive_flag      int32\n",
            "dtypes: int32(10)\n",
            "memory usage: 182.8 MB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3994318 entries, 0 to 3994317\n",
            "Data columns (total 4 columns):\n",
            " #   Column                Dtype  \n",
            "---  ------                -----  \n",
            " 0   Weight Capacity (kg)  float32\n",
            " 1   TE_wc                 float32\n",
            " 2   skew_0                float32\n",
            " 3   skew_1                float32\n",
            "dtypes: float32(4)\n",
            "memory usage: 91.4 MB\n"
          ]
        }
      ],
      "source": [
        "categorical_feat = t.cat_features.copy()\n",
        "numerical_feat = t.num_features.copy()\n",
        "\n",
        "X_train_cat = X_enc[categorical_feat].astype(\"int32\")\n",
        "X_train_num = X_enc[numerical_feat].astype(\"float32\")\n",
        "\n",
        "X_test_cat = test_enc[categorical_feat].astype(\"int32\")\n",
        "X_test_num = test_enc[numerical_feat].astype(\"float32\")\n",
        "\n",
        "X_train_cat.info()\n",
        "X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLdEjBBgwysI"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "#(units=512,exite_units=64, last_layer = 1, activation=\"relu\",  reg=0.001, dropout_rate=0.33)\n",
        "    categorical_features = t.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [64,128,256]),#\n",
        "              'last_layer': trial.suggest_int('last_layer',1,2),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': trial.suggest_categorical('reg', [0.00001,0.0001,0.001,0.01]),\n",
        "              \"exite_units\": trial.suggest_categorical('exite_units', [16,32,64]),#\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.51,step=0.03)\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = t.cat_features.copy()\n",
        "        numerical_feat = t.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num], batch_size=1024)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyp2r5IkwysI"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=t.direction_, sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec725dfe-2240-47ec-c194-81233022cfba",
        "id": "ZVi6cBIJwysI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-22 18:13:56,723] A new study created in memory with name: no-name-af0bc415-d4f8-4c0e-99f7-54cd884adcf2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-02-22 18:14:09,559] Trial 0 failed with parameters: {'units': 256, 'last_layer': 2, 'activation': 'silu', 'reg': 1e-05, 'exite_units': 32, 'dropout_rate': 0.39} because of the following error: FailedPreconditionError().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-44-f63d57ecfd44>\", line 4, in <lambda>\n",
            "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-43-8d9d85ac2be2>\", line 52, in objective_nn\n",
            "    model.fit([X_train_cat,X_train_num], y_train,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
            "    except TypeError as e:\n",
            "tensorflow.python.framework.errors_impl.FailedPreconditionError: Graph execution error:\n",
            "\n",
            "Detected at node StatefulPartitionedCall defined at (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "\n",
            "  File \"<ipython-input-45-3ce299ed4080>\", line 1, in <cell line: 0>\n",
            "\n",
            "  File \"<ipython-input-44-f63d57ecfd44>\", line 4, in tune_hyperparameters\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "\n",
            "  File \"<ipython-input-44-f63d57ecfd44>\", line 4, in <lambda>\n",
            "\n",
            "  File \"<ipython-input-43-8d9d85ac2be2>\", line 52, in objective_nn\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n",
            "\n",
            "DNN library initialization failed. Look at the errors above for more details.\n",
            "\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_13457]\n",
            "[W 2025-02-22 18:14:09,561] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-45-3ce299ed4080>\", line 1, in <cell line: 0>\n\n  File \"<ipython-input-44-f63d57ecfd44>\", line 4, in tune_hyperparameters\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"<ipython-input-44-f63d57ecfd44>\", line 4, in <lambda>\n\n  File \"<ipython-input-43-8d9d85ac2be2>\", line 52, in objective_nn\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_13457]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-3ce299ed4080>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_study\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcat_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-f63d57ecfd44>\u001b[0m in \u001b[0;36mtune_hyperparameters\u001b[0;34m(X, y, model_class, n_trials, n_splits_, n_repeats_, use_gpu)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-f63d57ecfd44>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtune_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits_\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#use_gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_splits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_repeats_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"KFold\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# Return the study object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-8d9d85ac2be2>\u001b[0m in \u001b[0;36mobjective_nn\u001b[0;34m(trial, X, y, n_splits, n_repeats, model, use_gpu, rs, fit_scaling, cv_strategy)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         model.fit([X_train_cat,X_train_num], y_train,\n\u001b[0m\u001b[1;32m     53\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-45-3ce299ed4080>\", line 1, in <cell line: 0>\n\n  File \"<ipython-input-44-f63d57ecfd44>\", line 4, in tune_hyperparameters\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 475, in optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 63, in _optimize\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n\n  File \"<ipython-input-44-f63d57ecfd44>\", line 4, in <lambda>\n\n  File \"<ipython-input-43-8d9d85ac2be2>\", line 52, in objective_nn\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_13457]"
          ]
        }
      ],
      "source": [
        "cat_study = tune_hyperparameters(X_enc, y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Trial 2 finished with value: 38.699286142985024\n",
        "* parameters: {'units': 512, 'last_layer': 2, 'activation': 'silu', 'num_transformer_heads': 4, 'transformer_units': 64, 'dropout_rate': 0.39, 'repeat_att': 1}"
      ],
      "metadata": {
        "id": "RWAsq27zwysI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bade4cc-7016-4f98-9dba-cd932012f5c9",
        "id": "PxgRBgt6wysJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RAYXidmYEsqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}