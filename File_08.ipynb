{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 90274,
          "databundleVersionId": 10951356,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30840,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1860.937789,
      "end_time": "2024-10-26T20:02:26.852331",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-10-26T19:31:25.914542",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabriziobasso/Colab_backup/blob/main/File_08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Time Series Forecasting for Retail Sales with MLP, ARIMA, KAN, and Chronos"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.008519,
          "end_time": "2024-10-26T19:31:29.283114",
          "exception": false,
          "start_time": "2024-10-26T19:31:29.274595",
          "status": "completed"
        },
        "tags": [],
        "id": "eFQAje2z1KTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "references:\n",
        "\n",
        "[github](https://github.com/ageron/handson-ml3/blob/main/15_processing_sequences_using_rnns_and_cnns.ipynb)\n",
        "\n",
        "\n",
        "[article](https://medium.com/@kylejones_47003/time-series-forecasting-for-retail-sales-with-mlp-arima-kan-and-chronos-258abbbf4779)\n",
        "\n",
        "[New Feature Colab](https://developers.googleblog.com/en/data-science-agent-in-colab-with-gemini/)"
      ],
      "metadata": {
        "id": "MOsObxB6SXtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessry to run LGBMRegressor\n",
        "!mkdir -p /etc/OpenCL/vendors && echo \"libnvidia-opencl.so.1\" > /etc/OpenCL/vendors/nvidia.icd"
      ],
      "metadata": {
        "id": "v0r46Q3q2iUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#!pip install -qq pytorch_tabnet\n",
        "!pip install optuna\n",
        "!pip install catboost\n",
        "#!pip install optuna-integration-pytorch-tabnet\n",
        "\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install keras --upgrade\n",
        "\n",
        "!pip install --upgrade category-encoders\n",
        "!pip install optuna-integration\n",
        "!pip install colorama\n",
        "#!pip install pyfiglet\n",
        "!pip install keras-tuner --upgrade\n",
        "!pip install keras-nlp\n",
        "!pip install BorutaShap\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install scikit-lego\n",
        "!pip install skops\n",
        "!pip install pykan\n",
        "!pip install chronos-forecasting\n",
        "\n",
        "#from pytorch_tabnet.tab_model import TabNetRegressor"
      ],
      "metadata": {
        "id": "VdgjH0IaaV61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from termcolor import colored\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import os\n",
        "from joblib import dump, load\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import iqr, skew, kurtosis, mode\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_style(\"dark\", {\"grid.color\": \".1\", \"grid.linestyle\": \":\", \"axes.facecolor\": \".9\"})\n",
        "\n",
        "from warnings import filterwarnings\n",
        "from termcolor import colored\n",
        "from warnings import filterwarnings; filterwarnings(action = 'ignore');\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin, clone\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_score, recall_score, log_loss, \\\n",
        "                            mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error, \\\n",
        "                            make_scorer,root_mean_squared_error\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, PowerTransformer, FunctionTransformer, \\\n",
        "                                  OrdinalEncoder, OneHotEncoder\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.base import clone\n",
        "\n",
        "from category_encoders import TargetEncoder, CatBoostEncoder, LeaveOneOutEncoder, OrdinalEncoder, CountEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold, RepeatedStratifiedKFold, \\\n",
        "                                    cross_val_score, RepeatedKFold\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.cluster import k_means\n",
        "from sklearn.feature_selection import RFECV, SequentialFeatureSelector, mutual_info_classif, mutual_info_regression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier, VotingRegressor, StackingRegressor, \\\n",
        "                             RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor, \\\n",
        "                             HistGradientBoostingClassifier, HistGradientBoostingRegressor \\\n",
        "\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor, early_stopping, log_evaluation\n",
        "import lightgbm as lgb\n",
        "\n",
        "from xgboost import XGBClassifier, XGBRegressor, DMatrix\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "import matplotlib as mpl\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "\n",
        "from keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Reshape\n",
        "from keras.models import Model\n",
        "from keras.losses import MeanSquaredError\n",
        "from keras.metrics import RootMeanSquaredError\n",
        "\n",
        "from keras.utils import FeatureSpace, plot_model\n",
        "\n",
        "# Import libraries for Hypertuning\n",
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch, GridSearch, BayesianOptimization"
      ],
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": true,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:33.914813Z",
          "iopub.execute_input": "2025-02-03T01:57:33.915083Z",
          "iopub.status.idle": "2025-02-03T01:57:42.908563Z",
          "shell.execute_reply.started": "2025-02-03T01:57:33.915055Z",
          "shell.execute_reply": "2025-02-03T01:57:42.907652Z"
        },
        "papermill": {
          "duration": 6.371992,
          "end_time": "2024-10-26T19:31:35.662482",
          "exception": false,
          "start_time": "2024-10-26T19:31:29.29049",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "NoIuwT_21KTa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set({\"axes.facecolor\"       : \"#ffffff\",\n",
        "         \"figure.facecolor\"     : \"#ffffff\",\n",
        "         \"axes.edgecolor\"       : \"#000000\",\n",
        "         \"grid.color\"           : \"#ffffff\",\n",
        "         \"font.family\"          : ['Cambria'],\n",
        "         \"axes.labelcolor\"      : \"#000000\",\n",
        "         \"xtick.color\"          : \"#000000\",\n",
        "         \"ytick.color\"          : \"#000000\",\n",
        "         \"grid.linewidth\"       : 0.5,\n",
        "         'grid.alpha'           :0.5,\n",
        "         \"grid.linestyle\"       : \"--\",\n",
        "         \"axes.titlecolor\"      : 'black',\n",
        "         'axes.titlesize'       : 12,\n",
        "#         'axes.labelweight'     : \"bold\",\n",
        "         'legend.fontsize'      : 7.0,\n",
        "         'legend.title_fontsize': 7.0,\n",
        "         'font.size'            : 7.5,\n",
        "         'xtick.labelsize'      : 7.5,\n",
        "         'ytick.labelsize'      : 7.5,\n",
        "        });\n",
        "\n",
        "sns.set_style(\"whitegrid\",{\"grid.linestyle\":\"--\", 'grid.linewidth':0.2, 'grid.alpha':0.5})\n",
        "# Set Style\n",
        "mpl.rcParams['figure.dpi'] = 120;\n",
        "\n",
        "# import font colors\n",
        "from colorama import Fore, Style, init\n",
        "\n",
        "# Making sklearn pipeline outputs as dataframe:-\n",
        "pd.set_option('display.max_columns', 100);\n",
        "pd.set_option('display.max_rows', 50);\n",
        "\n",
        "sns.despine(left=True, bottom=True, top=False, right=False)\n",
        "\n",
        "mpl.rcParams['axes.spines.left'] = True\n",
        "mpl.rcParams['axes.spines.right'] = False\n",
        "mpl.rcParams['axes.spines.top'] = False\n",
        "mpl.rcParams['axes.spines.bottom'] = True\n",
        "\n",
        "init(autoreset=True)"
      ],
      "metadata": {
        "id": "L_QF4gmj5M61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SPECIFIC LIBRARIES USED IN THE NOTEBOOK**"
      ],
      "metadata": {
        "id": "xxT96EmEIN-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import datetime\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.dates import DateFormatter\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from pandas_datareader import data as web\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from kan import KAN\n",
        "from chronos import ChronosPipeline\n",
        "import time"
      ],
      "metadata": {
        "id": "xoNEZx9ZIL22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Colab:#\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "87t8frSLdEsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "pZ1cBlKwHkh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Keras backend:\", keras.backend.backend())"
      ],
      "metadata": {
        "id": "Ta7Zcr6iHo6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retail forecasting is high-stakes. Misses mean stockouts, markdowns, or investor panic. ARIMA is the legacy tool: fast, interpretable, consistent. Newer models like LSTMs and KANs claim to capture nonlinearity. Transformers promise everything.\n",
        "\n",
        "We tested all of them using U.S. retail sales and updated the results through April 2025. The outcomes reflect real-world constraints: fixed forecast window, same input features, same evaluation horizon.\n",
        "\n",
        "We pulled seasonally adjusted U.S. Retail and Food Services Sales from the FRED API:"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.007142,
          "end_time": "2024-10-26T19:31:35.677236",
          "exception": false,
          "start_time": "2024-10-26T19:31:35.670094",
          "status": "completed"
        },
        "tags": [],
        "id": "OJLTekfn1KTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas_datareader.data as web\n",
        "import datetime\n",
        "\n",
        "start = datetime.datetime(1990, 1, 1)\n",
        "end = datetime.datetime(2025, 5, 1)\n",
        "series = web.DataReader(\"RSAFS\", \"fred\", start, end).dropna()[\"RSAFS\"]\n",
        "series.index.freq = \"MS\""
      ],
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:42.909948Z",
          "iopub.execute_input": "2025-02-03T01:57:42.91046Z",
          "iopub.status.idle": "2025-02-03T01:57:45.254458Z",
          "shell.execute_reply.started": "2025-02-03T01:57:42.910435Z",
          "shell.execute_reply": "2025-02-03T01:57:45.253807Z"
        },
        "papermill": {
          "duration": 0.671233,
          "end_time": "2024-10-26T19:31:36.355611",
          "exception": false,
          "start_time": "2024-10-26T19:31:35.684378",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "PEhAxeTp1KTb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "mean_val, std_val = series.mean(), series.std()\n",
        "norm_series = (series - mean_val) / std_val"
      ],
      "metadata": {
        "id": "3Y6J_K5XH1KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is in millions of U.S. dollars. We normalize it for neural networks but leave it raw for ARIMA. Our prediction window is 12 months. All models start their forecast at the same point: April 2024.\n",
        "\n",
        "We use a lag window of 24 months. That’s long enough to capture seasonal trends, short enough to avoid structural breaks."
      ],
      "metadata": {
        "id": "kBX31HvJF5kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "series.plot()\n",
        "plt.title(\"US Retail Sales and Food Services\")\n",
        "plt.ylabel(\"Millions of Dollars\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "He0vO488HL31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "keras.utils.set_random_seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Unified config\n",
        "prediction_length = 12\n",
        "window = 24"
      ],
      "metadata": {
        "id": "8fE4N3-LHL0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_lagged(series, window):\n",
        "    X, y = [], []\n",
        "    for i in range(len(series) - window):\n",
        "        X.append(series.iloc[i:i + window].values)\n",
        "        y.append(series.iloc[i + window])\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "aviY3jm_HLx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "split = series.index[-(window + prediction_length)]\n",
        "train_raw, test_raw = series[series.index <= split], series[series.index > split]\n",
        "train_norm, test_norm = norm_series[norm_series.index <= split], norm_series[norm_series.index > split]"
      ],
      "metadata": {
        "id": "FY0LPLMiHLux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw.shape, test_raw.shape"
      ],
      "metadata": {
        "id": "JF0w732F1RIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = make_lagged(train_norm, window)\n",
        "X_test, y_test = make_lagged(test_norm, window)"
      ],
      "metadata": {
        "id": "s0qg0pf1HLrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test#.shape"
      ],
      "metadata": {
        "id": "kKH5U8uVMtKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "cmH-1_UiHLok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_k = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "y_train_k = y_train.reshape(y_train.shape[0], 1)\n",
        "X_test_k = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)"
      ],
      "metadata": {
        "id": "hxnYJ8clHLls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test\n",
        "X_train.shape,y_train.shape,X_test_k.shape"
      ],
      "metadata": {
        "id": "HYkvU5V-0cqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Arima Model:"
      ],
      "metadata": {
        "id": "Tf-T-zfWLQZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm"
      ],
      "metadata": {
        "id": "5IUM_BFZMOA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sm.graphics.tsa.plot_acf(train_raw.values.squeeze(), lags=40)\n",
        "sm.graphics.tsa.plot_pacf(train_raw.values.squeeze(), lags=40, method=\"ywm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QXEMPS0UMPN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw.reset_index()"
      ],
      "metadata": {
        "id": "gVC7gLkRtn5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_raw)"
      ],
      "metadata": {
        "id": "CTHbljIvvd3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ARIMA\n",
        "start_arima = time.time()\n",
        "arima = SARIMAX(endog=train_raw, dates=train_raw.index, order=(2, 1, 0), seasonal_order=(1, 1, 1, 12)).fit()\n",
        "arima = ARIMA(train_raw, order=(2, 1, 0),seasonal_order=(1, 1, 1, 12)).fit()\n",
        "pred_arima = arima.forecast(steps=len(test_raw))\n",
        "pred_arima.index = test_raw.index\n",
        "arima_time = time.time() - start_arima"
      ],
      "metadata": {
        "id": "77cYseOlHLic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(arima.summary())"
      ],
      "metadata": {
        "id": "xxEewqq3HLfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_raw[-50:], color=\"royalblue\")\n",
        "plt.plot(test_raw, color=\"royalblue\", linestyle=\"dashed\")\n",
        "plt.plot(pred_arima, color=\"salmon\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q69Q2q9iHLc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Store Results**"
      ],
      "metadata": {
        "id": "3Dq61WA1y88I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_results(error_df, target, predic, model_name=\"insert_name\"):\n",
        "  df = error_df.copy()\n",
        "  t = target.copy()\n",
        "  p = predic.copy()\n",
        "\n",
        "  df[:][model_name]=0\n",
        "\n",
        "  df.loc[model_name,\"RMSE\"] = np.round(root_mean_squared_error(t,p),2)\n",
        "  df.loc[model_name,\"MAE\"] = np.round(mean_absolute_error(t,p)  ,2)\n",
        "  df.loc[model_name,\"MAPE\"] = np.round(mean_absolute_percentage_error(t,p),4)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "yyqvH6c-zCaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_raw.shape, pred_arima.shape)\n",
        "root_mean_squared_error(test_raw[:12],pred_arima[:12])\n",
        "\n",
        "df_error = pd.DataFrame(columns=[\"RMSE\",\"MAE\",\"MAPE\"])\n",
        "df_results = store_results(df_error, test_raw, pred_arima, model_name=\"ARIMA\")\n",
        "df_results"
      ],
      "metadata": {
        "id": "81CbdaV3tCv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_arima_is = arima.predict()\n",
        "\n",
        "plt.plot(train_raw, color=\"royalblue\", linestyle=\"dashed\")\n",
        "plt.plot(pred_arima_is, color=\"salmon\")"
      ],
      "metadata": {
        "id": "LRqtiFJcHLZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Neural Networks:"
      ],
      "metadata": {
        "id": "OgzvNIX_0S4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "ahead=35\n",
        "\n",
        "def split_inputs_and_targets(series, ahead=35, target_col=None):\n",
        "    return tf.expand_dims(series[:, :-ahead],-1), series[:, -ahead:]"
      ],
      "metadata": {
        "id": "NO1WD1yLQxry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = keras.utils.timeseries_dataset_from_array(\n",
        "                                                      norm_series,\n",
        "                                                      sequence_length=36+ahead,\n",
        "                                                      targets=None,\n",
        "                                                      sequence_stride=1,\n",
        "                                                      sampling_rate=1,\n",
        "                                                      batch_size=12,\n",
        "                                                      shuffle=False,\n",
        "                                                      seed=42,\n",
        "                                                      start_index=None,\n",
        "                                                      end_index=None\n",
        "                                                  ).map(split_inputs_and_targets)"
      ],
      "metadata": {
        "id": "2-e7zPHPHLWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_series_k = norm_series.values\n",
        "norm_series_k.shape"
      ],
      "metadata": {
        "id": "_XOuGXSgPspX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for a, b in sampler.take(1):\n",
        "  print(a.shape, b.shape)"
      ],
      "metadata": {
        "id": "5YtwxPDs0B_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_series_train_v0 = norm_series.iloc[:-ahead]\n",
        "norm_series_test = norm_series.iloc[-(36+ahead):]"
      ],
      "metadata": {
        "id": "ICbZn20i0B8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sampler = keras.utils.timeseries_dataset_from_array(\n",
        "                                                      norm_series_train_v0,\n",
        "                                                      sequence_length=36+ahead,\n",
        "                                                      targets=None,\n",
        "                                                      sequence_stride=1,\n",
        "                                                      sampling_rate=1,\n",
        "                                                      batch_size=12,\n",
        "                                                      shuffle=False,\n",
        "                                                      seed=42,\n",
        "                                                      start_index=None,\n",
        "                                                      end_index=None\n",
        "                                                  ).map(split_inputs_and_targets)\n",
        "\n",
        "test_sampler = keras.utils.timeseries_dataset_from_array(\n",
        "                                                      norm_series_test,\n",
        "                                                      sequence_length=36+ahead,\n",
        "                                                      targets=None,\n",
        "                                                      sequence_stride=1,\n",
        "                                                      sampling_rate=1,\n",
        "                                                      batch_size=12,\n",
        "                                                      shuffle=False,\n",
        "                                                      seed=42,\n",
        "                                                      start_index=None,\n",
        "                                                      end_index=None\n",
        "                                                  ).map(split_inputs_and_targets)"
      ],
      "metadata": {
        "id": "bcG7KkOv0B5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for a, b in test_sampler.take(1):\n",
        "  print(a.shape, b.shape)"
      ],
      "metadata": {
        "id": "XcjkV8Hb0B2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATE NUMPY VERSION FOR PYTORCH**"
      ],
      "metadata": {
        "id": "gikevTVGLQqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sampler_ = keras.utils.timeseries_dataset_from_array(\n",
        "                                                      norm_series_train_v0,\n",
        "                                                      sequence_length=36+ahead,\n",
        "                                                      targets=None,\n",
        "                                                      sequence_stride=1,\n",
        "                                                      sampling_rate=1,\n",
        "                                                      batch_size=1024,\n",
        "                                                      shuffle=False,\n",
        "                                                      seed=42,\n",
        "                                                      start_index=None,\n",
        "                                                      end_index=None\n",
        "                                                  ).map(split_inputs_and_targets)\n",
        "\n",
        "test_sampler_ = keras.utils.timeseries_dataset_from_array(\n",
        "                                                      norm_series_test,\n",
        "                                                      sequence_length=36+ahead,\n",
        "                                                      targets=None,\n",
        "                                                      sequence_stride=1,\n",
        "                                                      sampling_rate=1,\n",
        "                                                      batch_size=1024,\n",
        "                                                      shuffle=False,\n",
        "                                                      seed=42,\n",
        "                                                      start_index=None,\n",
        "                                                      end_index=None\n",
        "                                                  ).map(split_inputs_and_targets)"
      ],
      "metadata": {
        "id": "_u0mY0_o0QWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for xtrain_numpy, ytrain_numpy in train_sampler_.take(1):\n",
        "  print(xtrain_numpy.shape, ytrain_numpy.shape)\n",
        "\n",
        "xtrain_numpy = xtrain_numpy.numpy()\n",
        "ytrain_numpy = ytrain_numpy.numpy()\n",
        "\n",
        "for xtest_numpy, ytest_numpy in test_sampler_.take(1):\n",
        "  print(xtest_numpy.shape, ytest_numpy.shape)\n",
        "\n",
        "xtest_numpy = xtest_numpy.numpy()\n",
        "ytest_numpy = ytest_numpy.numpy()"
      ],
      "metadata": {
        "id": "Q3LJuEzf0QTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 LSTM"
      ],
      "metadata": {
        "id": "ZBdgUgDxN6BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential"
      ],
      "metadata": {
        "id": "vsXQ8gbPPd9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class lstm_model:\n",
        "  def __init__(self, input_shape, output_shape):\n",
        "    self.input_shape = input_shape\n",
        "    self.output_shape = output_shape\n",
        "    self.model = self.build_model()\n",
        "\n",
        "  def build_model(self,units=64):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units, activation=\"tanh\", input_shape=self.input_shape, kernel_regularizer=keras.regularizers.l2(0.001), dropout=0.2, return_sequences=True))\n",
        "    model.add(LSTM(units, activation=\"tanh\", input_shape=self.input_shape, kernel_regularizer=keras.regularizers.l2(0.001), dropout=0.2, return_sequences=True))\n",
        "    model.add(LSTM(units, activation=\"tanh\", input_shape=self.input_shape, kernel_regularizer=keras.regularizers.l2(0.001), dropout=0.2, return_sequences=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(Dense(self.output_shape[0]))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ],
      "metadata": {
        "id": "jB-BaE7f0QQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (36, 1)\n",
        "output_shape = (35, 1)\n",
        "lstm_mod = lstm_model(input_shape, output_shape)\n",
        "model_v0 = lstm_mod.build_model(32)"
      ],
      "metadata": {
        "id": "JeuGVeuy0QNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_v0.summary()"
      ],
      "metadata": {
        "id": "ZavbY6PK0Bzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "model_v0.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "              metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])"
      ],
      "metadata": {
        "id": "mW-iwuAx0BwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FIT THE MODEL:**"
      ],
      "metadata": {
        "id": "OQk6ZTkHTkkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain_numpy.shape"
      ],
      "metadata": {
        "id": "Ln7BppuBZzTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_v0.fit(xtrain_numpy[:280,],ytrain_numpy[:280],\n",
        "              validation_data=[xtrain_numpy[280:,],ytrain_numpy[280:]],\n",
        "              epochs=120,\n",
        "              batch_size=16,\n",
        "              callbacks=[keras.callbacks.ReduceLROnPlateau(patience=5, min_lr=0.00001, factor=0.5),\n",
        "                         keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True, monitor=\"val_RMSE\",\n",
        "                                                        start_from_epoch=3, mode=\"min\")])"
      ],
      "metadata": {
        "id": "6O-Ghi-uS238"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_pred = model_v0.predict(xtest_numpy)\n",
        "\n",
        "lstm_pred = (lstm_pred*std_val) + mean_val"
      ],
      "metadata": {
        "id": "AX3_TJhJS2yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_pred"
      ],
      "metadata": {
        "id": "59G3WH6PS2s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_test_stor = (xtest_numpy*std_val) + mean_val\n",
        "\n",
        "lstm_test_stor"
      ],
      "metadata": {
        "id": "Je2bqdejXVwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_lstm = pd.Series(lstm_pred.flatten(), index=test_raw.index)"
      ],
      "metadata": {
        "id": "g1HndmDzVdxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_raw[-50:], color=\"royalblue\")\n",
        "plt.plot(test_raw, color=\"royalblue\", linestyle=\"dashed\")\n",
        "plt.plot(pred_lstm, color=\"salmon\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BPYFGCwrS2qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_raw.shape, pred_lstm.shape)\n",
        "root_mean_squared_error(test_raw[:12],pred_arima[:12])\n",
        "\n",
        "df_results = store_results(df_results, test_raw, pred_lstm, model_name=\"LSTM\")\n",
        "df_results"
      ],
      "metadata": {
        "id": "_5HFm-2gS2nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PR6atgJ1S2kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OErunfAAS2hY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.X.shape, data.y.shape, data.X_test.shape"
      ],
      "metadata": {
        "id": "w1JQWVV9duYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.cat_features)"
      ],
      "metadata": {
        "id": "YuDLMDjOe6x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.X.info(),data.X_test.info(),"
      ],
      "metadata": {
        "id": "EunCWfhBfcnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Preprocessing\n",
        "Here we define a preprocessing class to handle missing values and perform some feature engineering."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.011742,
          "end_time": "2024-10-26T19:31:36.380662",
          "exception": false,
          "start_time": "2024-10-26T19:31:36.36892",
          "status": "completed"
        },
        "tags": [],
        "id": "BsolTJDo1KTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, data: Data):\n",
        "        self.data = data\n",
        "\n",
        "    def feature_preparation(self):\n",
        "        ''' User-define feature engineering and preprocessing'''\n",
        "        print('⇒ Train-Test-Original shapes before preprocessing:',\n",
        "              self.data.train.shape, self.data.test.shape, self.data.original.shape)\n",
        "\n",
        "        df = pd.concat([self.data.train, self.data.test, self.data.original], axis=0)\n",
        "        ## -- Operations\n",
        "        # Convert categoricals and handle missing\n",
        "        df = self.convert_categorical(df)\n",
        "        df = self.handle_nan(df)\n",
        "\n",
        "        # Retrieve train, test and original\n",
        "        self.data.train    = df[:len(self.data.train)].reset_index(drop=True)\n",
        "        self.data.test     = df[len(self.data.train):len(self.data.train)+\\\n",
        "                                len(self.data.test)].reset_index(drop=True).drop(columns=data.target)\n",
        "        self.data.original = df[len(self.data.train)+len(self.data.test):].reset_index(drop=True)\n",
        "        print('⇒ Train-Test-Original shapes after preprocessing:', self.data.train.shape, self.data.test.shape, self.data.original.shape)\n",
        "        print('⇒ Preprocessing finished')\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_categorical(df):\n",
        "        '''For converting data to categoricals'''\n",
        "        for c in df.select_dtypes(include=['object', 'category', 'bool', 'int']).columns:\n",
        "            df[c] = df[c].astype(str).fillna('missing').astype('int')\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def handle_nan(df):\n",
        "        for c in df.select_dtypes(exclude=['object', 'category', 'bool', 'int']).columns:\n",
        "            df[c] = df[c].fillna(df[c].median())\n",
        "\n",
        "        for c in df.select_dtypes(include=['object', 'category', 'bool', 'int']).columns:\n",
        "            df[c] = df[c].fillna(df[c].mode()[0])\n",
        "\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_fi():\n",
        "\n",
        "        def sorted_fi(fi): # fi is a list of feature importances\n",
        "            fi = list(zip(data.X.columns.to_list(), fi))\n",
        "            return sorted(fi, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Make a pipeline to transform X for those method which can't handle categorical and NaN natively\n",
        "        pipe    = make_pipeline(SimpleImputer(strategy='most_frequent'), CountEncoder(), StandardScaler())\n",
        "        X_FE    = pipe.fit_transform(data.X)\n",
        "\n",
        "        FI_LGBM = sorted_fi(LGBMRegressor(verbose=-1, random_state=CFG.SEED).fit(data.X, data.y).feature_importances_)\n",
        "\n",
        "        FI_CB   = sorted_fi(CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features).fit(data.X, data.y).get_feature_importance())\n",
        "\n",
        "        FI_RF   = sorted_fi(RandomForestRegressor(n_estimators=100, random_state=CFG.SEED).fit(X_FE, data.y).feature_importances_)\n",
        "\n",
        "        FI_MI   = sorted_fi(list(mutual_info_regression(X_FE, data.y)))\n",
        "\n",
        "        FIS     = [FI_LGBM, FI_CB, FI_RF, FI_MI]\n",
        "        titles  = ['LGBM FI', 'CB FI', 'RF FI', 'MI']\n",
        "\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(25, 10))\n",
        "        for i, FI in enumerate(FIS):\n",
        "            # Convert the list of tuples to a DataFrame for easier plotting\n",
        "            FI_df = pd.DataFrame(FI[:15], columns=['Feature', 'Importance'])\n",
        "            sns.barplot(x='Importance', y='Feature', data=FI_df, ax=axes[i], palette='Blues_r')\n",
        "            axes[i].set_title(titles[i])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "data = Data()\n",
        "pp   = Preprocessing(data)\n",
        "pp.feature_preparation()\n",
        "#pp.calculate_fi()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:45.255839Z",
          "iopub.execute_input": "2025-02-03T01:57:45.256124Z",
          "iopub.status.idle": "2025-02-03T01:57:46.490442Z",
          "shell.execute_reply.started": "2025-02-03T01:57:45.256101Z",
          "shell.execute_reply": "2025-02-03T01:57:46.489645Z"
        },
        "papermill": {
          "duration": 26.783101,
          "end_time": "2024-10-26T19:32:03.175845",
          "exception": false,
          "start_time": "2024-10-26T19:31:36.392744",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "25jm-nnR1KTe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.cat_features,data.num_features"
      ],
      "metadata": {
        "id": "gFvHEDvOe0oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.X.info()"
      ],
      "metadata": {
        "id": "EEWwdyAUgkYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modeling"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.014189,
          "end_time": "2024-10-26T19:32:03.237017",
          "exception": false,
          "start_time": "2024-10-26T19:32:03.222828",
          "status": "completed"
        },
        "tags": [],
        "id": "7OZ_SPtJ1KTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.0 Neural Network Model Functions:"
      ],
      "metadata": {
        "id": "aG58nE96v8Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_features_card = []\n",
        "for f in data.cat_features:\n",
        "  cat_features_card.append(1 + data.X[f].astype(\"int\").max())\n",
        "\n",
        "cat_features_card"
      ],
      "metadata": {
        "id": "XpEkN-4CzCKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.saving import register_keras_serializable\n",
        "\n",
        "#####################################################################################################################################################################################\n",
        "def build_model_v0_mm(units=512,last_layer = 1, activation=\"relu\",cat_features=data.cat_features,cat_features_card=cat_features_card):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model\n",
        "\n",
        "#####################################################################################################################################################################################\n",
        "def build_model_v0(units=512,last_layer = 1, activation=\"relu\",cat_features=data.cat_features,cat_features_card=cat_features_card):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(units, activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model\n",
        "\n",
        "#####################################################################################################################################################################################\n",
        "def build_model_v1(units=512,last_layer = 1, activation=\"relu\", cat_features=data.cat_features,cat_features_card=cat_features_card, dropout_rate=0.35, reg=0.001):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    reshaped_features = layers.Reshape((1, -1))(x)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x, flattened_attention])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model\n",
        "\n",
        "#####################################################################################################################################################################################\n",
        "def build_model_v2(units=512, last_layer=1, activation=\"relu\", cat_features=data.cat_features,cat_features_card=cat_features_card,\n",
        "                   repeat_att=2, dropout_rate=0.2, num_transformer_heads=4, transformer_units=64, reg=0.0001): # Reduced transformer_units\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(cat_features),))\n",
        "    embs = []\n",
        "    transformer_outputs = [] # List to store transformer outputs for each categorical feature\n",
        "\n",
        "    for j in range(len(cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:, j])\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "        embs.append(x)\n",
        "\n",
        "        # Reshape for Transformer (batch_size, 1, features) - Crucial!\n",
        "        reshaped_embedding = layers.Reshape((1, -1))(x)\n",
        "\n",
        "        # Transformer Layer for each categorical feature\n",
        "        for q in list(range(repeat_att)):\n",
        "          if q == 0:\n",
        "            attention_output = reshaped_embedding\n",
        "\n",
        "          attention_output_ = layers.MultiHeadAttention(num_heads=num_transformer_heads, key_dim=transformer_units,name=f\"mh_{j}_{q}\")(attention_output, attention_output)\n",
        "          attention_output_ = layers.LayerNormalization(name=f\"mh_ln1_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "          attention_output_ = layers.Dense(reshaped_embedding.shape[-1], activation=activation,name=f\"mh_dense_{j}_{q}\")(attention_output_)\n",
        "          attention_output = layers.LayerNormalization(name=f\"mh_ln2_{j}_{q}\")(attention_output + attention_output_) #ResNet_1\n",
        "\n",
        "        transformer_outputs.append(layers.Flatten()(attention_output)) # Store flattened transformer output\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    # Reshape for the Attention layer.  Crucial for keras.layers.Attention\n",
        "    # The Attention layer expects 3D tensors. Even if your \"sequence\"\n",
        "    # length is 1, you MUST add a dimension.\n",
        "\n",
        "    x_orig = layers.Concatenate(axis=-1)(embs+[x_input_nums])\n",
        "    reshaped_features = layers.Reshape((1, -1))(x_orig)\n",
        "\n",
        "    attention_output = layers.Attention()([reshaped_features, reshaped_features])  # Self-attention\n",
        "\n",
        "    # Flatten the attention output:\n",
        "    flattened_attention = layers.Flatten()(attention_output)\n",
        "\n",
        "    # Concatenate with original features (optional but often helpful):\n",
        "    x = layers.Concatenate(axis=-1)([x_orig, flattened_attention])\n",
        "\n",
        "    # Concatenate Transformer outputs and numerical features\n",
        "    all_features = layers.Concatenate(axis=-1)(transformer_outputs + [x])\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(all_features)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(units, activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = layers.Dense(int(units/last_layer), activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    #x = layers.Concatenate(axis=-1)([x_orig, x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model\n",
        "\n",
        "  ####################################################\n",
        "@register_keras_serializable()\n",
        "class exitesqueeze_layer(layers.Layer):\n",
        "    def __init__(self, exite_units,dropout_rate,activation,reg,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.exite_units = exite_units\n",
        "        self.activation=activation\n",
        "        self.reg=reg\n",
        "        self.dropout_rate=dropout_rate\n",
        "\n",
        "        self.reshaped_0 = layers.Reshape((-1, 1))\n",
        "        self.reshaped_1 = layers.Reshape((-1, ))\n",
        "\n",
        "        self.exite = layers.Dense(self.exite_units, activation=self.activation)\n",
        "        self.squeeze = layers.Dense(1, activation=\"linear\",kernel_regularizer=keras.regularizers.l2(reg))\n",
        "        self.lnorm_00 = layers.LayerNormalization()\n",
        "        self.lnorm_01 = layers.LayerNormalization()\n",
        "        self.drop = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.attention = layers.Attention()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.reshaped_0(inputs)\n",
        "        x = self.exite(x)\n",
        "        att_out = self.attention([x,x])\n",
        "        att_out = self.lnorm_00(att_out)\n",
        "        x = layers.add([x, att_out])\n",
        "        x = self.squeeze(x)\n",
        "        x = self.reshaped_1(x)\n",
        "\n",
        "        x = layers.multiply([x, inputs])\n",
        "\n",
        "        x = self.lnorm_01(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Remove build warnings\n",
        "    def build(self):\n",
        "        self.built = True\n",
        "\n",
        "    # Required for serialization:\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'exite_units': self.exite_units,\n",
        "            'dropout_rate': self.dropout_rate, # Include necessary config items\n",
        "            'activation': self.activation,\n",
        "            'reg': self.reg\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "def build_model_v3(units=512,exite_units=64, last_layer = 1, activation=\"relu\", reg=0.001, dropout_rate=0.33):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(data.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(data.cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    x_0 = layers.Concatenate(axis=-1, name=\"input_concat\")(embs+[x_input_nums])\n",
        "\n",
        "    es_0 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_0_concat\")([x_0,es_0])\n",
        "    x = layers.BatchNormalization(name=\"se_0_bn\")(x)\n",
        "\n",
        "    es_1 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_1_concat\")([x,es_1])\n",
        "    x = layers.BatchNormalization(name=\"se_1_bn\")(x)\n",
        "\n",
        "    es_2 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_2_concat\")([x,es_2])\n",
        "    x = layers.BatchNormalization(name=\"se_2_bn\")(x)\n",
        "\n",
        "    x_0 = layers.Dense(units, name=\"dense_0\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_0\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_0\")(x_0)\n",
        "\n",
        "    x_0 = layers.Dense(int(units/last_layer), name=\"dense_1\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_1\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_1\")(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([x_0,x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "hb64lyLLwCoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Stacking"
      ],
      "metadata": {
        "id": "OOAMQ6WiwE8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainModels:\n",
        "    def __init__(self, X, y, X_test, X_original, y_original, models):\n",
        "        self.models     = models\n",
        "        self.X          = X\n",
        "        self.y          = y\n",
        "        self.X_original = X_original\n",
        "        self.y_original = y_original\n",
        "        self.X_test     = X_test\n",
        "        self._OOF_train = pd.DataFrame()\n",
        "        self._OOF_test  = pd.DataFrame()\n",
        "        self.categorical_features = X.select_dtypes(include=['category', 'bool', 'category','int']).columns.to_list()\n",
        "        self.numerical_features = X.select_dtypes(include=['float']).columns.to_list()\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_cat(df_):\n",
        "        '''\n",
        "        A function to convert dtypes to categorical if the catboost with all categorical\n",
        "        features is called\n",
        "        '''\n",
        "        df = df_.copy()\n",
        "        for c in df.columns.to_list():\n",
        "            df[c] = df[c].astype(str).astype('category')\n",
        "        return df\n",
        "\n",
        "\n",
        "    def fit_model(self, name, model_, train_flag):\n",
        "        oof_train = np.zeros(self.X.shape[0])\n",
        "        oof_test  = np.zeros(self.X_test.shape[0])\n",
        "        scores_train = []\n",
        "        scores_val   = []\n",
        "\n",
        "        CB_CAT_FLAG = True if 'CB_CAT' in name.upper() else False\n",
        "\n",
        "        os.chdir('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Models/S5E2/layers_3_staked_models_opt')\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(CFG.CV.split(self.X, self.y)):\n",
        "            x_train, y_train = self.X.iloc[train_idx], self.y.iloc[train_idx]\n",
        "            x_val,   y_val   = self.X.iloc[val_idx],   self.y.iloc[val_idx]\n",
        "\n",
        "            # Adds the original data to training set only\n",
        "            if self.X_original is not None:\n",
        "                x_train = pd.concat([x_train, self.X_original], axis=0)\n",
        "                y_train = pd.concat([y_train, self.y_original], axis=0)\n",
        "\n",
        "            x_train[self.categorical_features] = x_train[self.categorical_features].astype(\"int\")\n",
        "            x_train[self.numerical_features] = x_train[self.numerical_features].astype(\"float\")\n",
        "\n",
        "            x_val[self.categorical_features] = x_val[self.categorical_features].astype(\"int\")\n",
        "            x_val[self.numerical_features] = x_val[self.numerical_features].astype(\"float\")\n",
        "\n",
        "            self.X_test[self.categorical_features] = self.X_test[self.categorical_features].astype(\"int\")\n",
        "            self.X_test[self.numerical_features] = self.X_test[self.numerical_features].astype(\"float\")\n",
        "\n",
        "            #print(x_train.info())\n",
        "            #print(x_val.info())\n",
        "            #print(self.X_test.info())\n",
        "\n",
        "            # -- Create a special block to convert all features to categorical\n",
        "            if CB_CAT_FLAG:\n",
        "                x_train = self.convert_cat(x_train)\n",
        "                x_val   = self.convert_cat(x_val)\n",
        "            if 'NN_' in name.upper():\n",
        "              model = model_\n",
        "            elif 'NNMM_' in name.upper():\n",
        "              model = model_\n",
        "            else:\n",
        "              model = clone(model_)\n",
        "\n",
        "#######################################################################################\n",
        "            if 'NN_' in name.upper():\n",
        "\n",
        "              model = model_\n",
        "\n",
        "              X_train_cat = x_train[self.categorical_features].astype(\"int32\")\n",
        "              X_train_num = x_train[self.numerical_features].astype(\"float32\")\n",
        "\n",
        "              X_valid_cat = x_val[self.categorical_features].astype(\"int32\")\n",
        "              X_valid_num = x_val[self.numerical_features].astype(\"float32\")\n",
        "\n",
        "              X_test_cat = self.X_test[self.categorical_features].astype(\"int32\")\n",
        "              X_test_num = self.X_test[self.numerical_features].astype(\"float32\")\n",
        "\n",
        "              # Compile the model\n",
        "              keras.utils.set_random_seed(42)\n",
        "\n",
        "              optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "              model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mse\"),\n",
        "                            metrics=[keras.metrics.RootMeanSquaredError(name=\"rmse\")])\n",
        "\n",
        "              # Fit the model\n",
        "              if train_flag==True:\n",
        "                model.fit([X_train_cat,X_train_num], y_train,\n",
        "                          validation_data=([X_valid_cat, X_valid_num], y_val),\n",
        "                          epochs=111,\n",
        "                          batch_size=2048,\n",
        "                          callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3, monitor=\"val_rmse\", factor=0.5, min_lr=5e-5),\n",
        "                                     keras.callbacks.EarlyStopping(patience=21, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                                    start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "                model.save(f'{name}_{CFG.VERSION}.keras') # Saving the model after training\n",
        "\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = keras.models.load_model(f'{name}_{CFG.VERSION}.keras') # Loading the saved model\n",
        "\n",
        "#######################################################################################\n",
        "            if 'NNMM_' in name.upper():\n",
        "\n",
        "              model = model_\n",
        "\n",
        "              X_train_cat = x_train[self.categorical_features].astype(\"int32\")\n",
        "              X_train_num = x_train[self.numerical_features].astype(\"float32\")\n",
        "\n",
        "              X_valid_cat = x_val[self.categorical_features].astype(\"int32\")\n",
        "              X_valid_num = x_val[self.numerical_features].astype(\"float32\")\n",
        "\n",
        "              X_test_cat = self.X_test[self.categorical_features].astype(\"int32\")\n",
        "              X_test_num = self.X_test[self.numerical_features].astype(\"float32\")\n",
        "\n",
        "              y_train_mm = (y_train-15)/135\n",
        "              y_val_mm = (y_val-15)/135\n",
        "\n",
        "              # Compile the model\n",
        "              keras.utils.set_random_seed(42)\n",
        "\n",
        "              optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "              model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mse\"),\n",
        "                            metrics=[keras.metrics.RootMeanSquaredError(name=\"rmse\")])\n",
        "\n",
        "              # Fit the model\n",
        "              if train_flag==True:\n",
        "                model.fit([X_train_cat,X_train_num], y_train_mm,\n",
        "                          validation_data=([X_valid_cat, X_valid_num], y_val_mm),\n",
        "                          epochs=111,\n",
        "                          batch_size=2048,\n",
        "                          callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3, monitor=\"val_rmse\", factor=0.5, min_lr=5e-5),\n",
        "                                     keras.callbacks.EarlyStopping(patience=21, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                                    start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "                model.save(f'{name}_{CFG.VERSION}.keras') # Saving the model after training\n",
        "\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = keras.models.load_model(f'{name}_{CFG.VERSION}.keras') # Loading the saved model\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "            elif 'CB' in name.upper():\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train,y_train,eval_set=[(x_val,y_val)], early_stopping_rounds=101)\n",
        "                model.save_model(f\"{name}_{fold}_{CFG.VERSION}.bin\")\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = CatBoostRegressor()\n",
        "                model.load_model(f\"{name}_{fold}_{CFG.VERSION}.bin\")\n",
        "\n",
        "            elif 'LGBMR' in name.upper():\n",
        "              early_stop = early_stopping(stopping_rounds=101)\n",
        "              print(self.categorical_features)\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train, y_train, eval_set=[(x_val, y_val)], callbacks=[early_stop],categorical_feature=self.categorical_features)\n",
        "                model.booster_.save_model(f'{name}_{fold}_{CFG.VERSION}.txt')\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = lgb.Booster(model_file=f'{name}_{fold}_{CFG.VERSION}.txt')\n",
        "\n",
        "            elif 'LGBMP' in name.upper():\n",
        "              # Fit the model\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train, y_train)\n",
        "                dump(model, f'{name}_{fold}_{CFG.VERSION}.pkl')\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = load(f'{name}_{fold}_{CFG.VERSION}.pkl')\n",
        "\n",
        "            elif 'XGBR' in name.upper():\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train, y_train, eval_set=[(x_val, y_val)],verbose=250)\n",
        "                model.save_model(f'{name}_{fold}_{CFG.VERSION}.json')\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = XGBRegressor()  # Create an instance of the model\n",
        "                model.load_model(f'{name}_{fold}_{CFG.VERSION}.json')\n",
        "\n",
        "            elif 'LGBM_2' in name.upper():\n",
        "              model.fit(x_train, y_train, eval_set=[(x_val, y_val)])\n",
        "\n",
        "\n",
        "            elif 'HGB_' in name.upper():\n",
        "              if train_flag==True:\n",
        "                model.fit(x_train, y_train)\n",
        "                dump(model, f'{name}_{fold}_{CFG.VERSION}.pkl')\n",
        "              else:\n",
        "                print(\"Load Model\")\n",
        "                model = load(f'{name}_{fold}_{CFG.VERSION}.pkl')\n",
        "\n",
        "            else:\n",
        "\n",
        "              model.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "            #################### Predictions\n",
        "            if 'NN_' in name.upper():\n",
        "              y_pred_train = model.predict([X_train_cat,X_train_num], batch_size=2048).ravel()\n",
        "              y_pred_val   = model.predict([X_valid_cat, X_valid_num], batch_size=2048).ravel()\n",
        "              y_pred_test  = model.predict([X_test_cat, X_test_num], batch_size=1024).ravel()\n",
        "\n",
        "            elif 'NNMM_' in name.upper():\n",
        "              y_pred_train = model.predict([X_train_cat,X_train_num], batch_size=2048).ravel()\n",
        "              y_pred_val   = model.predict([X_valid_cat, X_valid_num], batch_size=2048).ravel()\n",
        "              y_pred_test  = model.predict([X_test_cat, X_test_num], batch_size=1024).ravel()\n",
        "\n",
        "              y_pred_train = y_pred_train*135+15\n",
        "              y_pred_val   = y_pred_val*135+15\n",
        "              y_pred_test  = y_pred_test*135+15\n",
        "\n",
        "            else:\n",
        "              y_pred_train = model.predict(x_train).ravel()\n",
        "              y_pred_val   = model.predict(x_val).ravel()\n",
        "              y_pred_test  = model.predict(self.X_test if not CB_CAT_FLAG else self.convert_cat(self.X_test)).ravel()\n",
        "\n",
        "            oof_train[val_idx] = y_pred_val\n",
        "            oof_test   += y_pred_test/CFG.CV.get_n_splits()\n",
        "\n",
        "            train_score = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "            val_score   = np.sqrt(mean_squared_error(y_val, y_pred_val))\n",
        "\n",
        "            print(f'Fold {fold+1} → Training set Score: {train_score:.5f} | Validation set Score: {val_score:.5f}')\n",
        "\n",
        "            scores_train.append(train_score)\n",
        "            scores_val.append(val_score)\n",
        "\n",
        "        self._OOF_train[name] = oof_train\n",
        "        self._OOF_test[name]  = oof_test\n",
        "\n",
        "        os.chdir('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2')\n",
        "\n",
        "        print(colored(f'Overall → Training set Score: {np.mean(scores_train):.5f}±{np.std(scores_train):.7f} | Validation set Score: {np.mean(scores_val):.5f}±{np.std(scores_val):.7f}',\n",
        "              color='green', attrs=['bold', 'dark']))\n",
        "\n",
        "    def fit_models(self):\n",
        "        for name, model in list(self.models.items()):\n",
        "            print(colored(f'{\" \"*4} Fitting {name}', color='red', attrs=['dark', 'bold']))\n",
        "            self.fit_model(name, model[0], model[1])\n",
        "            print(\"\")\n",
        "\n",
        "    @property\n",
        "    def OOF_train(self):\n",
        "        return self._OOF_train\n",
        "    @property\n",
        "    def OOF_test(self):\n",
        "        return self._OOF_test\n",
        "\n",
        "    def save_predictions(self):\n",
        "        self._OOF_train.to_csv('OOF_train_many_models.csv', index=False)\n",
        "        self._OOF_test.to_csv('OOF_test_many_models.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:46.491421Z",
          "iopub.execute_input": "2025-02-03T01:57:46.491688Z",
          "iopub.status.idle": "2025-02-03T01:57:46.503925Z",
          "shell.execute_reply.started": "2025-02-03T01:57:46.491663Z",
          "shell.execute_reply": "2025-02-03T01:57:46.50294Z"
        },
        "papermill": {
          "duration": 0.044089,
          "end_time": "2024-10-26T19:32:03.295732",
          "exception": false,
          "start_time": "2024-10-26T19:32:03.251643",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "WypqJp7Q1KTh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.1 First Level of Stacking\n",
        "We define a bunch of tree-based methods with many different hyperparameters configurations to aggregate diversity."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.014633,
          "end_time": "2024-10-26T19:32:03.325194",
          "exception": false,
          "start_time": "2024-10-26T19:32:03.310561",
          "status": "completed"
        },
        "tags": [],
        "id": "mFxQScB81KTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2')"
      ],
      "metadata": {
        "id": "XEz8PpD8koh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.X.info(),data.test.info()\n",
        "data.X.select_dtypes(include=['category', 'bool', 'category','int']).columns.to_list()"
      ],
      "metadata": {
        "id": "Um7Q4e55lmwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgbm_params_1 = {'num_leaves': 103, 'min_child_samples': 36, 'subsample': 0.9131771240297577, 'subsample_freq': 2, 'colsample_bytree': 0.6190291906152294, 'reg_alpha': 0.03976551748855951, 'reg_lambda': 0.2576052197300848}\n",
        "lgbm_params_2 = {'max_depth': 5, 'learning_rate': 0.025, 'min_child_samples': 51, 'subsample': 0.90, 'subsample_freq': 1, 'colsample_bytree': 0.75, 'reg_alpha': 0.01, 'reg_lambda': 0.05}\n",
        "#lgbm_params_5 = {'max_depth': 5, 'learning_rate': 0.055, 'n_estimators': 3000, 'num_leaves': 31}\n",
        "#lgbm_params_6 = {'max_depth': 6, 'learning_rate': 0.01, 'n_estimators': 3000, 'num_leaves': 31}\n",
        "#lgbm_params_6 = {'max_depth': 9, 'learning_rate': 0.02, 'n_estimators': 2000, 'num_leaves': 31, \"subsample\":0.85, \"colsample_bytree\":0.85,\n",
        "#                 \"reg_alpha\":0.01, \"reg_lambda\":0.01, \"boosting_type\":\"gbdt\" }\n",
        "\n",
        "cb_cat_params_1   = {'iterations': 3000,'learning_rate': 0.025,'depth': 9, 'l2_leaf_reg': 0.004177701145518355,\n",
        "                     'bagging_temperature': 0.5, 'random_strength': 0.5,\"bootstrap_type\": \"Bayesian\",\n",
        "                     'cat_features': data.cat_features,'task_type': 'GPU',\n",
        "                     'random_seed':CFG.SEED,'verbose': 250,\"od_type\":'EBS',\"od_wait\":101,\"use_best_model\":True}\n",
        "\n",
        "cb_cat_params_2   = {'iterations': 1000,'learning_rate': 0.025,'depth': 4, 'l2_leaf_reg': 0.001,\n",
        "                     'bagging_temperature': 0.3, 'random_strength': 0.3,\"bootstrap_type\": \"Bayesian\",\n",
        "                     'cat_features': data.cat_features,'task_type': 'GPU',\n",
        "                     'random_seed':CFG.SEED,'verbose': 250,\"od_type\":'EBS',\"od_wait\":101,\"use_best_model\":True}\n",
        "\n",
        "xgb_params_1 = {'n_estimators': 3000,'learning_rate': 0.025,'max_depth': 9,'min_child_weight': 9,'subsample': 0.95,'colsample_bytree': 0.75,\n",
        "                'gamma': 0.5511841320880777,'reg_alpha': 0.006948944983035811,'reg_lambda': 0.0015661314558322087,\n",
        "                'objective': \"reg:squarederror\",'eval_metric': \"rmse\", \"early_stopping_rounds\":101, 'device':'gpu', 'tree_method': 'gpu_hist',\n",
        "                'random_state': CFG.SEED,'enable_categorical': True,\"verbosity\":0}\n",
        "\n",
        "xgb_params_2 = {'n_estimators': 1500,'learning_rate': 0.025,'max_depth': 5,'min_child_weight': 9,'subsample': 0.90,'colsample_bytree': 0.80,\n",
        "                'gamma': 0.10,'reg_alpha': 0.01,'reg_lambda': 0.01, 'objective': \"reg:squarederror\",'eval_metric': \"rmse\",\n",
        "                \"early_stopping_rounds\":101, 'device':'gpu', 'tree_method': 'gpu_hist','random_state': CFG.SEED,'enable_categorical': True,\"verbosity\":0}\n",
        "\n",
        "nn_params_v0 = {'units': 1024, 'last_layer': 2, 'activation': 'silu'}\n",
        "nn_params_v1 = {'units': 512, 'last_layer': 1, 'activation': 'relu', 'reg': 0.0002133354415296107, 'dropout_rate': 0.47090232042440616}\n",
        "nn_params_v2 = {'units': 256, 'last_layer': 1, 'activation': 'silu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.36}\n",
        "\n",
        "models={\n",
        "#        'LGBMR1_opt': [LGBMRegressor(learning_rate=0.02,boosting_type='gbdt',n_estimators=3000,objective=\"regression\",eval_metric=\"rmse\",device='gpu',verbose=-1,random_state= CFG.SEED,**lgbm_params_1),False],\n",
        "#        'LGBMR2_opt': [LGBMRegressor(verbose=-1, random_state=CFG.SEED, boosting_type='gbdt', device='cpu',n_estimators=1000,objective=\"regression\",eval_metric=\"rmse\", **lgbm_params_2),False],\n",
        "        #'LGBMR3': [LGBMRegressor(verbose=-1, random_state=CFG.SEED, data_sample_strategy='GOSS', device='gpu',n_estimators=211),False],\n",
        "        #'LGBMP4': [make_pipeline(CatBoostEncoder(), LGBMRegressor(verbose=-1, random_state=CFG.SEED, **lgbm_params_4, device='gpu')),False],\n",
        "        #'LGBMR5': [LGBMRegressor(verbose=-1, random_state=CFG.SEED, **lgbm_params_5, device='gpu'),False],\n",
        "        #'LGBMR6': [LGBMRegressor(verbose=-1, random_state=CFG.SEED, **lgbm_params_6, device='gpu'),False],\n",
        "\n",
        "#        'CB1_opt'   :[CatBoostRegressor(**cb_cat_params_1),False],\n",
        "#        'CB2_opt'   : [CatBoostRegressor(**cb_cat_params_2),False],\n",
        "    #    'CB3'   : [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features, grow_policy='Lossguide', loss_function='RMSE', task_type='GPU', iterations=3000),False],\n",
        "    #    'CB4'   : [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features, **cb_cat_params_4, loss_function='RMSE', task_type='GPU', iterations=3000),False],\n",
        "    #    'CB5'   : [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features, iterations=3000, task_type='GPU'),False],\n",
        "    #   'CB_CAT1': [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.test.columns.to_list(), iterations=750, task_type='GPU'),True],\n",
        "    #    'CB_CAT2': [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.test.columns.to_list(), **cb_cat_params_4, loss_function='RMSE', task_type='GPU'),True],\n",
        "\n",
        "#        'XGBR1_opt' : [XGBRegressor(**xgb_params_1),False],\n",
        "#        'XGBR2_opt' : [XGBRegressor(**xgb_params_2),False],\n",
        "\n",
        "\n",
        "#        'HGB_TE': [make_pipeline(TargetEncoder(), HistGradientBoostingRegressor(random_state=CFG.SEED, max_iter=5000,\n",
        "#                                                                              learning_rate=0.035, max_depth=8,validation_fraction=0.15,\n",
        "#                                                                              early_stopping=True, l2_regularization=0.03,\n",
        "#                                                                             )),False],\n",
        "\n",
        "      #  'HGB_CE': [make_pipeline(CountEncoder(), HistGradientBoostingRegressor(random_state=CFG.SEED, max_iter=5000,\n",
        "      #                                                                          learning_rate=0.055, max_depth=5,\n",
        "      #                                                                          early_stopping=True,  validation_fraction=0.15\n",
        "      #                                                                        )),True],\n",
        "\n",
        "#        \"NN_v0\" : [build_model_v0(**nn_params_v0),True],\n",
        "#        \"NN_v1\" : [build_model_v1(**nn_params_v1),False],\n",
        "#        \"NN_v2\" : [build_model_v3(**nn_params_v2),False],\n",
        "         \"NNMM_v0\" : [build_model_v0_mm(**nn_params_v0),True],\n",
        "        }\n",
        "\n",
        "# TM = TrainModels(X=data.X, y=data.y, X_test=data.X_test, X_original=None, y_original=None, models=models)\n",
        "# TM.fit_models()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T01:57:46.505448Z",
          "iopub.execute_input": "2025-02-03T01:57:46.505649Z",
          "iopub.status.idle": "2025-02-03T02:51:59.869417Z",
          "shell.execute_reply.started": "2025-02-03T01:57:46.505631Z",
          "shell.execute_reply": "2025-02-03T02:51:59.868592Z"
        },
        "papermill": {
          "duration": 1396.693239,
          "end_time": "2024-10-26T19:55:20.033429",
          "exception": false,
          "start_time": "2024-10-26T19:32:03.34019",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "CzGQ1DP71KTi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# TM.OOF_test.to_csv(\"Stacked_Models_step1_OOF_test.csv\")\n",
        "# TM.OOF_train.to_csv(\"Stacked_Models_step1_OOF_train.csv\")"
      ],
      "metadata": {
        "id": "kN8VXc4Ua6tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOF_train = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/Stacked_Models_step1_OOF_train.csv\", index_col=0)\n",
        "OOF_test = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/Stacked_Models_step1_OOF_test.csv\", index_col=0)"
      ],
      "metadata": {
        "id": "BA--HWmebXOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OOF_train[\"NNMM_v0\"] = TM.OOF_train[\"NNMM_v0\"].values\n",
        "# OOF_test[\"NNMM_v0\"] = TM.OOF_test[\"NNMM_v0\"].values\n",
        "OOF_train[\"Target\"] = data.y\n",
        "OOF_train.head(5)"
      ],
      "metadata": {
        "id": "Ad3zmhyTb0Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OOF_test.to_csv(\"Stacked_Models_step1_OOF_test.csv\")\n",
        "# OOF_train.to_csv(\"Stacked_Models_step1_OOF_train.csv\")"
      ],
      "metadata": {
        "id": "aPdzqZ6n4OuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOF_train.corr()"
      ],
      "metadata": {
        "id": "yde1IGghb-a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = OOF_train[\"Target\"]\n",
        "x2 = OOF_train[\"LGBMR1_opt\"]\n",
        "\n",
        "plt.scatter(x1,x2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MyDHUxbdcYgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOF_train_clipped = OOF_train.clip(15,150).copy()\n",
        "OOF_test_clipped = OOF_test.clip(15,150).copy()\n",
        "\n",
        "OOF_train_clipped.describe()"
      ],
      "metadata": {
        "id": "jMB0Y0qDSuGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOF_train_clipped.corr()"
      ],
      "metadata": {
        "id": "vsBLnhydTDAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOF_train_clipped.describe()"
      ],
      "metadata": {
        "id": "DFOpS6XwZ-Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.2 Second Level of Stacking\n",
        "We take the out-of-folds predictions from the previous models and fed them into simple linear methods like Ridge and Lasso. The alpha parameters have been tuned manually (i.e., trial and error)."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.033525,
          "end_time": "2024-10-26T19:55:20.101705",
          "exception": false,
          "start_time": "2024-10-26T19:55:20.06818",
          "status": "completed"
        },
        "tags": [],
        "id": "Tl19oKm91KTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models2 = {\n",
        "          'Ridge2':[ Ridge(alpha=1),True],\n",
        "          'Lasso2': [Lasso(alpha=0.0002),True],\n",
        "          'LGBM_2' : [LGBMRegressor(max_depth=2, random_state=CFG.SEED, device='gpu'),True],\n",
        "          }\n",
        "\n",
        "TM2 = TrainModels(X=OOF_train_clipped.drop(\"Target\",axis=1), y=data.y, X_test=OOF_test_clipped, X_original=None, y_original=None, models=models2)\n",
        "TM2.fit_models()\n",
        "data.submit(sub=TM2.OOF_test.mean(axis=1), desc='lasso_ridge_mean')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T02:51:59.870325Z",
          "iopub.execute_input": "2025-02-03T02:51:59.870552Z",
          "iopub.status.idle": "2025-02-03T02:52:39.627066Z",
          "shell.execute_reply.started": "2025-02-03T02:51:59.870532Z",
          "shell.execute_reply": "2025-02-03T02:52:39.626195Z"
        },
        "papermill": {
          "duration": 1.891352,
          "end_time": "2024-10-26T19:55:22.027553",
          "exception": false,
          "start_time": "2024-10-26T19:55:20.136201",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "AcuYSPyM1KTj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation set Score Unclipped: 38.35725±0.3112433\n",
        "\n",
        "Validation set Score Clipped: 38.35690±0.3115080"
      ],
      "metadata": {
        "id": "iuCTx3FHloUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TM2_OOF_train = TM2.OOF_train.copy()\n",
        "TM2_OOF_test = TM2.OOF_test.copy()"
      ],
      "metadata": {
        "id": "blVXJ6EtkRfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\",index_col=0)\n",
        "submission[\"Price\"] = TM2.OOF_test.mean(axis=1).values\n",
        "submission.to_csv(\"SUB_01_Second_Layer_clipped.csv\")\n",
        "second_layer_output = submission.copy()\n",
        "display(submission.head())"
      ],
      "metadata": {
        "id": "0C1DmHybk05n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1.3 Third Level of Stacking\n",
        "At this stage, the first level and second level out-of-fold predictions are incorporated into the original training data. This is then fed into another level of models to output the final stage of out-of-fold predictions which are finally averaged."
      ],
      "metadata": {
        "papermill": {
          "duration": 0.034722,
          "end_time": "2024-10-26T19:55:22.099225",
          "exception": false,
          "start_time": "2024-10-26T19:55:22.064503",
          "status": "completed"
        },
        "tags": [],
        "id": "0IuFkaqj1KTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.cat_features,data.num_features"
      ],
      "metadata": {
        "id": "JNXfvkL7e871"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.concat([data.X,      OOF_train.drop(\"Target\",axis=1), TM2_OOF_train], axis=1)\n",
        "test  = pd.concat([data.X_test, OOF_test,  TM2_OOF_test], axis=1)\n",
        "\n",
        "models3 = {\n",
        "          'LGBM_21' : [LGBMRegressor(verbose=-1, random_state=CFG.SEED, max_depth=4, device='gpu'),True],\n",
        "          'CB_3'   : [CatBoostRegressor(verbose=0, random_state=CFG.SEED, cat_features=data.cat_features, task_type='GPU'),True],\n",
        "          'Ridge_3': [make_pipeline(TargetEncoder(cols=data.cat_features), Ridge(alpha=5)),True],\n",
        "          'Lasso_3': [make_pipeline(TargetEncoder(cols=data.cat_features), Lasso(alpha=0.001)),True],\n",
        "          }\n",
        "\n",
        "TM3  = TrainModels(X=train, y=data.y, X_test=test, X_original=None, y_original=None, models=models3)\n",
        "TM3.fit_models()\n",
        "\n",
        "TM3.save_predictions()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T02:52:39.628014Z",
          "iopub.execute_input": "2025-02-03T02:52:39.628303Z",
          "iopub.status.idle": "2025-02-03T03:02:09.91606Z",
          "shell.execute_reply.started": "2025-02-03T02:52:39.628278Z",
          "shell.execute_reply": "2025-02-03T03:02:09.915226Z"
        },
        "papermill": {
          "duration": 423.130115,
          "end_time": "2024-10-26T20:02:25.264332",
          "exception": false,
          "start_time": "2024-10-26T19:55:22.134217",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "7XVP_opO1KTk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.submit(sub=TM3.OOF_test.mean(axis=1), desc='final_layer')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-02-03T03:02:09.91696Z",
          "iopub.execute_input": "2025-02-03T03:02:09.917291Z",
          "iopub.status.idle": "2025-02-03T03:02:10.278403Z",
          "shell.execute_reply.started": "2025-02-03T03:02:09.917259Z",
          "shell.execute_reply": "2025-02-03T03:02:10.277614Z"
        },
        "papermill": {
          "duration": 0.06412,
          "end_time": "2024-10-26T20:02:25.427822",
          "exception": false,
          "start_time": "2024-10-26T20:02:25.363702",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "Ui09Yz5y1KTk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "TM3.OOF_test.mean(axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-03T03:12:48.702102Z",
          "iopub.execute_input": "2025-02-03T03:12:48.702378Z",
          "iopub.status.idle": "2025-02-03T03:12:48.737359Z",
          "shell.execute_reply.started": "2025-02-03T03:12:48.702357Z",
          "shell.execute_reply": "2025-02-03T03:12:48.736785Z"
        },
        "id": "3UwUPsOx1KTl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "TM3.save_predictions()"
      ],
      "metadata": {
        "trusted": true,
        "id": "XMtQ8Rf-1KTl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "TM3.OOF_test.mean(axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "q-VVElGU1KTm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv(\"/content/drive/MyDrive/Exercises/Studies_Structured_Data/Data/S5E2/sample_submission.csv\",index_col=0)\n",
        "submission[\"Price\"] = TM3.OOF_test.mean(axis=1).values\n",
        "submission.to_csv(\"SUB_04_Final_Layer.csv\")\n",
        "final_layer_output = submission.copy()\n",
        "display(submission.head())\n",
        "submission[\"Price\"] = TM2.OOF_test.mean(axis=1).values\n",
        "submission.to_csv(\"SUB_04_Second_Layer.csv\")\n",
        "display(submission.head())\n",
        "second_layer_output = submission.copy()"
      ],
      "metadata": {
        "id": "DBNe-yhLnsdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOF_train_clipped = OOF_train_clipped.clip(15,150)\n",
        "OOF_test_clipped = OOF_test_clipped.clip(15,150)\n",
        "OOF_test_clipped.describe()"
      ],
      "metadata": {
        "id": "m-GKbMevoq9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission[\"Price\"] = OOF_test_clipped[\"NN_v0\"].values\n",
        "submission.to_csv(\"SUB_03_NN0.csv\")"
      ],
      "metadata": {
        "id": "z4Z1f8r7phKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_mean_squared_error(OOF_train_clipped[\"Target\"],OOF_train_clipped[\"NN_v0\"])"
      ],
      "metadata": {
        "id": "iCXcFwNSadBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TM3.OOF_train[\"Target\"]=data.y.values"
      ],
      "metadata": {
        "id": "tL54z0zIF4Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OOF_train_level_3 = TM3.OOF_train\n",
        "OOF_train_level_3\n",
        "\n",
        "all_forecast_train = pd.concat([OOF_train.drop(\"Target\",axis=1), TM2_OOF_train,OOF_train_level_3], axis=1)\n",
        "all_forecast_test = pd.concat([OOF_test,  TM2_OOF_test,TM3.OOF_test], axis=1)"
      ],
      "metadata": {
        "id": "3e-cRbjwF3-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_forecast_test.corr()"
      ],
      "metadata": {
        "id": "FHS-FAWYF377"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_forecast_train.to_csv(\"all_forecast_train.csv\")\n",
        "all_forecast_test.to_csv(\"all_forecast_test.csv\")"
      ],
      "metadata": {
        "id": "osBwUl30HXR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jReIEHz6wysG"
      },
      "source": [
        "#### **4.6.4 NeuralNetwork v3**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.cat_features,data.num_features"
      ],
      "metadata": {
        "id": "EyU08ZbXPTEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfBbW0LUwysH"
      },
      "outputs": [],
      "source": [
        "data.X.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class exitesqueeze_layer(layers.Layer):\n",
        "    def __init__(self, exite_units,dropout_rate,activation,reg):\n",
        "        super().__init__()\n",
        "\n",
        "        self.exite_units = exite_units\n",
        "        self.activation=activation\n",
        "        self.reg=reg\n",
        "\n",
        "        self.reshaped_0 = layers.Reshape((-1, 1))\n",
        "        self.reshaped_1 = layers.Reshape((-1, ))\n",
        "\n",
        "        self.exite = layers.Dense(self.exite_units, activation=self.activation)\n",
        "        self.squeeze = layers.Dense(1, activation=\"linear\",kernel_regularizer=keras.regularizers.l2(reg))\n",
        "        self.lnorm_00 = layers.LayerNormalization()\n",
        "        self.lnorm_01 = layers.LayerNormalization()\n",
        "        self.drop = layers.Dropout(rate=dropout_rate)\n",
        "        self.attention = layers.Attention()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.reshaped_0(inputs)\n",
        "        x = self.exite(x)\n",
        "        att_out = self.attention([x,x])\n",
        "        att_out = self.lnorm_00(att_out)\n",
        "        x = layers.add([x, att_out])\n",
        "        x = self.squeeze(x)\n",
        "        x = self.reshaped_1(x)\n",
        "\n",
        "        x = layers.multiply([x, inputs])\n",
        "\n",
        "        x = self.lnorm_01(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Remove build warnings\n",
        "    def build(self):\n",
        "        self.built = True"
      ],
      "metadata": {
        "id": "UJ5OQf-rDnO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(units=512,exite_units=64, last_layer = 1, activation=\"relu\", reg=0.001, dropout_rate=0.33):\n",
        "\n",
        "    x_input_cats = layers.Input(shape=(len(data.cat_features),))\n",
        "    embs = []\n",
        "    for j in range(len(data.cat_features)):\n",
        "        e = layers.Embedding(cat_features_card[j], int(np.ceil(np.sqrt(cat_features_card[j]))))\n",
        "        x = e(x_input_cats[:,j])\n",
        "        x = layers.Flatten()(x)\n",
        "        embs.append(x)\n",
        "\n",
        "    x_input_nums = layers.Input(shape=(len(data.num_features),))\n",
        "\n",
        "    x_0 = layers.Concatenate(axis=-1, name=\"input_concat\")(embs+[x_input_nums])\n",
        "\n",
        "    es_0 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_0_concat\")([x_0,es_0])\n",
        "    x = layers.BatchNormalization(name=\"se_0_bn\")(x)\n",
        "\n",
        "    es_1 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_1_concat\")([x,es_1])\n",
        "    x = layers.BatchNormalization(name=\"se_1_bn\")(x)\n",
        "\n",
        "    es_2 = exitesqueeze_layer(exite_units=exite_units,\n",
        "                              dropout_rate=dropout_rate,\n",
        "                              activation=activation,\n",
        "                              reg=reg)(x)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1, name=\"se_2_concat\")([x,es_2])\n",
        "    x = layers.BatchNormalization(name=\"se_2_bn\")(x)\n",
        "\n",
        "    x_0 = layers.Dense(units, name=\"dense_0\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_0\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_0\")(x_0)\n",
        "\n",
        "    x_0 = layers.Dense(int(units/last_layer), name=\"dense_1\", activation=activation, kernel_regularizer=keras.regularizers.l2(reg))(x_0)\n",
        "    x_0 = layers.BatchNormalization(name=\"bn_1\")(x_0)\n",
        "    x_0 = layers.Dropout(dropout_rate,name=\"do_1\")(x_0)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([x_0,x])\n",
        "\n",
        "    x = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = keras.Model(inputs=[x_input_cats,x_input_nums], outputs=x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "avZAgIvUwysH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_test = build_model(units=64,exite_units=16, last_layer = 2, activation=\"relu\", reg=0.001, dropout_rate=0.33)\n",
        "mod_test.summary()"
      ],
      "metadata": {
        "id": "cjfFlBj5wysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_model(mod_test, show_shapes=True, show_dtype=True, show_layer_names=True, rankdir=\"TB\")"
      ],
      "metadata": {
        "id": "gxWhVnRkwysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#t.cat_features_card,np.ceil(np.sqrt(t.cat_features_card)),len(t.cat_features)"
      ],
      "metadata": {
        "id": "hTJyGENowysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "697CMLBjwysI"
      },
      "source": [
        "##### 4.2.2 Optuna Optimization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujnWEIfcwysI"
      },
      "outputs": [],
      "source": [
        "categorical_feat = data.cat_features.copy()\n",
        "numerical_feat = data.num_features.copy()\n",
        "\n",
        "X_train_cat = data.X[categorical_feat].astype(\"int32\")\n",
        "X_train_num = data.X[numerical_feat].astype(\"float32\")\n",
        "\n",
        "X_test_cat = data.X_test[categorical_feat].astype(\"int32\")\n",
        "X_test_num = data.X_test[numerical_feat].astype(\"float32\")\n",
        "\n",
        "X_train_cat.info()\n",
        "X_train_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLdEjBBgwysI"
      },
      "outputs": [],
      "source": [
        "def objective_nn(trial, X, y, n_splits, n_repeats, model=build_model, use_gpu=True, rs=42, fit_scaling=False, cv_strategy=\"KFold\"):\n",
        "\n",
        "    model_class = model\n",
        "#(units=512,exite_units=64, last_layer = 1, activation=\"relu\",  reg=0.001, dropout_rate=0.33)\n",
        "    categorical_features = data.cat_features.copy()\n",
        "\n",
        "    num_cols = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    params = {'units': trial.suggest_categorical('units', [64,128,256]),#\n",
        "              'last_layer': trial.suggest_int('last_layer',1,2),#\n",
        "              'activation': trial.suggest_categorical('activation', [\"relu\",\"selu\",\"gelu\",\"silu\"]), #, reg=0.001, dropout_rate=0.33)\n",
        "              'reg': trial.suggest_categorical('reg', [0.00001,0.0001,0.001,0.01]),\n",
        "              \"exite_units\": trial.suggest_categorical('exite_units', [16,32,64]),#\n",
        "              'dropout_rate': trial.suggest_float('dropout_rate', 0.30, 0.51,step=0.03)\n",
        "              }\n",
        "\n",
        "    if cv_strategy == 'RepKFold':\n",
        "        kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "    elif cv_strategy == 'KFold':\n",
        "        kf = KFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"StratKFold\":\n",
        "        kf = StratifiedKFold(n_splits=n_splits, random_state=rs, shuffle=True)\n",
        "    elif cv_strategy == \"RepStratKFold\":\n",
        "        kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=rs)\n",
        "\n",
        "    rmse_scores = []\n",
        "\n",
        "    for idx_train, idx_valid in kf.split(X, y):\n",
        "\n",
        "        # Split the data into training and validation sets for the current fold\n",
        "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train].to_numpy()#.reshape(-1, 1)\n",
        "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid].to_numpy()#.reshape(-1, 1)\n",
        "\n",
        "        categorical_feat = data.cat_features.copy()\n",
        "        numerical_feat = data.num_features.copy()\n",
        "\n",
        "        X_train_cat = X_train[categorical_feat]\n",
        "        X_train_num = X_train[numerical_feat]\n",
        "\n",
        "        X_valid_cat = X_valid[categorical_feat]\n",
        "        X_valid_num = X_valid[numerical_feat]\n",
        "\n",
        "        # Create the model\n",
        "        keras.utils.set_random_seed(rs)\n",
        "        model = model_class(**params)\n",
        "\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
        "        model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError(name=\"mean_squared_error\"),\n",
        "                      metrics=[keras.metrics.RootMeanSquaredError(name=\"RMSE\")])\n",
        "\n",
        "        # Fit the model\n",
        "        model.fit([X_train_cat,X_train_num], y_train,\n",
        "                  validation_data=([X_valid_cat, X_valid_num], y_valid),\n",
        "                  epochs=25,\n",
        "                  batch_size=1024,\n",
        "                  callbacks=[keras.callbacks.ReduceLROnPlateau(patience=2, min_lr=0.00005),\n",
        "                              keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_rmse\",\n",
        "                                                            start_from_epoch=3, mode=\"min\")])\n",
        "\n",
        "        # Make predictions on the validation set\n",
        "        y_pred = model.predict([X_valid_cat, X_valid_num], batch_size=1024)\n",
        "\n",
        "        # Calculate the RMSE for the current fold\n",
        "        rmse_score = root_mean_squared_error(y_valid, y_pred)\n",
        "        rmse_scores.append(rmse_score)\n",
        "\n",
        "    # Calculate the mean RMSLE score across all folds\n",
        "    key_metric = np.mean(rmse_scores)\n",
        "\n",
        "    return key_metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyp2r5IkwysI"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tuning Hyperparameters with Optuna\n",
        "def tune_hyperparameters(X, y, model_class, n_trials, n_splits_ ,n_repeats_, use_gpu=True):  #use_gpu\n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
        "    study.optimize(lambda trial: objective_nn(trial, X, y, n_splits=n_splits_, n_repeats=n_repeats_, model=build_model, use_gpu=use_gpu, cv_strategy=\"KFold\"), n_trials=n_trials)\n",
        "    return study  # Return the study object\n",
        "\n",
        "# Step 3: Saving Best Results and Models\n",
        "def save_results(study, model_class, model_name):\n",
        "    best_params_file = f\"{model_name}_best_params.joblib\"\n",
        "    joblib.dump(study.best_params, best_params_file)\n",
        "    print(f\"Best parameters for {model_name} saved to {best_params_file}\")\n",
        "\n",
        "    verbose_file = f\"{model_name}_optuna_verbose.log\"\n",
        "    with open(verbose_file, \"w\") as f:\n",
        "        f.write(str(study.trials))\n",
        "    print(f\"Optuna verbose for {model_name} saved to {verbose_file}\")# usage with XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVi6cBIJwysI"
      },
      "outputs": [],
      "source": [
        "cat_study = tune_hyperparameters(data.X, data.y, model_class=build_model, n_trials=31, n_splits_ = 3 ,n_repeats_=3, use_gpu=True)\n",
        "#save_results(cat_study, TabNetClassifier, \"tabnet_ext\")\n",
        "cat_params = cat_study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Trial 19 finished with value: 38.69609437218236\n",
        "- parameters: {'units': 256, 'last_layer': 1, 'activation': 'silu', 'reg': 0.0001, 'exite_units': 32, 'dropout_rate': 0.36}."
      ],
      "metadata": {
        "id": "tSHfbDyPOWgl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TKkv5dORbuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}